"V1","V2","V3","V4"
"0.0886194286901087","0.0936585811581694","  1060","<p>I'll use an example so that you can reproduce the results </p>

<pre><code># mortality 
mort = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/cmort.dat""),start=1970, frequency=52)

# temperature
temp = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/temp.dat""), start=1970, frequency=52)

#pollutant particulates
part = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/part.dat""), start=1970, frequency=52)

temp = temp-mean(temp)
temp2 = temp^2
trend = time(mort)
</code></pre>

<p>Now, fit a model for mortality data</p>

<pre><code>fit = lm(mort ~ trend + temp + temp2 + part, na.action=NULL)
</code></pre>

<p>What I want now is to reproduce the result of the AIC command </p>

<pre><code>AIC(fit)
[1] 3332.282
</code></pre>

<p>According to R's help file for AIC, AIC = -2 * log.likelihood + 2 * npar.
If I'm correct I think that log.likelihood is given using the following formula:</p>

<pre><code>n = length(mort)
RSS = anova(fit)[length(anova(fit)[,2]),2] # there must be better ways to get this, anyway
(log.likelihood &lt;- -n/2*(log(2*pi)+log(RSS/n)+1))

 [1] -1660.135
</code></pre>

<p>This is approximately equal to</p>

<pre><code>logLik(fit)
'log Lik.' -1660.141 (df=6)
</code></pre>

<p>As far as I can tell, the number of parameters in the model are 5 (how can I get this number programmatically ??). So AIC should be given by:</p>

<pre><code>-2 * log.likelihood + 2 * 5
[1] 3330.271
</code></pre>

<p>Ooops, it seems like I should have used 6 instead of 5 as the number of parameters. What is wrong with those calculations? </p>
"
"0.108536190793863","0.101962548386916","  3412","<p>I have an experiment that I'll try to abstract here.  Imagine I toss three white stones in front of you and ask you to make a judgment about their position.  I record a variety of properties of the stones and your response.   I do this over a number of subjects.  I generate two models.  One is that the nearest stone to you predicts your response, and the other is that the geometric center of the stones predicts your response.  So, using lmer in R I could write.</p>

<pre><code>mNear   &lt;- lmer(resp ~ nearest + (1|subject), REML = FALSE)
mCenter &lt;- lmer(resp ~ center  + (1|subject), REML = FALSE)
</code></pre>

<p><strong>UPDATE AND CHANGE - more direct version that incorporates several helpful comments</strong></p>

<p>I could try</p>

<pre><code>anova(mNear, mCenter)
</code></pre>

<p>Which is incorrect, of course, because they're not nested and I can't really compare them that way.  I was expecting anova.mer to throw an error but it didn't.  But the possible nesting that I could try here isn't natural and still leaves me with somewhat less analytical statements.  When models are nested naturally (e.g. quadratic on linear) the test is only one way.  But in this case what would it mean to have asymmetric findings?</p>

<p>For example, I could make a model three:</p>

<pre><code>mBoth &lt;- lmer(resp ~ center + nearest + (1|subject), REML = FALSE)
</code></pre>

<p>Then I can anova.</p>

<pre><code>anova(mCenter, mBoth)
anova(mNearest, mBoth)
</code></pre>

<p>This is fair to do and now I find that the center adds to the nearest effect (the second command) but BIC actually goes up when nearest is added to center (correction for the lower parsimony).  This confirms what was suspected.</p>

<p>But is finding this sufficient?  And is this fair when center and nearest are so highly correlated?</p>

<p>Is there a better way to analytically compare the models when it's not about adding and subtracting explanatory variables (degrees of freedom)?</p>
"
"0.119991273679223","0.126814318375447","  3874","<p>I have data from patients treated with 2 different kinds of treatments during surgery.
I need to analyze its effect on heart rate. 
The heart rate measurement is taken every 15 minutes. </p>

<p>Given that the surgery length can be different for each patient, each patient can have between 7 and 10 heart rate measurements. 
So an unbalanced design should be used. 
I'm doing my analysis using R. And have been using the ez package to do repeated measure mixed effect ANOVA. But I do not know how to analyse unbalanced data. Can anyone help?</p>

<p>Suggestions on how to analyze the data are also welcomed.</p>

<p>Update:<br>
As suggested, I fitted the data using the <code>lmer</code> function and found that the best model is:</p>

<pre><code>heart.rate~ time + treatment + (1|id) + (0+time|id) + (0+treatment|time)
</code></pre>

<p>with the following result:</p>

<pre><code>Random effects:
 Groups   Name        Variance   Std.Dev. Corr   
 id       time        0.00037139 0.019271        
 id       (Intercept) 9.77814104 3.127002        
 time     treat0      0.09981062 0.315928        
          treat1      1.82667634 1.351546 -0.504 
 Residual             2.70163305 1.643665        
Number of obs: 378, groups: subj, 60; time, 9

Fixed effects:
             Estimate Std. Error t value
(Intercept) 72.786396   0.649285  112.10
time         0.040714   0.005378    7.57
treat1       2.209312   1.040471    2.12

Correlation of Fixed Effects:
       (Intr) time  
time   -0.302       
treat1 -0.575 -0.121
</code></pre>

<p>Now I'm lost at interpreting the result. 
Am I right in concluding that the two treatments differed in affecting heart rate? What does the correlation of -504 between treat0 and treat1 means?</p>
"
"0.10232890201933","0.108147614087175","  5250","<p>I am trying to analyse some data using a mixed effect model. The data I collected represent the weight of some young animals of different genotype over time.</p>

<p>I am using the approach proposed here:
<a href=""https://gribblelab.wordpress.com/2009/03/09/repeated-measures-anova-using-r/"">https://gribblelab.wordpress.com/2009/03/09/repeated-measures-anova-using-r/</a></p>

<p>In particular I'm using solution #2</p>

<p>So I have something like</p>

<pre><code>require(nlme)
model &lt;- lme(weight ~ time * Genotype, random = ~1|Animal/time, 
         data=weights)    
av &lt;- anova(model)
</code></pre>

<p>Now, I would like to have some multiple comparisons.
Using <code>multcomp</code> I can do:</p>

<pre><code>require(multcomp)
comp.geno &lt;- glht(model, linfct=mcp(Genotype=""Tukey""))
print(summary(comp.geno))
</code></pre>

<p>And, of course, I could do the same with time.</p>

<p>I have two questions:</p>

<ol>
<li>How do I use <code>mcp</code> to see the interaction between Time and Genotype?</li>
<li><p>When I run <code>glht</code> I get this warning:</p>

<p><code>covariate interactions found -- default contrast might be inappropriate</code></p>

<p>What does it mean? Can I safely ignore it? Or what should I do to avoid it?</p></li>
</ol>

<p><strong>EDIT:</strong>
I found <a href=""http://cran.r-project.org/web/packages/multcomp/vignettes/generalsiminf.pdf"">this PDF</a> that says:</p>

<blockquote>
  <p>Because it is impossible to determine the parameters of interest automatically in this case, mcp() in multcomp will by default generate comparisons for the main effects only, <strong>ignoring covariates and interactions</strong>. Since version 1.1-2, one can specify to average over interaction terms and covariates using arguments interaction_average = TRUE and covariate_average = TRUE respectively, whereas versions older than 1.0-0 automatically averaged over interaction terms. <strong>We suggest to the users, however, that they write out, manually, the set of contrasts they want.</strong> One should do this whenever there is doubt about what the default contrasts measure, which typically happens in models with higher order interaction terms. We refer to Hsu (1996), Chapter~7, and Searle (1971), Chapter~7.3, for further discussions and examples on this issue.</p>
</blockquote>

<p>I do not have access to those books, but maybe someone here has?</p>
"
"0.12041009266179","0.137861698644752","  6208","<p>I developed the ez package for R as a means to help folks transition from stats packages like SPSS to R. This is (hopefully) achieved by simplifying the specification of various flavours of ANOVA, and providing SPSS-like output (including effect sizes and assumption tests), among other features. The <code>ezANOVA()</code> function mostly serves as a wrapper to <code>car::Anova()</code>, but the current version of <code>ezANOVA()</code> implements only type-II sums of squares, whereas <code>car::Anova()</code> permits specification of either type-II or -III sums of squares. As I possibly should have expected, several users have requested that I provide an argument in <code>ezANOVA()</code> that lets the user request type-II or type-III. I have been reticent to do so and outline my reasoning below, but I would appreciate the community's input on my or any other reasoning that bears on the issue.</p>

<p>Reasons for <em>not</em> including a ""SS_type"" argument in <code>ezANOVA()</code>:</p>

<ol>
<li>The difference between type I, II, and III sum squares only crops up when data are unbalanced, in which case I'd say that more benefit is derived from ameliorating imbalance by further data collection than fiddling with the ANOVA computation.</li>
<li>The difference between type II and III applies to lower-order effects that are qualified by higher-order effects, in which case I consider the lower-order effects scientifically uninteresting. (But see below for possible complication of the argument)</li>
<li>For those rare circumstances when (1) and (2) don't apply (when further data collection is impossible and the researcher has a valid scientific interest in a qualified main effect that I can't currently imagine), one can relatively easily modify the <code>ezANOVA()</code> source or employ <code>car::Anova()</code> itself to achieve type III tests. In this way, I see the extra effort/understanding required to obtain type III tests as a means by which I can ensure that only those that really know what they're doing go that route.</li>
</ol>

<p>Now, the most recent type-III requestor pointed out that argument (2) is undermined by consideration of circumstances where extant but ""non-significant"" higher-order effects can bias computation of sums of squares for lower-order effects. In such cases it's imaginable that a researcher would look to the higher-order effect, and seeing that it is ""non-significant"", turn to attempting interpretation of the lower-order effects that, unbeknownst to the researcher, have been compromised. My initial reaction is that this is not a problem with sums of squares, but with p-values and the tradition of null hypothesis testing. I suspect that a more explicit measure of evidence, such as the likelihood ratio, might be more likely to yield a less ambiguous picture of the models supported consistent with the data. However, I haven't done much thinking on the consequence of unbalanced data for the computation of likelihood ratios (which indeed involve sums of squares), so I'll have to give this some further thought.</p>
"
"0.0957199230302734","0.101162829777814","  6912","<p>I came by the post ""<a href=""http://www.r-bloggers.com/post-hoc-pairwise-comparisons-of-two-way-anova/"">Post-hoc Pairwise Comparisons of Two-way ANOVA</a>"" (responding to <a href=""http://rtutorialseries.blogspot.com/2011/01/r-tutorial-series-two-way-anova-with.html"">this post</a>), which shows the following:</p>

<pre><code>dataTwoWayComparisons &lt;- read.csv(""http://www.dailyi.org/blogFiles/RTutorialSeries/dataset_ANOVA_TwoWayComparisons.csv"")

model1 &lt;- aov(StressReduction~Treatment+Age, data =dataTwoWayComparisons)
summary(model1) # Treatment is signif

pairwise.t.test(dataTwoWayComparisons$StressReduction, dataTwoWayComparisons$Treatment, p.adj = ""none"")
# no signif pair

TukeyHSD(model1, ""Treatment"")
# mental-medical   is the signif pair.
</code></pre>

<p>(Output is attached bellow)</p>

<p>Could someone please explain why the Tukey HSD is able to find a significant pairing while the paired (unadjusted pvalue) t-test fails in doing so?</p>

<p>Thanks.</p>

<hr>

<p>Here is the code output</p>

<pre><code>&gt; model1 &lt;- aov(StressReduction~Treatment+Age, data =dataTwoWayComparisons)
&gt; summary(model1) # Treatment is signif
            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
Treatment    2     18   9.000      11 0.0004883 ***
Age          2    162  81.000      99     1e-11 ***
Residuals   22     18   0.818                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; pairwise.t.test(dataTwoWayComparisons$StressReduction, dataTwoWayComparisons$Treatment, p.adj = ""none"")

        Pairwise comparisons using t tests with pooled SD 

data:  dataTwoWayComparisons$StressReduction and dataTwoWayComparisons$Treatment 

         medical mental
mental   0.13    -     
physical 0.45    0.45  

P value adjustment method: none 
&gt; # no signif pair
&gt; 
&gt; TukeyHSD(model1, ""Treatment"")
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = StressReduction ~ Treatment + Age, data = dataTwoWayComparisons)

$Treatment
                 diff         lwr        upr     p adj
mental-medical      2  0.92885267 3.07114733 0.0003172
physical-medical    1 -0.07114733 2.07114733 0.0702309
physical-mental    -1 -2.07114733 0.07114733 0.0702309

&gt; # mental-medical   is the signif pair.
</code></pre>
"
"0.10232890201933","0.108147614087175","  8545","<p>I have some problems in using (and finding) the Chow test for structural breaks in a regression analysis using R. I want to find out if there are some structural changes including another variable (represents 3 spatial subregions).</p>

<p>Namely, is the regression with the subregions better than the overall model. Therefore I need some statistical validation. </p>

<p>I hope my problem is clear, isn't it?</p>

<p>Kind regards<br>
marco</p>

<p>Toy example in R:</p>

<pre><code>library(mlbench) # dataset
data(""BostonHousing"")

# data preparation
BostonHousing$region &lt;- ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[2], 1, 
                        ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[3], 2,
                        ifelse(BostonHousing$medv &gt; 
                               quantile(BostonHousing$medv)[4], 3, 1)))

BostonHousing$region &lt;- as.factor(BostonHousing$region)

# regression without any subregion 
reg1&lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)

summary(reg1)

# are there structural breaks using the factor ""region"" which
# indicates 3 spatial subregions
reg2&lt;- lm(medv ~ crim + indus + rm + region, data=BostonHousing)
</code></pre>

<p>------- subsequent entry</p>

<p>I struggled with your suggested package ""strucchange"", not knowing how to use the ""from"" and ""to"" arguments correctly with my factor ""region"". Nevertheless, I found one hint to calculate it by hand (https://stat.ethz.ch/pipermail/r-help/2007-June/133540.html). This results in the following output, but now I am not sure if my interpetation is valid. The results from the example above below.</p>

<p>Does this mean that region 3 is significant different from region 1? Contrary, region 2 is not? Further, each parameter (eg region1:crim) represents the beta for each regime and the model for this region respectively? Finally, the ANOVA states that there is a signif. difference between these models and that the consideration of regimes leads to a better model?</p>

<p>Thank you for your advices!
Best Marco</p>

<pre><code>fm0 &lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)
summary(fm0)
fm1 &lt;- lm(medv  ~ region / (crim + indus + rm), data=BostonHousing)
summary(fm1)
anova(fm0, fm1)
</code></pre>

<p>Results:</p>

<pre><code>Call:
lm(formula = medv ~ region/(crim + indus + rm), data = BostonHousing)

Residuals:
       Min         1Q     Median         3Q        Max 
-21.079383  -1.899551   0.005642   1.745593  23.588334 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    12.40774    3.07656   4.033 6.38e-05 ***
region2         6.01111    7.25917   0.828 0.408030    
region3       -34.65903    4.95836  -6.990 8.95e-12 ***
region1:crim   -0.19758    0.02415  -8.182 2.39e-15 ***
region2:crim   -0.03883    0.11787  -0.329 0.741954    
region3:crim    0.78882    0.22454   3.513 0.000484 ***
region1:indus  -0.34420    0.04314  -7.978 1.04e-14 ***
region2:indus  -0.02127    0.06172  -0.345 0.730550    
region3:indus   0.33876    0.09244   3.665 0.000275 ***
region1:rm      1.85877    0.47409   3.921 0.000101 ***
region2:rm      0.20768    1.10873   0.187 0.851491    
region3:rm      7.78018    0.53402  14.569  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.008 on 494 degrees of freedom
Multiple R-squared: 0.8142,     Adjusted R-squared: 0.8101 
F-statistic: 196.8 on 11 and 494 DF,  p-value: &lt; 2.2e-16

&gt; anova(fm0, fm1)
Analysis of Variance Table

Model 1: medv ~ crim + indus + rm
Model 2: medv ~ region/(crim + indus + rm)
  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
1    502 18559.4                                 
2    494  7936.6  8     10623 82.65 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0723574605292422","0.0764719112901873","  9324","<p>I'd love a check if anyone is willing!</p>

<p>I am trying to see if there is a statistical difference in female size between sites. Over the years females were repeatedly sampled within sites. I have sampled females opportunistically. Meaning that females were sampled a different number of times between and within sites.</p>

<p>My formula is:</p>

<pre><code>&gt; lmerfit1&lt;-lmer(size ~ (1|FEMALE), data=Data)
&gt; lmerfit2&lt;-lmer(size ~ SITE+(1|FEMALE), data=Data)
&gt; anova(lmerfit1, lmerfit2)
Data: Data
Models:
lmerfit1: size ~ (1 | FEMALE)
lmerfit2: size ~ SITE + (1 | FEMALE)
         Df    AIC    BIC  logLik Chisq Chi Df Pr(&gt;Chisq)
lmerfit1  3 2167.8 2179.6 -1080.9                        
lmerfit2  4 2169.8 2185.5 -1080.9     0      1          **1**
</code></pre>

<p>A p value of <strong>1</strong> leaves me concerned. The other female traits I ran thru this same formula made sense.</p>

<p>thanks! </p>
"
"0.0626633989716535","0.0662266178532522","  9692","<p>In a (one or multi) way anova model, once a new individual is assigned to a treatment, the predicted value for him is calculated using the coefficients of the ANOVA model (simply assigning the treatment mean value to the individual). </p>

<p>How should I construct a confidence (or prediction) interval for that predicted value? Can the <code>predict</code> function be used in R for an <code>aov</code> model? </p>

<p>There must be something that I'm missing here, as I can't find anything like this in my books. Thank you.</p>
"
"0.0626633989716535","0.0662266178532522","  9712","<p>I'm in the process of learning R, in the hope of replacing everything I do in SPSS/Sigmplot with R. It's going well so far :) I've got to the point of running a repeated-measures ANOVA, but have come unstuck when trying to plot the results</p>

<p>I've worked out how to plot a set of means using ggplot2, but now I'm unsure of how to plot the standard error as error bars. I've seen a number of guides with different implementations, and none of them seem to be appropriate (or even agree with each other). Many people use standard deviations, which is not what I am after. Others have different methods of computing the standard error, so I'm unsure of the best way to proceed. </p>

<p>What I have so far is this:</p>

<pre><code>qplot(CATEGORIES, means, shape=factor(ANOTHER_CATEGORY), facets=MORE_CATEGORIES ~ ., data=alldata)
</code></pre>

<p>I was wondering if someone could point me in the right direction in terms of how to get the standard errors from a repeated-measures ANOVA in R, and then how to translate this into error bars in ggplot?</p>

<p>Thanks!</p>
"
"0.192342632631684","0.203279781135864"," 11079","<p>I need an help because I donÂ´t know if the command for the ANOVA analysis I am 
performing in R is correct. Indeed using the function aov I get the following error: <code>In aov (......) Error() model is singular</code></p>

<p>The structure of my table is the following: subject, stimulus, condition, sex, response</p>

<p>Example:</p>

<pre><code>subject  stimulus condition sex    response
subject1    gravel  EXP1    M      59.8060
subject2    gravel  EXP1    M      49.9880
subject3    gravel  EXP1    M      73.7420
subject4    gravel  EXP1    M      45.5190
subject5    gravel  EXP1    M      51.6770
subject6    gravel  EXP1    M      42.1760
subject7    gravel  EXP1    M      56.1110
subject8    gravel  EXP1    M      54.9500
subject9    gravel  EXP1    M      62.6920
subject10   gravel  EXP1    M      50.7270
subject1    gravel  EXP2    M      70.9270
subject2    gravel  EXP2    M      61.3200
subject3    gravel  EXP2    M      70.2930
subject4    gravel  EXP2    M      49.9880
subject5    gravel  EXP2    M      69.1670
subject6    gravel  EXP2    M      62.2700
subject7    gravel  EXP2    M      70.9270
subject8    gravel  EXP2    M      63.6770
subject9    gravel  EXP2    M      72.4400
subject10   gravel  EXP2    M      58.8560
subject11   gravel  EXP1    F      46.5750
subject12   gravel  EXP1    F      58.1520
subject13   gravel  EXP1    F      57.4490
subject14   gravel  EXP1    F      59.8770
subject15   gravel  EXP1    F      55.5480
subject16   gravel  EXP1    F      46.2230
subject17   gravel  EXP1    F      63.3260
subject18   gravel  EXP1    F      60.6860
subject19   gravel  EXP1    F      59.4900
subject20   gravel  EXP1    F      52.6630
subject11   gravel  EXP2    F      55.7240
subject12   gravel  EXP2    F      66.4220
subject13   gravel  EXP2    F      65.9300
subject14   gravel  EXP2    F      61.8120
subject15   gravel  EXP2    F      62.5160
subject16   gravel  EXP2    F      65.5780
subject17   gravel  EXP2    F      59.5600
subject18   gravel  EXP2    F      63.8180
subject19   gravel  EXP2    F      61.4250
.....
.....
.....
.....
</code></pre>

<p>As you can notice each subject repeated the evaluation in 2 conditions (EXP1 and EXP2).</p>

<p>What I am interested in is to know if there are significant differences between 
the evaluations of the males and the females.</p>

<p>This is the command I used to perform the ANOVA with repeated measures:</p>

<pre><code>aov1 = aov(response ~ stimulus*sex + Error(subject/(stimulus*sex)), data=scrd)
summary(aov1)
</code></pre>

<p>I get the following error:</p>

<pre><code>&gt; aov1 = aov(response ~ stimulus*sex + Error(subject/(stimulus*sex)), data=scrd)
Warning message:
In aov(response ~ stimulus * sex + Error(subject/(stimulus * sex)),  :
Error() model is singular
&gt; summary(aov1)

Error: subject
          Df  Sum Sq Mean Sq F value Pr(&gt;F)
sex        1  166.71  166.72   1.273  0.274
Residuals 18 2357.29  130.96               

Error: subject:stimulus
              Df Sum Sq Mean Sq F value Pr(&gt;F)    
stimulus       6 7547.9 1257.98 35.9633 &lt;2e-16 ***
stimulus:sex   6   94.2   15.70  0.4487 0.8445    
Residuals    108 3777.8   34.98                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Error: Within
           Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 420 9620.6  22.906               
&gt; 
</code></pre>

<p>The thing is that looking at the data it is evident for me that there is a 
difference between male and females, because for each stimulus I always get
a mean higher for the males rather than the females. 
Therefore the ANOVA should indicate significant differences....</p>

<p>Is there anyone who can suggest me where I am wrong?</p>

<p>Finally, I know that in R there are two libraries on linear mixed models called 
nlme and lme4, but I have never used it so far and I donÂ´t know if I have to utilize it for my case.
Is it the case to utilize it? If yes, could you please provide a quick R example
of a command which could solve my problem?</p>

<p>Thanks in advance!</p>

<p>Best regards</p>

<hr>

<p>Dear all, 
I am stuck now ;-( Indeed I understood everything you suggested me but still I donÂ´t get significance in the ANOVA results, and definitively there is an error, because results cannot be non-significant. Indeed looking at the means for each stimulus, it is possible to notice that males gave always higher evaluations than females.</p>

<p>To prove this I discarded for a moment the effect of the repeated measures, and I performed an ANOVA separately on both the two conditions (EXP1 and EXP2) during which the evaluations were given.
What I get is significant differences between males and female, in both EXP1 and EXP2.</p>

<p>Now, why when I perform the ANOVA with repeated measures I donÂ´t get the same behavior?</p>

<p>My design is the following:
-sex is a between-subjects factor (with two levels)
-stimulus is a within-subjects factor (with 3 assumed levels)
-condition is a within-subjects factor (with 2 levels)
-all factors are fully crossed</p>

<p>I tried, both the ways suggested but without achieving significance: </p>

<pre><code>mDf &lt;- aggregate(response ~ subject + sex, data=scrd, FUN=mean)
summary(aov(response ~ sex, data=mDf))     # ANOVA with just the between-effect
</code></pre>

<p>and</p>

<pre><code>aov1 = aov(response ~ sex*stimulus*condition + Error(subject/(stimulus*condition)), data=scrd)
summary(aov1)
</code></pre>

<p>Instead if I perform the ANOVA on the two subtables of EXP 1 and 2 I get significant differences. </p>

<pre><code>table_EXP1 &lt;- subset(scrd, condition == ""EXP1"")
table_EXP2 &lt;- subset(scrd, condition == ""EXP2"")


fit_table_EXP1 &lt;- lm(response ~ stimulus*sex, data=table_EXP1) 
summary(fit_table_EXP1 )
anova(fit_table_EXP1 )


fit_table_EXP2 &lt;- lm(response ~ stimulus*sex, data=table_EXP2) 
summary(fit_table_EXP2)
anova(fit_table_EXP2)
</code></pre>

<p>....how can this be possible?...it is a contraddiction....</p>

<p>HELP!</p>

<p>Please enlighten me!</p>

<p>Thanks in advance</p>

<p>Cheers</p>
"
"0.141099489220774","0.157894736842105"," 11113","<p>I am using ANOVA with repeated measures to test significance between males and females results of an experiment during which participants had to evaluate 7 stimuli in 2 conditions (EXP1 and EXP2).</p>

<p>The problem is that even if from results it is clear that there are significant differences between males and females, I donÂ´t get significance in the ANOVA results.
Definitively there is an error, because results cannot be non-significant. Indeed looking at the means for each stimulus, it is possible to notice that males gave always higher evaluations than females.</p>

<p>To prove this I discarded for a moment the effect of the repeated measures, and I performed an ANOVA separately on both the two conditions (EXP1 and EXP2) during which the evaluations were given.
What I get is significant differences between males and female, in both EXP1 and EXP2.</p>

<p>Now, why when I perform the ANOVA with repeated measures I donÂ´t get the same behavior?</p>

<p>The structure of my table is the following: subject, stimulus, condition, sex, response. The design is the following:</p>

<ol>
<li>sex is a between-subjects factor (with two levels)</li>
<li>stimulus is a within-subjects factor (with 3 assumed levels)</li>
<li>condition is a within-subjects factor (with 2 levels)</li>
<li>all factors are fully crossed</li>
</ol>

<p>Example:</p>

<pre><code>subject  stimulus condition sex        response
subject1    gravel  EXP1    M      59.8060
subject2    gravel  EXP1    M      49.9880
subject3    gravel  EXP1    M      73.7420
subject4    gravel  EXP1    M      45.5190
subject5    gravel  EXP1    M      51.6770
subject6    gravel  EXP1    M      42.1760
subject7    gravel  EXP1    M      56.1110
subject8    gravel  EXP1    M      54.9500
subject9    gravel  EXP1    M      62.6920
subject10   gravel  EXP1    M      50.7270
subject1    gravel  EXP2    M      70.9270
subject2    gravel  EXP2    M      61.3200
subject3    gravel  EXP2    M      70.2930
subject4    gravel  EXP2    M      49.9880
subject5    gravel  EXP2    M      69.1670
subject6    gravel  EXP2    M      62.2700
subject7    gravel  EXP2    M      70.9270
subject8    gravel  EXP2    M      63.6770
subject9    gravel  EXP2    M      72.4400
subject10   gravel  EXP2    M      58.8560
subject11   gravel  EXP1    F      46.5750
subject12   gravel  EXP1    F      58.1520
subject13   gravel  EXP1    F      57.4490
subject14   gravel  EXP1    F      59.8770
subject15   gravel  EXP1    F      55.5480
subject16   gravel  EXP1    F      46.2230
subject17   gravel  EXP1    F      63.3260
subject18   gravel  EXP1    F      60.6860
subject19   gravel  EXP1    F      59.4900
subject20   gravel  EXP1    F      52.6630
subject11   gravel  EXP2    F      55.7240
subject12   gravel  EXP2    F      66.4220
subject13   gravel  EXP2    F      65.9300
subject14   gravel  EXP2    F      61.8120
subject15   gravel  EXP2    F      62.5160
subject16   gravel  EXP2    F      65.5780
subject17   gravel  EXP2    F      59.5600
subject18   gravel  EXP2    F      63.8180
subject19   gravel  EXP2    F      61.4250
.....
.....
.....
.....
</code></pre>

<p>As you can notice each subject repeated the evaluation in 2 conditions (EXP1 and EXP2).</p>

<p>What I am interested in is to know if there are significant differences between 
the evaluations of the males and the females (both at global level and for each stimulus).</p>

<p>This is the command I used to perform the ANOVA with repeated measures:</p>

<pre><code>aov1 = aov(response ~ sex*stimulus*condition + Error(subject/(stimulus*condition)), data=scrd)
summary(aov1)
</code></pre>

<p>Doing so I donÂ´t get significance for the differences between males and females.</p>

<p>Instead if I perform the ANOVA on the two subtables of EXP 1 and 2 I get significant differences. </p>

<pre><code>table_EXP1 &lt;- subset(scrd, condition == ""EXP1"")
table_EXP2 &lt;- subset(scrd, condition == ""EXP2"")


fit_table_EXP1 &lt;- lm(response ~ stimulus*sex, data=table_EXP1) 
anova(fit_table_EXP1)


fit_table_EXP2 &lt;- lm(response ~ stimulus*sex, data=table_EXP2) 
anova(fit_table_EXP2)
</code></pre>

<p>How can this be possible? Is it a contradiction?</p>
"
"NaN","NaN"," 12398","<p>I am new to statistics and I currently deal with ANOVA. I carry out an ANOVA test in R using</p>

<pre><code>aov(dependendVar ~ IndependendVar)
</code></pre>

<p>I get â€“ among others â€“ an F-value and a p-value. </p>

<p>My null hypothesis ($H_0$) is that all group means are equal. </p>

<p>There is a lot of information available on <a href=""http://onlinestatbook.com/2/analysis_of_variance/one-way.html"">how F is calculated</a>, but I don't know how to read an F-statistic and how F and p are connected. </p>

<p>So, my questions are:</p>

<ol>
<li>How do I determine the critical F-value for rejecting $H_0$?</li>
<li>Does each F have a corresponding p-value, so they both mean basically the same? (e.g., if $p&lt;0.05$, then $H_0$ is rejected) </li>
</ol>
"
"0.0626633989716535","0.0441510785688348"," 12993","<p>Let's say I have a simple 2x2 factorial experiment that I want to do ANOVA on. Like this, for example:</p>

<pre><code>d   &lt;- data.frame(a=factor(sample(c('a1','a2'), 100, rep=T)),
                  b=factor(sample(c('b1','b2'), 100, rep=T)));
d$y &lt;- as.numeric(d$a)*rnorm(100, mean=.75, sd=1) +
       as.numeric(d$b)*rnorm(100, mean=1.2, sd=1) +
       as.numeric(d$a)*as.numeric(d$b)*rnorm(100, mean=.5, sd=1) +
       rnorm(100);
</code></pre>

<ol>
<li><p>In the absence of a significant interaction, by default (i.e. <code>contr.treatment</code>) the output of <code>Anova()</code> is the overall significance of <code>a</code> over all levels of <code>b</code> and of <code>b</code> over all levels of <code>a</code>, is that right?</p></li>
<li><p>How should I specify a contrast that would allow me to test the significance of effect <code>a</code> with <code>b</code> being held constant at level b1, of effect <code>a</code> with <code>b</code> being held constant at level b2, and of the interaction <code>a:b</code>? </p></li>
</ol>
"
"0.0542680953969316","0.0573539334676404"," 13788","<p>I have a data set of $n$ benchmarks and $m$ subsamples in each benchmark.  I run these benchmarks and their subsamples on $p$ subject machines.
The 'individual' studied by the subsamples are the same for each subject machine, and the benchmarks are the same for each subject machine.</p>

<p>How do I carry out an ANOVA in R in this situation?</p>

<p>Mainly I want to compute total mean and confidence intervals. I don't care about sub sample means at all, but I want to recognise the replication there in the final confidence and means. I may care about benchmark means though.
I can't work out how to setup this anova in R. I want to be able to replicate the means by manual calculation.</p>

<p>I have tried <code>glm</code>, <code>anova</code>, <code>aov</code>, and <code>lme</code> but I'm totally confused.
I think ANOVA results should be equivalent for two subject machines to the nested mean of machine/benchmark/checkpoint, but the means don't come out the same when I try them.</p>

<p>Edit: </p>

<p>I'm starting to get a clue from <a href=""http://zoonek2.free.fr/UNIX/48_R/13.html"">http://zoonek2.free.fr/UNIX/48_R/13.html</a></p>
"
"0.127911127524163","0.1441968187829"," 14947","<p>I am trying to run a repeated measures Anova in R followed by some specific
contrasts on that dataset. I think the correct approach would be to use
<code>Anova()</code> from the car package.</p>

<p>Lets illustrate my question with the example taken from <code>?Anova</code> using the
<code>OBrienKaiser</code> data (Note: I ommited the gender factor from the example):<br>
We have a design with one between subjects factor, treatment (3 levels: control,
A, B), and 2 repeated-measures (within subjects) factors,
phase (3 levels: pretest, posttest, followup) and hour (5 levels: 1 to 5).</p>

<p>The standard ANOVA table is given by (in difference to example(Anova) I switched
to Type 3 Sums of Squares, that is what my field wants):</p>

<pre><code>require(car)
phase &lt;- factor(rep(c(""pretest"", ""posttest"", ""followup""), c(5, 5, 5)),
levels=c(""pretest"", ""posttest"", ""followup""))
hour &lt;- ordered(rep(1:5, 3))
idata &lt;- data.frame(phase, hour)
mod.ok &lt;- lm(cbind(pre.1, pre.2, pre.3, pre.4, pre.5, post.1, post.2, post.3, post.4, post.5, fup.1, fup.2, fup.3, fup.4, fup.5) ~ treatment, data=OBrienKaiser)
av.ok &lt;- Anova(mod.ok, idata=idata, idesign=~phase*hour, type = 3)
summary(av.ok, multivariate=FALSE)
</code></pre>

<p>Now, imagine that the highest order interaction would have been significant
(which is not the case) and we would like to explore it further with the
following contrasts:<br>
<strong>Is there a difference between hours 1&amp;2 versus hours 3 (contrast 1) and between
hours 1&amp;2 versus hours 4&amp;5 (contrast 2) in the treatment conditions (A&amp;B
together)?</strong><br>
In other words, how do I specify these contrasts:</p>

<ol>
<li><code>((treatment %in% c(""A"", ""B"")) &amp; (hour %in% 1:2))</code> versus <code>((treatment %in% c(""A"", ""B"")) &amp; (hour %in% 3))</code></li>
<li><code>((treatment %in% c(""A"", ""B"")) &amp; (hour %in% 1:2))</code> versus <code>((treatment %in% c(""A"", ""B"")) &amp; (hour %in% 4:5))</code></li>
</ol>

<p>My idea would be to run another ANOVA ommitting the non-needed treatment
condition (control):</p>

<pre><code>mod2 &lt;- lm(cbind(pre.1, pre.2, pre.3, pre.4, pre.5, post.1, post.2, post.3, post.4, post.5, fup.1, fup.2, fup.3, fup.4, fup.5) ~ treatment, data=OBrienKaiser, subset = treatment != ""control"")
av2 &lt;- Anova(mod2, idata=idata, idesign=~phase*hour, type = 3)
summary(av2, multivariate=FALSE)
</code></pre>

<p>However, I still have no idea how to set up the appropriate
within-subject contrast matrix comparing hours 1&amp;2 with 3 and 1&amp;2 with 4&amp;5.
And I am not sure if omitting the non-needed treatment group is indeed a good
idea as it changes the overall error term. </p>

<p>Before going for <code>Anova()</code> I was also thinking going for <code>lme</code>. However, there are
small differences in F and p values between textbook ANOVA and what is returned
from <code>anove(lme)</code> <a href=""http://stats.stackexchange.com/q/14088/442"">due to possible negative variances in standard ANOVA (which are not allowed in <code>lme</code></a>). Relatedly, somebody pointed me to <code>gls</code> which allows for fitting repeated measures ANOVA, however, it has no contrast argument.</p>

<p>To clarify: I want an F or t test (using type III sums of squares) that answers whether or not the desired contrasts are significant or not.</p>

<hr>

<p><strong>Update:</strong></p>

<p>I already <a href=""http://thread.gmane.org/gmane.comp.lang.r.general/237681"" rel=""nofollow"">asked a very similar question on R-help, there was no answer</a>.</p>

<p><a href=""http://thread.gmane.org/gmane.comp.lang.r.general/115048/focus=115302"" rel=""nofollow"">A similar questions was posed on R-help some time ago.</a> However, the answers did also not solve the problem.</p>

<hr>

<p><strong>Update (2015):</strong></p>

<p>As this question still generates some activity, specifying theses and basically all other contrasts can now be done relatively easy with the <code>afex</code> package in combination with the <code>lsmeans</code> package as described in the <a href=""https://cran.rstudio.com/web/packages/afex/vignettes/anova_posthoc.html"" rel=""nofollow"">afex vignette</a>.</p>
"
"0.135368413338721","0.132846856888409"," 14978","<p>When running a repeated measures ANOVA in SPSS, it's possible to 'Save' the residuals as new variables in the data editor.</p>

<p>But the values output do not match the residuals given in R, and seem to be residuals for a between-subjects model. Unless I am missing something? Is SPSS giving the wrong residuals?</p>

<p>Example in R:</p>

<pre><code>set.seed(1)  # hopefully this keeps things the same every time!

  # create a data frame with each line representing one subject,
  # and create first and second observations for some experiment

DF &lt;- data.frame(participant=factor(1:5), first=rnorm(5, 10, 5), second=rnorm(5, 20, 5))

DF
</code></pre>

<p>-</p>

<pre><code>  participant     first   second
1           1  6.867731 15.89766
2           2 10.918217 22.43715
3           3  5.821857 23.69162
4           4 17.976404 22.87891
5           5 11.647539 18.47306
</code></pre>

<p>-</p>

<pre><code>  # reshape it for an ANOVA in R
DFlong &lt;- reshape(DF, direction=""long"", varying=c(""first"", ""second""), v.names=""value"", idvar=""participant"", times=c(1, 2), timevar=""group"")

DFlong
</code></pre>

<p>-</p>

<pre><code>    participant group     value
1.1           1     1  6.867731
2.1           2     1 10.918217
3.1           3     1  5.821857
4.1           4     1 17.976404
5.1           5     1 11.647539
1.2           1     2 15.897658
2.2           2     2 22.437145
3.2           3     2 23.691624
4.2           4     2 22.878907
5.2           5     2 18.473058
</code></pre>

<p>-</p>

<pre><code>my.aov &lt;- aov(value ~ group + Error( participant / group ), DFlong)
summary(my.aov)
</code></pre>

<p>-</p>

<pre><code>Error: participant
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals  4 86.474  21.619               

Error: participant:group
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
group      1 251.469 251.469  19.871 0.01118 *
Residuals  4  50.619  12.655                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>-</p>

<pre><code>my.aov$""participant:group""$residuals
</code></pre>

<p>-</p>

<pre><code>        6          7          8          9         10 
0.7066837 -1.0533061 -5.5440267  3.6252135 -2.2654355 
</code></pre>

<p>-</p>

<pre><code># import into SPSS:
write.table(DF, ""C:/test.txt"", row.names=FALSE)
</code></pre>

<p>Then load SPSS, and run:</p>

<pre><code>GET DATA  /TYPE = TXT
 /FILE = 'C:\test.txt'
 /DELCASE = LINE
 /DELIMITERS = "" ""
 /QUALIFIER = '""'
 /ARRANGEMENT = DELIMITED
 /FIRSTCASE = 2
 /IMPORTCASE = ALL
 /VARIABLES =
 participant F1.0
 first F16.14
 second F16.13
 .
CACHE.
EXECUTE.
DATASET NAME DataSet1 WINDOW=FRONT.
</code></pre>

<p>Now change the variable types to scale (in the 'variables' tab - I don't know the syntax for this). Then run:</p>

<pre><code>GLM
  first second
  /WSFACTOR = factor1 2 Polynomial
  /METHOD = SSTYPE(3)
  /SAVE = RESID
  /CRITERIA = ALPHA(.05)
  /WSDESIGN = factor1 .
</code></pre>

<p>Or, do the above SPSS commands using the GUI: File->Read text data... find C:\test.txt, import it, remember to specify that the file has variable names as the first case, and run:</p>

<ol>
<li><p>Analyze->General Linear Model->Repeated Measures...</p></li>
<li><p>Set number of levels to 2</p></li>
<li><p>Put variables into analysis, 'first' and 'second'.</p></li>
<li><p>Open 'Save...' dialog box, check 'Residuals->Unstandardized'</p></li>
<li><p>Run analysis, SPSS creates two variables of residuals:</p>

<pre><code>RES_1    RES_2
-3.78    -4.78
  .27     1.76
-4.82     3.02
 7.33     2.20
 1.00    -2.20
</code></pre></li>
</ol>

<p>Note these values are different to R. So has SPSS got it wrong?</p>
"
"0.0808981002113217","0.0170996392014192"," 15223","<p>I ran an ANOVA finding for example an interaction between gender and grade than I want to know in what grades boys and girls differ, but in many cases I find (adjusted) p-values of 0 and 1. How / why is this possible? Doesn't seem right...</p>

<pre><code>as.factor(gender)                     1     16    16.2    2.6377  0.104396    
as.factor(grade)                      7  50077  7153.9 1165.4184 &lt; 2.2e-16 ***
as.factor(gender):as.factor(grade)    7    132    18.9    3.0795  0.003056 ** 
Residuals                          7747  47555     6.1                        
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = rating ~ as.factor(gender) * as.factor(grade), data = users_c[users_c$grade %in% 1:8, ])

$`as.factor(gender)`
           diff        lwr        upr     p adj
m-f -0.09135851 -0.2016276 0.01891058 0.1043964

$`as.factor(grade)`
         diff        lwr       upr     p adj
2-1 0.3823566 -0.5454435  1.310157 0.9169296
3-1 1.9796023  1.1649854  2.794219 0.0000000
4-1 3.9558543  3.1534606  4.758248 0.0000000
5-1 5.7843111  4.9829529  6.585669 0.0000000
6-1 7.0752044  6.2708610  7.879548 0.0000000
7-1 8.4868609  7.6776332  9.296089 0.0000000
8-1 9.3867231  8.5626511 10.210795 0.0000000
3-2 1.5972457  1.0395026  2.154989 0.0000000
4-2 3.5734976  3.0337642  4.113231 0.0000000
5-2 5.4019544  4.8637616  5.940147 0.0000000
6-2 6.6928478  6.1502200  7.235476 0.0000000
7-2 8.1045042  7.5546625  8.654346 0.0000000
8-2 9.0043665  8.4329024  9.575831 0.0000000
4-3 1.9762520  1.6694948  2.283009 0.0000000
5-3 3.8047088  3.5006705  4.108747 0.0000000
6-3 5.0956021  4.7837806  5.407424 0.0000000
7-3 6.5072586  6.1830461  6.831471 0.0000000
8-3 7.4071208  7.0474558  7.766786 0.0000000
5-4 1.8284568  1.5588754  2.098038 0.0000000
6-4 3.1193501  2.8410202  3.397680 0.0000000
7-4 4.5310066  4.2388618  4.823151 0.0000000
8-4 5.4308688  5.0998193  5.761918 0.0000000
6-5 1.2908933  1.0155630  1.566224 0.0000000
7-5 2.7025498  2.4132612  2.991838 0.0000000
8-5 3.6024120  3.2738803  3.930944 0.0000000
7-6 1.4116565  1.1141985  1.709114 0.0000000
8-6 2.3115187  1.9757711  2.647266 0.0000000
8-7 0.8998622  0.5525763  1.247148 0.0000000

$`as.factor(gender):as.factor(grade)`
                diff         lwr        upr     p adj
m:1-f:1  0.005917865 -1.77842639  1.7902621 1.0000000
f:2-f:1  0.318074165 -1.28953805  1.9256864 0.9999988
m:2-f:1  0.442924925 -1.11597060  2.0018205 0.9998619
f:3-f:1  1.769000750  0.35262166  3.1853798 0.0020136
m:3-f:1  2.174229216  0.76569156  3.5827669 0.0000147
f:4-f:1  3.738998543  2.34268666  5.1353104 0.0000000
m:4-f:1  4.163719997  2.77146170  5.5559783 0.0000000
f:5-f:1  5.769586591  4.37599400  7.1631792 0.0000000
m:5-f:1  5.816721075  4.42497532  7.2084668 0.0000000
f:6-f:1  7.169439003  5.77317769  8.5657003 0.0000000
m:6-f:1  7.000924045  5.60308216  8.3987659 0.0000000
f:7-f:1  8.330142924  6.92683436  9.7334515 0.0000000
m:7-f:1  8.674488370  7.26930678 10.0796700 0.0000000
f:8-f:1  9.535307293  8.11198164 10.9586329 0.0000000
m:8-f:1  9.251081088  7.82191240 10.6802498 0.0000000
f:2-m:1  0.312156300 -1.12690148  1.7512141 0.9999959
m:2-m:1  0.437007060 -0.94741539  1.8214295 0.9995001
f:3-m:1  1.763082885  0.54136279  2.9848030 0.0000892
m:3-m:1  2.168311350  0.95569081  3.3809319 0.0000001
f:4-m:1  3.733080678  2.53468294  4.9314784 0.0000000
m:4-m:1  4.157802132  2.96412989  5.3514744 0.0000000
f:5-m:1  5.763668726  4.56844048  6.9588970 0.0000000
m:5-m:1  5.810803210  4.61772882  7.0038776 0.0000000
f:6-m:1  7.163521138  5.96518233  8.3618599 0.0000000
m:6-m:1  6.995006180  5.79482611  8.1951862 0.0000000
f:7-m:1  8.324225059  7.11768240  9.5307677 0.0000000
m:7-m:1  8.668570505  7.45984987  9.8772911 0.0000000
f:8-m:1  9.529389428  8.29962271 10.7591561 0.0000000
m:8-m:1  9.245163223  8.00863850 10.4816879 0.0000000
m:2-f:2  0.124850760 -1.02282435  1.2725259 1.0000000
f:3-f:2  1.450926585  0.50586965  2.3959835 0.0000172
m:3-f:2  1.856155050  0.92289131  2.7894188 0.0000000
f:4-f:2  3.420924378  2.50621691  4.3356318 0.0000000
m:4-f:2  3.845645832  2.93713824  4.7541534 0.0000000
f:5-f:2  5.451512425  4.54096139  6.3620635 0.0000000
m:5-f:2  5.498646910  4.59092496  6.4063689 0.0000000
f:6-f:2  6.851364838  5.93673457  7.7659951 0.0000000
m:6-f:2  6.682849880  5.76580854  7.5998912 0.0000000
f:7-f:2  8.012068759  7.08671595  8.9374216 0.0000000
m:7-f:2  8.356414205  7.42822339  9.2846050 0.0000000
f:8-f:2  9.217233128  8.26179669 10.1726696 0.0000000
m:8-f:2  8.933006923  7.96888762  9.8971262 0.0000000
f:3-m:2  1.326075825  0.46649985  2.1856518 0.0000150
m:3-m:2  1.731304290  0.88471145  2.5778971 0.0000000
f:4-m:2  3.296073618  2.46998162  4.1221656 0.0000000
m:4-m:2  3.720795071  2.90157332  4.5400168 0.0000000
f:5-m:2  5.326661665  4.50517434  6.1481490 0.0000000
m:5-m:2  5.373796150  4.55544575  6.1921465 0.0000000
f:6-m:2  6.726514078  5.90050756  7.5525206 0.0000000
m:6-m:2  6.557999120  5.72932364  7.3866746 0.0000000
f:7-m:2  7.887217999  7.04935402  8.7250820 0.0000000
m:7-m:2  8.231563445  7.39056617  9.0725607 0.0000000
f:8-m:2  9.092382368  8.22140761  9.9633571 0.0000000
m:8-m:2  8.808156163  7.92766524  9.6886471 0.0000000
m:3-f:3  0.405228465 -0.13578346  0.9462404 0.4221367
f:4-f:3  1.969997793  1.46166478  2.4783308 0.0000000
m:4-f:3  2.394719246  1.89762897  2.8918095 0.0000000
f:5-f:3  4.000585840  3.49977062  4.5014011 0.0000000
m:5-f:3  4.047720325  3.55206739  4.5433733 0.0000000
f:6-f:3  5.400438253  4.89224417  5.9086323 0.0000000
m:6-f:3  5.231923295  4.71940255  5.7444440 0.0000000
f:7-f:3  6.561142174  6.03389412  7.0883902 0.0000000
m:7-f:3  6.905487620  6.37327442  7.4377008 0.0000000
f:8-f:3  7.766306543  7.18788499  8.3447281 0.0000000
m:8-f:3  7.482080337  6.88942637  8.0747343 0.0000000
f:4-m:3  1.564769328  1.07871270  2.0508260 0.0000000
m:4-m:3  1.989490781  1.51520464  2.4637769 0.0000000
f:5-m:3  3.595357375  3.11716862  4.0735461 0.0000000
m:5-m:3  3.642491860  3.16971239  4.1152713 0.0000000
f:6-m:3  4.995209787  4.50929846  5.4811211 0.0000000
m:6-m:3  4.826694830  4.33626022  5.3171294 0.0000000
f:7-m:3  6.155913709  5.65010831  6.6617191 0.0000000
m:7-m:3  6.500259155  5.98928021  7.0112381 0.0000000
f:8-m:3  7.361078078  6.80213257  7.9200236 0.0000000
m:8-m:3  7.076851872  6.50319055  7.6505132 0.0000000
m:4-f:4  0.424721453 -0.01192015  0.8613631 0.0668946
f:5-f:4  2.030588047  1.58971048  2.4714656 0.0000000
m:5-f:4  2.077722532  1.64271796  2.5127271 0.0000000
f:6-f:4  3.430440460  2.98119847  3.8796825 0.0000000
m:6-f:4  3.261925502  2.80779484  3.7160562 0.0000000
f:7-f:4  4.591144381  4.12045589  5.0618329 0.0000000
m:7-f:4  4.935489827  4.45924616  5.4117335 0.0000000
f:8-f:4  5.796308750  5.26892973  6.3236878 0.0000000
m:8-f:4  5.512082545  4.96913148  6.0550336 0.0000000
f:5-m:4  1.605866594  1.17800058  2.0337326 0.0000000
m:5-m:4  1.653001078  1.23118920  2.0748130 0.0000000
f:6-m:4  3.005719006  2.56923916  3.4421989 0.0000000
m:6-m:4  2.837204048  2.39569420  3.2787139 0.0000000
f:7-m:4  4.166422928  3.70789927  4.6249466 0.0000000
m:7-m:4  4.510768373  4.04654394  4.9749928 0.0000000
f:8-m:4  5.371587296  4.85503631  5.8881383 0.0000000
m:8-m:4  5.087361091  4.55492128  5.6198009 0.0000000
m:5-f:5  0.047134485 -0.37906079  0.4733298 1.0000000
f:6-f:5  1.399852412  0.95913504  1.8405698 0.0000000
m:6-f:5  1.231337454  0.78563790  1.6770370 0.0000000
f:7-f:5  2.560556334  2.09799705  3.0231156 0.0000000
m:7-f:5  2.904901779  2.43669086  3.3731127 0.0000000
f:8-f:5  3.765720703  3.24558412  4.2858573 0.0000000
m:8-f:5  3.481494497  2.94557538  4.0174136 0.0000000
f:6-m:5  1.352717928  0.91787572  1.7875601 0.0000000
m:6-m:5  1.184202970  0.74431204  1.6240939 0.0000000
f:7-m:5  2.513421849  2.05645683  2.9703869 0.0000000
m:7-m:5  2.857767295  2.39508230  3.3204523 0.0000000
f:8-m:5  3.718586218  3.20341827  4.2337542 0.0000000
m:8-m:5  3.434360013  2.90326187  3.9654582 0.0000000
m:6-f:6 -0.168514958 -0.62249009  0.2854602 0.9968060
f:7-f:6  1.160703921  0.69016548  1.6312424 0.0000000
m:7-f:6  1.505049367  1.02895400  1.9811447 0.0000000
f:8-f:6  2.365868290  1.83862318  2.8931134 0.0000000
m:8-f:6  2.081642085  1.53882109  2.6244631 0.0000000
f:7-m:6  1.329218879  0.85401081  1.8044269 0.0000000
m:7-m:6  1.673564325  1.19285330  2.1542753 0.0000000
f:8-m:6  2.534383248  2.00296656  3.0657999 0.0000000
m:8-m:6  2.250157043  1.70328327  2.7970308 0.0000000
m:7-f:7  0.344345446 -0.15203755  0.8407284 0.5648416
f:8-f:7  1.205164369  0.65953016  1.7507986 0.0000000
m:8-f:7  0.920938164  0.36023867  1.4816377 0.0000022
f:8-m:7  0.860818923  0.31038540  1.4112524 0.0000101
m:8-m:7  0.576592718  0.01122178  1.1419637 0.0401330
m:8-f:8 -0.284226205 -0.89329509  0.3248427 0.9688007
</code></pre>
"
"NaN","NaN"," 16032","<p><a href=""http://stats.stackexchange.com/questions/11887/is-this-design-a-one-way-repeated-measures-anova-or-not"">Here</a> I described my general situation. How to calculate 95%CI between means in R?</p>
"
"0.12041009266179","0.116652206545559"," 16601","<p>I've just compared the ANOVA tables generated by SPSS and Statistica  with the aov table provided by summary(aov.model). They yield identical between Subject effects (e.g., NativeLanguage(English vs Other), but different F ratios for within subject effects (e.g., Class(animate vs inanimate), aov F-ratios being consistently smaller and more conservative.  The interaction of the within-between factors again yields identical terms. I am stumped. HOw can this be? Any suggestion? Here are more details:</p>

<p>I was analyzing the RTs lexdec data base from the languageR package. I did some minor data adjustments. E.g., To get a sense for RTs I reversed the log transform exp(lexdec$RT) then removed Error responses and RT outliers. Using ddply I obtained condition means for NativeLanguage and Class for each subject (the data frame is shown at the bottom ob my post). Analyzing these data, I obtained  different aov and STATISTICA summaries. Specifically for the within-subjects factor Class was p~.18 with aov and p~.06 with STATISTICA and SPSS, with larger Class SS values shown by the two commercial packages (569) than by aov (301).</p>

<p>I've tried to make the two anova outputs (shown below) look transparent but it appears that the posting format does not match the format shown in the question window.</p>

<pre><code>&gt; C1.anova &lt;- aov(RT ~ (Class * NativeLanguage)
+  + Error(Subject/Class) + (NativeLanguage), data=C1 )
&gt; summary(C1.anova)

Error: Subject
               Df Sum Sq Mean Sq F value  Pr(&gt;F)  
NativeLanguage  1  81413   81413  6.2973 0.02131 *
Residuals      19 245637   12928                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Error: Subject:Class
                     Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
Class                 1  301.51  301.51  1.9661 0.176994   
Class:NativeLanguage  1 2175.86 2175.86 14.1880 0.001305 **
Residuals            19 2913.83  153.36                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>STATISTICA</p>

<pre><code>                       SS  DF        MS         F          p
Intercept        15387240   1  15387240  1190.202   0.000000
NativeLanguage      81413   1     81413     6.297   0.021311
Error              245637  19     12928     
Class                 569   1       569     3.709   0.069217
Class*NativeLanguage 2176   1      2176    14.188   0.001305
Error                2914  19       153     
</code></pre>

<p>Data:</p>

<pre><code>   Subject  Class NativeLanguage       RT
1       A1 animal        English 557.6410
2       A1  plant        English 548.4687
3       A2 animal        English 533.4737
4       A2  plant        English 511.7941
5       A3 animal          Other 598.9545
6       A3  plant          Other 602.4118
7        C animal        English 562.8864
8        C  plant        English 560.0588
9        D animal          Other 630.1464
10       D  plant          Other 604.0286
11       I animal          Other 542.1219
12       I  plant          Other 533.1666
13       J animal          Other 565.4324
14       J  plant          Other 513.2333
15       K animal        English 492.4500
16       K  plant        English 517.7333
17      M1 animal        English 481.8372
18      M1  plant        English 497.9687
19      M2 animal          Other 671.6666
20      M2  plant          Other 655.8750
21       P animal          Other 640.7209
22       P  plant          Other 610.0286
23      R1 animal        English 552.9744
24      R1  plant        English 545.4242
25      R2 animal        English 636.8864
26      R2  plant        English 675.1714
27      R3 animal        English 607.8572
28      R3  plant        English 614.9428
29       S animal        English 599.9285
30       S  plant        English 586.6286
31      T1 animal        English 580.0500
32      T1  plant        English 583.2857
33      T2 animal          Other 892.5526
34      T2  plant          Other 862.1000
35       V animal          Other 736.2619
36       V  plant          Other 718.3529
37      W1 animal        English 517.0465
38      W1  plant        English 539.2727
39      W2 animal        English 639.1363
40      W2  plant        English 666.7143
41       Z animal          Other 725.3750
42       Z  plant          Other 706.2069
</code></pre>
"
"0.130778311815021","0.138214738143788"," 17014","<p>I am still struggling with the transition from STATISTICA/SPSS to R. 
To get a feel for R, I used the ChickWeight data from the R package.
There is one between-chick/subjects factor, <code>Diet</code>, and one within-chick/subjects factor. <code>Time</code>, which I used to analyze weight.</p>

<p>Using the R manual, I used the <code>aov()</code> function: </p>

<pre><code>Chicken.anova &lt;- aov(weight ~ (Time * Diet) + Error(Chick/Time) + (Diet), data=ChickWeight)
</code></pre>

<p>It yielded one set of effects with <code>Chick</code> as error term. The effects included the between chick effect <code>Diet</code> (as expected) and also within-subjects effects (unexpected); <code>aov()</code> also yielded a second set of effects, with the interaction of <code>Chick</code> with <code>Time</code> as error term. </p>

<p>When I reshaped the data, so that <code>Time</code> could be treated as a repeated measure by STATISTICA, and then used the between subjects factor <code>Diet</code> and the within-subject factor <code>Time</code> to analyze weight, the anova yielded a different (more familiar) effect pattern. The between subjects factor now listed only <code>Diet</code> as an effect, with an F value of 7.14, and <code>Time(DAYS)</code> and the interaction of <code>Diet</code> with <code>Time</code> (<code>Time</code> with <code>DAYS</code>) were listed within subjects. I thought I understood anovas reasonably well, but I am not able to understand the benefit of the <code>aov()</code> result over the STATISTICA result. What accounts for the difference? The outcomes of the <code>aov()</code> and STATISTICA analyses are shown below. </p>

<p>R code &amp; results</p>

<pre><code>####aov() 
summary(Chicken.anova &lt;- aov(weight ~ (Time * Diet)
+    + Error(Chick/Time) + (Diet), data=ChickWeight))

Error: Chick
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
Time       1  89700   89700 12.4668 0.0009853 ***
Diet       3 121337   40446  5.6212 0.0023723 ** 
Time:Diet  1   2482    2482  0.3450 0.5599840    
Residuals 44 316586    7195                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Error: Chick:Time
          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
Time       1 1962914 1962914 348.4347 &lt; 2.2e-16 ***
Time:Diet  3   84222   28074   4.9834  0.004464 ** 
Residuals 46  259142    5634                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>STATISTICA results</p>

<pre><code>        SS  Degr. of    MS  F   p
            Freedom         
Intercept   5567063 1   5567063     1425.22 0.000000
Diet        83716   3   27905       7.144   0.000489
Error       179681  46  3906        
DAYS        1346422 9   149602      259.603 0.000000
DAYS*Diet   76737   27  2842        4.932   0.000000
Error       238578  414 576     
</code></pre>
"
"0.0361787302646211","0.0382359556450936"," 18404","<p>I want to regress two series (one big series divided in half) with the mean of the big series. 
I do that because I would like to ""investigate"" the relationship between those two subseries and the mean.</p>

<p>Does this make any sense for you?</p>

<p>When running the code below, I don't understand why I don't get p-value:</p>

<pre><code>&gt; x  = rnorm(200)
&gt; m  = mean(x) 
&gt; anova(lm(rep(m, 100) ~ x[1:100]), lm(rep(m, 100) ~ x[101:200])) 
Analysis of Variance Table

Model 1: rep(m, 100) ~ x[1:100]
Model 2: rep(m, 100) ~ x[101:200]
  Res.Df RSS Df Sum of Sq F Pr(&gt;F)
1     98   0                      
2     98   0  0         0   
</code></pre>
"
"0.0957199230302734","0.086710996952412"," 18579","<p>We analyze blood vessel samples of patients in an organ bath which gives us force values. Each subject (patient) donates two types of blood vessels (pulmonary vein and pulmonary artery). The organ bath measurements include dose-response curves with a pharmaceutical, i.e. different concentrations are applied. From what I understand, this should be analyzed using a two-way RM ANOVA test. There are two variables (vessel type and concentration), and all levels of both are applied to each subject. The data table looks like this:</p>

<pre><code>subject  vessel  conc  force
20110818 PA      -12   0
20110818 PV      -12   0
20110818 PA      -11   0.09
20110818 PV      -11   0.15
...
</code></pre>

<p>There are no missing values or anything like that to consider.</p>

<p>Perusing several R tutorials, I came up with the following query:</p>

<pre><code>aov.ex2=aov(force~(vessel*conc)+Error(subject/(vessel*conc)),data=data.ex2)
</code></pre>

<p>However, the results are somewhat unclear to me:</p>

<pre><code>&gt; summary(aov.ex2)

Error: subject
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals  1 15.370  15.370               

Error: subject:vessel
       Df Sum Sq Mean Sq
vessel  1 166.72  166.72

Error: subject:concentration
              Df Sum Sq Mean Sq
concentration  1 3134.3  3134.3

Error: subject:vessel:concentration
                     Df Sum Sq Mean Sq
vessel:concentration  1 148.32  148.32

Error: Within
           Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 219   3031  13.840               
</code></pre>

<p>This is quite a lot of information, but in contrast to the nice examples on the web I can't seem to find the useful part of the information: p values and whether or not there are significant differences. Is there something wrong with my input, or does the result simply mean there is nothing interesting to see in these data?</p>
"
"0.0511644510096651","0.0270369035217938"," 18738","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/12398/how-to-interpret-f-and-p-value-in-anova"">How to interpret F- and p-value in ANOVA?</a>  </p>
</blockquote>



<p>I found that I can use ANOVA also for ONE Model, doing something like:</p>

<pre><code>&gt; anova(lm(a~b))
Analysis of Variance Table

Response: a
           Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    
b           1 0.002679 0.0026791  11.191 0.0009001 ***
Residuals 398 0.095282 0.0002394                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I know that ANOVA check the means BUT what test is that if I use only ONE model?
If the p.value is above 0.05 it means that the regression fit good?</p>
"
"0.0626633989716535","0.0441510785688348"," 18951","<p>What does it mean ""The means are statistically equals?"" I ask this question because I'm studing ANOVA (<code>anova()</code> function in R) and I know that this test checks if the means of N groups are statistically equals.</p>

<p>My doubt concerns what ""statistically equals"" means because if I have two series like:</p>

<pre><code>A &lt;- c(2,4,12,14)
</code></pre>

<p>the mean is <strong>8</strong></p>

<p>But if I do</p>

<pre><code>B &lt;- c(-20,-10, 10, 52)
</code></pre>

<p>the mean is always <strong>8</strong> but the number are totally differents.</p>

<p>So the question is: does it check if the means are similar and stop OR check if the means are similar AND also check if the number that ""generate"" this mean are similar?</p>

<p>Thank you</p>
"
"0.140119619801808","0.148087219439773"," 19175","<p>I'm re-analyzing a colleague's data. The data and R code <a href=""https://gist.github.com/1409791"" rel=""nofollow"">are here</a>.</p>

<p>It's a 2x2x2x2x3 completely within-Ss design. One of the predictor variables, <code>cue</code>, is a two-level variable that when collapsed to a difference score reflects a theory-pertinent value. She previously collapsed <code>cue</code> to a difference score within each subject and condition, then computed an ANOVA, yielding an MSE that she could then use for planned comparisons of each condition's mean difference score against zero. You'll have to trust me that she wasn't fishing and did indeed have good theoretical basis for doing all 24 tests.</p>

<p>I thought I'd see if there was any difference when instead using mixed effects models to represent the data. As shown in the code, I took two approaches:</p>

<p>Method 1 - Model the data as a 2x2x2x2x3 design, obtain <em>a posteriori</em> samples from this model, compute the <code>cue</code> difference score for each condition within each sample, compute the 95% prediction interval for the cue difference score within each condition.</p>

<p>Method 2 - Collapse <code>cue</code> to a difference score within each subject and condition, model the data as a 2x2x2x3 design, obtain <em>a posteriori</em> samples from this model, compute the 95% prediction interval for the cue difference score within each condition.</p>

<p>It appears that method 1 yields broader prediction intervals than method 2 with the consequence that if one uses overlap with zero as a criterion for ""significance"", only 25% of the cuing scores are ""significant"" under method 1 while 75% of cuing scores are ""significant"" under method 2. Noteably, the patterns of significance obtained by method 2 are more akin to the original ANOVA-based results than are the patterns obtained by method 1.</p>

<p>Any idea what's going on here?</p>
"
"0.0626633989716535","0.0441510785688348"," 19966","<p>In a simple experiment I asked participants to make acceptability judgements (9-point Likert scale) when they observed different animated virtual characters. I have two factors: character (7 levels) and motion (5 levels). </p>

<p>Two-way ANOVA showed significant interaction between the above factors, so I want to explore this interaction better. I've run a post-hoc Tukey test and looked at critical differences between mean acceptability ratings for all characters, and all levels of motion. It's a large matrix (rough example can be seen in <a href=""http://stackoverflow.com/questions/8391783/visualize-critical-values-pairwise-comparisons-from-posthoc-tukey-in-r"">this post in stackoverflow</a>) that might not be entirely clear for the interpretation. It's been suggested to me that maybe I could have visualize or statistically explore this interaction between characters and motion better. </p>

<p>Any suggestions how I could do that (preferably in R)?</p>
"
"0.119991273679223","0.115285743977679"," 20026","<p>I'm familiar with post-hoc testing with <code>ANOVA</code> for exploring differences between a sequence of groups, but recently I've been reading about Change Point Analysis (especially the <code>R</code> packages <code>bcp</code>, <code>changepoint</code> and <code>strucchange</code>). </p>

<p>It looks like those packages only handle data where there is one data point per unit of time. I'm curious if they can be used with data where there are multiple data points per unit of time. Here's some example data representing the measurement of a single continuous variable on a number of specimens that have been dated to specific moments in time (no repeated measurements):</p>

<pre><code>a&lt;-data.frame(time=""1000"",x=rnorm(10,12,3))
b&lt;-data.frame(time=""2000"",x=rnorm(50,13,4))
c&lt;-data.frame(time=""3500"",x=rnorm(50,12,4))
d&lt;-data.frame(time=""5000"",x=rnorm(7,14,5))
e&lt;-data.frame(time=""7000"",x=rnorm(20,10,3))
f&lt;-data.frame(time=""7500"",x=rnorm(15,11,3))
g&lt;-data.frame(time=""9000"",x=rnorm(15,10,5))
h&lt;-data.frame(time=""9500"",x=rnorm(35,30,2))
i&lt;-data.frame(time=""10000"",x=rnorm(30,28,4))
a2i&lt;-rbind(a,b,c,d,e,f,g,h,i) 

library(ggplot2)
a2i$time&lt;-as.numeric(levels(a2i$time))[a2i$time] 
ggplot(a2i,aes(time,x))+stat_smooth()+geom_point()
</code></pre>

<p><img src=""http://i.stack.imgur.com/cASXS.png"" alt=""enter image description here""></p>

<p>Here's what I'd be most grateful for some advice on...</p>

<p>Q1. Would it be valid to do the Change Point Analysis on a vector like the means or medians of the groups? That would allow me to start with a 'one data point per unit of time' input format which would suit the <code>R</code> packages, as I understand them. I've seen it done with environmental data like monthly gas concentrations (from daily observations), but I thought I'd check.  </p>

<p>Q2. Is there a kind of Change Point Analysis that I can do on the raw data in <code>a2g</code> that will give me some measures of the probabilities of changes across the sequence? For example, something that will detect the change from time=9000 to time=9500, using all the data points in the sample? I'm guessing that if it was possible, someone would already have implemented and I just need a pointer to the relevant function.</p>

<p>Q3. In case Q2 can be answered 'yes', would the method change if the distribution of each group's values was non-normal (unlike my sample data)?</p>

<p>Q4. If Change Point Analysis is completely the wrong approach here, please let me know. I'm basically just curious about methods other than <code>ANOVA</code> for these kinds of data. Any other suggestions would be most welcome.</p>
"
"0.0738495239084239","0.0624390541054463"," 20305","<p>After finding a significant treatment in two-way anova, how do I report where the differences are? Every text I have read has left me at: Ah, the results are significant, so let's move on.  In one-way anova I could use Tukey's HSD to find the means that differed, making my report much more effective. Right now I don't know how to do that with two-factor anova.</p>

<p>Here is an example from a text done in R:</p>

<p>Is there a difference in Friday tardy rates at different plants?</p>

<pre><code>&gt; mlate
        day  plant absences
1   march-4 plant1       19
2  march-11 plant1       22
3  march-18 plant1       20
4   march-4 plant2       18
5  march-11 plant2       20
6  march-18 plant2       16
7   march-4 plant3       27
8  march-11 plant3       32
9  march-18 plant3       28
10  march-4 plant4       22
11 march-11 plant4       27
12 march-18 plant4       26

&gt; anova(lm(absences ~ plant+day, data=mlate))
Analysis of Variance Table

Response: absences
          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
plant      3 216.250  72.083  41.191 0.0002134 ***
day        2  30.167  15.083   8.619 0.0172128 *  
Residuals  6  10.500   1.750                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>So I can see that both the plant and the day have an effect on worker's being late, but how do I compare the means like I would with a tukey's hsd in a one-way test?</p>
"
"0.154266624185072","0.163038662681506"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.10232890201933","0.108147614087175"," 21071","<p>I'm testing for difference in a continuous outcome under three different conditions. </p>

<p>Under condition A I take a measurement of the outcome. I do this twice for the same sample. Example values could be 2.2, 2.1. These are ""technical"" replicates that come from the same biological source</p>

<p>I do the same for four ""biological"" replicates for condition A:</p>

<pre><code>A1, measure 1: 2.2
A1, measure 2: 2.1
A2, measure 1: 2.0
A2, measure 2: 2.1
A3, measure 1: 1.9
A3, measure 2: 1.8
A4, measure 1: 1.5
A4, measure 2: 1.6
</code></pre>

<p>I also have conditions B, C, and D, with two ""technical"" replicates in each of four ""biological"" replicates.</p>

<p>How would I test test for mean differences (ANOVA) that best accounts for both the technical and biological variation? I wouldn't want to fit a model counting each measurement as a separate observation, because each pair comes from the same biological sample. I'm assuming there must be a better way than just averaging over the pairs.</p>

<p>Bonus: how do you do this in R?</p>

<p>Assuming I have data that looks like this:</p>

<pre><code>&gt; data
   condition sample measurement outcome
1          A      1           1     2.2
2          A      1           2     2.1
3          A      2           1     2.0
4          A      2           2     2.1
5          A      3           1     1.9
6          A      3           2     1.8
7          A      4           1     1.5
8          A      4           2     1.6
9          B      1           1     1.7
10         B      1           2     1.6
11         B      2           1     1.5
12         B      2           2     1.6
13         B      3           1     1.4
14         B      3           2     1.3
15         B      4           1     1.0
16         B      4           2     1.1
17         C      1           1     2.4
18         C      1           2     2.3
19         C      2           1     2.2
20         C      2           2     2.3
21         C      3           1     2.1
22         C      3           2     2.0
23         C      4           1     1.7
24         C      4           2     1.8
</code></pre>

<p>I probably wouldn't want to do something like this:</p>

<pre><code>summary(lm(outcome~condition, data=data))
</code></pre>

<p>Thanks in advance.</p>
"
"0.12041009266179","0.137861698644752"," 21112","<p>I am trying to reproduce the anova table for a 2x2 cross-over design. I used the data listed in tables 2.1 and 2.2 of the ""Design and Analysis of cross-over trials"" book, by Jones and Kenward. I am giving all the code so that you can reproduce the calculations.     </p>

<pre><code>g1AB &lt;- read.table(textConnection(""
Label Per1 Per2
7 121.905 116.667
8 218.5 200.5
9 235 217.143
13 250 196.429
14 186.19 185.5
15 231.563 221.842
17 443.25 420.5
21 198.421 207.692
22 270.5 213.158
28 360.476 384
35 229.75 188.25
36 159.091 221.905
37 255.882 253.571
38 279.048 267.619
41 160.556 163
44 172.105 182.381
58 267 313
66 230.75 211.111
71 271.19 257.619
76 276.25 222.105
79 398.75 404
80 67.778 70.278
81 195 223.158
82 325 306.667
86 368.077 362.5
89 228.947 227.895
90 236.667 220
"")-&gt;con,header=T)
close(con)

g2BA &lt;- read.table(textConnection(""
Label Per1 Per2
3 138.333 138.571
10 225 256.25
11 392.857 381.429
16 190 233.333
18 191.429 228
23 226.19 267.143
24 201.905 193.5
26 134.286 128.947
27 238 248.5
29 159.5 140
30 232.75 276.563
32 172.308 170
33 266 305
39 171.333 186.333
43 194.737 191.429
47 200 222.619
51 146.667 183.81
52 208 241.667
55 208.75 218.81
59 271.429 225
68 143.81 188.5
70 104.444 135.238
74 145.238 152.857
77 215.385 240.476
78 306 288.333
83 160.526 150.476
84 353.81 369.048
85 293.889 308.095
99 371.190 404.762
"")-&gt;con,header=T)
close(con)

n1 &lt;- nrow(g1AB)
n2 &lt;- nrow(g2BA)
p &lt;- 2 # periods
</code></pre>

<p>Some quantities that may be useful:</p>

<pre><code>y11. &lt;- sum(g1AB$Per1)
y12. &lt;- sum(g1AB$Per2)
y21. &lt;- sum(g2BA$Per1)
y22. &lt;- sum(g2BA$Per2)
y1.. &lt;- sum(c(g1AB$Per1,g1AB$Per2))
y2.. &lt;- sum(c(g2BA$Per1,g2BA$Per2))
y... &lt;- y1.. + y2..

y11.bar &lt;- 1/n1 * y11.
y12.bar &lt;- 1/n1 * y12.
y21.bar &lt;- 1/n2 * y21.
y22.bar &lt;- 1/n2 * y22.

y1..bar &lt;- 1/(p*n1)*(y11.+y12.)
y2..bar &lt;- 1/(p*n2)*(y21.+y22.)

y...bar &lt;- 1/(p*(n1+n2)) * (y1.. + y2..)
</code></pre>

<p>In order to perform the analysis of variance in R, I created the following dataset</p>

<pre><code>mydata1 &lt;- data.frame(PEFR=g1AB$Per1,Subjects=g1AB$Label,Time=1,Groups=""AB"",Treatment=1)
mydata2 &lt;- data.frame(PEFR=g2BA$Per1,Subjects=g2BA$Label,Time=1,Groups=""BA"",Treatment=2)
mydata3 &lt;- data.frame(PEFR=g1AB$Per2,Subjects=g1AB$Label,Time=2,Groups=""AB"",Treatment=2)
mydata4 &lt;- data.frame(PEFR=g2BA$Per2,Subjects=g2BA$Label,Time=2,Groups=""BA"",Treatment=1)
mydata &lt;- rbind(mydata1,mydata2,mydata3,mydata4)
mydata$Subjects&lt;-factor(mydata$Subjects)
</code></pre>

<p>(I have read that) The correct anova table is obtained using the following command</p>

<pre><code>res &lt;- summary(PEFR.aov &lt;- aov(PEFR~Groups+Time+Treatment+Error(Subjects), data=mydata))

Error: Subjects
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Groups     1  10573   10573   0.899  0.347
Residuals 54 634866   11757               

Error: Within
          Df Sum Sq Mean Sq F value  Pr(&gt;F)   
Time       1    480   479.6   1.470 0.23061   
Treatment  1   3026  3026.1   9.276 0.00359 **
Residuals 54  17617   326.2 
</code></pre>

<p>1) Can you please explain to me the use of the Error term in the formula?</p>

<p>2) The anova table is the same to that of table 2.10 of the book, except the SS for Time (Period) which has a value of 396.858 instead of 480. According to Table 2.8 this quantity is calculated using</p>

<pre><code>&gt; n1*n2/(2*(n1+n2))*(y11.bar-y12.bar+y21.bar-y22.bar)^2
[1] 396.8583
</code></pre>

<p>Also, the Treatment SS is calculated by</p>

<pre><code>n1*n2/(2*(n1+n2))*(y11.bar-y12.bar-y21.bar+y22.bar)^2
[1] 3026.12
</code></pre>

<p>If the formula was specified like this</p>

<pre><code>PEFR~Groups+Treatment+Time+Error(Subjects)
</code></pre>

<p>the SS for Time would be correct, but the Treatment SS would have a value of 3109. How can I get 3026 for Treatment and 397 for Time, as I see in the book? What should be taken care of in the order the variables are specified in the formula? </p>

<p>Thank you for your time.  </p>
"
"0.0626633989716535","0.0662266178532522"," 21692","<p>Please help!
I have recently been criticized for using pairwise comparisons to explain all three levels of a factor within a negative binomial GLM rather than all levels at once. I was told that it is ""long-winded"" and ""uneccessary"". I was under the impression that in GLMs one cannot bulk all levels of a factor together to obtain a test statistic and corresponding p-value.</p>

<p>Obviously if a factor is ""insignificant"" at any level then carrying out a post-hoc analysis is pointless. My levels all have there own p-values therefore I discussed these values from the below global model. I was told to do an ANOVA instead which I don't believe is suitable for overdispersed, zero-inflated data.</p>

<p>p-value for all levels of a factor anyone?</p>

<p>(Below, lower field layer 0, upper field layer 1 and change1 is in intercept)</p>

<pre><code>    Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
   -2.4284  -0.7956  -0.3862   0.4045   2.4233  

    Coefficients:
                            Estimate Std. Error z value Pr(&gt;|z|)   
    (Intercept)                    4.3410884  1.8219786   2.383  0.01719 * 
    Height                         0.0373584  0.0119929   3.115  0.00184 **
    Width                         -0.0007891  0.0008246  -0.957  0.33859   
    MeanMin                       -0.1731877  0.1404434  -1.233  0.21752   
    as.factor(Site_Treat)2        -0.4080256  0.2480438  -1.645  0.09998 . 
    as.factor(Change)2            -0.4940398  0.1755487  -2.814  0.00489 **
    as.factor(Change)3            -0.1613766  0.1763677  -0.915  0.36019   
    as.factor(Lower_Field_Layer)1  0.4873488  0.2931585   1.662  0.09643 . 
    as.factor(Lower_Field_Layer)2 -0.3292409  0.3717863  -0.886  0.37585   
    as.factor(Upper_Field_Layer)2 -0.0081040  0.3257734  -0.025  0.98015   
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

   (Dispersion parameter for Negative Binomial(4.7795) family taken to be 1)

    Null deviance: 96.392  on 46  degrees of freedom
    Residual deviance: 47.968  on 37  degrees of freedom
    AIC: 403.94
</code></pre>

<p>Best wishes,
Platypezid</p>
"
"0.0808981002113217","0.0854981960070962"," 22972","<p>I am measuring the X, Y, Z variables of i subjects in k states, and for each subject and state i am taking j measurements. What is the proper way to test for differences between the k states?</p>

<p>It seems likes a one way repeated measure ANOVA problem, however how should I deal with the fact that in each subject and within each state i take j measurements of X, Y, Z? Should I take the mean of the Xj, Yj, Zj within each subject and state and with this mean do a one way repeated measures ANOVA?</p>

<p>Many thanks?</p>
"
"0.119991273679223","0.115285743977679"," 23197","<p>I'm analyzing data from an unbalanced factorial experiment both with <code>SAS</code> and <code>R</code>. Both <code>SAS</code> and <code>R</code> provide similar Type I sum of squares but their Type III sum of squares are different from each other. Below are <code>SAS</code> and <code>R</code> codes and outputs. </p>

<pre><code>DATA ASD;
INPUT Y T B;
DATALINES;
 20 1 1
 25 1 2
 26 1 2
 22 1 3
 25 1 3
 25 1 3
 26 2 1
 27 2 1
 22 2 2
 31 2 3
;

PROC GLM DATA=ASD;
CLASS T B;
MODEL Y=T|B;
RUN;
</code></pre>

<p><strong>Type I SS from SAS</strong>    </p>

<pre><code>Source  DF       Type I SS     Mean Square    F Value    Pr &gt; F
T       1     17.06666667     17.06666667       9.75    0.0354
B       2     12.98000000      6.49000000       3.71    0.1227
T*B     2     47.85333333     23.92666667      13.67    0.0163
</code></pre>

<p><strong>Type III SS from SAS</strong></p>

<pre><code>Source  DF     Type III SS     Mean Square    F Value    Pr &gt; F
T       1     23.07692308     23.07692308      13.19    0.0221
B       2     31.05333333     15.52666667       8.87    0.0338
T*B     2     47.85333333     23.92666667      13.67    0.0163
</code></pre>

<p><strong>R Code</strong></p>

<pre><code>Y &lt;- c(20, 25, 26, 22, 25, 25, 26, 27, 22, 31)
T &lt;- factor(x=rep(c(1, 2), times=c(6, 4)))
B &lt;- factor(x=rep(c(1, 2, 3, 1, 2, 3), times=c(1, 2, 3, 2, 1, 1)))
Data &lt;- data.frame(Y, T, B)
Data.lm &lt;- lm(Y~T*B, data = Data)
anova(Data.lm)
drop1(Data.lm,~.,test=""F"") 
</code></pre>

<p><strong>Type I SS from R</strong>    </p>

<pre><code>Analysis of Variance Table

Response: Y
          Df Sum Sq Mean Sq F value  Pr(&gt;F)  
T          1 17.067  17.067  9.7524 0.03543 *
B          2 12.980   6.490  3.7086 0.12275  
T:B        2 47.853  23.927 13.6724 0.01629 *
Residuals  4  7.000   1.750                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p><strong>Type III SS from R</strong></p>

<pre><code>Single term deletions

Model:
Y ~ T * B
       Df Sum of Sq    RSS     AIC F value  Pr(&gt;F)  
&lt;none&gt;               7.000  8.4333                  
T       1    28.167 35.167 22.5751 16.0952 0.01597 *
B       2    20.333 27.333 18.0552  5.8095 0.06559 .
T:B     2    47.853 54.853 25.0208 13.6724 0.01629 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Am I missing something here? If not which one is correct Type III SS?</p>
"
"0.140394095324012","0.148377302419909"," 23276","<p>We know that a paired <i>t</i>-test is just a special case of one-way repeated-measures (or within-subject) ANOVA as well as linear mixed-effect model, which can be demonstrated with lme() function the nlme package in R as shown below.</p>

<pre><code>#response data from 10 subjects under two conditions
x1&lt;-rnorm(10)
x2&lt;-1+rnorm(10)

# Now create a dataframe for lme
myDat &lt;- data.frame(c(x1,x2), c(rep(""x1"", 10), rep(""x2"", 10)), rep(paste(""S"", seq(1,10), sep=""""), 2))
names(myDat) &lt;- c(""y"", ""x"", ""subj"")
</code></pre>

<p>When I run the following paired t-test:</p>

<pre><code>t.test(x1, x2, paired = TRUE)
</code></pre>

<p>I got this result (you will get a different result because of the random generator):</p>

<pre><code>t = -2.3056, df = 9, p-value = 0.04657
</code></pre>

<p>With the ANOVA approach we can get the same result:</p>

<pre><code>summary(aov(y ~ x + Error(subj/x), myDat))

# the F-value below is just the square of the t-value from paired t-test:
          Df  F value Pr(&gt;F)
x          1  5.3158  0.04657
</code></pre>

<p>Now I can obtain the same result in lme with the following model, assuming a positive-definite symmetrical correlation matrix for the two conditions:</p>

<pre><code>summary(fm1 &lt;- lme(y ~ x, random=list(subj=pdSymm(form=~x-1)), data=myDat))

# the 2nd row in the following agrees with the paired t-test
# (Intercept) -0.2488202 0.3142115  9 -0.7918878  0.4488
# xx2          1.3325786 0.5779727  9  2.3056084  0.0466
</code></pre>

<p>Or another model, assuming a compound symmetry for the correlation matrix of the two conditions:</p>

<pre><code>summary(fm2 &lt;- lme(y ~ x, random=list(subj=pdCompSymm(form=~x-1)), data=myDat))

# the 2nd row in the following agrees with the paired t-test
# (Intercept) -0.2488202 0.4023431  9 -0.618428  0.5516
# xx2          1.3325786 0.5779727  9  2.305608  0.0466
</code></pre>

<p>With the paired t-test and one-way repeated-measures ANOVA, I can write down the traditional cell mean model as</p>

<pre><code>Yij = Î¼ + Î±i + Î²j + Îµij, i = 1, 2; j = 1, ..., 10
</code></pre>

<p>where i indexes condition, j indexes subject, Y<sub>ij</sub> is the response variable, Î¼ is constant for the fixed effect for overall mean, Î±<sub>i</sub> is the fixed effect for condition, Î²<sub>j</sub> is the random effect for subject following N(0, Ïƒ<sub>p</sub><sup>2</sup>) (Ïƒ<sub>p</sub><sup>2</sup> is population variance), and Îµ<sub>ij</sub> is residual following N(0, Ïƒ<sup>2</sup>) (Ïƒ<sup>2</sup> is within-subject variance).</p>

<p>I thought that the cell mean model above would not be appropriate for the lme models, but the trouble is that I can't come up with a reasonable model for the two lme() approaches with the correlation structure assumption. The reason is that the lme model seems to have more parameters for the random components than the cell mean model above offers. At least the lme model provides exactly the same F-value, degrees of freedom, and p-value as well, which gls cannot. More specifically gls gives incorrect DFs due to the fact that it does not account for the fact that each subject has two observations, leading to much inflated DFs. The lme model most likely is overparameterized in specifying the random effects, but I don't know what the model is and what the parameters are. So the issue is still unresolved for me. </p>
"
"0.173507095076498","0.175400453519478"," 23833","<h2>The data</h2>

<p>Suppose we have a dataset <code>d</code> with two between-subject factors (i.e., groups), <code>group</code> and <code>condition</code>, and two within-subject factors (i.e., repeated-measures factors), <code>topic</code> and <code>problem</code> (I uploaded the data to pastebin, so everybody should be able to obtain it):</p>

<pre><code>&gt; d &lt;- read.table(url(""http://pastebin.com/raw.php?i=4hRFyaRj""), colClasses = c(rep(""factor"", 6), ""numeric""))
&gt; str(d)
'data.frame':   2928 obs. of  6 variables:
  $ code     : Factor w/ 183 levels &quot;A03U&quot;,&quot;A08C&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
  $ group    : Factor w/ 2 levels ""control"",""experimental"": 2 2 2 2 2 2 2 2 2 2 ...
  $ condition: Factor w/ 3 levels &quot;alternatives&quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
  $ topic    : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 2 2 2 2 3 3 ...
  $ problem  : Factor w/ 4 levels &quot;AC&quot;,&quot;DA&quot;,&quot;MP&quot;,..: 3 4 1 2 3 4 1 2 3 4 ...
  $ mean     : num  94.5 94.5 86.5 84.5 80 46.5 73.5 43.5 51 39 ...
</code></pre>

<p>The data is from a behavioral experiment in which participants in six groups (2 levels of <code>group</code> times 3 levels of <code>condition</code>) worked on 16 tasks (for each of 4 <code>topics</code> 4 different <code>problems</code>). Allocation of participants to group/condition was fully random. Presentation of tasks was random insofar that problem was blocked within topic (i.e., for each topic all problems where presented sequentially), but order of problem and topic was random.<br>
<strong>Update:</strong> The factor identifying the participant (in which topic and problem are nested) is <code>code</code>. </p>

<h2>The Problem</h2>

<p>How can I fit this dataset using <code>lme</code>?<br>
(Sidenote: I would also consider using <code>lme4</code>, but I am kind of afraid of not having p-values, if there is something easily digestible as p-values, I would also consider <code>lme4</code> an option).</p>

<p>So far I managed to fit an <code>lme</code> model with only one within-subject factor, but not two (see below).</p>

<h2>What I tried</h2>

<p>I can fit an <code>lme</code> model if I have just <a href=""http://stats.stackexchange.com/a/10909/442"">one within-subject factor</a>:</p>

<pre><code>require(nlme)
 m1 &lt;- lme(mean ~ condition*group*problem, random = ~1|code/problem, 
           data = d, subset = topic == ""1"")

anova(m1)
                        numDF denDF F-value p-value
(Intercept)                 1   531   12101  &lt;.0001
condition                   2   177      31  &lt;.0001
group                       1   177       2  0.2178
problem                     3   531      35  &lt;.0001
condition:group             2   177       1  0.3672
condition:problem           6   531      24  &lt;.0001
group:problem               3   531       1  0.2180
condition:group:problem     6   531       2  0.0281
</code></pre>

<p>This (especially the df) nicely correspond with the results from an standard ANOVA (using
<code>ez</code>):</p>

<pre><code>require(ez)
ezANOVA(subset(d, topic == ""1""), dv = .(mean), wid = .(code), between = .(condition, group), within = .(problem))$ANOVA

Warning: Data is unbalanced (unequal N per group). Make sure you specified a well-considered value for the type argument to ezANOVA().
                   Effect DFn DFd     F                             p p&lt;.05     ges
2               condition   2 177 30.69 0.000000000003611248905859672     * 0.13079
3                   group   1 177  1.53 0.217821969825403999321267179       0.00374
5                 problem   3 531 34.85 0.000000000000000000014254103     * 0.10028
4         condition:group   2 177  1.01 0.367225806638525886782531416       0.00492
6       condition:problem   6 531 24.40 0.000000000000000000000000142     * 0.13503
7           group:problem   3 531  1.48 0.217959293081550348203379031       0.00472
8 condition:group:problem   6 531  2.38 0.028119961573665430004664856     * 0.01499
</code></pre>

<p>Trying to fit this data with two within-subject factors in <code>lme</code> fails (either per code, or per dfs):</p>

<pre><code>m2 &lt;- lme(mean ~ condition*group*problem*topic, random = ~1|code/(problem*topic), data = d)
# fails: Error in getGroups.data.frame(dataMix, groups) : 
#  Invalid formula for groups

m3 &lt;- lme(mean ~ condition*group*problem*topic, random = ~1|code/problem/topic, data = d)
# the next model takes some time (probably already an indicator, that it is the wrong model)
# and produces wrong denominator df!

# with both factors as ANOVA
m4 &lt;- ezANOVA(d, dv = .(mean), wid = .(code), between = .(condition, group), within = .(problem, topic))

#effects are the same
all(row.names(anova(m3))[-1] == m4$ANOVA$Effect)

#denominator dfs are not:
anova(m3)$denDF[-1] == m4$ANOVA$DFd

# only for effects with topic:
row.names(anova(m3))[-1][!(anova(m3)$denDF[-1] == m4$ANOVA$DFd)]
</code></pre>

<p><strong>UPDATE</strong>: As the precise error or nesting is somewhat unclear I here provide the equivalent <code>aov</code> call (this is the ""standard"" model via <code>aov</code>), which matches the results from <code>ezANOVA</code>. The critical error term is <code>Error(code/(problem*topic))</code>:</p>

<pre><code>m5 &lt;- aov(mean ~ (condition*group*problem*topic) + Error(code/(problem*topic)), d)
summary(m5)
</code></pre>
"
"0.173507095076498","0.175400453519478"," 25632","<h1>Books to Learn Statistics using R</h1>

<h2>What exactly is the book I'm looking for.</h2>

<p>What I am looking for is a book that teaches you statistics while using R to give you hands-on experience and thus end up helping you learn R together. I've seen on amazon many books that attempts to do that, but not with R. Examples are Minitab and SAS.</p>

<h2>Are the R Book and Statistical Computing an option? - <em>Still not answered</em>.</h2>

<p><a href=""http://rads.stackoverflow.com/amzn/click/0470510242"">The R Book</a> and <a href=""http://rads.stackoverflow.com/amzn/click/0471560405"">Statistical Computing: An Introduction to Data Analysis using S-Plus</a> seems viable, but a reader opinion here would be helpful and welcome.</p>

<h2>How the book relate to statistics courses?</h2>

<p>To be even more precise on what I was looking for, consider these two courses learning outcomes on statistics from a math department at the university Im currently a student:</p>

<p><a href=""http://www.stevens.edu/ses/math/courses/ma331/index.php"">Intermediate Statistics</a> and <a href=""http://www.stevens.edu/ses/math/courses/ma222/index.php"">Probability &amp; Statistics</a>, that is, I'm looking in a book a normal statistics course going to intermediate level but rather than just board and paper having you learning and using R instead. That also means I am looking for a book that assume I want to learn statistics from the beginning. </p>

<h2>This book is for researchers too.</h2>

<p>I am also a software engineer researcher, but I guess the current situation where you are found with mountains of data and want to learn statistics to go on writing code to automate that is pretty much applicable to many other fields. </p>

<p>That means I'm am not interested on learning every single detail of every single property for every single curve, but am more concerned on making sense of data for my research domain, although I would not mind if the book wanted to go deep on that. </p>

<p>As a final motivation, I find myself reading scientific papers in different sort of communities that claim results based on statistical inference while there is no readable proof if the statistics assumptions/constraints are being violated or not. </p>

<p>A R book that is not much about statistics won't ensure I am not following up on this practice, which is also why I decided looking for a book that is akin to a statistics course using R rather than playing around with a overview book. </p>

<h2>Related questions in Cross Validated.</h2>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/1668/what-books-provide-an-overview-of-computational-statistics-as-it-applies-to-comp"">What books provide an overview of computational statistics as it applies to computer science?</a> - Differs that the question looks for an overview while this is to learn statistics using R.</li>
<li><a href=""http://stats.stackexchange.com/questions/614/open-source-statistical-textbooks"">Open source statistical books</a> gives a list of open source (open books) available online. </li>
</ul>

<h2>Answers and feedback for this question.</h2>

<h3>@Julie</h3>

<p>Suggested books were few I already come across but are an example that unfortunately doesn't suits me:</p>

<p><a href=""http://rads.stackoverflow.com/amzn/click/0387790535"">Introductory Statistics with R</a>, <a href=""http://rads.stackoverflow.com/amzn/click/1584884509"">Using R for Introductory Statistics</a>, <a href=""http://rads.stackoverflow.com/amzn/click/0470022981"">Statistics: An Introduction using R</a> are few of the books that I already looked on amazon but are about an statistics overview or make assumptions that requires previous statistics knowledge. The problem with overview books is mostly about not calling attention to the assumptions, constraints and provide enough explanation to result in make sense of the information. </p>

<p>If you believe there is no book that could fit on this needing as well or think the R book or the Statistical Computing: An Introduction to Data Analysis using S-Plus would fit this, I would also appreciate this type of answer.</p>

<h3>@Christopher Aden</h3>

<p>Introduction to Probability and Statistics Using R seems to be the closest one but still broad general to what I was looking for.</p>

<p><em>What I was expecting for is a book such as <a href=""http://rads.stackoverflow.com/amzn/click/1429224266"">David S. Moore, The Basics of Statistics</a> because:</em></p>

<ul>
<li>It covers all statistics subjects. </li>
<li>It uses two tools, miniTab and other to give hands-on learning on the just explained method.</li>
<li>It very much highlight assumptions and constraints. This is very important for a researcher who has not taken a in depth statistics course and want to use statistics. Hardly overview books will cover them, which is dangerous for researchers.
<ul>
<li>You can see the book table of contents <a href=""http://www.whfreeman.com/Catalog/product/basicpracticeofstatistics-fifthedition-moore/tableofcontents"">here</a>. Notice how the focus is statistics and the tool usage is to improve understanding and get the student to know how to use tools to do the statistics after learning in an easier way. Its not about the tool, its about statistics! </li>
</ul></li>
</ul>

<p><em>I want exactly the same thing, but using R.</em></p>

<h3>@Gregory Demin</h3>

<p>It uses R as pedagogy examples, assumes you want to learn statistics and best of all, it is open source. Unfortunately, does not cover ANOVA nor ANCOVA, or more advanced subjects.</p>

<h3>@Peter Ellis</h3>

<p>Good suggestion for a textbook that covers what is wanted in this question.</p>

<h2>Books in the asker opinion that answer the question.</h2>

<p>@Peter Ellis and @Gregory Demin.</p>

<h2>Collection of R Books on Amazon</h2>

<p>Amazon discussion about R books for different students background may be found <a href=""http://www.amazon.com/gp/richpub/syltguides/fullview/R3ET07XHWH16TF"">here</a>. </p>

<h2>Video Lectures teaching Statistics using R</h2>

<p>Google Tech Talks from 2007 that also motivated this question and covers more about Data Mining rather than statistics but using R together <a href=""http://www.youtube.com/watch?v=zRsMEl6PHhM&amp;feature=BFa&amp;list=LL6zEW1-HcVxlw7hOFjJHneg&amp;lf=mh_lolz"">here</a>.</p>
"
"0.149168726281763","0.139103721018664"," 27396","<p>This is continuation for a series of questions (<a href=""http://stackoverflow.com/questions/10330314/pointrange-plot-with-boxplot-type-grouping"">1</a>, <a href=""http://stats.stackexchange.com/questions/27248/monte-carlo-nonparametric-confidence-intervals-for-mean-estimate"">2</a>). I have a data set from an experiment with 2x2 design. A replicate consists of 20-50 normally distributed replicate measurements. Each treatment combination has 3 of these replicates. Measured average response for each replicate seems not to be normally distributed. </p>

<p><img src=""http://i.stack.imgur.com/tfJEe.png"" alt=""enter image description here"">
Figure 1. Example of the problem. Error bars show standard error of mean.</p>

<p>Since this is an experiment and I am supposed to talk about an effect of a treatment, I would like to combine the variation within measurements (error bars) and between replicates (dots in the figure) into one number / treatment combination and use the error bars with one mean value to show the overall effect of a treatment (a dot instead of three). Ideally I would like to use confidence intervals of some sort (<a href=""http://stackoverflow.com/questions/10330314/pointrange-plot-with-boxplot-type-grouping"">asked here</a>). I have understood that since this is an unbalanced nested design, I should use mixed models or a simple 2-way ANOVA for mean values, but how to compress the variation on two levels into one figure?</p>

<p>Would <a href=""http://www.phidot.org/software/mark/docs/book/pdf/app_2.pdf"" rel=""nofollow"">the delta method</a> be something for me?</p>

<p>R code with an example of the data:</p>

<pre><code>library(ggplot2)

x1 &lt;- c(rnorm(50, 14,3),rnorm(35, 7,1),rnorm(40, 15,9))
x2 &lt;- c(rnorm(43, 6,3),rnorm(32, 7,1),rnorm(40, 8,4))
x3 &lt;- c(rnorm(50, 15,5), rnorm(50, 10,7), rnorm(50, 13,9))
x4 &lt;- c(rnorm(26, 14,2), rnorm(43, 25,10), rnorm(45, 15,9))

dx1 &lt;- data.frame(Treatment = rep(""T1"", length(x1)), Temp = rep(10, length(x1)), Rep = rep(c(1,2,3), times = c(50,35,40)), Meas = x1)
dx2 &lt;- data.frame(Treatment = rep(""T1"", length(x2)), Temp = rep(20, length(x2)), Rep = rep(c(4,5,6), times = c(43,32,40)), Meas = x2)
dx3 &lt;- data.frame(Treatment = rep(""T2"", length(x3)), Temp = rep(10, length(x3)), Rep = rep(c(7,8,9), times = c(50,50,50)), Meas = x3)
dx4 &lt;- data.frame(Treatment = rep(""T2"", length(x4)), Temp = rep(20, length(x4)), Rep = rep(c(10,11,12), times = c(26,43,45)), Meas = x4)

# Entire data set

dat &lt;- rbind(dx1,dx2,dx3,dx4)

# Plot overview

w &lt;- ggplot(dat, aes(x = factor(Rep), y = Meas))
w + geom_boxplot(aes(fill = factor(Temp)))

# Averages (with se)

aveg &lt;- aggregate(Meas ~ Treatment + Temp + Rep, data = dat, FUN = mean)
se &lt;- function(x) sd(x)/sqrt(length(x))
SE &lt;- aggregate(Meas ~ Treatment + Temp + Rep, data = dat, FUN = se)

dat2 &lt;- merge(aveg, SE, by = c(""Treatment"", ""Temp"", ""Rep""), sort = F)

colnames(dat2)[colnames(dat2) %in% grep(""\\.x"", colnames(dat2), value = T)] &lt;- ""mean""
colnames(dat2)[colnames(dat2) %in% grep(""\\.y"", colnames(dat2), value = T)] &lt;- ""se""

# Plot entire data set

p &lt;- ggplot(dat2, aes(x = Treatment, y = mean, ymax = mean + se/2, 
ymin = mean - se/2))

p + geom_pointrange(aes(color = factor(Temp)), 
position=position_dodge(width=0.50), size = 1)
</code></pre>
"
"0.0957199230302734","0.086710996952412"," 27945","<p>What is the meaning and effect of %in% in a model formula?</p>

<p>It is apparently used for nesting of one variable into another in a variety of analysis (manova, anova, regressions) in a few published articles.</p>

<p>From ?formula, b%in%a is a:b, so why use %in%?<br>
How is a:b nesting?</p>

<p>I am probably mistaken, but my understanding is that nesting b in a should not lead to the same mean square as the interaction of a and b denoted by a:b?</p>

<pre><code>library(lme4)  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>with(sleepstudy, Days%in%Subject)
  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ...  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fit&lt;-aov(data=sleepstudy, Reaction~Days + Days%in%Subject)
anova(fit)


               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
 Days           1 162703  162703  193.23 &lt; 2.2e-16 ***
 Days:Subject  17 269685   15864   18.84 &lt; 2.2e-16 ***
 Residuals    161 135567     842
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
anova(fm1)


      Df Sum Sq Mean Sq F value
 Days  1  29986   29986  45.785
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction~Days + Days%in%Subject + (1|Subject), sleepstudy)
anova(fm1)

Analysis of Variance Table
             Df Sum Sq Mean Sq  F value
Days          1 162703  162703 248.4233
Days:Subject 17  73391    4317   6.5916
</code></pre>
"
"0.135368413338721","0.143065845879825"," 28486","<p>I've performed a three-way repeated measures ANOVA; what post-hoc analyses are valid? </p>

<p>This is a fully balanced design (2x2x2) with one of the factors having a within-subjects repeated measure. I'm aware of multivariate approaches to repeated measures ANOVA in R, but my first instinct is to proceed with a simple aov() style of ANOVA:</p>

<pre><code>aov.repeated &lt;- aov(DV ~ IV1 * IV2 * Time + Error(Subject/Time), data=data)
</code></pre>

<p>DV = response variable</p>

<p>IV1 = independent variable 1 (2 levels, A or B)</p>

<p>IV2 = independent variable 2 (2 levels, Yes or No)</p>

<p>IV3 = Time (2 levels, Before or After)</p>

<p>Subject = Subject ID (40 total subjects, 20 for each level of IV1: nA = 20, nB = 20)</p>

<pre><code>summary(aov.repeated)

    Error: Subject
          Df Sum Sq Mean Sq F value   Pr(&gt;F)   
IV1       1   5969  5968.5  4.1302 0.049553 * 
IV2       1   3445  3445.3  2.3842 0.131318   
IV1:IV2   1  11400 11400.3  7.8890 0.007987 **
Residuals 36  52023  1445.1                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Error: Subject:Time
               Df Sum Sq Mean Sq F value   Pr(&gt;F)   
Time            1    149   148.5  0.1489 0.701906   
IV1:Time        1    865   864.6  0.8666 0.358103   
IV2:Time        1  10013 10012.8 10.0357 0.003125 **
IV1:IV2:Time    1    852   851.5  0.8535 0.361728   
Residuals      36  35918   997.7                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Alternatively, I was thinking about using the nlme package for a lme style ANOVA:</p>

<pre><code>aov.repeated2 &lt;- lme(DV ~ IV1 * IV2 * Time, random = ~1|Subject/Time, data=data)
summary(aov.repeated2)

Fixed effects: DV ~ IV1 * IV2 * Time 
                                Value Std.Error DF   t-value p-value
(Intercept)                      99.2  11.05173 36  8.975972  0.0000
IV1                              19.7  15.62950 36  1.260437  0.2156
IV2                              65.9  15.62950 36  4.216385  0.0002 ***
Time                             38.2  14.12603 36  2.704228  0.0104 *
IV1:IV2                         -60.8  22.10346 36 -2.750701  0.0092 **
IV1:Time                        -26.2  19.97722 36 -1.311494  0.1980
IV2:Time                        -57.8  19.97722 36 -2.893295  0.0064 **
IV1:IV2:Time                     26.1  28.25206 36  0.923826  0.3617
</code></pre>

<p>My first instinct post-hoc of significant 2-way interactions with Tukey contrasts using glht() from multcomp package:</p>

<pre><code>data$IV1IV2int &lt;- interaction(data$IV1, data$IV2)
data$IV2Timeint &lt;- interaction(data$IV2, data$Time)

aov.IV1IV2int &lt;- lme(DV ~ IV1IV2int, random = ~1|Subject/Time, data=data)
aov.IV2Timeint &lt;- lme(DV ~ IV2Timeint, random = ~1|Subject/Time, data=data)

IV1IV2int.posthoc &lt;- summary(glht(aov.IV1IV2int, linfct = mcp(IV1IV2int = ""Tukey"")))
IV2Timeint.posthoc &lt;- summary(glht(aov.IV2Timeint, linfct = mcp(IV2Timeint = ""Tukey"")))

IV1IV2int.posthoc
#A.Yes - B.Yes == 0        0.94684   
#B.No - B.Yes == 0         0.01095 * 
#A.No - B.Yes == 0         0.98587    I don't care about this
#B.No - A.Yes == 0         0.05574 .  I don't care about this
#A.No - A.Yes == 0         0.80785   
#A.No - B.No == 0          0.00346 **

IV2Timeint.posthoc 
#No.After - Yes.After == 0           0.0142 *
#Yes.Before - Yes.After == 0         0.0558 .
#No.Before - Yes.After == 0          0.5358   I don't care about this
#Yes.Before - No.After == 0          0.8144   I don't care about this
#No.Before - No.After == 0           0.1941  
#No.Before - Yes.Before == 0         0.8616
</code></pre>

<p>The main problem I see with these post-hoc analyses are some comparisons that aren't useful for my hypotheses.</p>

<p>Any suggestions for an appropriate post-hoc analysis are greatly appreciated, thanks.</p>

<p><strong>Edit:</strong> <a href=""http://stats.stackexchange.com/questions/5250/multiple-comparisons-on-a-mixed-effects-model"">Relevant question and answer that points toward testing manual contrast matrices</a></p>
"
"0.0964766140389895","0.0892172298385518"," 28601","<p>I am working on my master thesis at the moment and planned on running the statistics with SigmaPlot. However, after spending some time with my data I came to the conclusion that SigmaPlot might not be fit for my problem (I may be mistaken) so I started my first attempts in R, which did not exactly make it easier.</p>

<p>The plan was to run a simple TWO-WAY-ANOVA on my data which results from 3 different proteins and 8 different treatments on those, so my two factors are proteins and treatments. I tested for normality using both</p>

<pre><code>&gt; shapiro.test(time)
</code></pre>

<p>and</p>

<pre><code>&gt; ks.test(time, ""norm"", mean=mean(time), sd=sqrt(var(time)))
</code></pre>

<p>In both cases (maybe not surprising) I ended up with a non-normal distribution.</p>

<p>Which left me with the first questions of which test to use for equality of variances. I came up with</p>

<pre><code>&gt; chisq.test(time)
</code></pre>

<p>and the result was, that I don't have equality of variance in my data either.</p>

<p>I tried different data transformations (log, center, standardization), all of which did not solve my problems with the variances.</p>

<p>Now I am at a loss, how to conduct the ANOVA for testing which proteins and which treatments differ significantly from each other. I found something about a Kruskal-Walis-Test, but only for one factor (?). I also found things about ranking or randamization, but not yet how to implement those techniques in R.</p>

<p>Does anyone have a suggestion what I should do?</p>

<p>Edit: thank you for your answers, I am a little overwhelmed by the reading (it just seems getting more and more instead of less), but I will of course keep going.</p>

<p>Here an example of my data, as suggested (I am very sorry for the format, I couldn't figure out another solution or place to put a file. I am still new to this all.):  </p>

<pre><code>protein treatment   time  
A   con 2329.0  
A   HY  1072.0  
A   CL1 4435.0  
A   CL2 2971.0  
A   CL1-HY sim  823.5  
A   CL2-HY sim  491.5  
A   CL1+HY mix  2510.5  
A   CL2+HY mix  2484.5  
A   con 2454.0  
A   HY  1180.5  
A   CL1 3249.7  
A   CL2 2106.7  
A   CL1-HY sim  993.0  
A   CL2-HY sim  817.5  
A   CL1+HY mix  1981.0  
A   CL2+HY mix  2687.5  
B   con 1482.0  
B   HY  2084.7  
B   CL1 1498.0  
B   CL2 1258.5  
B   CL1-HY sim  1795.7  
B   CL2-HY sim  1804.5  
B   CL1+HY mix  1633.0  
B   CL2+HY mix  1416.3  
B   con 1339.0  
B   HY  2119.0  
B   CL1 1093.3  
B   CL2 1026.5  
B   CL1-HY sim  2315.5  
B   CL2-HY sim  2048.5  
B   CL1+HY mix  1465.0  
B   CL2+HY mix  2334.5  
C   con 1614.8  
C   HY  1525.5  
C   CL1 426.3  
C   CL2 1192.0  
C   CL1-HY sim  1546.0  
C   CL2-HY sim  874.5  
C   CL1+HY mix  1386.0  
C   CL2+HY mix  364.5  
C   con 1907.5  
C   HY  1152.5  
C   CL1 639.7  
C   CL2 1306.5  
C   CL1-HY sim  1515.0  
C   CL2-HY sim  1251.0  
C   CL1+HY mix  1350.5  
C   CL2+HY mix  1230.5
</code></pre>
"
"0.170928433982631","0.151744244666721"," 28649","<p>What (with justification) is a valid post-hoc for a two-way ANOVA main effect, if no interaction is present?</p>

<p>Example two-way fixed effect ANOVA:</p>

<pre><code>&gt; aov.example &lt;- aov(Response ~ IV1 * IV2, data=data)
&gt; summary(aov.example)

              Df Sum Sq Mean Sq F value   Pr(&gt;F)   
IV1            1  13.10  13.099  0.7222 0.40547  
IV2            4 315.56  78.891  4.3498 0.01081 *
IV1:IV2        4 141.00  35.251  1.9436 0.14240  
Residuals     20 362.74  18.137          
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I see a few general options, but I'm unsure which is most appropriate:</p>

<ol>
<li><p><strong>Post-hoc with a multiple comparison test using two-way model</strong></p>

<pre><code>&gt; TukeyHSD(aov.example, which=""IV2"")

Tukey multiple comparisons of means
   95% family-wise confidence level

Fit: aov(formula = Response ~ IV1 * IV2, data = data)

$IV2
               diff        lwr        upr     p adj
B-A      -2.1485711  -9.506162  5.2090200 0.9031415
C-A      -2.3382727  -9.695864  5.0193184 0.8733517
D-A       1.4732257  -5.884365  8.8308168 0.9735981
E-A      -8.0515205 -15.409112 -0.6939294 0.0277241
C-B      -0.1897016  -7.547293  7.1678895 0.9999912
D-B       3.6217968  -3.735794 10.9793879 0.5905067
E-B      -5.9029494 -13.260541  1.4546416 0.1559460
D-C       3.8114984  -3.546093 11.1690895 0.5439632
E-C      -5.7132478 -13.070839  1.6443432 0.1785266
E-D      -9.5247462 -16.882337 -2.1671552 0.0074740
</code></pre></li>
<li><p><strong>Post-hoc with a multiple comparison test using one-way model</strong></p>

<pre><code>&gt; TukeyHSD(aov(Response ~ IV2, data=data))

Tukey multiple comparisons of means
   95% family-wise confidence level

Fit: aov(formula = Response ~ IV2, data = data)

$IV2
               diff        lwr        upr     p adj
B-A      -2.1485711  -9.858179  5.5610365 0.9224401
C-A      -2.3382727 -10.047880  5.3713349 0.8976378
D-A       1.4732257  -6.236382  9.1828333 0.9794638
E-A      -8.0515205 -15.761128 -0.3419130 0.0375667
C-B      -0.1897016  -7.899309  7.5199060 0.9999933
D-B       3.6217968  -4.087811 11.3314044 0.6456949
E-B      -5.9029494 -13.612557  1.8066581 0.1951992
D-C       3.8114984  -3.898109 11.5211060 0.6014671
E-C      -5.7132478 -13.422855  1.9963597 0.2212053
E-D      -9.5247462 -17.234354 -1.8151387 0.0102178
</code></pre>

<p>Overall, in this example the significant comparisons do not change, but the adjusted p-values are lower using the two-way ANOVA model. Which is most appropriate for a two-way main effect post-hoc?</p></li>
<li><p><strong>Post-hoc with subset levels of other independent variable</strong> </p></li>
</ol>

<p>I'm pretty sure this is not valid, given the lack of any effect across the other independent variable. However, when one level of IV1 has larger effects between levels in IV2 compared to other IV1 levels, wouldn't this strongly skew the overall analysis? In other words, even though overall there is a main effect, could this effect not be significant in the the other level of IV1?</p>

<pre><code>    &gt; aov.level1 &lt;- aov(Response ~ IV2, data=data[1:15,])
    &gt; summary(aov.level1)

                Df Sum Sq Mean Sq F value  Pr(&gt;F)  
    IV2          4 334.55  83.639  3.8413 0.03837 *
    Residuals   10 217.74  21.774                  

    &gt; aov.level2 &lt;- aov(Response ~ IV2, data=data[16:30,])
    &gt; summary(aov.level2)

                Df Sum Sq Mean Sq F value Pr(&gt;F)  
    IV2          4 122.01  30.503  2.1037 0.1551
    Residuals   10 145.00  14.500 
</code></pre>

<p>Level 1 of IV1 has a significant IV2 main effect, but Level 2 does not. There are also differences in significant multiple comparisons. My concern is that I'm committing a Type I error using the previous two methods.</p>

<pre><code>    &gt; TukeyHSD(aov.level1)

    Tukey multiple comparisons of means
        95% family-wise confidence level

    Fit: aov(formula = Response ~ IV2, data = data[1:15, ])

    $IV2
                     diff        lwr         upr     p adj
    B-A       -7.86237771 -20.401217  4.67646157 0.3054028
    C-A       -7.90634218 -20.445181  4.63249709 0.3008150
    D-A       -0.94962690 -13.488466 11.58921237 0.9989922
    E-A      -12.55848654 -25.097326 -0.01964727 0.0496014
    C-B       -0.04396448 -12.582804 12.49487479 1.0000000
    D-B        6.91275080  -5.626088 19.45159008 0.4167559
    E-B       -4.69610883 -17.234948  7.84273044 0.7342625
    D-C        6.95671528  -5.582124 19.49555455 0.4111062
    E-C       -4.65214436 -17.190984  7.88669492 0.7404793
    E-D      -11.60885964 -24.147699  0.92997964 0.0729541

    &gt; TukeyHSD(aov.level2)

    Tukey multiple comparisons of means
        95% family-wise confidence level

    Fit: aov(formula = Response ~ IV2, data = data[16:30, ])

    $IV2
                   diff        lwr       upr     p adj
    B-A       3.5652355  -6.667185 13.797656 0.7795224
    C-A       3.2297968  -7.002624 13.462218 0.8321507
    D-A       3.8960783  -6.336343 14.128499 0.7231230
    E-A      -3.5445545 -13.776975  6.687866 0.7829174
    C-B      -0.3354387 -10.567860  9.896982 0.9999634
    D-B       0.3308428  -9.901578 10.563264 0.9999654
    E-B      -7.1097900 -17.342211  3.122631 0.2257310
    D-C       0.6662815  -9.566139 10.898702 0.9994435
    E-C      -6.7743513 -17.006772  3.458070 0.2618999
    E-D      -7.4406328 -17.673054  2.791788 0.1942037
</code></pre>

<p><a href=""http://stats.stackexchange.com/questions/16760/why-does-post-hoc-test-on-pair-of-group-means-become-significant-when-looking-at"">This post has some relevant information.</a></p>
"
"0.135670238492329","0.124266855846554"," 28876","<p>I've been running some power simulations for a one-way ANOVA in R, and my problem is that the results from the simulation doesn't match the result from g*power or <em>pwr.anova.test</em> from the ""pwr""-package. As an example, let's compare simulated power to analytical power using these values:</p>

<pre><code>group_size &lt;- c(40,40,40)
means &lt;- c(0.2,0,-0.2)
sds &lt;- c(1,1,1)
</code></pre>

<p>Analytical power analysis, f = 0.1632993 is calculated from the means and standard deviations above. </p>

<pre><code>size &lt;- 10
plot_df &lt;- data.frame()
power &lt;- 0
while(power &lt; 0.80) {
  power &lt;- pwr.anova.test(k=3, n=size, f= 0.1632993, sig.level=0.05)$power
  plot_df &lt;- rbind(plot_df, data.frame(""n"" = size, ""power"" = power))
  size &lt;- size + 2
  print(power)
}
</code></pre>

<p>Simulated power analysis</p>

<pre><code>set.seed(1001)
run_sim &lt;- function() {
# generate all data
create_sim_data &lt;- function(i) {
  #stdev bias correction
  c4 &lt;- (sqrt(2/(group_size[1] - 1))) * (gamma(group_size[1]/2)/gamma((group_size[1] - 1)/2))
  sds2 &lt;- sds / c4
  # pre-allocate matrix
  test_matrix &lt;- matrix(nrow=sims, ncol=sum(group_size))
  # nested loops to create simulated data for all runs
   for(j in 1:sims) {
     for(i in 1:length(group_size)) {
       # col_start &amp; cold_end is used to have the different groups on the same row
       col_start &lt;- sum(group_size[1:i])-(group_size[i]-1)
       col_end &lt;- cumsum(group_size)[i]
       # generate data with rnorm
       test_matrix[j,col_start:col_end] &lt;- rnorm(group_size[i], mean = means[i], sd = sds2[i])
     }
   }
    return(test_matrix)
}
# extract results from simulations
get_power &lt;- function() {
  sig &lt;- rep(NA, sims)
  eta_2 &lt;- rep(NA, sims)
  omega_2 &lt;- rep(NA, sims)
  for(i in 1:sims) {
    # perform ANOVA on data
    result &lt;- summary(aov(test_matrix[i,] ~ group))
    # calculate effect size
    eta_2[i] &lt;- result[[1]]$'Sum Sq'[1] / sum(result[[1]]$'Sum Sq')
    omega_2[i] &lt;- (result[[1]]$'Sum Sq'[1] - (result[[1]]$Df[1] * result[[1]]$'Mean Sq'[2])) / (sum(result[[1]]$'Sum Sq') + result[[1]]$'Mean Sq'[2])
        # get p-value from ANOVA
        sig_result &lt;- result[[1]]$'Pr(&gt;F)'[1]
    # check sig.level
    sig[i] &lt;- sig_result &lt; 0.05
  }
  out &lt;- list(""power"" = mean(sig), ""eta_2"" = mean(eta_2), ""omega"" = mean(omega_2))
}

power &lt;- 0
plot_df &lt;- data.frame()
eta &lt;- NULL
omega &lt;- NULL
# repeat the simulation until the desired power is found
while(power &lt; 0.8) {
  # regenerate grouping as group_size increases 
  group &lt;- c(rep(1, group_size[1]), rep(2,group_size[2]), rep(3,group_size[3]))
  # create data matrix
  test_matrix &lt;- create_sim_data()
  # get anova power
  result &lt;- get_power()
  # extract power value
  power &lt;- result$power
      # save eta-squared each iteration
      eta &lt;- rbind(eta, result$eta_2)
  # save eta-squared each iteration
  omega &lt;- rbind(omega, result$omega)
  cat(""power ="", power, ""group size ="", group_size,""\n\n"")
  # save group size and power for each iteration
  plot_df &lt;- rbind(plot_df, data.frame(""group_n"" = group_size[1], ""power"" = power))
  # increase group size with 2
  group_size &lt;- group_size + 2
}

out &lt;- list(""power"" = plot_df, ""f"" = sqrt(eta / (1 - eta)), ""omega"" = omega)
return(out)
}
sims &lt;- 1000
sim &lt;- run_sim()
</code></pre>

<p>This will generate a difference of about 10 % between the two methods (the short line is from simulation) </p>

<p><img src=""http://i.stack.imgur.com/REjgP.png"" alt=""plot""></p>

<p>My thoughts are that the difference is due to Cohen's <em>f</em> being an biased estimator of the population effect size. But how should I interpret my results, are my simulations overestimating the power? If so, how can I get it to match the output from the analytical power estimation. </p>

<p><strong>To summarize my question: why doesn't the two methods give the same output when fed with the same means and standard deviations?</strong></p>

<p>I'd be glad for any pointers were I wen't wrong. Thanks in advance!</p>
"
"0.149168726281763","0.148377302419909"," 29280","<p>I can't get the same results in R as in GraphPad Prism for repeated measures anova.</p>

<p>The experiment was a stimulation time course, so I have as DV=response and as factor ""time"" within groups, also I add a factor sample for each experiment</p>

<pre><code>data &lt;- read.csv(""http://dl.dropbox.com/u/4828275/datos.csv"")
options(contrasts=c(""contr.sum"",""contr.poly""))

## Convert variables to factor
data &lt;- within(data, {
sample &lt;- factor(sample)
time &lt;- factor(time)
})
aov &lt;- aov(response~time+sample, data=data)
summary(glht(aov, linfct=mcp(time=""Dunnett"")))

     Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Dunnett Contrasts


Fit: lme.formula(fixed = response ~ time, data = data, random = ~1 | 
sample)

Linear Hypotheses:
            Estimate Std. Error z value Pr(&gt;|z|)
2 - 0 == 0    1.1789     2.0800   0.567        1
5 - 0 == 0    1.2966     2.0800   0.623        1
10 - 0 == 0   1.0555     2.0800   0.507        1
15 - 0 == 0   0.4317     2.0800   0.208        1
30 - 0 == 0   0.2148     2.0800   0.103        1
(Adjusted p values reported -- bonferroni method)
</code></pre>

<p>For repeated measures I have this code</p>

<pre><code>aov.repeated &lt;- ezANOVA(
  data
  , dv = .(response)
  , wid = .(time)
  , within = .(sample)
  , type = 1
  , return_aov = TRUE
)$aov
</code></pre>

<p>The GraphPad Prism results for the same data was</p>

<pre><code>Table Analyzed  Data 1              

Repeated Measures ANOVA                 
  P value   0.0415              
  P value summary   *               
  Are means signif. different? (P &lt; 0.05)   Yes             
  Number of groups  6               
  F 2.863               
  R square  0.4172              

 Was the pairing significantly effective?                   
  R square  0.1980              
  F 2.119               
  P value   0.1162              
  P value summary   ns              
  Is there significant matching? (P &lt; 0.05) No              

ANOVA Table SS  df  MS      
  Treatment (between columns)   130.6   5   26.12       
  Individual (between rows) 77.30   4   19.32       
  Residual (random) 182.4   20  9.121       
  Total 390.3   29          

Dunnett's Multiple Comparison Test  Mean Diff.  q   Significant? P &lt; 0.05?  Summary 95% CI of diff
  0 vs 2    -2.861  1.498   No  ns  -8.085 to 2.362
  0 vs 5    -5.777  3.024   Yes *   -11.00 to -0.5531
  0 vs 10   -6.009  3.146   Yes *   -11.23 to -0.7855
  0 vs 15   -4.621  2.419   No  ns  -9.844 to 0.6029
  0 vs 30   -2.581  1.351   No  ns  -7.805 to 2.642
</code></pre>

<p>How can I get the same results as above in R?
Is there a way to get Dunnett's Multiple Comparison Test in aov.repeated? </p>
"
"0.0255822255048325","0.0540738070435875"," 29329","<p>I was wondering, what is the meaning of operators in anova or regression formulas in R</p>

<p>For example</p>

<ul>
<li>""<strong>+</strong>"" aov &lt;- aov(x~time+sample, data=data) -> repeated mesures anova?</li>
<li>""<strong>*</strong>"" aov &lt;- aov(x~time*sample, data=data) -> two way anova?</li>
<li>""<strong>/</strong>"" aov &lt;- aov(x~time/sample, data=data) -> ?</li>
<li>""<strong>:</strong>"" aov &lt;- aov(x~time:sample, data=data) -> ?</li>
</ul>

<p>And also are there more operators for this kind of formulas?</p>
"
"0.0511644510096651","0.0540738070435875"," 29479","<p>How do I get the 95% confidence interval for ANOVA level coefficients? For comparison with a constant value, not multiple comparison (like MMC).</p>

<p>I tried to take the coefficient's SE from the model:</p>

<pre><code>&gt; m1 = lm(formula = TrendAdd ~ 0 + Migrace)
&gt; c = coef(summary(m1))
&gt; c
              Estimate  Std. Error     t value  Pr(&gt;|t|)
MigraceB -0.0084214286 0.006555969 -1.28454367 0.2019195
MigraceD  0.0032250000 0.007510806  0.42938134 0.6685694
MigraceR  0.0006068966 0.007889737  0.07692228 0.9388391
&gt; c_low = c[,1] - 1.96*c[,2]
&gt; c_high = c[,1] + 1.96*c[,2]
&gt; c_low
   MigraceB    MigraceD    MigraceR 
-0.02127113 -0.01149618 -0.01485699 
&gt; c_high
   MigraceB    MigraceD    MigraceR 
0.004428271 0.017946180 0.016070782 
</code></pre>

<p>But I'm not sure this is correct! It yields different result than when I run t-test on each level of <code>Migrace</code> (t.test intervals are larger):</p>

<pre><code>&gt; t.test(TrendAdd[Migrace == ""B""])

    One Sample t-test

data:  TrendAdd[Migrace == ""B""] 
t = -1.206, df = 41, p-value = 0.2347
alternative hypothesis: true mean is not equal to 0 
95 percent confidence interval:
 -0.022523452  0.005680595 
sample estimates:
   mean of x 
-0.008421429 
</code></pre>

<p>Why is there a difference and how to do it correctly?</p>
"
"0.108536190793863","0.114707866935281"," 29981","<p>Let's have some linear model, for example just simple ANOVA:</p>

<pre><code># data generation
set.seed(1.234)                      
Ng &lt;- c(41, 37, 42)                    
data &lt;- rnorm(sum(Ng), mean = rep(c(-1, 0, 1), Ng), sd = 1)      
fact &lt;- as.factor(rep(LETTERS[1:3], Ng)) 

m1 = lm(data ~ 0 + fact)
summary(m1)
</code></pre>

<p>Result is as follows:</p>

<pre><code>Call:
lm(formula = data ~ 0 + fact)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.30047 -0.60414 -0.04078  0.54316  2.25323 

Coefficients:
      Estimate Std. Error t value Pr(&gt;|t|)    
factA  -0.9142     0.1388  -6.588 1.34e-09 ***
factB   0.1484     0.1461   1.016    0.312    
factC   1.0990     0.1371   8.015 9.25e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8886 on 117 degrees of freedom
Multiple R-squared: 0.4816,     Adjusted R-squared: 0.4683 
F-statistic: 36.23 on 3 and 117 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Now I try two different methods to estimate confidence interval of these parameters</p>

<pre><code>c = coef(summary(m1))

# 1st method: CI limits from SE, assuming normal distribution
cbind(low = c[,1] - qnorm(p = 0.975) * c[,2], 
    high = c[,1] + qnorm(p = 0.975) * c[,2])

# 2nd method
confint(m1)
</code></pre>

<h2>Questions:</h2>

<ol>
<li>What is the distribution of estimated linear regression coefficients? Normal or $t$?</li>
<li>Why do both methods yield different results? Assuming normal distribution and correct SE, I'd expect both methods to have the same result.</li>
</ol>

<p>Thank you very much!</p>

<p>data ~ 0 + fact</p>

<p><strong>EDIT after an answer</strong>:</p>

<p>The answer is exact, this will give exactly the same result as <code>confint(m1)</code>!</p>

<pre><code># 3rd method
cbind(low = c[,1] - qt(p = 0.975, df = sum(Ng) - 3) * c[,2], 
    high = c[,1] + qt(p = 0.975, df = sum(Ng) - 3) * c[,2])
</code></pre>
"
"0.0542680953969316","0.0764719112901873"," 31218","<p>I would like to compare multiple means using a method such as an ANOVA. This would have to be followed up with either Scheffe's test or the Games-Howell post-hoc method (my data is non-normal - to different degrees and can't all be transformed to normality using the same transformation - heteroscedastic and has greatly differing sample sizes).</p>

<p>In their Opthalmic and Physiological Optics paper, Armstrong et al. (2000) state that these tests can only be performed after an ANOVA returns a significant result. My question is this - how can an ANOVA be appropriately applied to data such as mine, which require these post-hoc tests simply because they violate so many assumptions of the ANOVA? It seems odd to me that they should advocate the use of an ANOVA before post-hoc tests that are used in place of others because of the violations of the ANOVA assumptions.</p>

<p>Also - on a side note, does anyone have any advice as to whether to use the above approach or to use the multcomp package available in R?</p>

<p>Many thanks!</p>
"
"0.108536190793863","0.101962548386916"," 32072","<p>Just when I thought I'd had a grip on how to do an analysis of variance this particular data set had me startled: it's a collection of response times (in ms) to a linguistic input. To be precise, it's part of a reading time experiment, and I'm trying to see if there's a significant effect of two factors.</p>

<p>My experiment had a 2x2 factorial design, with 2 factors binary <code>Conflicting</code> and <code>ContextPresent</code>. The only value I'm interested in for the purpose of this question is one particular random variable, a response time. No transformations have been done on it, except removal of outliers via 3-sigma rule (I used mean + 3 * standard deviation to determine outliers.)</p>

<p>So I run my anova in R:</p>

<pre><code>&gt; anova(lm(TextDisplay9.RT ~ Conflicting * ContextPresent, data=items.cropped))
Analysis of Variance Table

Response: TextDisplay9.RT
                            Df   Sum Sq Mean Sq F value  Pr(&gt;F)
Conflicting                  1   111185  111185  7.0591 0.00808 **
ContextPresent               1    73591   73591  4.6723 0.03102 *
Conflicting:ContextPresent   1      352     352  0.0223 0.88128
Residuals                  651 10253667   15751
</code></pre>

<p>Jolly ho, I get pretty good results for a main effect on both my factors, and no interaction. That's fine. But here's the corresponding box-and-whiskers graph:</p>

<pre><code>&gt; ggplot(items.cropped,aes(Conflicting,TextDisplay9.RT)) + geom_boxplot() +
  facet_grid(.~ ContextPresent)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R9d2V.png"" alt=""Boxplot of suspiciously equal looking stuff""></p>

<p>So this plot actually makes it seem like there <em>shouldn't</em> be a main effect of either variable! They're all too similar! Yes, the scale is rather squished, because of the outliers, but the means are really really close!</p>

<pre><code>&gt; with(items.cropped,mean(items.cropped[Conflicting==""semantic conflict"" &amp; ContextPresent == ""mentioned in context"",]$TextDisplay9.RT,na.rm=T))
    [1] 431.8659
    &gt; with(items.cropped,mean(items.cropped[Conflicting==""semantic conflict"" &amp; ContextPresent == ""not mentioned in context"",]$TextDisplay9.RT,na.rm=T))
[1] 454.5305
&gt; with(items.cropped,mean(items.cropped[Conflicting==""no semantic conflict"" &amp; ContextPresent == ""mentioned in context"",]$TextDisplay9.RT,na.rm=T))
    [1] 407.485
    &gt; with(items.cropped,mean(items.cropped[Conflicting==""no semantic conflict"" &amp; ContextPresent == ""not mentioned in context"",]$TextDisplay9.RT,na.rm=T))
[1] 427.2188
</code></pre>

<p>Does it sound possible that there could be a main effect? Or did I somehow misuse ANOVAs? I could provide the data if needed!</p>

<p>Thanks very much for any suggestions.</p>
"
"0.0808981002113217","0.0683985568056769"," 32087","<p>My study design involves a control and 2 test groups plus some covariates. Each group consists of around 20 observations. In total I look at around 1,000 variables.</p>

<p>I created a linear model using the <code>lm()</code> function in R including 2 covariates. After that I thought I would include another covariate because doing a PCA plot earlier showed a slight effect on that covariate. However, after adding this covariate to the model 50% of the significant hits are now different. I was actually assuming that it would pretty much identical as the effect was hardly seen in the PCA.</p>

<p>Could it be that I have overfitted the model? Or is the effect simple just not shown in the PCA plot but is there?</p>

<p>I just compared the two models using <code>anova(lm1, lm2)</code> and the p-value is significant which I think means that the third covariate adds significant information to the model?</p>

<pre><code>lm1 &lt;- lm(var ~ factor_of_interest + cov1 + cov2)
lm2 &lt;- lm(var ~ factor_of_interest + cov1 + cov2 + cov3)
anova(lm1, lm2)
</code></pre>
"
"0.0808981002113217","0.0683985568056769"," 32094","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/32087/linear-model-overfitting-due-to-too-many-covariates"">Linear model overfitting  due to too many covariates</a>  </p>
</blockquote>



<p>My study design involves a control and 2 test groups plus some covariates.
Each group consists of around 20 observations. In total I look at around a 1000 variables.</p>

<p>I created a linear model using the <code>lm</code> function in R including 2 covariates. After that I thought I include another covariate because doing a PCA plot earlier showed a slight effect on that covariate. However, after adding this covariate to the model 50% of the significant hits are now different. I was actually assuming that it would pretty much identical as the effect was hardly seen in the PCA. </p>

<p>Could it be that I have overfitted the model? Or is the effect simple just not shown in the PCA plot but is there? </p>

<p>I just compared the two models using <code>anova(lm1, lm2)</code> and the p-value is significant which I think means that the third covariate adds significant information to the model?</p>

<pre><code>lm1 &lt;- lm(var ~ factor_of_interest + cov1 + cov2)
lm2 &lt;- lm(var ~ factor_of_interest + cov1 + cov2 + cov3)

anova(lm1, lm2)
</code></pre>
"
"0.0626633989716535","0.0662266178532522"," 32099","<p>I need to compare results from data with different residual DF as my $x$ variable has different levels. The following is just an example (in R, for demonstration purpose, but this is not a R question):</p>

<pre><code># first case 
set.seed (123)
data1 &lt;- data.frame (y = rnorm (100, 5, 2), 
 x = sample (c(""A"", ""B""), 100, replace = T))
anova(lm(y~ x, data = data1))
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value Pr(&gt;F)
x          1   2.07  2.0669  0.6177 0.4338
Residuals 98 327.89  3.3459               

# second case: 
 set.seed (123)
data2 &lt;- data.frame (y = rnorm (100, 5, 2), 
 x = sample (c(""A"", ""B"", ""C"", ""D"", ""E""), 100, replace = T))
anova(lm(y~ x, data = data2))
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value Pr(&gt;F)
x          4   4.89  1.2224  0.3572 0.8384
Residuals 95 325.07  3.4218       
</code></pre>

<p>Here I have two different DF for the residuals (95 vs. 96) and $x$ (1 vs. 4): Is it valid to compare p-values as such? I know that the F-test considers $x$ and residual while calculating p-value. Is there any extra-caution needed? </p>
"
"0.0723574605292422","0.0382359556450936"," 32188","<p>I am doing a one way ANOVA (per species) with custom contrasts.</p>

<pre><code>     [,1] [,2] [,3] [,4]
0.5    -1    0    0    0
5       1   -1    0    0
12.5    0    1   -1    0
25      0    0    1   -1
50      0    0    0    1
</code></pre>

<p>where I compare intensity 0.5 against 5, 5 against 12.5 and so on. These is the data I'm working on</p>

<p><img src=""http://i.stack.imgur.com/L7uVk.png"" alt=""enter image description here""></p>

<p>with the following results</p>

<pre><code>Generalized least squares fit by REML
  Model: dark ~ intensity 
  Data: skofijski.diurnal[skofijski.diurnal$species == ""niphargus"", ] 
       AIC      BIC    logLik
  63.41333 67.66163 -25.70667

Coefficients:
            Value Std.Error  t-value p-value
(Intercept) 16.95 0.2140872 79.17334  0.0000
intensity1   2.20 0.4281744  5.13809  0.0001
intensity2   1.40 0.5244044  2.66970  0.0175
intensity3   2.10 0.5244044  4.00454  0.0011
intensity4   1.80 0.4281744  4.20389  0.0008

 Correlation: 
           (Intr) intns1 intns2 intns3
intensity1 0.000                      
intensity2 0.000  0.612               
intensity3 0.000  0.408  0.667        
intensity4 0.000  0.250  0.408  0.612 

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-2.3500484 -0.7833495  0.2611165  0.7833495  1.3055824 

Residual standard error: 0.9574271 
Degrees of freedom: 20 total; 15 residual
</code></pre>

<p>16.95 is the global mean for ""niphargus"". In intensity1, I'm comparing means for intensity 0.5 against 5.</p>

<p>If I understood this right, the coefficient for intensity1 of 2.2 should be half the difference between means of intensity levels 0.5 and 5. However, my hand calculations don't match those of the summary. Can anyone chip in what am I doing wrong?</p>

<pre><code>ce1 &lt;- skofijski.diurnal$intensity
    levels(ce1) &lt;- c(""0.5"", ""5"", ""0"", ""0"", ""0"")
    ce1 &lt;- as.factor(as.character(ce1))
    tapply(skofijski.diurnal$dark, ce1, mean)
       0    0.5      5 
  14.500 11.875 13.000 
diff(tapply(skofijski.diurnal$dark, ce1, mean))/2
      0.5       5 
  -1.3125  0.5625 
</code></pre>
"
"NaN","NaN"," 34069","<p>I need to do some simple mean comparisons between groups (basic ANOVA F-tests) on data with missing values. I use the <a href=""http://cran.r-project.org/web/packages/mice/index.html"">mice</a> package in R for multiple imputation, but I can only pool results for the linear model coefficients, or the $R^2$.</p>

<p>Does anyone know how to combine to pool multiple F-statistics from each linear model fit? Or, how can I compute the standard errors for the F-test?</p>
"
"0.0886194286901087","0.0780488176318078"," 34180","<p>This is probably a very simple question for anyone using R, but I am fairly new to it (and stats in general) and I can't figure this simple computation out.</p>

<p>I have 3 varieties of plants (A, B, C) tested with 2 different nutrient supplies, and the weights of the plants are measured. The data is balanced. (Here is an example of dataset: <a href=""http://dl.dropbox.com/u/1755762/example.csv"" rel=""nofollow"">http://dl.dropbox.com/u/1755762/example.csv</a>)</p>

<p>I can compare the means between genotypes and treatments, with for example <code>bargraph.CI</code> from the <code>sciplot</code> package, which I like because of the automatic SE bars:</p>

<pre><code>bargraph.CI(Genotype, Weight, Treatment, legend=T, ylab=""Dry weight (g)"", xlab=""Genotype"")
</code></pre>

<p>which would produce something like that (just to have an idea at what I am looking at):</p>

<p><img src=""http://i.stack.imgur.com/IUb2o.png"" alt=""enter image description here""></p>

<p>I would then complement this with the classes from a Tukey HSD post-hoc test with <code>HSD.test</code> from the <code>agricolae</code> package (if Anova showed a GenotypeÃ—Treatment interaction):</p>

<pre><code>int &lt;- interaction(Genotype, Treatment)
HSD.test(aov(Weight~int), ""int"", group=T)
</code></pre>

<p>But <strong>how can I compare the conservation rates between the two treatments for each genotypes?</strong> By this I mean: compare, for each genotype, the ratio ""Treatment 1 weight mean over Treatment 2 weight mean"" (this would be just one bar per genotype, showing the weight conservation rate between the two treatments).</p>

<p>It would be great to get a graph similar to what <code>bargraph.CI</code> produces, but I don't know how to determine if the differences between those weight conservation rates are significant: would standard error bars be relevant here?</p>
"
"0.131126630557311","0.146734796413356"," 34496","<p>I have a set of experiments that I'd like to run some significance tests over.  It looks like:</p>

<pre><code>Word Feature Model Test Score Correct Total
allow.v woc hac zellig 0.382353 26 68
allow.v woc hac bellig 1.000000 0 0
allow.v woc kmeans zellig 0.382353 26 68
allow.v woc kmeans bellig 1.000000 0 0
run.v woc eigen zellig 0.308824 21 68
run.v woc eigen bellig 1.000000 0 0
run.v woc agglo zellig 0.323529 22 68
</code></pre>

<p>This combines two different types of tests, which i'm calling zellig and bellig for a lack of better names right now.  Using this data, I considered treating each score as a response variable that is dependent on the Feature and Model, with each as an observation.  With this format, it's pretty easy to do standard evaluations like:</p>

<pre><code>zelligData &lt;- subset(senseData, Test == ""zellig"")
anova(lm(Score ~ Feature * Model, zelligData))
pairwise.t.test(zelligData$Score, zelligData$Feature, p.adj=""none"")
pairwise.t.test(zelligData$Score, zelligData$Model, p.adj=""none"")
</code></pre>

<p>That's well and good, however, Score is actually an aggregate value of many observed events, it's the average number of times each combination resulted a correct evaluation.  The Correct and Total fields count the two numbers used to generate Score.  And now, I'd like to do significance tests for each word with each event counted by Total treated as an observation.  Concretely, I'd like to use the above commands on data that looks like:</p>

<pre><code>Word Feature Model Test Score
allow.v woc hac zellig 1
allow.v woc hac zellig 1
allow.v woc hac zellig 1
allow.v woc hac zellig 1
....
allow.v woc hac zellig 0
allow.v woc hac zellig 0
allow.v woc hac zellig 0
allow.v.pos hac zellig 1
allow.v.pos hac zellig 1
allow.v.pos hac zellig 1
...
allow.v.pos hac zellig 0
allow.v.pos hac zellig 0
allow.v.pos hac zellig 0
...
</code></pre>

<p>However, I'd like to avoid creating this secondary view of the data, as some words have a very large number of events and it seems wasteful to turn the aggregate values into the expanded form.</p>

<p>Is it at all possible to perform anova and paired t-test using my already aggregated counts of events?  Would this evaluation be different from just using the aggregated scores as they are?  I imagine they would as the aggregated Score value doesn't indicate how many times events there really were in total.</p>

<p>Thanks in advance!</p>

<p>Edit:  After reading <a href=""http://www.ablongman.com/graziano6e/text_site/MATERIAL/Stats/manfactr.htm"" rel=""nofollow"">how to do ANOVA manually</a>, I should rephrase this question.  Is there anyway to pass in the matrix of summary statistics between each factor, as described in Section 2 (Summary Statistics) of the instructions for doing it manually, to a function in R and have it compute the ANOVA result?</p>
"
"0.0626633989716535","0.0662266178532522"," 35778","<p>I have a 2 level repeated measures DV of accuracy, and a covariate of response bias. As response bias increases, accuracy level 1 increases while level 2 decreases.  I want to see if there is a difference in the means of the groups after controlling for response bias. </p>

<p>I can't do it with an ANCOVA.  Can I just manually calculate expected values based on a regression equation, and run an ANOVA?  </p>

<p>I'd like to do it in R or SPSS, so specifics for either would be welcome, but not necessary.</p>
"
"0.0626633989716535","0.0662266178532522"," 37466","<p>I am taking a graduate course in Applied Statistics that uses the following textbook (to give you a feel for the level of the material being covered): <a href=""http://amzn.com/0471072044"">Statistical Concepts and Methods</a>, by G. K. Bhattacharyya and R. A. Johnson.</p>

<p>The Professor requires us to use SAS for the homeworks. </p>

<p>My question is that: is there a Java library(ies), that can be used instead of SAS for problems typically seen in such classes.</p>

<p>I am currently trying to make do with <a href=""http://commons.apache.org/math/"">Apache Math Commons</a> and though I am impressed with the library (it's ease of use and understandability) it seems to lack even simple things such as the ability to draw histograms (thinking of combining it with a charting library).</p>

<p>I have looked at Colt, but my initial interest died down pretty quickly. </p>

<p>Would appreciate any input -- and I've looked at similar questions on Stackoverflow but have not found anything compelling.</p>

<p>NOTE: I am aware of R, SciPy and Octave and java libraries that make calls to them -- I am looking for a Java native library or set of libraries that can together provide the features I'm looking for.</p>

<p>NOTE: The topics covered in such a class typically include: one-samle and two-sample tests and confidence intervals for means and medians, descriptive statistics, goodness-of-fit tests, one- and two-way ANOVA, simultaneous inference, testing variances, regression analysis, and categorical data analysis.</p>
"
"0.130444267050272","0.127256952595156"," 40385","<p>I would like to test in what regression fits my data best. My dependent variable is a count, and has a lot of zeros. </p>

<p>And I would need some help to determine what model and family to use (poisson or quasipoisson, or zero-inflated poisson regression), and how to test the assumptions.</p>

<ol>
<li>Poisson Regression: as far as I understand, the strong assumption is that dependent variable mean = variance. How do you test this? How close together do they have to be? Are unconditional or conditional mean and variance used for this? What do I do if this assumption does not hold? </li>
<li>I read that if variance is greater than mean we have overdispersion, and a potential way to deal with this is including more independent variables, or family=quasipoisson. Does this distribution have any other requirements or assumptions? What test do I use to see whether (1) or (2) fits better - simply <code>anova(m1,m2)</code>?</li>
<li>I also read that negative-binomial distribution can be used when overdispersion appears. How do I do this in R? What is the difference to quasipoisson?</li>
<li><p>Zero-inflated Poisson Regression: I read that using the vuong test checks what models fits better.  </p>

<p><code>&gt; vuong (model.poisson, model.zero.poisson)</code></p>

<p>Is that correct? What assumptions does a zero-inflated regression have? </p></li>
<li><p><a href=""http://www.ats.ucla.edu/stat/"">UCLA's Academic Technology Services, Statistical Consulting Group</a> has a <a href=""http://www.ats.ucla.edu/stat/R/dae/zipoisson.htm"">section</a> about zero-inflated Poisson Regressions, and test the zeroinflated model (a) against the standard poisson model (b):  </p>

<p><code>&gt; m.a &lt;- zeroinfl(count ~ child + camper | persons, data = zinb)</code><br>
<code>&gt; m.b &lt;- glm(count ~ child + camper, family = poisson, data = zinb)</code><br>
<code>&gt; vuong(m.a, m.b)</code></p></li>
</ol>

<p>I don't understand what the <code>| persons</code> part of the first model does, and why you can compare these models. I had expected the regression to be the same and just use a different family. </p>
"
"0.10232890201933","0.0946291623262781"," 40920","<p>I have this data </p>

<pre><code>data &lt;- as.data.frame(cbind(y,x1,x2,pre))
data$x1 &lt;- as.factor(data$x1)
data$x2 &lt;- as.factor(data$x2)

data$x1 &lt;- C(data$x1, contr.treatment, base=3)
data$x2 &lt;- C(data$x2, contr.treatment, base=2)

y x1 x2 pre
1  16  1  1  14
2  15  1  1  13
3  14  1  2  14
4  13  1  2  13
5  12  2  1  12
6  11  2  1  12
7  11  2  2  13
8  13  2  2  13
9  10  3  1  10
10 11  3  1  11
11 11  3  2  11
12  9  3  2  10

'data.frame':   12 obs. of  4 variables:
 $ y  : num  16 15 14 13 12 11 11 13 10 11 ...
     $ x1 : Factor w/ 3 levels ""1"",""2"",""3"": 1 1 1 1 2 2 2 2 3 3 ...
  ..- attr(*, ""contrasts"")= num [1:3, 1:2] 1 0 0 0 1 0
  .. ..- attr(*, ""dimnames"")=List of 2
  .. .. ..$ : chr  ""1"" ""2"" ""3""
      .. .. ..$ : chr  ""1"" ""2""
 $ x2 : Factor w/ 2 levels ""1"",""2"": 1 1 2 2 1 1 2 2 1 1 ...
      ..- attr(*, ""contrasts"")= num [1:2, 1] 1 0
      .. ..- attr(*, ""dimnames"")=List of 2
      .. .. ..$ : chr  ""1"" ""2""
  .. .. ..$ : chr ""1""
     $ pre: num  14 13 14 13 12 12 13 13 10 11 ...
</code></pre>

<p>And I fitted the following model</p>

<pre><code>lm(y ~ x1 + x2 + x1*x2)
</code></pre>

<p>My design matrix 'x' is</p>

<pre><code>x &lt;- as.matrix(cbind(rep(1,12), data$pre, c(1,1,1,1,0,0,0,0,0,0,0,0), 
c(0,0,0,0,1,1,1,1,0,0,0,0), c(1,1,0,0,1,1,0,0,1,1,0,0), 
c(1,1,0,0,0,0,0,0,0,0,0,0), c(0,0,0,0,1,1,0,0,0,0,0,0)))

x
  [,1] [,2] [,3] [,4] [,5] [,6] [,7]
 [1,]    1   14    1    0    1    1    0
 [2,]    1   13    1    0    1    1    0
 [3,]    1   14    1    0    0    0    0
 [4,]    1   13    1    0    0    0    0
 [5,]    1   12    0    1    1    0    1
 [6,]    1   12    0    1    1    0    1
 [7,]    1   13    0    1    0    0    0
 [8,]    1   13    0    1    0    0    0
 [9,]    1   10    0    0    1    0    0
[10,]    1   11    0    0    1    0    0
[11,]    1   11    0    0    0    0    0
[12,]    1   10    0    0    0    0    0
</code></pre>

<p>I'm trying to use this design to reproduce the following table:</p>

<pre><code>Source DF Squares Mean Square F Value Pr &gt; F
Model 6 44.79166667 7.46527778 12.98 0.0064
Error 5 2.87500000 0.57500000
Corrected Total 11 47.66666667

Source DF Type III SS Mean Square F Value Pr &gt; F
pre 1 3.12500000 3.12500000 5.43 0.0671
x1 2 4.58064516 2.29032258 3.98 0.0923
x2 1 3.01785714 3.01785714 5.25 0.0706
x1*x2 2 1.25000000 0.62500000 1.09 0.4055
</code></pre>

<p>The first part is fine</p>

<pre><code>XtX &lt;- t(x) %*% x
XtXinv &lt;- solve(XtX)
betahat &lt;- XtXinv %*% t(x) %*% y

H &lt;- x %*% XtXinv %*% t(x) 
IH &lt;- (diag(1,12) - H)
yhat &lt;- H %*% y 
e &lt;- IH %*% y
ybar &lt;- mean(y)

MSS &lt;- t(betahat) %*% t(x) %*% y - length(y)*(ybar^2) 
ESS &lt;- t(e) %*% e 
TSS &lt;- MSS + ESS 

dfM &lt;- sum(diag(H)) - 1 
dfE &lt;- sum(diag(IH)) 
dfT &lt;- dfM + dfE 

MSM &lt;- MSS/dfM 
MSE &lt;- ESS/dfE 

Ftest &lt;- MSM / MSE
pr &lt;- 1 - pf(Ftest, dfM, dfE)
</code></pre>

<p>The contrast coefficient matrix for 'pre' seems correct. </p>

<pre><code>L &lt;- matrix(c(0,1,0,0,0,0,0), 1, 7, byrow=T)
Lb &lt;- L %*% betahat 
LXtXinvLt &lt;- round(L %*% XtXinv %*% t(L), digits=4) 
SSpre &lt;- t(Lb) %*% solve(LXtXinvLt) %*% (Lb) 
MSpre &lt;- SSpre / 1 
Fpre &lt;- MSpre / MSE 
PRpre &lt;- 1 - pf(Fpre, 1, 12-7)
</code></pre>

<p>But I can't understand how to define the contrast coefficient matrix for x1, x2, and x1*x2. What's the problem with the rest of my code? Below an example for how I think I should calculate for x1</p>

<pre><code>L &lt;- matrix(c(0,0,1,1,0,0,0), 1, 7, byrow=T)
Lb &lt;- L %*% betahat 
LXtXinvLt &lt;- round(L %*% XtXinv %*% t(L), digits=4) 
SSX1 &lt;- t(Lb) %*% solve(LXtXinvLt) %*% (Lb) 
MSX1 &lt;- SSX1 / 1 
FX1 &lt;- MSX1 / MSE 
PRX1 &lt;- 1 - pf(FX1, 1, 12-7) 
</code></pre>

<p>The result I get for the second part of the ANOVA table is</p>

<pre><code>pre     1    3.1250 6.0000  3.6822  5.4348 0.06711
x1      2    1.0439 3.9189 -3.4291  0.9078 0.46098  
x2      1    0.2500 3.1250 -4.1457  0.4348 0.53881  
x1:x2   2    1.2500 4.1250 -2.8141  1.0870 0.40554 
</code></pre>

<p>Thanks!</p>
"
"0.108536190793863","0.114707866935281"," 41123","<p>I feel overwhelmed after attempting to dig into the literature on how to run my mixed model analysis following it up with using AIC to select the best model or models.  I do not think my data is that complicated, but I am looking for confirmation that what I have done is correct, and then advise on how to proceed.  I am unsure if I should be using lme or lmer and then with either of those, if I should be using REML or ML.</p>

<p>I have a value of selection and I want to know which covariates best influence that value and allow for predictions.  Here's some made up example data and my code for my test that I am working with:</p>

<pre><code>ID=as.character(rep(1:5,3))
season=c(""s"",""w"",""w"",""s"",""s"",""s"",""s"",""w"",""w"",""w"",""s"",""w"",""s"",""w"",""w"")
time=c(""n"",""d"",""d"",""n"",""d"",""d"",""n"",""n"",""n"",""n"",""n"",""n"",""d"",""d"",""d"")
repro=as.character(rep(1:3,5))
risk=runif(15, min=0, max=1.1)
comp1=rnorm(15, mean = 0, sd = 1)
mydata=data.frame(ID, season, time, repro, risk, comp1)
c1.mod1&lt;-lmer(comp1~1+(1|ID),REML=T,data=mydata)
c1.mod2&lt;-lmer(comp1~risk+(1|ID),REML=T,data=mydata)
c1.mod3&lt;-lmer(comp1~season+(1|ID),REML=T,data=mydata)
c1.mod4&lt;-lmer(comp1~repro+(1|ID),REML=T,data=mydata)
c1.mod5&lt;-lmer(comp1~time+(1|ID),REML=T,data=mydata)
c1.mod6&lt;-lmer(comp1~season+repro+time+(1|ID),REML=T,data=mydata)
c1.mod7&lt;-lmer(comp1~risk+season+season*time+(1|ID),REML=T,data=mydata)
</code></pre>

<p>I have ~19 models that explore this data with various combinations and up to a 2 way interaction terms, but always with ID as a random effect and comp1 as my dependent variable.  </p>

<ul>
<li>Q1. Which to use? lme or lmer? does it matter?</li>
</ul>

<p>In both of these, I have the option to use ML or REML - and I get drastically different answers - using ML followed by AIC I end up with 6 models all with similar AIC values and the model combinations simply do not make sense, whereas REML results in 2 of the most likely models being the best.  However, when running REML I cannot use anova any longer.  </p>

<ul>
<li>Q2. is the main reason to use ML over REML because of use with ANOVA?
This is not clear to me.</li>
</ul>

<p>I am still not able to run stepAIC or I do not know of another way to narrow down those 19 models.</p>

<ul>
<li>Q3. is there a way to use stepAIC at this point?</li>
</ul>
"
"0.0886194286901087","0.0624390541054463"," 41182","<p>I am working on R, and have multiple data.frames. They all look like this one:</p>

<pre><code> head(Stim.Adjs)
      LogFreq    Word   PhonCV    FreqDev  LengthDev
6316    2.673  enkele VC-CV-CV -0.1794029 -1.0608453
12638   2.415 laatste CVVC-CCV -0.2207496 -0.7113483
12639   2.415 laatste CVVC-CCV -0.2207496 -0.7113483
1633    2.248    bang      CVC -0.2475127 -1.7598394
3167    2.243   bezig  CVV-CVC -0.2483140 -1.4103424
25467   2.243 vroeger CCVV-CVC -0.2483140 -0.7113483
</code></pre>

<p>These are list of words extracted from a corpus, and the columns 4 and 5 represent measures of standard deviation of Frequency and Word length respectively. What I want to do is to have all lists with a similar frequency deviation and word length deviation (two separate controls). So far I had been doing this by comparing each dimension in pairs of list using t.tests, and then removing values from the lists so that the t.tests showed no significant difference in means. </p>

<p>Now that I have more than two lists, I'm a bit confused as to how to test if the means are equal or not. At first I though of an ANOVA, but I don't have a predictor variable, just 3 independent means (for each of the two values I want to control). </p>

<p>The question is, then, what alternative to the t.test could I use that allows me to compare the means of independent vectors. Help with the function for this in R is much appreciated.</p>

<p>Thanks in advance.</p>
"
"0.10232890201933","0.0946291623262781"," 43361","<p>As a follow-up to <a href=""http://stats.stackexchange.com/questions/41390/test-for-effects-of-two-categorical-variables-on-a-binary-response-variable"">this question</a>, I have the following data:</p>

<pre><code>   Site Treatment Survival
1   BED        DN      1.0
2   BED        DN      1.0
3   BED        DN      1.0
4   BED        MB      1.0
5   BED        MB      1.0
6   BED        MB      0.9
7   BED    Forest      0.4
8   BED    Forest      0.5
9   BED    Forest      0.4
10  BRO        DN      0.9
11  BRO        DN      1.0
12  BRO        DN      1.0
13  BRO        MB      1.0
14  BRO        MB      1.0
15  BRO        MB      1.0
16  BRO    Forest      1.0
17  BRO    Forest      1.0
18  BRO    Forest      1.0
19  LAP        DN      0.8
20  LAP        DN      0.4
21  LAP        DN      0.6
22  LAP        MB      0.5
23  LAP        MB      1.0
24  LAP        MB      0.7
25  LAP    Forest      0.2
26  LAP    Forest      0.2
27  LAP    Forest      0.4
</code></pre>

<p>on which I ran a binomial glm :</p>

<pre><code>&gt; glm.out &lt;- glm(Survival~Site*Treatment, data=surv,
    family=""binomial"", weights=rep(10, nrow(surv)))
&gt; anova(glm.out, test=""Chisq"")

Analysis of Deviance Table
Model: binomial, link: logit
Response: Survival
Terms added sequentially (first to last)

               Df Deviance Resid. Df Resid. Dev P(&gt;|Chi|)    
NULL                              26    138.254              
Site            2   63.098        24     75.155 1.988e-14 ***
Treatment       2   42.991        22     32.164 4.620e-10 ***
Site:Treatment  4   13.874        18     18.290  0.007707 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>All effects are significant, so now I want to do post-hoc comparisons.  I have discovered the <code>glht</code> function in the multcomp package, which seems to do what I want.  I read somewhere that with 2 independent variables, you should set <code>interaction_average=TRUE</code> to get a result equivalent to a TukeyHSD for a linear model.</p>

<pre><code>&gt; Treat.comp &lt;- glht(glm.out, mcp(Treatment=""Tukey"", interaction_average=TRUE))
&gt; summary(Treat.comp)
</code></pre>

<p>which gives me these strange results:</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses
Multiple Comparisons of Means: Tukey Contrasts
Fit: glm(formula = Survival ~ Site * Treatment, family = ""binomial"", 
    data = surv.san, weights = rep(10, nrow(surv.san)))

Linear Hypotheses:
                 Estimate Std. Error z value Pr(&gt;|z|)
DN - MB == 0       -0.202   3316.127   0.000        1
Forest - MB == 0   -1.886   3316.127  -0.001        1
Forest - DN == 0   -1.684   3316.127  -0.001        1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>Why are my P-values = 1 for all comparisons even though the Treatment effect was highly significant?  </p>

<p>If I try using <code>glht</code> with <code>interaction_average=FALSE</code> I get more reasonable results but with a warning message:</p>

<pre><code>Warning message:
In mcp2matrix(model, linfct = linfct) :
  covariate interactions found -- default contrast might be inappropriate
</code></pre>

<p>Can someone help me to understand what I am doing wrong?  Thank-you!!!</p>
"
"0.0886194286901087","0.0936585811581694"," 45759","<p>I am trying to see if there are differences between two groups (background genetic mutants) and some values associated to them. Every group has 4 different gene types with two replicates each and its respective value. </p>

<pre><code>&gt; data
bg              type     value
a            3motif 0.7371446
a            3motif 0.7519591
a     captured_exon 0.8030102
a     captured_exon 0.8110690
a  helicase_protein 0.7331316
a  helicase_protein 0.7479729
a replicase_protein 0.6897468
a replicase_protein 0.6967064
b            3motif 0.6934017
b            3motif 0.7076932
b     captured_exon 0.7725613
b     captured_exon 0.7816896
b  helicase_protein 0.6938436
b  helicase_protein 0.7153722
b replicase_protein 0.6630786
b replicase_protein 0.6831855
</code></pre>

<p>I'd first like to see if there are differences between the different types of each group (background). So, while looking for a possible answer I found that the following anova test would fit on my needs:</p>

<pre><code>&gt;aov(value~bg+Error(type), data = data)

Call:
    aov(formula = value ~ bg + Error(type), data = data)

Grand Mean: 0.7300979 

Stratum 1: type

Terms:
                 Residuals
Sum of Squares  0.02462776
Deg. of Freedom          3

Residual standard error: 0.09060494 

Stratum 2: Within

Terms:
                         bg   Residuals
Sum of Squares  0.004222238 0.001159024
Deg. of Freedom           1          11

Residual standard error: 0.01026479 
Estimated effects are balanced
</code></pre>

<p>But I don't know how to iterpret the results, would you please help me?</p>
"
"NaN","NaN"," 45878","<p>I was doing ANOVA in SPSS and then in R and to my huge surprise, the results from the latest version of R were incorrect.</p>

<p>When I use the function <code>model.tables(x,""means"")</code> to get descriptive statistics, the independent variable means by the second dependent are slightly incorrect (e.g. 129 instead of 130.27).</p>

<p>My question is what could cause the problem? I am a novice to R but using the same data, SPSS gets the result correctly, so something is obviously wrong.</p>

<p><code>head(data)</code>:</p>

<pre><code>  skupina pohlavie zodpovedny
1       1        1        152
2       1        1        118
3       2        2         88
4       2        1        140
</code></pre>

<p>Code:</p>

<pre><code>x &lt;- aov(zodpovedny ~ pohlavie*skupina,data=data)
model.tables(x,""means"")
</code></pre>

<p>Problem illustrated:</p>

<p><img src=""http://i.stack.imgur.com/g19cG.jpg"" alt=""This is unfortunate.""></p>
"
"0.0895377892669139","0.0811107105653813"," 46755","<p>Consider for instance the balanced one-way random effects ANOVA model:
$$(y_{ij} \mid \mu_i) \sim_{\text{iid}} {\cal N}(\mu_i, \sigma^2_w), \quad j=1,\ldots,J, 
\qquad 
\mu_i \sim {\cal N}(\mu, \sigma^2_b), \quad i=1,\ldots,I. $$</p>

<p>In <a href=""http://stats.stackexchange.com/questions/41177/prediction-interval-for-the-one-way-random-effect-anova"">this post</a> I asked how to get a prediction interval and I found an answer in a published paper by Lin &amp; Liao. The approach is similar to the case of the simple Gaussian sample model and it runs as follows. Denote by $\sigma_{\textrm{tot}}^2=\sigma^2_b+\sigma^2_w$  the total variance and by 
$\tau^2=\sigma^2_b+\frac{\sigma^2_w}{J}$ the variance of the sample mean $\bar{y}_{i\bullet}$. Then, considering a new observation $y^{\textrm{new}}$ one has $$
y^{\textrm{new}} - \bar{y}_{\bullet\bullet} \sim 
{\cal N}\left(0, \sigma^2_{\textrm{tot}} + \frac{\tau^2}{I}\right).$$ 
The variance can also be written as 
 $$  \left(1+\frac{1}{I}\right)\tau^2 + \left(1-\frac{1}{J}\right)\sigma^2_w$$ and it is estimated without bias by $\frac{I+1}{I(I-1)}\frac{SS_b}{J} + \frac{SS_w}{IJ}$ where $SS_b$ is the between sum of squares and $SS_w$ is the within sum of squares. Moreover $SS_w$, $SS_b$, $y^{\textrm{new}}$ and $\bar{y}_{\bullet\bullet}$ are independent. Then we have the distributional approximation
$$\frac{y^{\textrm{new}} - \bar{y}_{\bullet\bullet}}{\sqrt{\frac{I+1}{I(I-1)}\frac{SS_b}{J} + \frac{SS_w}{IJ}}} \approx \mathrm{t}_\nu,$$ where $\nu$ are the Satterthwaite degrees of freedom.  </p>

<p>Now I wonder whether it is possible to get the prediction standard error (or alternatively an approximation) $se=\sqrt{\frac{I+1}{I(I-1)}\frac{SS_b}{J} + \frac{SS_w}{IJ}}$ with R and/or SAS. I am not only interested in this example, but also in more general mixed models. </p>

<p><a href=""http://stats.stackexchange.com/questions/46644/prediction-for-one-way-random-effect-anova-with-rms-package"">Here</a> I asked whether it is possible with Frank Harrell's <code>Gls</code> package but the author himself replies that this package does not handle predictions for a model ""without predictor"". </p>

<p>Below is a numerical example for the balanced one-way random effect ANOVA:</p>

<pre><code>ranovapred &lt;- function(y, group, conf=0.95){
    group &lt;- factor(group)
    means &lt;- aggregate(y~group, FUN=mean)$y  # groups means
    I &lt;- length(levels(group))
    J &lt;- length(y)/I
    sizes &lt;- table(group) # groups sizes
    if(!all(as.numeric(sizes)==J)){ stop(""balanced only!"") }
    ssw &lt;- crossprod(y-rep(means, times=sizes))  # within sum of squares
    ssb &lt;- J*crossprod(means-mean(y)) # beween sum of squares
    a &lt;- (1/J*(1+1/I))/(I-1)
    b &lt;- 1/I/J 
    v &lt;- a*ssb+b*ssw # estimates the variance of (Ynew-Ybar)
    nu &lt;- v^2/((a*ssb)^2/(I-1)+(b*ssw)^2/I/(J-1)) # Satterthwaite degrees of freedom
    alpha &lt;- 1-conf
    bounds &lt;- mean(y) + c(-1,1)*sqrt(v)*qt(1-alpha/2,nu)
return(list(bounds=bounds, std.error=sqrt(v), ss=c(ssw=ssw,ssb=ssb)) )
}
# fictive data
set.seed(421)
I &lt;- 3
J &lt;- 4 
dd &lt;- data.frame(y=rpois(I*J,10), group=gl(I,J))

&gt; dd
    y group
1  12     1
2  11     1
3  13     1
4  14     1
5  10     2
6   8     2
7   8     2
8  12     2
9   8     3
10  6     3
11  6     3
12  9     3
&gt; ranovapred(y=dd$y,dd$group)
$bounds
[1] -1.136421 20.636421

$std.error
         [,1]
[1,] 3.338538

$ss
  ssw   ssb 
22.75 55.50 
</code></pre>
"
"0.140119619801808","0.128342256847803"," 47692","<p>I have a run an unbalanced 2x2x2x2 Type II ANOVA in R and am having trouble following up on the results. Here is the output:</p>

<pre><code>       Effect DFn DFd             F           p p&lt;.05             ges
2           cond   1 127  3.2359349424 0.074414031       0.0110769653158
3             sf   1 127  1.6981345415 0.194889782       0.0058436648717
5             ba   1 127  1.5404865586 0.216833055       0.0012264293759
9             tt   1 127  1.9253260611 0.167700584       0.0054448755666
4        cond:sf   1 127  0.1846599042 0.668127012       0.0006387833954
6        cond:ba   1 127  5.8799698820 0.016721105     * 0.0046651103251
7          sf:ba   1 127  1.4992638464 0.223051114       0.0011936498636
10       cond:tt   1 127  0.5266890712 0.469337439       0.0014954062256
11         sf:tt   1 127  0.0768302867 0.782090431       0.0002184199961
13         ba:tt   1 127 11.5004885802 0.000927851     * 0.0087996011237
8     cond:sf:ba   1 127  0.0042138896 0.948344162       0.0000033589171
12    cond:sf:tt   1 127  0.1197411309 0.729888009       0.0003403692520
14    cond:ba:tt   1 127  0.3878677814 0.534539033       0.0002993221940
15      sf:ba:tt   1 127  0.0001339682 0.990783282       0.0000001034158
16 cond:sf:ba:tt   1 127  0.4820119706 0.488780454       0.0003719473651
</code></pre>

<p><strong>cond</strong> and <strong>sf</strong> are between-subjects factors. <strong>ba</strong> and <strong>tt</strong> are within (or repeated). The unbalanced nature of the experiment has meant that I have used a type II anova.</p>

<p>You can see that we have a suggestively significant main effect of <strong>cond</strong> (2) and two significant interactions (6 &amp; 13). I have graphed the interactions and they seem logical, but I am sure that <strong>cond</strong> is contributing somewhat to the interactions.</p>

<p>I am at a loss at how to proceed. I suppose I wish to do some kind of post-hoc analysis concentrating on the interactions. I have investigated a number of different R packages (afex, phia, contrasts etc), but have yet to work out what I am actually doing with these interactions. </p>

<p>My data looks like this:</p>

<pre><code>str(xx)
'data.frame':   524 obs. of  6 variables:
$ ba  : Factor w/ 2 levels ""before"",""after"": 1 1 1 1 1 1 1 1 1 1 ...
    $ tt  : Factor w/ 2 levels ""targ"",""calm"": 1 1 1 1 1 1 1 1 1 1 ...
$ p   : Factor w/ 131 levels ""1"",""2"",""3"",""4"",..: 4 8 9 10 13 18 19 22 25 29 ...
    $ cond: Factor w/ 2 levels ""Control"",""Spider"": 1 1 1 1 1 1 1 1 1 1 ...
$ sf  : Factor w/ 2 levels ""Fear"",""No-Fear"": 1 1 1 1 1 1 1 1 1 1 ...
    $ eda : num  1.478 -0.56 -0.27 -0.902 -0.483 ...
</code></pre>

<p>Moreover consider <a href=""http://books.google.co.uk/books/about/Foundations_of_Behavioral_Statistics.html?id=8sLOa8vHl7YC"" rel=""nofollow"">Thompson (2006)</a> :</p>

<blockquote>
  <p>As noted by Rosnow and Rosenthal (1989a), the cell means â€œare the
  combined effects of the interaction, the row effects [a main effect],
  the column effects [a second main effect], and the grand meanâ€ (p.
  144). By the same token, simple post hoc tests of the cell means also
  do not yield insight about the origins of interaction effects,
  because the interaction effects are not uniquely a function of the
  cell means (Boik, 1979).</p>
</blockquote>

<p>So I guess post-hoc t-tests (with adjusted p-values) are out? </p>

<p><strong>Update:</strong> Following advice I have <em>found</em> from <a href=""http://stats.stackexchange.com/users/442/henrik"">@henrik</a> (<a href=""https://groups.google.com/forum/?fromgroups=#!topic/ez4r/RpwYT6pEva0"" rel=""nofollow"">here</a>) I have been investigating the phia package and the <strong>testInteractions</strong> function. I have been getting some results (for a type III anova - so not the type II I am after) but, again, I am way out of my depth here:</p>

<p>e.g., </p>

<pre><code>&gt; testInteractions(m2[[""lm""]], pairwise = ""ba"", ""cond"", idata = m2[[""idata""]],     adjustment = ""none"")              
Multivariate Test: Pillai test statistic
P-value adjustment method: none
                      Value Df test stat approx F num Df den Df  Pr(&gt;F)  
after-before : Control -0.31910  1  0.045305   6.0268      1    127 0.01544 *
after-before :  Spider  0.14243  1  0.008045   1.0300      1    127 0.31208  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p><strong>Latest Update:</strong></p>

<p>I seem to be getting further bogged down with this. Using lmer as referenced <a href=""http://www.uni-kiel.de/psychologie/rexrepos/Univariate/ANOVA/anovaMixed.html"" rel=""nofollow"">here</a> gives this:</p>

<pre><code>fit.1 &lt;-lmer(eda ~ 1 + cond * sf * ba *tt +(ba *tt | p), xx)
</code></pre>

<p>which leads to:</p>

<pre><code>&gt; anova(fit.1)
Analysis of Variance Table
          Df  Sum Sq Mean Sq F value
cond           1 0.00330 0.00330  0.0158
sf             1 0.04236 0.04236  0.2035
ba             1 0.07408 0.07408  0.3559
tt             1 0.34651 0.34651  1.6649
cond:sf        1 0.05633 0.05633  0.2707 
cond:ba        1 2.38017 2.38017 11.4359
sf:ba          1 0.45349 0.45349  2.1789
cond:tt        1 0.03832 0.03832  0.1841
sf:tt          1 0.03197 0.03197  0.1536
ba:tt          1 2.39403 2.39403 11.5025
cond:sf:ba     1 0.02595 0.02595  0.1247 
cond:sf:tt     1 0.00667 0.00667  0.0320
cond:ba:tt     1 0.08078 0.08078  0.3881
sf:ba:tt       1 0.00003 0.00003  0.0001
cond:sf:ba:tt  1 0.10034 0.10034  0.4821
</code></pre>

<p>Which seems to be confirming the earlier finding of a significant interaction between cond * ba and ba * tt (but is it type II?). Still not sure if any of this is correct.</p>

<p><strong>To clarify then: I am looking for advice about what to do next, in terms of understanding the significant interactions.</strong></p>
"
"0.272013559690274","0.287481023438573"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.0361787302646211","0.0382359556450936"," 49924","<p>I'm learning R and trying to understand how <code>lm()</code> handles factor variables &amp; how to make sense of the ANOVA table. I'm fairly new to statistics, so please be gentle with me.</p>

<p>Here's some movie data from Rotten Tomatoes. I'm trying to model the score of each movie based on the mean scores for all of the movies in 4 groups: those rated G, PG, PG-13, and R.</p>

<pre><code>download.file(""http://www.rossmanchance.com/iscam2/data/movies03RT.txt"", destfile = ""./movies.txt"")
movies &lt;- read.table(""./movies.txt"", sep = ""\t"", header = T, quote = """")
lm1 &lt;- lm(movies$score ~ as.factor(movies$rating))
anova(lm1)
</code></pre>

<p>and the ANOVA output:</p>

<pre><code>## Analysis of Variance Table
## 
## Response: movies$score
##                           Df Sum Sq Mean Sq F value Pr(&gt;F)
## as.factor(movies$rating)   3    570     190    0.92   0.43
## Residuals                136  28149     207
</code></pre>

<p>I understand how to get all the numbers in this table, EXCEPT <code>Sum Sq</code> and <code>Mean Sq</code> for <code>as.factor(movies$rating)</code>. Can someone please explain how that <code>Sum Sq</code> is calculated from my data? I know that <code>Mean Sq</code>is just <code>Sum Sq</code> divided by <code>Df</code>.</p>
"
"0.177380818842608","0.172469820421908"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0626633989716535","0.0220755392844174"," 50180","<p>In my research I have performed a series of measurements on 5 different brands of blocks. Each block has been inspected for deformation under incremental forces (20, 30, 40, 50, 60, 70, 80, 90, 100, 110 and 120 N). The deformation for each force was measured 3 times and the mean values were assigned to each brand for a specific amount of force. I was successful in creating linear regression graphs for these 5 different brands.</p>

<p>Now my wish is to see whether a brand makes a significant difference in deformation values and to perform a post-hoc analysis to compare brands among themselves. In other words to compare the linear regression lines. Sorry if what I am saying makes no sense.</p>

<p>So far, I have tried the following commands:</p>

<pre><code>anova(lm(Deformation~Force*Brand, data=Data))
lm(Deformation~Force, data=Data))

# and
aov.data = aov(Deformation~Force*Brand, Data)
</code></pre>

<p>I have gotten suspiciously low p-values (<em>*</em>) which clearly indicates that I might be doing something wrong. I would be grateful if you could help me with this issue.</p>

<pre><code>Force   Brand   Deformation  
20  Brand1  0.65  
30  Brand1  1.23  
40  Brand1  1.25  
50  Brand1  2.39  
60  Brand1  2.45  
70  Brand1  2.93  
80  Brand1  3.13  
90  Brand1  3.57  
100 Brand1  4.68  
110 Brand1  4.84  
120 Brand1  5.33  
20  Brand2  1.24  
30  Brand2  1.11  
40  Brand2  1.6  
50  Brand2  2.13  
60  Brand2  2.69  
70  Brand2  3.60  
80  Brand2  3.90  
90  Brand2  3.99  
100 Brand2  4.51  
110 Brand2  4.74  
120 Brand2  5.98  
20  Brand3  1.21  
30  Brand3  1.37  
40  Brand3  2.56  
50  Brand3  2.49  
60  Brand3  3.17  
70  Brand3  3.33  
80  Brand3  3.38  
90  Brand3  4.2  
100 Brand3  4.22  
110 Brand3  5.22  
120 Brand3  6.28  
20  Brand4  0.92  
30  Brand4  0.89  
40  Brand4  1.2  
50  Brand4  1.67  
60  Brand4  1.98  
70  Brand4  2.25  
80  Brand4  3.8  
90  Brand4  4.17  
100 Brand4  4.94  
110 Brand4  5.4  
120 Brand4  5.76  
20  Brand5  0.69  
30  Brand5  1.26  
40  Brand5  1.61  
50  Brand5  2.17  
60  Brand5  2.07  
70  Brand5  3.35  
80  Brand5  3.27  
90  Brand5  4.13  
100 Brand5  4.25  
110 Brand5  4.59  
120 Brand5  5  
</code></pre>
"
"NaN","NaN"," 50598","<p>How do I interpret the following output from R for a two-way repeated measures ANOVA</p>

<pre><code>Model &lt;- aov(value ~ BWRatio * NumGraBoolean + Error(participant/(BWRatio * NumGraBoolean)))
</code></pre>

<p><img src=""http://i.stack.imgur.com/kQCo7.png"" alt=""enter image description here""></p>

<p>In particular, what does all these Error terms mean and what does their p-value signify? Please excuse my lack of understanding repeated measures and ANOVA in general, but what do the residuals here represent as well?</p>
"
"0.130778311815021","0.128342256847803"," 51489","<p>Let's say I have an experiment with three within-subject factors, A, B, &amp; C. The data looks like this.</p>

<pre><code> s  a  b  c
 1  1  1  1
 1  1  1  2
 1  1  2  1
 1  1  2  2
 1  2  1  1
 1  2  1  2
 1  2  2  1
 1  2  2  2
</code></pre>

<p>Simple enough. I have 49 subjects. Now, to do this ANOVA in R, I use</p>

<pre><code>m1 &lt;- aov(score ~ a*b*c + Error(subject/(a*b*c)), data)
summary(m1, type=3)
... (clipped) ...
Error: s:b:c
                  Df Sum Sq Mean Sq F value  Pr(&gt;F)   
b:c                1  4.608   4.608   8.121 0.00643 **
Residuals         48 27.236   0.567  
</code></pre>

<p>That looks fine, and matches SPSS's repeated measures GLM. All is well.</p>

<p>We can also do a mixed model in R using <code>lme4</code> and get the exact same results as this, as well as a mixed model done in JMP.</p>

<pre><code>m2 &lt;- lmer(score~a*b*c + (a*b*c|s), data)
library(car); Anova(m2, type=3, test.statistic""F"")
... (clipped) ...
                     F Df Df.res    Pr(&gt;F) 
b:c             8.1206  1 48.000  0.006430 ** 
</code></pre>

<p>I can do the same thing in SAS and get similar results. </p>

<pre><code>proc mixed data=mixedexample method=reml covtest;
    class a b c s;
    model score = a|b|c; 
    random intercept a|b|c/sub=s;
run;
</code></pre>

<p>I can do the ANOVA, by hand in Stata:</p>

<pre><code>anova score a / s|a ///
            b / s#b ///
            c / s#c ///
            a#b / s#a#b ///
            a#c / s#a#c /// 
            b#c / s#b#c /// 
            a#b#c 
</code></pre>

<p>I leave off the full interaction error term so it is the residual. Though, for some reason, the <code>df</code> of the main effect of <code>a</code> is twice the size of the others. But that's not the question I have. </p>

<p>My question is: how do I do the <code>lmer</code> and <code>proc mixed</code> version of the full LMM in Stata? The simple version is </p>

<pre><code>xtmixed score a##b##c || s:, reml
</code></pre>

<p>But how do I add the fully crossed error terms to <code>xtmixed</code> in the same way I do in <code>lmer</code> by adding <code>+ (a*b*c|s)</code>? The data is balanced with no missing values, so the LMM should be the same as the repeated-measures ANOVA, right? Why can't I do this in Stata? </p>

<p>I barely know the basics of LMMs, but this is a Stata question. I'm just trying to figure out all the different ways of performing these two models. Also, if anyone knows a simpler way of doing the univariate ANOVA in Stata without specifying every single error term by hand? This may not even be the ""right"" way of doing this procedure, but because I get the same output everywhere else, how do I get Stata to do the same thing as R's <code>lmer</code>, SAS's <code>proc mixed</code>, and JMP?</p>
"
"0.0886194286901087","0.0936585811581694"," 51826","<p>I have a question on how a statistician would normally interpret an anova output. Say I have anova output from R.</p>

<pre><code>&gt; summary(fitted_data)

Call:
lm(formula = V1 ~ V2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.74004 -0.33827  0.04062  0.44064  1.22737 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.11405    0.32089   6.588  1.3e-09 ***
V2           0.03883    0.01277   3.040  0.00292 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.6231 on 118 degrees of freedom
Multiple R-squared: 0.07262,    Adjusted R-squared: 0.06476 
F-statistic:  9.24 on 1 and 118 DF,  p-value: 0.002917 

&gt; anova(fit)
Analysis of Variance Table

Response: V1
           Df Sum Sq Mean Sq F value   Pr(&gt;F)   
V2          1  3.588  3.5878  9.2402 0.002917 **
Residuals 118 45.818  0.3883                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>From the above, I guess the most important value is Pr(>F), right? So this Pr, is less than 0.05 (95% level). How should my ""explain"" this? Do I explain it in ""association"", ie, V2 and V1 are associated (or not) ? or in terms of ""significance""? I always felt that I couldn't understand when people say ""This value is significant...."". So what is ""significant""? Is there a more intuitive form of explanation? like ""I am 95% confident that ...."" . </p>

<p>Also, is the Pr value the only important piece of information? or can i also look at residuals and the rest of the output to ""explain"" the result? thanks</p>
"
"0.119991273679223","0.115285743977679"," 52583","<p>I have done an experiment of how different animal species affect nutrient fluxes in sediments. I had a number of experimental units (sediment boxes) to which different animal species were added. I then measured nutrient fluxes in these units. Each unit was measured five times. I have used the ezANOVA package for R to do a repeated measures ANOVA with two factors (between subjects factor â€œSpeciesâ€ and within subject factor â€œTimeâ€). 
Here's an example of my input:</p>

<pre><code>&gt; ezANOVA(data=NoP_3_5,dv=.(AcPO4),wid=.(Subject),within=.(Time),between=.(Species),return_aov=T)
$ANOVA
    Effect DFn DFd           F            p p&lt;.05        ges
2      Species   2   6 10.60384830 1.072453e-02     * 0.50875722
3         Time   2  12 27.88590570 3.081718e-05     * 0.76667542
4 Species:Time   4  12  0.08093179 9.867291e-01       0.01871588

$`Mauchly's Test for Sphericity`
        Effect         W           p p&lt;.05
3         Time 0.1439855 0.007866779     *
4 Species:Time 0.1439855 0.007866779     *

$`Sphericity Corrections`
        Effect       GGe       p[GG] p[GG]&lt;.05       HFe       p[HF] p[HF]&lt;.05
3         Time 0.5387889 0.001349263         * 0.5630403 0.001103048         *
4 Species:Time 0.5387889 0.933492515           0.5630403 0.939138882          

$aov

Call:
aov(formula = formula(aov_formula), data = data)

Grand Mean: -0.004276663 

Stratum 1: Subject

Terms:
                     Species    Residuals
Sum of Squares  0.0004410742 0.0001247870
Deg. of Freedom            2            6

Residual standard error: 0.004560464 
Estimated effects may be unbalanced

Stratum 2: Subject:Time

Terms:
                        Time Species:Time    Residuals
Sum of Squares  0.0013994206 0.0000081229 0.0003011028
Deg. of Freedom            2            4           12

Residual standard error: 0.005009181 
Estimated effects may be unbalanced
</code></pre>

<p>I want to use the $aov output to do a Tukey HSD post hoc test of the between factor (""Species""). However, it does not work:</p>

<pre><code>mod &lt;- ezANOVA(data=NoP_3_5,dv=.(AcPO4),wid=.(Subject),within=.(Time),between=.(Species),return_aov=T)
    &gt; TukeyHSD(mod$aov)
Error in UseMethod(""TukeyHSD"") : 
  no applicable method for 'TukeyHSD' applied to an object of class ""c('aovlist', 'listof')""
</code></pre>

<p>I have also tried to use the aov-command directly:</p>

<pre><code>&gt; mod2 &lt;- aov(AcPO4~(Species)+Error(Subject/Time)+(Species),data=NoP_3_5)
&gt; TukeyHSD(mod2)
Error in UseMethod(""TukeyHSD"") : 
  no applicable method for 'TukeyHSD' applied to an object of class ""c('aovlist', 'listof')""
</code></pre>

<p>I have tried to use lme() to specify the anova-model with the aim to produce something that TukeyHSD() will accept without success.</p>

<p>Any help would be greatly appreciated!</p>
"
"0.0895377892669139","0.108147614087175"," 53312","<p>From the documentation for <code>anova()</code>: </p>

<blockquote>
  <p>When given a sequence of objects, â€˜anovaâ€™ tests the models against one another in the order specified...</p>
</blockquote>

<p>What does it mean to test the models against one another? And why does the order matter?</p>

<p>Here is an example from the <a href=""http://www.genabel.org/sites/default/files/pdfs/GenABEL-tutorial.pdf"">GenABEL tutorial</a>:</p>

<pre><code>    &gt;  modelAdd = lm(qt~as.numeric(snp1))
    &gt;  modelDom = lm(qt~I(as.numeric(snp1)&gt;=2))
    &gt;  modelRec = lm(qt~I(as.numeric(snp1)&gt;=3))
     anova(modelAdd, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ as.numeric(snp1)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(&gt;Chi)
    1   2372 2320                      
    2   2371 2320  1    0.0489     0.82
     anova(modelDom, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ I(as.numeric(snp1) &gt;= 2)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(&gt;Chi)
    1   2372 2322                      
    2   2371 2320  1      1.77     0.18
     anova(modelRec, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ I(as.numeric(snp1) &gt;= 3)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(&gt;Chi)  
    1   2372 2324                        
    2   2371 2320  1      3.53    0.057 .
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>How do I interpret these output?
Thanks!</p>
"
"0.0626633989716535","0.0662266178532522"," 53432","<p>I have 3 categorical variables (CVa, CVb, CVc) all 0 or 1. Two continuous variables (IV1, IV2) are confounding my observational study. The multiple regression </p>

<pre><code>lm(DV ~ CVa + CVb + CVc + CVa:CVb + CVa:CVc + IV1 + IV2)
</code></pre>

<p>is showing great significance for CVa</p>

<pre><code>              Estimate   Std. Error t value Pr(&gt;|t|)
(Intercept)  -1.414684   1.498886  -0.944  0.35233
CVa1         -0.841076   0.256946  -3.273  0.00255 **
CVb1         -0.413594   0.168753  -2.451  0.01990 * 
CVc1         -0.328669   0.183652  -1.790  0.08298 . 
IV1          -0.011768   0.006519  -1.805  0.08049 . 
IV2           0.487658   0.211015   2.311  0.02743 * 
CVa1:CVb1     0.321766   0.238869   1.347  0.18743   
CVa1:CVc1     0.741290   0.259402   2.858  0.00744 **
</code></pre>

<p>I thought that ANCOVA (between factor CVa) must also show significance, but</p>

<pre><code>summary(aov(DV ~ CVa + CVb + CVc + CVa:CVb + CVa:CVc + IV1 + IV2))
</code></pre>

<p>is not showing any significance for CVa</p>

<pre><code>          Df Sum Sq Mean Sq F value  Pr(&gt;F)   
CVa        1  0.368  0.3681   3.093 0.08817 . 
CVb        1  0.427  0.4275   3.593 0.06709 . 
CVc        1  0.015  0.0148   0.125 0.72629   
IV1        1  0.585  0.5849   4.916 0.03384 * 
IV2        1  0.693  0.6935   5.828 0.02166 * 
CVa:CVb    1  0.126  0.1262   1.061 0.31069   
CVa:CVc    1  0.972  0.9716   8.166 0.00744 **
Residuals 32  3.807  0.1190
</code></pre>

<p>Am I doing ANOVA instead of ANCOVA? If yes, how do I control for IV1, IV2 to get that F-value they usually report in papers?</p>

<p>Just in case, <code>lsmeans(m2,pairwise ~ CVa * CVb)</code> reports that main effect of CVa is significant when controlled for IV1, IV2</p>

<pre><code>$`CVa:CVb pairwise differences`
               estimate        SE df  t.ratio p.value
0, 0 - 1, 0  0.47043119 0.1725208 32  2.72681 0.04807
</code></pre>
"
"0.0626633989716535","0.0662266178532522"," 55662","<p>I am doing an ANCOVA model in order to explain the gap in a given distance. So I have a control group, and I have several quantitative variables, right now I am trying to evaluate the impact of the quantitative variables individually. But I think the results I get are incoherent, lest see:</p>

<pre><code>                           Df Sum Sq Mean Sq F value   Pr(&gt;F)    
as.factor(groupe)           1  738.8   738.8  21.931 1.03e-05 ***
BASE0008                    1   36.6    36.6   1.087  0.29992    
as.factor(groupe):BASE0008  1  270.0   270.0   8.015  0.00576 ** 
Residuals                  87 2930.9    33.7 
</code></pre>

<p>These are the results from the ANOVA table, we could say that the group has a significant effect so does the interaction, but when I look at the results of the regression model, I find this:</p>

<pre><code>                              Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)                    1.16666    3.38404   0.345  0.73111   
as.factor(groupe)NAV           4.48191    3.79515   1.181  0.24084   
BASE0008                       0.26027    0.08651   3.009  0.00343 **
as.factor(groupe)NAV:BASE0008 -0.26902    0.09503  -2.831  0.00576 **
</code></pre>

<p>Well, the interaction is still relevant, but it looks like there is not effect of the group and the quantitative variable is more important to determinate the output of the experiment. I want to know if my interpretation is accurate: What can I say about the group? </p>
"
"0.0626633989716535","0.0662266178532522"," 55840","<p>I (or rather, my girlfriend whom I'm trying to convert to free software) am just getting started with R.</p>

<p>She wants to do a two-way ANOVA on her data on mussels and the threads they release in self-defense.  Her experiment has four conditions and in each condition two sets of data were collected (number of threads and average length).</p>

<p>I am by no means a statistician; I only spent a few weeks learning the very basics of R as a programming language, but I know it is a powerful, industry-grade piece of software.</p>

<p>I know I can import data through a command such as</p>

<pre><code>&gt; m10c = read.csv('.../mussels-only-10c.csv')
&gt; m10c
   Tank MusselNo ThreadsNo AvgLength
1     1        1         2      1.70
2     1        2         0      0.00
3     1        3         7      1.48
4     2        1        26      2.12
5     2        2         0      0.00
6     2        3        50      0.69
7     3        1         6      1.37
8     3        2        22      1.50
9     3        3         0      0.00
10    4        1         2      1.00
11    4        2         6      1.98
12    4        3        23      1.21
</code></pre>

<p>but I don't have a firm enough knowledge of statistics to know what to do with it.</p>

<p>A few questions: What is her hypothesis? What are her dependent and independent variables? Is the data independent (did you control for pseudoreplication)?</p>

<p><strong>Hypothesis</strong> There will be significant differences in the thread count and length of mussels in the 10c condition than the 15c condition, and between those stored with snails and those without (but not as drastic).  Expecting more threads in the warmer condition.</p>

<p><strong>Independent Variables</strong> Whether there are snails there or not; temperature</p>

<p><strong>Dependent Variables</strong> Number; length of the threads</p>

<p><strong>""Is the data independent?""</strong>  She doesn't know enough about it to say whether or not she controlled it - this is an undergraduate research project.  She doesn't <em>think</em> it's relevant to an ANOVA.</p>
"
"0.108536190793863","0.101962548386916"," 56055","<p>I have data from a randomized survey experiment in which each respondent was assigned to one of 4 groups, one of which can be considered a ""control"" or ""no treatment"" group. The key question asked in the survey was a binary one: i.e. each respondent was faced with a choice between two products given some stimulus based on the assigned group. Of course, there are several other questions to be controlled for (demographics, pre-existing preferences, etc.).</p>

<p>I want to know what effect, if any, being in a particular group had on the respondent's choice for that key question, controlling for the other factors. Since my response variable is categorical I can't use ANOVA (at least R doesn't appear willing to let me have a non-numeric response variable). I have tried to do a logistic regression but it seems like the structure of my data means that this would result in the respondents in each group being compared to the rest of the respondents which seems like it would be incorrect.</p>

<p>My data resembles the following in structure:</p>

<pre><code>| Id | Group | Product Chosen | ... (other variables)
| 1  |     1 | A              | ...
| 2  |     4 | B              | ...
| 3  |     3 | B              | ...
| 4  |     2 | B              | ...
| 5  |     1 | A              | ...
| 5  |     2 | B              | ...
| 5  |     4 | A              | ...
| 5  |     3 | B              | ...
</code></pre>

<p>etc.</p>

<p>In case it is relevant, I have been using R for my analysis.</p>

<p><strong>Update:</strong> Just so it's clear, my working hypothesis is that respondents in non-control groups were more likely to choose product A than B (and less importantly, but similarly, that respondents in group 2 were more likely than those in group 3, and those in group 3 were more likely than those in group 4).</p>
"
"0.0886194286901087","0.0936585811581694"," 56303","<p>First time posting here, so thank you ahead of time for your help.  I'd like to estimate the variances associated with two factors in a relatively simple, but unbalanced GLS model, and I am unsure how to extract that information (or how to manually calculate it).  For the two factors I'm using, DS3 and DS4, DS3 has four levels (WW - ZZ) and DS4 has seven levels (AA - GG).  I've been using the nlme package; the model itself and model output are below.</p>

<pre><code>&gt; library(nlme)
&gt; temp_gls4 = gls(output~DS3+DS4,data=temp_month,weights=varIdent(form=~1|DS4))
&gt; summary(temp_gls4)
Generalized least squares fit by REML
  Model: output ~ DS3 + DS4 
  Data: temp_month 
     AIC      BIC    logLik
1199.769 1268.615 -582.8845

Variance function:
Structure: Different standard deviations per stratum
Formula: ~1 | DS4 
Parameter estimates:
       AA        BB        CC        DD        EE        FF        GG 
1.0000000 1.2900663 1.1351448 0.9501666 0.7463385 0.8933227 0.9123444 

Coefficients:
                Value Std.Error   t-value p-value
(Intercept)  3.853300 0.1232730 31.258279  0.0000
DS3XX       -0.053908 0.1505280 -0.358129  0.7204
DS3YY       -0.119480 0.1505280 -0.793737  0.4278
DS3ZZ       -0.146255 0.1552625 -0.941988  0.3467
DS4BB        0.934175 0.2081654  4.487655  0.0000
DS4CC       -0.093199 0.1098115 -0.848717  0.3965
DS4DD        0.831305 0.2011411  4.132945  0.0000
DS4EE        1.867642 0.2538358  7.357675  0.0000
DS4FF        1.429805 0.1680183  8.509820  0.0000
DS4GG        0.050992 0.1779757  0.286510  0.7746
</code></pre>

<p>I know that in a traditional balanced least squared situation you can pretty easily calculate the variance of DS3 and DS4 by manipulating the mean squares (e.g. var(DS3)=(MS(DS3)-MSE)/7), but the data here aren't balanced and I don't know the math behind the GLS model and REML as compared to an traditional least squared model.  I've tried using the anova command, but it just produces the F statistics associated with each factor.  Any help you can provide would be appreciated!</p>

<p>Jon</p>
"
"0.0886194286901087","0.0936585811581694"," 56380","<p>The <code>lme4</code> package in R includes the <code>cake</code> dataset. </p>

<pre><code>library(lme4)
head(cake[,2:4], 20)
   recipe temperature angle
1       A         175    42
2       A         185    46
3       A         195    47
4       A         205    39
5       A         215    53
6       A         225    42
7       B         175    39
8       B         185    46
9       B         195    51
10      B         205    49
11      B         215    55
12      B         225    42
13      C         175    46
14      C         185    44
15      C         195    45
16      C         205    46
17      C         215    48
18      C         225    63
19      A         175    47
20      A         185    29
</code></pre>

<p>I've analysed the <code>cake</code> dataset using two different models below. The first model is a 2 factor ANOVA:</p>

<pre><code>summary(aov(angle ~ temperature + recipe, cake))
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
temperature   5   2100   420.1   6.918 4.37e-06 ***
recipe        2    135    67.5   1.112     0.33    
Residuals   262  15908    60.7                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...and the second is a mixed effects model, with <code>temperature</code> as a random effect:</p>

<pre><code>lmer(angle ~ recipe + (1| temperature), data=cake, REML=F)
Linear mixed model fit by maximum likelihood 
Formula: angle ~ recipe + (1 | temperature) 
   Data: cake 
  AIC  BIC logLik deviance REMLdev
 1893 1911 -941.7     1883    1877
Random effects:
 Groups      Name        Variance Std.Dev.
 temperature (Intercept)  6.4399  2.5377  
 Residual                60.2560  7.7625  
Number of obs: 270, groups: temperature, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)   33.122      1.320  25.093
recipeB       -1.478      1.157  -1.277
recipeC       -1.522      1.157  -1.315

Correlation of Fixed Effects:
        (Intr) recipB
recipeB -0.438       
recipeC -0.438  0.500
</code></pre>

<p>Is someone able to provide a summary of what the mixed effect model has done differently to the ANOVA?</p>
"
"0.153493353028995","0.153209119956831"," 57304","<p>I am trying to test if richness varies by treatment (Severity) over time (Block) using a repeated measures ANOVA in R. Any suggestions on how to do this correctly? I have tried, but get an error message stating I have an extra = in the formula:</p>

<pre><code>TWP.aov &lt;- aov(Richness ~ Severity * Year + Error(Plot/Severity), data = TWP)
summary(TWP.aov) 
</code></pre>

<p>Here is a subset of the data (called TWP which I read in as a csv file):</p>

<pre><code>Block       Severity    Plot    Richness
2003-2004   High        A       18
2003-2004   High        B       24
2003-2004   High        C       21
2005-2006   High        A       28
2005-2006   High        B       24
2005-2006   High        C       20
2007-2009   High        A       14
2007-2009   High        B       27
2007-2009   High        C       29
2003-2004   Low         A       12
2003-2004   Low         B       10
2003-2004   Low         C       14
2005-2006   Low         A       18
2005-2006   Low         B       16
2005-2006   Low         C       14
2007-2009   Low         A       8
2007-2009   Low         B       19
2007-2009   Low         C       20
</code></pre>

<p>When I input the above subset I get the correct summary table: </p>

<pre><code>Error: Plot
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals  2  49.33   24.67        

Error: Plot:Severity
          Df Sum Sq Mean Sq F value Pr(&gt;F)  
Severity   1 304.22  304.22   85.56 0.0115 *
Residuals  2   7.11    3.56                 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: Within
               Df Sum Sq Mean Sq F value Pr(&gt;F)
Block           2  43.00  21.500   0.745  0.505
Severity:Block  2   1.44   0.722   0.025  0.975
Residuals       8 230.89  28.861  
</code></pre>

<p>My actual data's summary table looks like this though:</p>

<pre><code>Error: Plot
               Df Sum Sq Mean Sq F value Pr(&gt;F)  
Severity        1    248  247.66   4.013 0.0479 *
Block           2    493  246.67   3.997 0.0214 *
Severity:Block  2    244  122.17   1.980 0.1436  
Residuals      99   6110   61.71                 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>As of June 12, 2013: As an update, I still have not resolved the ""Error() model is singular"" issue with the aov function, so I have tried to run this in the nlme package using the following code:</p>

<pre><code>TWP.lme &lt;- lme(Richness ~Severity * Block, random = ~1|(Plot/Severity), data = TWP)
    summary(TWP.lme)
    anova(TWP.lme)
</code></pre>

<p>I do not get any error message, and I believe I have structured the error term correctly.  I have checked the results visually against the graphs of the data and all appears correct.
Is there a way to check the results are correct? Why is there no error message regarding the model is singular?</p>
"
"0.10232890201933","0.0946291623262781"," 57844","<p>I have a data set that is repeated-measures ratings of four devices (two two-level variables) for 6 dimensions (ease, confidence, comfort, control, size and fit), with a two-level between-subjects variable. Like so: </p>

<pre><code>ptcip / grp / fdback / dur / assem / accep / freq / t.vs.r / attrib / meas / d.rating
1    RA   binary    short       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating       ease    9         2
1    RA   binary    short       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating confidence    7         1
1    RA   binary    short       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating    comfort    6         4
1    RA   binary    short       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating    control    5         3
1    RA   binary    short       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating       size    7        -1
1    RA   binary    short       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating        fit    6         0
1    RA   binary      med       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating       ease    9         6
1    RA   binary      med       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating confidence    5         6
1    RA   binary      med       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating    comfort    6         5
1    RA   binary      med       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating    control    9        -1
1    RA   binary      med       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating       size    2        -1
1    RA   binary      med       &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; rating        fit    8        -2
</code></pre>

<p>I need to run separate ANOVAs on each of the dimensions (we know from other analyses that the dimensions are rated differently, so including it as a factor wouldn't be very informative). What's the most efficient way to write this--split each of the dimensions off into separate data frames, or specify aov to be run on data iff attrib == ""ease""? Seems like the latter would be better, but I'm new to R.</p>

<p>The model I am currently specifying is</p>

<pre><code>aov(d.ratings ~ grp*fdback*dur + Error(particip/(fdback*dur))+grp, PDdonly)
</code></pre>

<p>but as that collapses over the six dimensions, it's not actually meaningful atm.</p>
"
"0.108536190793863","0.0892172298385518"," 58059","<p>I think this means an unequal sample in different conditions.  But it seems to mean something else. . .</p>

<p>I have a data set like below</p>

<pre><code>particip    group   device  width   length  accep   thresh  rating  d-rating
1           RA      Dingo   nom     nom     Y       5       8       3
1           RA      Dingo   nom     long    Y       4       6       2
1           RA      Dingo   fat     nom     Y       4       6       2
1           RA      Dingo   fat     long    N       6       4      -2
</code></pre>

<p>and I'm running an ANOVA on it like so</p>

<pre><code>aov.AMIDS_d &lt;- aov(d.rating ~ group*device*width*length + Error(particip/(device*width*length))+group,data.AMIDS_d) 
</code></pre>

<p>This works ok until I try to print the condition means like so</p>

<pre><code>print(model.tables(aov.AMIDS_d,""means""),digits=3)
</code></pre>

<p>and it says</p>

<pre><code>Error in model.tables.aovlist(aov.AMIDS_d, ""means"") : design is unbalanced so cannot proceed
</code></pre>

<p>According to the design, it ought to be balanced, so I need to check my data structure. I tried</p>

<pre><code>table(data.AMIDS_d[,2:5])
</code></pre>

<p>to give a table of observations per condition and got this</p>

<pre><code>, , width = fat, length = long

     device
group Dingo SNAR
   NR    12   12
   NV    12   12
   RA    12   12

, , width = nom, length = long

     device
group Dingo SNAR
   NR    12   12
   NV    12   12
   RA    12   12

, , width = fat, length = nom

     device
group Dingo SNAR
   NR    12   12
   NV    12   12
   RA    12   12

, , width = nom, length = nom

     device
group Dingo SNAR
   NR    12   12
   NV    12   12
   RA    12   12
</code></pre>

<p>which looks both correct and balanced. So what is causing the unbalanced design error?</p>
"
"0.0886194286901087","0.0936585811581694"," 58225","<p>I am conducting a psycholinguistic experiment. Each trial consists of the subject responding to a word by pressing a button.The design of my experiment is as follows:</p>

<p>5 blocks of a 100 trials each (each trial is a response). In each block, 50 trials are <code>Regular</code> and 50 are <code>Random</code>. Response time (RT) is the dependent variable. This experiment is partly a learning experiment. I expect RTs to be faster in the <code>Regular</code> condition because there is learning which allows for faster responses.     </p>

<p>I therefore want to test the difference in mean response time (RT) between the two conditions of my experiment.I know that the model for the ANOVA would be something like </p>

<pre><code>RTs ~ Type of Trial (`Regular` and `Random`) + ...
</code></pre>

<p>I have two related questions</p>

<ol>
<li><p>Should I use <code>Trial</code> or <code>Block</code> as the other factor? (or both?) Normally I would compare mean RT for each condition across blocks, but it is possible that within blocks, there is already changes in RT between trials. So, is <code>Block</code>, <code>Trial</code> or both my factor? (besides the condition factor). </p></li>
<li><p>Since I expect variation across participants, should I include <code>Subject</code> as an Error variable in the model?</p></li>
</ol>

<p>I am doing my analysis in R, in case anyone wants/can provide some advice in that format, but also general statistical advise is much appreciated.</p>
"
"0.161796200422643","0.170996392014192"," 58321","<p>I need some help with the statistical analysis of a study of a particular surgery to remove a particular cancer. I am using the statistical program R to conduct my analysis. My data are saved in the object <code>study_data</code>.</p>

<h3>Data</h3>

<pre><code># Create reproducible example data
set.seed(50)

study_data &lt;- data.frame(
              Patient_ID = 1:500,
              Institution = sample(c(""New York"",""San Francisco"",""Houston"",""Chicago""),500,T),
              Gender = sample(c(""Male"",""Female""),500,T),
              Race = sample(c(""White"",""Black"",""Hispanic"",""Asian""),500,T),
              Tumor_grade = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Pathologic_stage = sample(c(""P0"",""Pa"",""Pis"",""P1"",""P2a"",""P2b"",""P3a"",""P3b"",""P4a"",""P4b""),500,T),
              Treatment_arm = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Surgery_age = round(runif(500,20,100)),
              Nodes_removed = round(runif(500,1,130)))
</code></pre>

<p>Here is what the data look like:</p>

<pre><code># Peak at the first six lines of the data
head(study_data)

  Patient_ID   Institution Gender     Race Tumor_grade Pathologic_stage Treatment_arm Surgery_age Nodes_removed
1          1       Houston   Male Hispanic         One              P2b           Two          77           130
2          2 San Francisco Female Hispanic       Three               Pa           Two          38           112
3          3      New York Female    Black        Four               P0          Four          90            90
4          4       Chicago   Male Hispanic         Two              Pis          Four          46             4
5          5       Houston Female    Black        Four              P2a          Four          96           114
6          6      New York   Male    Black       Three              P3b          Four          92             7
</code></pre>

<h3>My interest</h3>

<p>I am interested in learning more about what variables are associated with the number of lymph nodes removed during the surgery. My first thought was to simply stratify the data by a particular variable and then calculate the median number of nodes removed.</p>

<p>For example, to see if the institution at which the surgery was performed mattered, I could write:</p>

<pre><code>cbind(do.call(rbind, by(study_data$Nodes_removed, study_data$Institution, summary)))

              Min. 1st Qu. Median  Mean 3rd Qu. Max.
Chicago          1   25.50   65.5 64.48   98.75  129
Houston          1   40.00   71.0 69.26  100.00  130
New York         4   36.00   67.0 67.96  100.00  129
San Francisco    3   36.75   61.0 65.76   99.00  127
</code></pre>

<p>This lets me compare the median nodes removed in each institutional city.</p>

<h3>My question</h3>

<p>I would like to fully examine the association between all of my variables and the outcome <code>Nodes_removed</code>.</p>

<ol>
<li>Should I just do these simple summary statistics for all of my variables?</li>
<li>Do I need to perform some sort of hypothesis test for all of the associations to say whether or not the summary statistics differ? For example, should I calculate a median and a confidence interval for each comparison?</li>
<li>Or should I be using t-tests to compare one group to another?</li>
<li>In the case of a multi-level variable, should I use ANOVA?</li>
<li>Is there any role for linear regression analysis here? </li>
<li>If I wanted to build a single model that includes every possible predictor variable, what method should I use?</li>
</ol>

<p>For example, say that I am most interested in the association between the age at which the surgery was performed, <code>Surgery_age</code>, and <code>Nodes_removed</code>. However, I would like to adjust this association for potential confounders like gender, race, tumor grade, treatment arm, etc. What is the best way for me to do this?</p>

<p>Thanks for any advice you can give!</p>
"
"0.102966471440162","0.108821437516502"," 58700","<p>I am having some trouble running an Anova on categorical variables in R and matching SPSS output. What I need to do is run an anova on the dataset below (its a made up data set).  But, I need to know if the mean of each category is significantly from the total mean of all races.  </p>

<pre><code>Satisfaction    Race
3   Asian
4   Cacasion
5   African American
2   Other 
5   African American
3   African American
4   African American
5   African American
2   Asian
3   African American
1   Cacasion
1   Cacasion
1   Cacasion
5   Other 
5   Other 
5   Other 
5   African American
5   Asian
4   Asian
5   Other 
5   Other 
5   Other 
1   Cacasion
4   Cacasion
</code></pre>

<p>For example, the mean of all races is 3.5 :</p>

<pre><code>&gt; mean(test$Satisfaction)
[1] 3.5 
</code></pre>

<p>What I would like to know is if the mean score for each race is significantly different from the total mean of 3.5 and the p-value.</p>

<p>I ran an Anova in R with the following model, but R will set one catagory as the refernce and test is against the others :</p>

<pre><code>&gt; lm.test &lt;- lm(test$Satisfaction ~ test$Race)
&gt; summary(lm.test)

Call:
lm(formula = test$Satisfaction ~ test$Race)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.5714 -1.0000  0.4286  0.8482  2.0000 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         3.8571     0.5023   7.679 2.18e-07 ***
test$RaceAsian     -0.6071     0.8330  -0.729   0.4745    
    test$RaceCacasion  -1.8571     0.7394  -2.512   0.0207 *  
test$RaceOther      0.7143     0.7103   1.006   0.3266    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 1.329 on 20 degrees of freedom
Multiple R-squared: 0.391,  Adjusted R-squared: 0.2997 
F-statistic:  4.28 on 3 and 20 DF,  p-value: 0.01732 
</code></pre>

<p>The output is telling me that the mean for African American is 3.8571 and is significantly different from the mean of the caucasian group.  It is not different from the mean of group Asian and Other.  </p>

<p>Is there a way for me to set the intercept to 3.5 in R and get significant compared to the mean and not the reference group.  Or should I be using another tests altogether?  My stats isn't that great so if its another tests a brief explain on which test and how to run it in R would be great.  </p>
"
"0.0886194286901087","0.0936585811581694"," 58918","<p>I have 3(treatment)x3(period)x6(sequence) crossover design data. I have used SAS PROC MIXED procedure and I got the results with following code:</p>

<pre><code>data cross;
input yield id treatment period sequence;
cards;
72  1   1   1   1
73  1   2   2   1
77  1   3   3   1
75  2   2   1   2
78  2   3   2   2
70  2   1   3   2
75  3   3   1   3
77  3   1   2   3
73  3   2   3   3
64  4   1   1   4
68  4   3   2   4
71  4   2   3   4
80  5   2   1   5
72  5   1   2   5
80  5   3   3   5
74  6   3   1   6
76  6   2   2   6
70  6   1   3   6
58  7   1   1   4
62  7   3   2   4
67  7   2   3   4
64  8   2   1   5
56  8   1   2   5
60  8   3   3   5
72  9   3   1   6
69  9   2   2   6
66  9   1   3   6
76  10  2   1   2
79  10  3   2   2
65  10  1   3   2
61  11  1   1   1
50  11  2   2   1
60  11  3   3   1
71  12  3   1   3
72  12  1   2   3
75  12  2   3   3
;
PROC MIXED data=cross;
CLASSES id treatment period sequence;
MODEL yield= sequence period treatment period*treatment / solution;
RANDOM id(sequence);
*lsmeans period/alpha=0.05 cl diff adjust=tukey;
run;
</code></pre>

<p>Results:</p>

<pre><code>               Effect              DF      DF    F Value    Pr &gt; F

               sequence             5       6       0.88    0.5479
               period               2      16       0.24    0.7863
               treatment            2      16       7.23    0.0058
               treatment*period     4      16       3.32    0.0368
</code></pre>

<p>But when I used <code>lme</code> function from <code>nlme</code> package in R, I got the following results:</p>

<p>R code:</p>

<pre><code>anova(cross &lt;- lme(yield ~ sequence +treatment + period + period*treatment,data=cross, random= ~1|id/sequence,correlation=NULL,method=""REML""))
</code></pre>

<p>Results:</p>

<pre><code>               numDF denDF   F-value p-value
(Intercept)        1    16 1086.7422  &lt;.0001
sequence           5     6    0.5755  0.7196
treatment          2    16    7.2270  0.0058
period             2    16    0.2441  0.7863
treatment:period   4    16    3.3186  0.0368
</code></pre>

<p>The results are same for treatment, period and treatment-period interaction but I got different results for sequence. And I don't know why? Has anyone got any idea?</p>
"
"0.130444267050272","0.137861698644752"," 59608","<p>I performed an experiment on coral colonies (10 colonies, randomly chosen in repeated measurements); manipulating aragonite/carbon chemistry and temperature (fixed effect) to analyse effects on the respiration rate (response variable). </p>

<p>I used the standard ""aov"" argument to test for significances: </p>

<pre><code>&gt; aovARAGT&lt;-aov(RR~HiC*HiT+SIZE, data=neu) #HiC = param. ARAG / HiT = param. T
&gt; summary(aovARAGT)
        Df  Sum Sq Mean Sq F value  Pr(&gt;F)   
HiC          1 0.02391 0.02391   2.088 0.17411   
HiT          1 0.19208 0.19208  16.769 0.00149 **
SIZE         1 0.02423 0.02423   2.116 0.17145   
HiC:HiT      1 0.00191 0.00191   0.167 0.69043   
Residuals   12 0.13746 0.01145                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Now I want to test for random effects on the corals respiration rate using a glm, size of the colonies is another variable I included.</p>

<p>I am unsure how to interpret the results and to say whether the individual animal has a significant effect on the response variable or not. My best guess is to compare a model with random effect with a model without random effects, but I don't have a clue how this comparison is made.</p>

<pre><code>&gt; coralglm&lt;-lme(RR~TEMP*ARAG+SIZE, random= ~ 1 | ANIMAL, data=neu, method=""ML"")
&gt; summary(coralglm)
Linear mixed-effects model fit by maximum likelihood
Data: neu 
AIC       BIC   logLik
-15.18325 -9.350752 14.59162

Random effects:
Formula: ~1 | ANIMAL
 (Intercept)  Residual
StdDev: 2.142837e-06 0.1025639

Fixed effects: RR ~ TEMP * ARAG + SIZE 
         Value Std.Error DF    t-value p-value
(Intercept)  2.2967729 2.5158883  7  0.9129073  0.3916
TEMP        -0.0906237 0.0865013  5 -1.0476570  0.3428
ARAG        -0.2221899 1.6720758  5 -0.1328827  0.8995
SIZE         0.0017142 0.0014612  7  1.1731477  0.2791
TEMP:ARAG    0.0087810 0.0572029  5  0.1535061  0.8840
Correlation: 
  (Intr) TEMP   ARAG   SIZE  
TEMP      -0.996                     
ARAG      -0.963  0.959              
SIZE       0.073 -0.155 -0.051       
TEMP:ARAG  0.958 -0.955 -0.999  0.046

Standardized Within-Group Residuals:
   Min         Q1        Med         Q3        Max 
-2.3657936 -0.2605501  0.2769216  0.7377278  1.1827805 

Number of Observations: 17
Number of Groups: 9 
</code></pre>

<p>From these two outputs a couple of questions arise:</p>

<ol>
<li>From the ANOVA output I read that only temperature has a significant effect on the response variable - correct?</li>
<li>Why is Temp not significant in the glm and are the outputs of these methodically very different approaches even comparable - do I use both for my statistics?</li>
<li>How do I find out if the individuality of the corals has a significant random impact on my response variable?</li>
</ol>
"
"0.125326797943307","0.121415466064296"," 59861","<p><strong>Data structure:</strong>
I have two datasets from two protected areas that differ in protection status. Both areas contain 43 and 37 sites each. </p>

<p><strong>Question:</strong>
I would like to know which test would be the best for testing whether the PA status has had an effect on:  </p>

<ol>
<li>the first axis of a PCoA (principal coordinates analysis) - i.e. species composition turnover (derived by constructing a bray curtis dissimilarity matrix) and </li>
<li>species richness per site (a continuous variable). </li>
</ol>

<p><strong>Problem:</strong>
I understand that there is pseudoreplication present in this as I only have two areas. From what I have read, it seems that I either have to use an ANCOVA / GLM / mixed-effect model, where I define PA status as both a random effect and a fixed effect. I intended to nest sites within PA, but it seems that as there is only one datapoint per site it will not work as a nested object. </p>

<p>For those familiar with R, here are some codes I have tried:</p>

<pre><code>pcoaPAanovadata1 &lt;- read.csv(""PCoA\\data\\
                              combined data PCoA axis 1 with distance variables.csv"", 
                              header=T)

str(pcoaPAanovadata1)
'data.frame': 80 obs. of 7 variables:
PCOA:    num -0.2215 -0.3521 -0.0611 0.3434 -0.3624 ...
PA.stat: Factor w/ 2 levels ""N"",""P"": 1 1 1 1 1 1 1 1 1 1 ...
village: num 33.6 33.7 39.9 37.9 34 ...
road:    num 4.18 3.8 0.89 0.1 3.43 5.49 1.86 5.04 0.79 0.88 ...
track:   num 8.11 6.48 3.11 2.71 4.49 5.35 1.25 4.03 7.62 6.77 ...
site:    Factor w/ 80 levels ""M1_11"",""M1_17"",..: 1 2 3 4 5 6 7 8 9 10 ...
rich:    num 3.27 1.79 7.31 0.82 1.79 1.82 2.45 0.82 5.47 2.79 ...
</code></pre>

<p>compare community composition turnover at different PAs:
below specifies a null model where the slope deviates as a result of the random effect </p>

<pre><code>z0 &lt;- lmer(rich ~ 1, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z0)
z1 &lt;- lme(rich ~ pastat, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z1)
anova(z0,z1)
</code></pre>

<p>impacts of distance variables:</p>

<pre><code>zz &lt;- lme(pcoa ~ road, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(zz)
</code></pre>

<p>The errors I get from the lme(linear mixed effect model):</p>

<pre><code>Warning message:
In pt(-abs(tTable[, ""t-value""]), tTable[, ""DF""]) : NaNs produced
</code></pre>

<p>The error I get from the ANOVA:</p>

<pre><code>Warning message:
In anova.lme(z0, z1) :
fitted objects with different fixed effects. REML comparisons are not meaningful.
</code></pre>

<p>Firstly, I was hoping to just clarify whether the test I am running is correct. Secondly, it'd be great if someone could tell me what the errors mean. I apologise if my question is poorly phrased, I am relatively new to R and the statistics I am using. </p>
"
"0.140119619801808","0.138214738143788"," 60108","<p>My question is very closely related to a previous post 
<a href=""http://stats.stackexchange.com/questions/51520"">Specifying the Error() term in repeated measures ANOVA in R</a>. However, I would like to get more insight into how to define the error term.</p>

<p>Suppose I have a two-way repeated ANOVA, The factor for between group effect is the Treatment (control vs. placebo), while Time is the within group effect measured repeatedly over 4 times (T1~T4). Patients ID are recorded as Subject. Here I borrowed the data from an example from the tutorial in <a href=""http://gjkerns.github.io/R/2012/01/20/power-sample-size.html"" rel=""nofollow"">http://gjkerns.github.io/R/2012/01/20/power-sample-size.html</a>
so the data looks like this</p>

<pre><code> Time Subject Method      NDI
 0min    1     Treat 51.01078
 15min   1     Treat 47.12314
 48hrs   1     Treat 26.63542
 96hrs   1     Treat 20.78196
 0min    2     Treat 42.61345
 15min   2     Treat 32.77171
</code></pre>

<p>To apply ANOVA:</p>

<pre><code>aovComp &lt;- aov(NDI ~ Time*Method + Error(Subject/Time), theData)
summary(aovComp)
Error: Subject
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Method     1    113   112.7   0.481  0.491
Residuals 58  13579   234.1              

Error: Subject:Time        
            Df Sum Sq Mean Sq F value  Pr(&gt;F)    
Time          3  13963    4654 103.789 &lt; 2e-16 ***
Time:Method   3   1221     407   9.074 1.3e-05 ***
Residuals   174   7803      45 
</code></pre>

<p>I have also tried the other error term:</p>

<pre><code>aovComp1 &lt;- aov(NDI ~ Time*Method + Error(Subject), theData)
summary(aovComp1)

Error: Subject      
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Method     1    113   112.7   0.481  0.491
Residuals 58  13579   234.1               

Error: Within
             Df Sum Sq Mean Sq F value  Pr(&gt;F)    
Time          3  13963    4654 103.789 &lt; 2e-16 ***
Time:Method   3   1221     407   9.074 1.3e-05 ***
Residuals   174   7803      45
</code></pre>

<p>Can someone help me explaining the differences between these two error terms? If the first term is the correct one, what does the results from the second error term mean?                  </p>

<p><strong>Update by @amoeba:</strong> The two outputs are the same so it seems that in this case there is no difference, but the question remains as to what is the difference <em>in principle</em>. Are <code>Error(subject)</code> and <code>Error(subject/time)</code> always the same thing?</p>
"
"0.0723574605292422","0.0573539334676404"," 60546","<p>I've calculated an ANOVA in R using the <code>aov()</code> function, then calculated it semi-manually using my own code. I'm interested in why my mean sum of squares between groups (0.634443) is not the same as that outputted by <code>aov()</code> (0.4145), and yet my mean sum of squares within groups is the same as that outputted by <code>aov()</code>?</p>

<pre><code># create fake data
set.seed(50)
x = data.frame(length=rnorm(9), 
               site = c(rep('a', 3), 
                        rep('b', 3), 
                        rep('c', 3)), stringsAsFactors=F)
# ANOVA summary
summary(aov(length ~ factor(site), x))
# Df Sum Sq Mean Sq F value Pr(&gt;F)
# factor(site)  2  0.829  0.4145   0.514  0.622
# Residuals     6  4.840  0.8067 

# calculate sum of squares within, between and total
sumlength &lt;- sum(x$length)   
    n = 3
    overallMean &lt;- sumlength/n
    aMean &lt;- mean(x$length[x$site==""a""])
    bMean &lt;- mean(x$length[x$site==""b""])
    cMean &lt;- mean(x$length[x$site==""c""]) 

ssw &lt;- sum((x$length[x$site == 'a'] - aMean) * (x$length[x$site == 'a'] - aMean)) + 
            sum((x$length[x$site == 'b'] - bMean) * (x$length[x$site == 'b'] - 
            bMean)) + sum((x$length[x$site == 'c'] - cMean) * (x$length[x$site == 
            'c'] - cMean))  
sst &lt;- sum((x$length - overallMean) * (x$length - overallMean))  
ssb &lt;- ((aMean - overallMean) * (aMean - overallMean) + (bMean - overallMean) * 
        (bMean - overallMean) + (cMean - overallMean) * (cMean - overallMean)) * 3

meanSsb &lt;- ssb/2     # 0.634443
meanSsw &lt;- ssw/6     # 0.8066979
</code></pre>
"
"0.109082976072021","0.126814318375447"," 60833","<p>I have a dependent variable which is measured in two different areas (DV1,DV2).  I want to see if the relationship my independent variable (IV) has with each DV is transferable to the other DV i.e. if we know the relationship between DV1 and IV, can this be used to successfully predict DV2, and vice versa.</p>

<p>People have advised me to carry out a MANOVA in order to do this, with Y containing DV1 and DV2 and x containing IV.  I am a little confused with MANOVA however, as I am not sure if it is telling me what I want to know.  Can someone please explain exactly what a MANOVA analysis can tell me about my data?  I have posted my two outputs below.  </p>

<p>Some code I have used:</p>

<pre><code>Y &lt;- cbind(DV1,DV2)
fit &lt;- manova(Y ~ E)

summary(fit)
</code></pre>

<p>gives me the output:</p>

<pre><code>                   Df  Pillai approx F num Df den Df    Pr(&gt;F)    
E                  1 0.40252    13.81      2     41 2.598e-05 ***
Residuals          42                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p><code>summary.aov(fit)</code> gives me:</p>

<pre><code>Response 1 :
                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    
E                  1  1.123 1.12299  27.826 4.341e-06 ***
Residuals          42  1.695 0.04036                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Response 2 :
                   Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
E                  1 0.41248 0.41248  7.5361 0.008862 **
Residuals          42 2.29884 0.05473                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>For reference, I am actually using a pgls based MANOVA using code from ""Outomuro, D., D.C. Adams, and F. Johansson. 2013. Evolution of wing shape in ornamented-winged damselflies. Evolutionary Biology.""  But I thought for illustratve purposes it would be easier to stick to the normal non-phylogenetic MANOVA.</p>
"
"0.0738495239084239","0.0936585811581694"," 62756","<p>I have 3 within-subject factors, namely <code>offset</code> (1px, ..., 5px), <code>side</code> (left, right) and <code>color</code> (red, green), which define the characteristics of the stimulus in a reaction time experiment. The DV is reaction time <code>RT</code>. The design is fully balanced. </p>

<p>I ran a repeated measures ANOVA in R, like this: </p>

<pre><code>options(contrasts = c(""contr.helmert"", ""contr.poly""))

simon.aov &lt;- aov(median.RT ~ color*side*offset + Error(VP / (color*side*offset)), data=dfa)
</code></pre>

<p>The results revealed a significant main effect of the <code>color</code>, as well as a significant interaction <code>color x side</code> and a significant 3-way interaction <code>color x side x offset</code>.<br>
My primary focus lies on the interactions. <strong>Specifically, I want to know on which of the 5 offsets (i.e. on which levels of the third factor) the 2-way interaction <code>color x side</code> reaches significance.</strong> </p>

<p>I am by no means familiar with post-hoc contrasts and multiple comparisons, but this question is the gist of the thesis that I'm working on. So my progress depends on an adequate test to examine this question. </p>

<p>I highly appreciate any help on which test to run, and how to do this most efficiently in R. </p>

<h2>Edit:</h2>

<p>I'm sorry I didn't provide any plots earlier.  </p>

<p>@John: Here is the plot you requested.</p>

<p><img src=""http://i.stack.imgur.com/Z6alg.png"" alt=""Plot 1 of 3-way anova results""> </p>

<p>However, I believe, that this following plot rather clarifies my question: </p>

<p><img src=""http://i.stack.imgur.com/wCtKH.png"" alt=""Plot 2 of 3-way anova results""></p>

<p>It seems like there is no <code>color x side</code> interaction at the first 3 levels of <code>offset</code>, but this interaction emerges at <code>offset</code> 4 and 5. This is what the plot seems to imply, however I don't know how to prove it statistically. </p>
"
"0.0723574605292422","0.0573539334676404"," 63357","<p>Consider a model with a continuous response variable and a categorical explanatory variable.  I appreciate that in R, a summary.lm output of an anova on this data gives you rows that represent the mean value of each factor level.  The significance stars represent the significance of the difference between the mean of each level and the ""intercept"", which represents the mean of the first level of the factor.</p>

<p>What I am wondering is what do significance stars on this intercept term represent?  Simply that the mean of this particular factor level is different from zero?</p>
"
"0.10232890201933","0.0811107105653813"," 63369","<p>I'm running mixed design ANOVA using R.  Somehow, I got incorrect values for the Df.   There are two factors: <code>time</code> (within subjects), and <code>task</code> (between subjects).  <code>Time</code> has four levels and <code>task</code> has three levels.  I expected to find Df 3 for <code>time</code> and Df 2 for <code>task</code>.  However, the ANOVA result I got showed 1 for the task Df under ""Error:subject"".  </p>

<p>Why does this happen?  </p>

<p>The dataset looks like this:</p>

<pre><code>&gt; data
subject  response  time  task  
1        1.0076499 time1 task1  
2        0.9939471 time1 task1  
3        0.9981924 time1 task1
4        0.9960073 time1 task1
5        1.0064204 time1 task1
6        0.9990992 time1 task1
7        0.9959636 time1 task1
8        0.9922857 time1 task2
9        1.0007059 time1 task2
10       0.9952433 time1 task2
11       0.9976965 time1 task2
12       0.9957487 time1 task2
13       1.0013376 time1 task2
14       0.9894985 time1 task2
15       1.0049207 time1 task3
16       0.9904670 time1 task3
17       1.0015606 time1 task3
18       1.0086701 time1 task3
19       0.9970305 time1 task3
20       0.9847713 time1 task3
21       0.9990247 time1 task3
1        1.0047615 time2 task1
2        0.9930625 time2 task1
3        1.0449758 time2 task1
4        1.0332383 time2 task1
5        1.0117085 time2 task1
6        0.9849849 time2 task1
7        0.9486615 time2 task1
8        0.9795459 time2 task2
9        0.9803583 time2 task2
</code></pre>

<p>The script I wrote is here:</p>

<pre><code>data = read.csv(""anova_data.csv"", header=T)
anova_mixed = aov(response~(task*time)+Error(subject/time)+task, data)
summary(anova_mixed)
</code></pre>

<p>Then, the result:</p>

<pre><code>Error: subject
     Df   Sum Sq  Mean Sq
task  1 0.001163 0.001163

Error: subject:time
     Df   Sum Sq   Mean Sq
time  3 0.002001 0.0006671

Error: Within
          Df   Sum Sq   Mean Sq F value  Pr(&gt;F)   


task       2 0.005760 0.0028798   6.628 0.00234 **
time       3 0.000273 0.0000909   0.209 0.88976   
task:time  6 0.001679 0.0002798   0.644 0.69468   
Residuals 68 0.029544 0.0004345                   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.16969328660358","0.179342528949657"," 63464","<p>I have an unbalanced repeated measures data set to analyse, and I've read that the way most statistical packages handle this with ANOVA (i.e. type III sum of squares) is wrong. Therefore, I would like to use a mixed effects model to analyse these data. I have read a lot about mixed models in <code>R</code>, but I am still very new to <code>R</code> and mixed effect models and not very confident I am doing things right. Note that I can't yet entirely divorce myself of ""traditional"" methods, and still need $p$-values and post hoc tests.</p>

<p>I would like to know if the following approach makes sense, or if I am doing something horribly wrong. Here's my code:</p>

<pre><code># load packages
library(lme4)
library(languageR)
library(LMERConvenienceFunctions)
library(coda)
library(pbkrtest)

# import data
my.data &lt;- read.csv(""data.csv"")

# create separate data frames for each DV &amp; remove NAs
region.data &lt;- na.omit(data.frame(time=my.data$time, subject=my.data$subject, dv=my.data$dv1))

# output summary of data
data.summary &lt;- summary(region.data)

# fit model
# ""time"" is a factor with three levels (""t1"", ""t2"", ""t3"")
region.lmer &lt;- lmer(dv ~ time + (1|subject), data=region.data)

# check model assumptions
mcp.fnc(region.lmer)

# remove outliers (over 2.5 standard deviations)
rm.outliers &lt;- romr.fnc(region.lmer, region.data, trim=2.5)
region.data &lt;- rm.outliers$data
region.lmer &lt;- update(region.lmer)

# re-check model assumptions
mcp.fnc(region.lmer)

# compare model to null model
region.lmer.null &lt;- lmer(dv ~ 1 + (1|subject), data=region.data)
region.krtest &lt;- KRmodcomp(region.lmer, region.lmer.null)

# output lmer summary
region.lmer.summary &lt;- summary(region.lmer)

# run post hoc tests
t1.pvals &lt;- pvals.fnc(region.lmer, ndigits=10, withMCMC=TRUE)

region.lmer &lt;- lmer(dv ~ relevel(time,ref=""t2"") + (1|subject), data=region.data)
t2.pvals &lt;- pvals.fnc(region.lmer, ndigits=10, withMCMC=TRUE)

region.lmer &lt;- lmer(dv ~ relevel(time,ref=""t3"") + (1|subject), data=region.data)
t3.pvals &lt;- pvals.fnc(region.lmer, ndigits=10, withMCMC=TRUE)

# Get mcmc mean and 50/95% HPD confidence intervals for graphs
# repeated three times and stored in a matrix (not shown here for brevity)
as.numeric(t1.pvals$fixed$MCMCmean)
as.numeric(t1.pvals$fixed$HPD95lower)
as.numeric(t1.pvals$fixed$HPD95upper)
HPDinterval(as.mcmc(t1.pvals$mcmc),prob=0.5)
    HPDinterval(as.mcmc(t1.pvals$mcmc),prob=0.5)
</code></pre>

<p>Some specific questions I have:</p>

<ol>
<li>Is this a valid way of analysing mixed effects models? If not, what
should I be doing instead.</li>
<li>Are the criticism plots output by mcp.fnc good enough for verifying
model assumptions, or should I be taking additional steps.</li>
<li>I get that for mixed models to be valid, the data need respect
assumptions of normality and homoscedasticity. How to I judge what
is ""approximately normal"" and what is not by looking at the
criticism plots generated by mcp.fnc? Do I just need to get a feel
for this, or is their a prescribed way of doing things? How robust
are mixed models in respect to these assumptions?</li>
<li>I need to assess differences between the three time points for ~20
characteristics (biomarkers) of the subjects in my sample. Is
fitting and testing separate models for each acceptable so long as I
report all undertaken tests (significant or not), or do I need any
form of correction for multiple comparisons.</li>
</ol>

<p>To be a little more precise in regards to the experiment, here are some more details. We followed a number of participants longitudinally as they underwent a treatment. We measured a number of biomarkers before the start of the treatment and at two time points after. What I'd like to see is if there are difference in these biomarkers between the three time points.</p>

<p>I am basing most of what I am doing here on this <a href=""http://www.bodo-winter.net/tutorial/bw_LME_tutorial.pdf"">tutorial</a>, but made some changes based on my needs and things I read. The changes I made are:</p>

<ol>
<li>relevel the ""time"" factor to obtain t1-t2, t2-t3, and t1-t3 comparisons with pvals.fnc (from the languageR package)</li>
<li>compare my mixed model to the null model using an approximate F-test based on a Kenward-Roger's approach (using the pbkrtest package) rather than a likelihood ratio test (because I read, that Kenward-Roger's is better regarded right now)</li>
<li>Use the LMERConvenienceFunctions package to check assumptions and remove outliers (because I read that mixed models are very sensitive to outliers)</li>
</ol>
"
"0.0808981002113217","0.0854981960070962"," 63649","<p>I have used <code>lme</code> and <code>ezAnova</code> to analyse data from a 2$\times$3 repeated-measures experiment. Theoretically those are two different ways to perform the same analysis. However, the resulting $F$-statisics and DF differ and I am lost in why.</p>

<p>Here is the exact data and output:
I have a data set with 2 independent variables (<code>marker_lang</code> and <code>congruency</code>) and the dependent variable <code>RT</code>: Both IV are repeated and completely crossed (thus 6 conditions overall). The data are not collapsed to cell means, meaning that per condition and subject I have several data points.</p>

<p>Here is what I did with ezAnova: </p>

<pre><code>ezANOVA(subset(data.mark.afc, !is.na(afc.RT)), dv=afc.RT, wid=subjectID, 
within=.(marker_lang,congruency), within_full=.(marker_lang,congruency), detailed=1, type=3)
</code></pre>

<p>And the output: </p>

<pre><code>$Anova
              Effect DFn DFd          SSn        SSd            F            p   p&lt;.05          ges
1            (Intercept)   1  24 84879098.819 1892110.06 1076.6278430 1.881814e-21     * 0.9762134497
2            marker_lang   1  24    36392.804   80595.30   10.8371986 3.071336e-03     * 0.0172922873
3             congruency   2  48    25426.393   47319.45   12.8960382 3.292730e-05     * 0.0121448066
4 marker_lang:congruency   2  48     1160.152   48150.91    0.5782581 5.647333e-01       0.0005606399
</code></pre>

<p>Here is what I did with lme:</p>

<pre><code>basemodel &lt;- lme(data=subset(cdata, !is.na(afc.RT)), afc.RT~1,
random=~1|subjectID/congruency/marker_lang, method=""ML"")

langmodel &lt;- update(basemodel, .~. + marker_lang)

angcongmodel &lt;- update(langmodel, .~. + congruency)

fulmodel &lt;- update(langcongmodel, .~. +marker_lang:congruency)
</code></pre>

<p>And the anova-tables for the lme analysis: </p>

<pre><code>anova(fulmodel)
                   numDF denDF   F-value p-value
(Intercept)                1  4715 1112.3468  &lt;.0001
marker_lang                1    72   24.8917  &lt;.0001
congruency                 2    48    8.3902  0.0008
marker_lang:congruency     2    72    0.4475  0.6410

anova(basemodel, langmodel, langcongmodel, fulmodel)
          Model df      AIC      BIC    logLik   Test   L.Ratio p-value
basemodel         1  5 64203.43 64235.88 -32096.72                         
langmodel         2  6 64185.06 64224.00 -32086.53 1 vs 2 20.366313  &lt;.0001
langcongmodel     3  8 64173.27 64225.19 -32078.64 2 vs 3 15.790082  0.0004
fulmodel          4 10 64176.38 64241.28 -32078.19 3 vs 4  0.892535  0.6400
</code></pre>

<p>I would expect the $F$, DF, and $p$-values for corresponding effects to be the same, which is not the case. This seems not an issue of different anova-types, as I tried out different types for <code>ezAnova</code>. None yield the same result as anova of <code>fulmodel</code>.</p>

<p>Any help/ideas will be greatly appreciated!</p>
"
"0.0511644510096651","0.0540738070435875"," 63669","<p>I currently am conducting a study where I have three variables: one that is binary, and two numerical ratio variables. Each of the subjects in my study has values for each of the three variables. </p>

<p>Variables:  </p>

<ul>
<li>Condition (binary): Values 0 and 1</li>
<li>Pre (ratio)</li>
<li>Post (ratio)</li>
</ul>

<p>I want to test if there is a significant difference between the the pre and post variables of the <code>0</code> control group and the <code>1</code> experimental group. Both groups have 103 subjects. The data meet all typical ANOVA assumptions such as normality and the like. I was thinking of nesting the variables as follows and then running a two-way ANOVA. </p>

<p>Variable 1 is <code>exposed</code> / <code>not</code> and variable 2 is <code>pre</code> / <code>post</code>. People are nested in Variable 1 (meaning that each person gives both pre and post information for either exposed or not exposed conditions)</p>

<p>Would this be the correct way to approach this problem? Also how would I implement this statistical analysis, preferably in R or SPSS?</p>
"
"0.0915257523912551","0.108821437516502"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"0.184476051596313","0.18746719611077"," 64010","<p>I am wondering what the exact relationship between partial $R^2$ and coefficients in a linear model is and whether I should use only one or both to illustrate the importance and influence of factors.</p>

<p>As far as I know, with <code>summary</code> I get estimates of the coefficients, and with <code>anova</code> the sum of squares for each factor - the proportion of the sum of squares of one factor divided by the sum of the sum of squares plus residuals is partial $R^2$ (the following code is in <code>R</code>).</p>

<pre><code>library(car)
mod&lt;-lm(education~income+young+urban,data=Anscombe)
    summary(mod)

Call:
lm(formula = education ~ income + young + urban, data = Anscombe)

Residuals:
    Min      1Q  Median      3Q     Max 
-60.240 -15.738  -1.156  15.883  51.380 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -2.868e+02  6.492e+01  -4.418 5.82e-05 ***
income       8.065e-02  9.299e-03   8.674 2.56e-11 ***
young        8.173e-01  1.598e-01   5.115 5.69e-06 ***
urban       -1.058e-01  3.428e-02  -3.086  0.00339 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 26.69 on 47 degrees of freedom
Multiple R-squared:  0.6896,    Adjusted R-squared:  0.6698 
F-statistic: 34.81 on 3 and 47 DF,  p-value: 5.337e-12

anova(mod)
Analysis of Variance Table

Response: education
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
income     1  48087   48087 67.4869 1.219e-10 ***
young      1  19537   19537 27.4192 3.767e-06 ***
urban      1   6787    6787  9.5255  0.003393 ** 
Residuals 47  33489     713                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The size of the coefficients for 'young' (0.8) and 'urban' (-0.1, about 1/8 of the former, ignoring '-') does not match the explained variance ('young' ~19500 and 'urban' ~6790, i.e. around 1/3).</p>

<p>So I thought I would need to scale my data because I assumed that if a factor's range is much wider than another factor's range their coefficients would be hard to compare:</p>

<pre><code>Anscombe.sc&lt;-data.frame(scale(Anscombe))
mod&lt;-lm(education~income+young+urban,data=Anscombe.sc)
summary(mod)

Call:
lm(formula = education ~ income + young + urban, data = Anscombe.sc)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.29675 -0.33879 -0.02489  0.34191  1.10602 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.084e-16  8.046e-02   0.000  1.00000    
income       9.723e-01  1.121e-01   8.674 2.56e-11 ***
young        4.216e-01  8.242e-02   5.115 5.69e-06 ***
urban       -3.447e-01  1.117e-01  -3.086  0.00339 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5746 on 47 degrees of freedom
Multiple R-squared:  0.6896,    Adjusted R-squared:  0.6698 
F-statistic: 34.81 on 3 and 47 DF,  p-value: 5.337e-12

anova(mod)
Analysis of Variance Table

Response: education
          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
income     1 22.2830 22.2830 67.4869 1.219e-10 ***
young      1  9.0533  9.0533 27.4192 3.767e-06 ***
urban      1  3.1451  3.1451  9.5255  0.003393 ** 
Residuals 47 15.5186  0.3302                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1    
</code></pre>

<p>But that doesn't really make a difference, partial $R^2$ and the size of the coefficients (these are now <em>standardized coefficients</em>) still do not match:</p>

<pre><code>22.3/(22.3+9.1+3.1+15.5)
# income: partial R2 0.446, Coeff 0.97
9.1/(22.3+9.1+3.1+15.5)
# young:  partial R2 0.182, Coeff 0.42
3.1/(22.3+9.1+3.1+15.5)
# urban:  partial R2 0.062, Coeff -0.34
</code></pre>

<p><strong>So is it fair to say that 'young' explains three times as much variance as 'urban' because partial $R^2$ for 'young' is three times that of 'urban'?</strong> Why is the coefficient of 'young' then not three times that of 'urban' (ignoring the sign)?</p>

<p>I suppose the answer for this question will then also tell me the answer to my initial query: Should I use partial $R^2$ or coefficients to illustrate the relative importance of factors? (Ignoring direction of influence - sign - for the time being.)</p>

<p><strong>Edit:</strong></p>

<p>Partial eta-squared appears to be another name for what I called partial $R^2$. <a href=""http://www.inside-r.org/packages/cran/heplots/docs/etasq"">etasq {heplots}</a> is a useful function that produces similar results:</p>

<pre><code>etasq(mod)
          Partial eta^2
income        0.6154918
young         0.3576083
urban         0.1685162
Residuals            NA
</code></pre>
"
"0.149399459174937","0.149122807017544"," 64715","<p>Using R, I created groups of individuals with trait values. Then I simulated a treatment that modified their trait value (see below). Finally I run a one-way Anova on them using the individuals traits value as dependent variable and the groups as independent variable. I want to know how does my <strong>treatment magnitude can be translated into Effect Size</strong> in order to compare my results with what I would expect using the pwr package. I don't want to measure the effect size afterward, I want to know what effect size did I simulated. I'm not sure of which words I should use, I might have better to call it the expected effect size given the treatment I simulated.</p>

<p>Here is the definition of effect size for Anova as defined by Cohen.</p>

<pre><code>Effect.size = sqrt(Epsilon.over.all.i(p.i *(mu.i - mu)^2)/sigma.square)
    mu.i = the mean of the group i
    mu = the mean of the overall
    p.i = n.i / N
    n.i = the number of individuals in group i
    N = the total number of individuals
    sigma square = the error variance within groups
</code></pre>

<p>You'll find it more explicitly written directly on my <a href=""http://www.statmethods.net/stats/power.html"" rel=""nofollow"">source</a> (one third of the page in the ANOVA section)</p>

<p>Here is my R script:</p>

<pre><code># Create individuals and groups.    
N=100 # sample size
nb.groups = 4
sd=1
My.data = data.frame(
group=rep(1:nb.groups,n/nb.groups),
Trait=rnorm(n=N, mean=0,sd=sd)
)

# Simulate a treatment

Treatment = 0.55
</code></pre>

<p>Way 1 of simulating a treatment      </p>

<pre><code># Adding a fixed value if Treatment = 4, nb.groups=4:
  # group 1: nothing is added
  # group 2: 1/3 * 4 is added
  # group 3: 2/3 * 4 is added
  # group 4: 3/3 * 4 is added

for (i in 2:nb.groups){
     My.data[which(My.mat[,1]==i),2] = 
     My.data[which(My.mat[,1]==i),2] + Treatment*((i-1)/(nb.groups-1))
}
</code></pre>

<p>Way 2 of simulating a treatment    </p>

<pre><code># Adding a value drawn from a random distribution which mean equals the fixed value we added in Way 1 and sd always equals my parameter ""sd""

for (i in 1:nb.groups){
     My.data[which(My.mat[,1]==i),2] =
     My.data[which(My.mat[,1]==i),2] + rnorm(N/nb.groups,mean=Treatment*((i-1)/(nb.groups-1)),sd=sd)
}
</code></pre>

<p>I repeat my question: Both for way 1 and way 2, I'm trying to find a function that ""translate"" the parameter ""Treatment"" into the common ""Effect size"".</p>

<p>In order to use the above formula for effect-size, I must be able to infer the expected mu.i, mu and sigma.square but I don't know how to do that !</p>

<p>Here is the ANOVA I ran:</p>

<pre><code>aov(my.data$Trait~my.data$group)
</code></pre>

<p>P.s. I already posted this question but did not get any answer or comment. So I try again. Please let me know if my question is unanswerable.</p>
"
"0.0886194286901087","0.0624390541054463"," 66378","<p>One often hears to say <em>""more than 70% variability is explained by ...""</em> What exactly is meant by this? Th proportion of the sum of squares (SSE), or mean sum of squares (MSE)? For example in the following anova table:</p>

<pre><code>                                    Df Sum Sq Mean Sq F value Pr(&gt;F)    
as.factor(site)                    444   8357   18.82   163.1 &lt;2e-16 ***
as.factor(year)                     12    569   47.43   410.9 &lt;2e-16 ***
as.factor(month)                     5    863  172.53  1494.8 &lt;2e-16 ***
as.factor(year):as.factor(month)    60    769   12.82   111.1 &lt;2e-16 ***
Residuals                        34188   3946    0.12                   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
7176 observations deleted due to missingness
</code></pre>

<p>could we say that most of the variability was explained by <code>site</code>? We see that site covers most of the SSE but as there is a lot of sites, the MSE for site is almost the lowest in the table. </p>

<p>And how would I interpret this in practice? I want to know where is the variability, whether it varies mostly accross time or space. Is the <code>site</code> actually the biggest source of variability, or is it a <code>month</code> and <code>year</code>? Shall I read SSE or MSE column for this purpose?</p>

<p>PS: please note I am not a professional statistician, so if you are about to respond with a lot of math then please make also some simple summary for dummies :-)</p>
"
"0.10232890201933","0.108147614087175"," 66745","<p>So I've been reading various online tutorials and I've not come across and example which is similar to what I'm trying to do. I've thought up this example because I was struggling to explain with words alone. It's really basic but hopefully illustrates the concept:</p>

<pre><code>movie_eg=data.frame(score_children=c(1.1,2.5,3.9,1.5,4.3),
                    score_adults=c(12.5,13.1,18.1,13.1,18.8),
                    brad_hrs=c(0.1,0.2,0.3,0.1,0.6),
                    angelina_hrs=c(0.2,0.6,0.8,0.9,0.1),
                    cgi_budget=c(0.9,0.1,0.2,0.3,0.8),
                    seat_cost=c(0.1,0.3,0.6,0.1,0.9))
rownames(movie_eg)=c(""shrek"",""bourne"",""lockstock"",""scream"",""sharknado"")
</code></pre>

<p>So in reality I have over 400 ""movies"" each with 2 continuous outcomes (""scores"") and 4 predictive measurements (""brad"",""ang"",""budget"",""seats"") which are also continuous. The two ""scores"" are moderately correlated across ""movies"".</p>

<p>I'd like to see if there is a strong connection between any/all of the predictive values and the two outcomes - so here, what (if anything) influences both ""scores""?</p>

<p>Therefore, I believe that the ""dependent variables"" are the two outcomes and the ""independent variables"" are the four predictions.</p>

<p>So I would do:
<code>summary.aov(manova(cbind(brad_hrs,angelina_hrs,cgi_budget,seat_cost)~cbind(score_children,score_adults),data=movie_eg))</code></p>

<p>and then see all P values are greater than 0.05, so nothing had a strong effect on score? But that seat_cost was the best predictor (although still not significant)? </p>

<pre><code>Response seat_cost :
                                    Df  Sum Sq  Mean Sq F value  Pr(&gt;F)  
cbind(score_children, score_adults)  2 0.45417 0.227085  17.583 0.05381 .
Residuals                            2 0.02583 0.012915                   
</code></pre>

<p>(Probably worth mentioning in reality the ""scores"" are measures of drug response and the predictions are measures of different cellular functions, so really nothing to do with movies!)</p>
"
"0.130778311815021","0.138214738143788"," 67643","<p>I have an experiment where several subjects (subjects $= S_1,S_2,...,S_m $) were asked to perform a set of tasks (tasks $= T_1, T_2, T_3,...,T_n$) using both their left ($L$) and right ($R$) arms. Each task for each arm was repeated $r$ times. The response is measured in a variable called 'measure'. Unfortunately, there were some tasks and repetitions missing. </p>

<p>I tried using the aov function in R to perform a repeated measures ANOVA analysis, but later found out that this is not appropriate for unbalanced designs. Here is the sample of what I did:</p>

<pre><code>&gt; summary(aov(measure ~ arm*task + Error(subject/(arm*task)), data=all_data))

Error: subject
          Df Sum Sq Mean Sq F value Pr(&gt;F)
arm        1 0.3240  0.3240   0.398  0.573
task       4 0.1426  0.0357   0.044  0.994
Residuals  3 2.4397  0.8132               

Error: subject:arm
          Df Sum Sq Mean Sq F value Pr(&gt;F)  
arm        1 0.0023 0.00234   0.074 0.8027  
task       4 0.9972 0.24931   7.941 0.0601 .
arm:task   1 0.0112 0.01117   0.356 0.5928  
Residuals  3 0.0942 0.03139                 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Error: subject:task
          Df Sum Sq Mean Sq F value Pr(&gt;F)
task       5 0.3898 0.07795   1.652  0.172
arm:task   4 0.1482 0.03706   0.785  0.542
Residuals 35 1.6511 0.04718               

Error: subject:arm:task
          Df Sum Sq  Mean Sq F value Pr(&gt;F)
arm:task   5 0.0352 0.007032   0.351  0.878
Residuals 35 0.7013 0.020036               

Error: Within
           Df Sum Sq  Mean Sq F value Pr(&gt;F)
Residuals 203  1.288 0.006345               
Warning message:
In aov(measure ~ arm * task + Error(subject/(arm * task)),  :
  Error() model is singular
</code></pre>

<p>Why am I getting the warning message: ""Error() model is singular""?</p>

<p>I also found that for unbalanced design it better to use the function anova() from the ""car"" package. I tried to search for the documentation for anova(), but I am very confused about how to use this function. Which model must I use for testing the following hypothesis:
1. Within-subjects there is no difference between the left and right arms.
2. Within-subjects there is no difference between the different tasks.
3. There is no interaction effect between arm and task within a subject.</p>

<p>It would also be very helpful if someone could point me to suitable books that can help me learn about these concepts.</p>
"
"0.0723574605292422","0.0764719112901873"," 67897","<p>using this code</p>

<pre><code>library(MASS)
n = c(300, 200, 100) 
group = rep(1:3, n)
x=c(rnbinom(300, size=2, mu=2.47), rnbinom(200, size=2, mu=2.27),
rnbinom(100, size=2, mu=2.27))
glm1 = glm.nb(x ~ factor(group))
</code></pre>

<p>I have created three neg. binomial distributions and analysed them using a glm. The independent variable (group) is meant to be categorial. When I use <code>anova(glm1)</code>, I get an p-value (Pr (>Chi)) of 0.07. However, when I use <code>summary(glm1)</code>, I get a p-value (Pr (>|z|) of 0.2 for factor 2 and 0.02 for factor 3. Thus, depending on if I use <code>summary()</code> or <code>anova()</code>, the factor is significant or it's not. I have three questions and would be very glad if someone could help me with one of these.</p>

<ol>
<li>Can I conclude if group is a significant factor or not?</li>
<li>Residuals are not distributed normally. Can I use <code>anova(glm1)</code>
nonetheless?</li>
<li>Apparently <code>summary()</code> gives me an intercept and two factors. Do the
two factors relate to the difference between group 1 and 2
respectively between group 1 and 3?</li>
</ol>

<p>I would really appreciate your help, even only a small hint on one of these question would be great. Please tell me, if any information is missing or if I didn't express myself comprehensibly.</p>
"
"0.0957199230302734","0.101162829777814"," 68104","<p>I have observations of four different groups of people. The dependent variable are count data (medical emergencies in the past 2 months). For each group, the dependent variable follows a negative binomial distribution with a maximum at 0. Now I would like to examine, if the factor ""group"" (meant to be categorical) is a significant factor for the number of reported medical emergencies. What I have done so far: I conducted a glm using <code>glm.nb</code> in R and examined the result of <code>glm.nb</code> using <code>anova()</code>. I would really appreciate, if someone could confirm for me, if this procedure is feasible for a negative binomial distributed response variable and a categorical factor. </p>

<p>My second question: <code>anova()</code> with the result of the <code>glm.nb</code> produces the warning: <code>tests made without re-estimating 'theta'</code>. As far as I understand, theta is a dispersion parameter for the negative binomial distribution. However, the distributions of observations in each of the four groups have very different dispersions. Does R calculate a ""mean"" theta for all four distributions? </p>
"
"0.10232890201933","0.0946291623262781"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.130444267050272","0.116652206545559"," 71058","<p>I'm struggling with interpreting the output from R for a two-way repeated measures ANOVA.
I used <code>m1.emax.aov &lt;- aov(mean_emax_norm ~ (cond5*stride) + Error(dog_id/(cond5*stride)), df3.emg.m1)</code> and the summary tables are throwing me.</p>

<p>I'm used to just seeing a summary table with the main effects and interaction listed once; each with their own error term. I don't know how to interpret the output below. I did not expect to see cond5 for Error:dog_id, stride for Error:dog_id:cond5, or cond5:stride for Error:dog_id:stride.</p>

<p>How do you interpret, for instance, stride is significant for Error:dog_id:cond5 but not for Error:dog_id:stride?</p>

<p>Any guidance would be greatly appreciated.</p>

<pre><code>summary(m1.emax.aov)

Error: dog_id
      Df Sum Sq Mean Sq F value Pr(&gt;F)  
cond5      2  706.5   353.3   12.69  0.011 *
Residuals  5  139.1    27.8                 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: dog_id:cond5
          Df Sum Sq Mean Sq F value Pr(&gt;F)   
cond5      4  48.30   12.07   3.453 0.0216 * 
stride     1  36.07   36.07  10.316 0.0035 **
Residuals 26  90.91    3.50                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: dog_id:stride
             Df Sum Sq Mean Sq F value Pr(&gt;F)
stride        2   7.42   3.709   0.343  0.717
cond5:stride  3   9.99   3.331   0.308  0.819
Residuals    11 119.02  10.820               

Error: dog_id:cond5:stride
             Df Sum Sq Mean Sq F value Pr(&gt;F)  
cond5:stride  8  103.8  12.975   2.498 0.0224 *
Residuals    52  270.1   5.194                 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.135670238492329","0.143384833669101"," 71914","<p>Hopefully this is a question that someone here can answer for me on the nature of decomposing sums of squares from a mixed-effects model fit with <code>lmer</code> (from the <a href=""http://cran.r-project.org/web/packages/lme4/index.html"">lme4</a> R package).</p>

<p>First off I should say that I am aware of the controversy with using this approach, and in practise I would be more likely to use a bootstrapped LRT to compare models (as suggested by Faraway, 2006). However, I am puzzled at how to replicate the results, and so for my own sanity I thought I would ask here.</p>

<p>Basically, I am getting to grips with using mixed-effects models fit by the <code>lme4</code> package. I know that you can use the <code>anova()</code> command to give a summary of sequentially testing the fixed-effects in the model. As far as I know this is what Faraway (2006) refers to as the 'Expected mean squares' approach. What I want to know is how are the sums of squares calculated?</p>

<p>I know that I could take the estimated values from a particular model (using <code>coef()</code>), assume that they are fixed, and then make tests using the sums of squares of model residuals with and without the factors of interest. This is fine for a model containing a single within-subject factor. However, when implementing a split-plot design the sums of squares value I get is equivalent to the value produced by R using <code>aov()</code> with an appropriate <code>Error()</code> designation. However, this is <em>not</em> the same as the sums of squares produced by the <code>anova()</code> command on the model object, despite the fact that the F-ratios are the same. </p>

<p>Of course this makes complete sense as there is no need for the <code>Error()</code> strata in a mixed-model. However, this must mean that the sums of squares are penalised somehow in a mixed-model in order to provide appropriate F-ratios. How is this achieved? And how does the model somehow correct the between-plot sum of squares but not correct the within-plot sum of squares. Evidently this is something that is necessary for a classical split-plot ANOVA that was achieved by designating different error values for the different effects, so how does a mixed-effect model allow for this?</p>

<p>Basically, I want to be able to replicate the results from the <code>anova()</code> command applied to a lmer model object myself to verify the results and my understanding, however, at present I can achieve this for a normal within-subject design but not for the split-plot design and I can't seem to find out why this is the case.   </p>

<p>As an example:</p>

<pre><code>library(faraway)
library(lme4)
data(irrigation)

anova(lmer(yield ~ irrigation + variety + (1|field), data = irrigation))

Analysis of Variance Table
           Df Sum Sq Mean Sq F value
irrigation  3 1.6605  0.5535  0.3882
variety     1 2.2500  2.2500  1.5782

summary(aov(yield ~ irrigation + variety + Error(field/irrigation), data = irrigation))

Error: field
           Df Sum Sq Mean Sq F value Pr(&gt;F)
irrigation  3  40.19   13.40   0.388  0.769
Residuals   4 138.03   34.51               

Error: Within
          Df Sum Sq Mean Sq F value Pr(&gt;F)
variety    1   2.25   2.250   1.578  0.249
Residuals  7   9.98   1.426               
</code></pre>

<p>As can be seen above all the F-ratios agree. The sums of squares for variety also agree. However, the sums of squares for irrigation do not agree, however it appears the lmer output is scaled. So what does the anova() command actually do?</p>
"
"0.0886194286901087","0.0624390541054463"," 72202","<p>I'm struggling to extract F-values and/or p-values from an ANOVA with all random effects or an ANOVA with both mixed and random effects.</p>

<p>I would expect to be able to get F- and p-values regarding the variance of the random effects based on the expected mean squares. Maybe the  <code>aov()</code> function just doesn't return what I'm looking for? Or I'm specifying my model incorrectly?</p>

<p>Here's a simple example where I want to have <code>cyl</code> as a fixed effect and <code>carb</code> as a random effect:</p>

<pre><code>data(mtcars)
summary(aov(formula = mpg ~ cyl + Error(carb), data = mtcars))
</code></pre>

<p>But no p-values for the random effect are reported.</p>

<pre><code>Error: carb
    Df Sum Sq Mean Sq
cyl  1  341.8   341.8

Error: Within
          Df Sum Sq Mean Sq F value  Pr(&gt;F)    
cyl        1  492.1   492.1   48.85 1.1e-07 ***
Residuals 29  292.2    10.1                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.177238857380217","0.156097635263616"," 72453","<p>I have a problem like the following:</p>

<p>1) There are six measurements for each individual with large within-subject variance </p>

<p>2) There are two groups (Treatment and Control)</p>

<p>3) Each group consists of 5 individuals</p>

<p>4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.</p>

<p>The data looks like this:
<img src=""http://i.stack.imgur.com/55V9J.png"" alt=""http://s10.postimg.org/p9krg6f3t/examp.png""></p>

<p>And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. <strong>This ignores within-subject variability</strong>:</p>

<pre><code> n.simulations&lt;-10000
    pvals=matrix(nrow=n.simulations,ncol=1)
    for(k in 1:n.simulations){
      subject=NULL
      for(i in 1:10){
        subject&lt;-rbind(subject,as.matrix(rep(i,6)))
      }
      #set.seed(42)

      #Sample Subject Means
      subject.means&lt;-rnorm(10,100,2)

      #Sample Individual Measurements
      values=NULL
      for(sm in subject.means){
        values&lt;-rbind(values,as.matrix(rnorm(6,sm,20)))
      }

      out&lt;-cbind(subject,values)

      #Split into GroupA and GroupB
      GroupA&lt;-out[1:30,]
      GroupB&lt;-out[31:60,]

      #Add effect size to GroupA
      GroupA[,2]&lt;-GroupA[,2]+0

      colnames(GroupA)&lt;-c(""Subject"", ""Value"")
      colnames(GroupB)&lt;-c(""Subject"", ""Value"")

      #Calculate Individual Means and SDS
      GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
      for(i in 1:length(unique(GroupA[,1]))){
        GroupA.summary[i,1]&lt;-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
        GroupA.summary[i,2]&lt;-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      }
      colnames(GroupA.summary)&lt;-c(""Mean"",""SD"")


      GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
      for(i in 1:length(unique(GroupB[,1]))){
        GroupB.summary[i,1]&lt;-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
        GroupB.summary[i,2]&lt;-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      }
      colnames(GroupB.summary)&lt;-c(""Mean"",""SD"")

      Summary&lt;-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
      colnames(Summary)[1]&lt;-""Group""

      pvals[k]&lt;-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
    }
</code></pre>

<p>And here is code for plots:</p>

<pre><code>#Plots
par(mfrow=c(2,2))
boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
        ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
        xlab=""Subject"", ylab=""Value"")
stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
#abline(h=mean(GroupA[,2]), lty=2, lwd=3)

for(i in 1:length(unique(GroupA[,1]))){
  m&lt;-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
  ci&lt;-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]

  points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.2,
           ci[1],i-.2,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
        ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
        xlab=""Subject"", ylab=""Value"")
stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
#abline(h=mean(GroupB[,2]), lty=2, lwd=3)

for(i in 1:length(unique(GroupB[,1]))){
  m&lt;-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
  ci&lt;-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]

  points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.2,
           ci[1],i-.2,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
        ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
        main=""Individual Averages"")
stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)

points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
segments(.9,
         t.test(GroupA.summary[,1])$conf.int[1],.9,
             t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
)

points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
segments(1.9,
         t.test(GroupB.summary[,1])$conf.int[1],1.9,
             t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
)
legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
     main=c(paste(""# sims="", n.simulations),
            paste(""% Sig p-values="",100*length(which(pvals&lt;0.05))/length(pvals)))
)
</code></pre>

<p>Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.</p>

<p>So what is the correct way to analyze this data?</p>

<p><strong>Bonus:</strong></p>

<p>The example above is a simplification. For the actual data: </p>

<p>1) The within-subject variance is positively correlated with the mean. </p>

<p>2) Values can only be multiples of two. </p>

<p>3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. </p>

<p>4) Number of Subjects in each group are not necessarily equal. </p>

<p>Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.</p>

<p><strong>EDIT:</strong></p>

<p>Ok, here is what <em>actual</em> data looks like. There is also three groups rather than two:</p>

<p><img src=""http://i.stack.imgur.com/k1xWd.png"" alt=""enter image description here""></p>

<p>dput() of data:</p>

<pre><code>structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
    NULL, c(""Group"", ""Subject"", ""Value"")))
</code></pre>

<p><strong>EDIT 2:</strong></p>

<p>In response to Henrik's answer:
So if I instead perform anova followed by TukeyHSD procedure on the individual averages as shown below, I could interpret this as underestimating my p-value by about 3-4x? </p>

<p>My goal with this part of the question is to understand how I, as a reader of a journal article, can better interpret previous results given their choice of analysis method. For example they have those ""stars of authority"" showing me 0.01>p>.001. So if i accept 0.05 as a reasonable cutoff I should accept their interpretation? The only additional information is mean and SEM.</p>

<pre><code>#Get Invidual Means
summary=NULL
for(i in unique(dat[,2])){
sub&lt;-which(dat[,2]==i)
summary&lt;-rbind(summary,cbind(
dat[sub,1][3],
dat[sub,2][4],
mean(dat[sub,3]),
sd(dat[sub,3])
)
)
}
colnames(summary)&lt;-c(""Group"",""Subject"",""Mean"",""SD"")

TukeyHSD(aov(summary[,3]~as.factor(summary[,1])+ (1|summary[,2])))

#      Tukey multiple comparisons of means
#        95% family-wise confidence level
#    
#    Fit: aov(formula = summary[, 3] ~ as.factor(summary[, 1]) + (1 | summary[, 2]))
#    
#    $`as.factor(summary[, 1])`
#             diff       lwr       upr     p adj
#    2-1 -0.672619 -4.943205  3.597967 0.9124024
#    3-1  7.507937  1.813822 13.202051 0.0098935
#    3-2  8.180556  2.594226 13.766885 0.0046312
</code></pre>

<p><strong>EDIT 3:</strong>
I think we are getting close to my understanding. Here is the simulation described in the comments to @Stephane:</p>

<pre><code>#Get Subject Means
means&lt;-aggregate(Value~Group+Subject, data=dat, FUN=mean)

#Initialize ""dat2"" dataframe
dat2&lt;-dat

#Initialize within-Subject sd
s&lt;-.001
pvals=matrix(nrow=10000,ncol=2)

for(j in 1:10000){
#Sample individual measurements for each subject
temp=NULL
for(i in 1:nrow(means)){
temp&lt;-c(temp,rnorm(6,means[i,3], s))
}

#Set new values
dat2[,3]&lt;-temp

#Take means of sampled values and fit to model
dd2 &lt;- aggregate(Value~Group+Subject, data=dat2, FUN=mean)
fit2 &lt;- lm(Value~Group, data=dd2)

#Save sd and pvalue
pvals[j,]&lt;-cbind(s,anova(fit2)[[5]][5])

#Update sd
s&lt;-s+.001
}

plot(pvals[,1],pvals[,2], xlab=""Within-Subject SD"", ylab=""P-value"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/gMMDY.png"" alt=""enter image description here""></p>
"
"0.0626633989716535","0.0441510785688348"," 73096","<p>This is a cross-post (<a href=""http://stackoverflow.com/questions/19432964/anova-error-in-levelsxx"">http://stackoverflow.com/questions/19432964/anova-error-in-levelsxx</a>) about an error I received in R while trying to run an ANOVA on my data. But error aside, I need help understanding why an ANOVA can't deal with my data and what other statistical models could be applied instead.</p>

<p>So here's my objective: I have 3 people (speaker) who recorded a bunch of words that I analyzed. The analysis yielded 3 continuous variables: skewness, kurtosis and Center of Gravity (CoG)*. I need to find out what combinations of these 3 variables best model the difference between each speaker. For example, are skewness and CoG together more significant than just CoG in finding the difference between speakers?</p>

<p>I have a basic knowledge of stats, but erring on the side of assuming I'm an idiot might be better for any complex explanations.</p>

<p>Thanks in advance!</p>

<ul>
<li>The skewness is a measure for how much the shape of the spectrum below the center of gravity is different from the shape above the mean frequency.</li>
<li>The kurtosis is a measure for how much the shape of the spectrum around the center of gravity is different from a Gaussian shape.</li>
<li>The center of gravity is a measure for how high the frequencies in a spectrum are on average weighted by their energy.</li>
</ul>
"
"0.130444267050272","0.137861698644752"," 74971","<p>When you collect data from participants in an experiment, sometimes you can collect repeated responses for <em>the same condition</em>, e.g., in R:</p>

<pre><code>set.seed(2012) # keep the example the same each time.

data.full &lt;- data.frame(id=gl(10, 4),
                        condition=gl(2, 40),
                        response=c(rnorm(40), rnorm(40, 1)))
head(data.full)

# Output:
#   id condition    response
# 1  1         1 -0.77791825
# 2  1         1 -0.57787590
# 3  1         1  0.66325605
# 4  1         1  0.08802235
# 5  2         1  1.25707865
# 6  2         1 -0.62977450
</code></pre>

<p>To analyse this (i.e. does condition predict response) I would normally take the mean response for each participant, for each condition. I would do this on the basis that we are supposed to be generalizing from a sample to a population, i.e. there should be one 'estimate' response from each participant for each condition, and the collection of these single responses (for each condition) is our sample, then we do an analysis which generalizes to the population.</p>

<p>I would transform the data e.g. like this:</p>

<pre><code>library(plyr)
data.means &lt;- ddply(data.full, .(id, condition),
                    summarize,
                    mean.response=mean(response))
head(data.means)

# Output:
#   id condition mean.response
# 1  1         1    -0.1511289
# 2  1         2     0.8658770
# 3  2         1     0.1510842
# 4  2         2     0.0129323
# 5  3         1     0.1857577
# 6  3         2     0.9859697
</code></pre>

<p>And then proceed with the within-subjects analysis (note the same process would apply if there were more conditions or a 2x2 design etc.), e.g.:</p>

<pre><code>aov1 &lt;- aov(mean.response ~ condition + Error(id/condition), data=data.means)
summary(aov1) # F = 4.2, p = .07, not significant
</code></pre>

<p>However, I've been told that with linear mixed-effects models, you can include all the underlying data on the basis that the lme models can include correlated data. My understanding was that they could include correlated data meant they could include responses from the same participants (within-subjects effects modelled as random effects), not that you could include the underlying data that gives the participant response estimate.</p>

<p>My question is, can you include the underlying data collected from the multiple responses of each participant in the <em>same condition</em>, i.e. can you do this:</p>

<pre><code>library(nlme)
lme1 &lt;- lme(response ~ 1, random= ~ 1|id/condition, data=data.full, method=""ML"")
lme2 &lt;- update(lme1, .~. + condition)
anova(lme1, lme2)

# X(1) = 3.19, p = .07, not significant
</code></pre>

<p>Or should you do this:</p>

<pre><code>lme1 &lt;- lme(mean.response ~ 1, random= ~ 1|id/condition, data=data.means, method=""ML"")
lme2 &lt;- update(lme1, .~. + condition)
anova(lme1, lme2)

# X(1) = 5.25, p = .02, significant
</code></pre>

<p>Which is the correct approach?</p>
"
"NaN","NaN"," 76059","<p>I am learning R and have been experimenting with analysis of variance.  I have been running both</p>

<pre><code>kruskal.test(depVar ~ indepVar, data=df)
</code></pre>

<p>and </p>

<pre><code>anova(lm(depVar ~ indepVar, data=dF))
</code></pre>

<p>Is there a practical difference between these two tests?  My understanding is that they both evaluate the null hypothesis that the populations have the same mean.  Thanks in advance</p>
"
"0.0511644510096651","0.0540738070435875"," 76609","<p>I am running an experiment testing reaction times under different conditions. I have a data sample located <a href=""http://chymera.eu/data/test/ER_aov.csv"" rel=""nofollow"">here</a> and I have added a graphical plot of my data below in order to ease your understanding:</p>

<p><img src=""http://i.stack.imgur.com/owWXn.png"" alt=""enter image description here""></p>

<p>I would like to check whether weak emotion recognition has significantly higher error rates than all other conditions. I was told that the proper way to do this was a repeated measurement ANOVA. I have found out that this can be done via R's <code>stats::aov()</code> function.</p>

<p>I am interfacing with R via RPy and you may see my exact code under <a href=""http://nbviewer.ipython.org/urls/gist.github.com/TheChymera/7474588/raw/a8244233365b7cce81e58457a6571325daecc1cf/ER-aov"" rel=""nofollow"">this notebook</a>.</p>

<p>I am getting the following resulting summary:</p>

<pre><code>Error: ID
          Df Sum Sq  Mean Sq F value Pr(&gt;F)
Residuals  6  0.022 0.003666               

Error: Within
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)   
COI        6 0.02628 0.004379   3.468 0.0083 **
Residuals 36 0.04547 0.001263                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>How does this help me address my issue?</p>

<p>Additionally, in a discussion resulting from <a href=""http://stats.stackexchange.com/questions/74967/data-in-one-condition-significantly-different-from-data-in-other-conditions-test"">this other question</a> I have been told that while anova is acceptable in some cases (such as this one) linear models such as the one generated by <code>nlme::lme()</code> are preferable.</p>

<p>I have used that function in <a href=""http://nbviewer.ipython.org/urls/gist.github.com/TheChymera/7396334/raw/187786dc7375b6a4255b2b54fc4d269bd846f32d/nlme-aov"" rel=""nofollow"">this other notebook</a>, and the output reads as follows:</p>

<pre><code>Fixed effects: ER ~ COI 
                 Value  Std.Error DF  t-value p-value
(Intercept) 0.01928571 0.01514819 36 1.273137  0.2111
COIem-hard  0.07028571 0.01899579 36 3.700069  0.0007
COIsc-11    0.00403687 0.01899579 36 0.212514  0.8329
COIsc-15    0.01700000 0.01899579 36 0.894935  0.3768
COIsc-19    0.00417857 0.01899579 36 0.219974  0.8271
COIsc-23    0.00432488 0.01899579 36 0.227676  0.8212
COIsc-27    0.00417857 0.01899579 36 0.219974  0.8271
</code></pre>

<p>how am I to interpret those p-values in the context of the point I'm trying to make?
Also, why is my first COI (COIem-easy) absent from the list?</p>

<p>As a general point I would also be very happy to hear which of these 2 approaches you advise I should use. </p>
"
"0.0723574605292422","0.0573539334676404"," 76733","<p>I am trying to determine whether the data depicted in the following figure shows a plateau phase for higher scramblings.</p>

<p><img src=""http://i.stack.imgur.com/QaBpZ.png"" alt=""enter image description here""></p>

<p>As I have often been encouraged to use linear models instead of anova - I have decided to tackle this issue with lme4.</p>

<p>The output I get for my latex document from the <a href=""http://cran.r-project.org/web/packages/texreg/index.html"" rel=""nofollow"">texreg</a> <a href=""http://cran.r-project.org/web/packages/lme4/index.html"" rel=""nofollow"">lme4</a> parser is as follows:</p>

<pre><code>                     Model 1 
</code></pre>

<p>(Intercept)              : 2.03 &nbsp;&nbsp; [1.78; 2.28]<sup>*</sup>


      <br></p>

<p>COIsc-14                 : -0.23 &nbsp;&nbsp; [-0.38; -0.08]<sup>*</sup>


   <br></p>

<p>COIsc-18                 : -0.23 &nbsp;&nbsp; [-0.38; -0.08]<sup>*</sup>


   <br></p>

<p>COIsc-22                 : -0.19 &nbsp;&nbsp; [-0.34; -0.04]<sup>*</sup>


   <br></p>

<p>COIsc-26                 : -0.32 &nbsp;&nbsp; [-0.47; -0.17]<sup>*</sup>


   <br></p>

<p>COIsc-6                  : 0.45 &nbsp;&nbsp; [0.30; 0.60]<sup>*</sup>


      <br></p>

<p>AIC                      : 1697.17                        <br></p>

<p>BIC                      : 1735.04                        <br></p>

<p>Log Likelihood           : -840.59                        <br></p>

<p>Deviance                 : 1681.17                        <br></p>

<p>Num. obs.                : 840                            <br></p>

<p>Num. groups: ID          : 7                              <br></p>

<p>Variance: ID.(Intercept) : 0.08                           <br></p>

<p>Variance: Residual       : 0.41                           <br></p>

<p><sup>*</sup>0 outside the confidence interval</p>

<pre><code>                     Model 1 
</code></pre>

<p>My confidence interval is 0.95 . Can I make the point I am trying to make by the fact that the 14, 18, 22, and 26 values are all in each other's confidence intervals?</p>

<p>Also, what information do I get from </p>

<pre><code>Variance: ID.(Intercept) : 0.08 

Variance: Residual : 0.41 
</code></pre>

<p>? Does that mean that 0.51 of my variance is explained by my categories, 0.08 by my IDs (which I define as random) and 0.41 by unidentified sources? 0.41 sounds like a lot, is this a good model then?</p>
"
"0.0626633989716535","0.0441510785688348"," 76814","<p>I want to use both anova and linear models to test the assumption that my some of my categories have different means than the rest.</p>

<p>I am using stats::aov for anova and nlme::lme for linear modelling. The full code is available in <a href=""http://nbviewer.ipython.org/urls/gist.github.com/TheChymera/7516431/raw/e9a80f5a2f21c6097b182bf4d6b086a5e7e76048/ER-nlm-aov"" rel=""nofollow"">this notebook</a>.</p>

<p>Basically I end up getting:</p>

<pre><code>Error: ID
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)
Residuals  6 0.01198 0.001997               

Error: Within
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)  
COI        7 0.01926 0.002752    1.92 0.0903 .
Residuals 42 0.06020 0.001433 
</code></pre>

<p>and</p>

<pre><code>Fixed effects: ER ~ COI 
                      Value  Std.Error DF   t-value p-value
(Intercept)      0.00200000 0.01465712 42 0.1364525  0.8921
COIemotion-hard  0.05600000 0.02023699 42 2.7672098  0.0084
COIscrambling-06 0.04218045 0.02023699 42 2.0843242  0.0433
COIscrambling-10 0.02094737 0.02023699 42 1.0351029  0.3065
COIscrambling-14 0.00685714 0.02023699 42 0.3388420  0.7364
COIscrambling-18 0.02085714 0.02023699 42 1.0306445  0.3086
COIscrambling-22 0.04885714 0.02023699 42 2.4142493  0.0202
COIscrambling-26 0.02742857 0.02023699 42 1.3553681  0.1825
</code></pre>

<p>Which means that lme is telling me 3 groups may significantly lie outside the range of the others, while aov is telling me the probability of ANY GROUP AT ALL being different from the others is not significant (using a p=0.05 cutoff).</p>

<p>What am I to make of this? 
Do I understand these results correctly?</p>
"
"0.149168726281763","0.148377302419909"," 76918","<p>I have a question about how to do analysis of an experiment that has already been done, I hope you can help me with some advice!</p>

<p>I will try to keep it as simple as possible, but will give some detail so you know what I'm talking about!</p>

<p>What has been done is a ""screening trial"" to look at the activity of about 50 subjects (fungi) as antagonists (against a pest), the 50 individuals are members of groups (species), but some groups have many more members than others</p>

<p>I have results of several types of screening tests for each of the 50 individuals, with reps of each.  The screening tests look at different aspects, like growth rate, direct effects, and indirect effects.  </p>

<p>I can rank the isolates by their results in each screening test, and there looks like a lot of variability.</p>

<p>I want to be able to report the findings of screens, for each screening test and also to see if some individuals are in top ranks in different screening tests (and also the opposite, if some are great at some tests but not at others). I think what I want is to know if the results of the tests correlate for each individual....? </p>

<p>I am not sure how to say - this individual is the best - how can I tell if it is different than the next in the rank?
If I list the top ten from each screening test, I would like to know that they are statistically different from those I excluded from the list.  I would also like to compare them as groups, to be able to say, this species was the best, but with different numbers of representatives within the species, I dont think I could do this (please advise)</p>

<p>This seems like it would be a common research experiment, for example, for testing drugs in medical experiments, so I am looking for examples of what others have done to present this type of result.</p>

<p>I have seen a similar experiment to what I have to analyse but that had been done on a small scale, and the researchers used ANOVA to test differences among individuals and among groups, and some posthoc test to give each group little letters designating their means different than other groups.  </p>

<p>This seems to be unwieldy for 50 subjects, and I'm not sure about this.... I think I need some kind of mixed model regression to put all the test results in a model to test for correlation/covariance, but my understanding is weak!</p>

<p>I have been learning R and would like to do this analysis using R.</p>

<p>Can you give me advice/suggestions?  I would appreciate any help in understanding and clarifying this problem and solutions!  </p>
"
"0.0723574605292422","0.0764719112901873"," 77891","<p>I ran a repeated design whereby I tested 30 males and 30 females across three different tasks. I want to understand how the behaviour of males and females is different and how that depends on the task. I used both the lmer and lme4 package to investigate this, however, I am stuck with trying to check assumptions for either method. The code I run is</p>

<pre><code>lm.full &lt;- lmer(behaviour ~ task*sex + (1|ID/task), REML=FALSE, data=dat)
lm.full2 &lt;-lme(behaviour ~ task*sex, random = ~ 1|ID/task, method=""ML"", data=dat)
</code></pre>

<p>I checked if the interaction was the best model by comparing it with the simpler model without the interaction and running an anova:</p>

<pre><code>lm.base1 &lt;- lmer(behaviour ~ task+sex+(1|ID/task), REML=FALSE, data=dat)
lm.base2 &lt;- lme(behaviour ~ task+sex, random= ~1|ID/task), method=""ML"", data=dat)
anova(lm.base1, lm.full)
anova(lm.base2, lm.full2)
</code></pre>

<p>Q1: Is it ok to use these categorical predictors in a linear mixed model?<br/>
Q2: Do I understand correctly it is fine the outcome variable (""behaviour"") does not need to be normally distributed itself (across sex/tasks)?<br/>
Q3: How can I check homogeneity of variance? For a simple linear model I use <code>plot(LM$fitted.values,rstandard(LM))</code>. Is using <code>plot(reside(lm.base1))</code> sufficient?<br/>
Q4: To check for normality is using the following code ok?</p>

<pre><code>hist((resid(lm.base1) - mean(resid(lm.base1))) / sd(resid(lm.base1)), freq = FALSE); curve(dnorm, add = TRUE)
</code></pre>
"
"0.10232890201933","0.0946291623262781"," 78999","<p>Imagine the following data:</p>

<pre><code>ds &lt;- data.frame(x=1:10,y=1:10,z=rep(c(""A"",""B""),each=5))
</code></pre>

<p>the means for groups in <code>z</code> are:</p>

<pre><code>library(plyr)
ddply(ds, ""z"", function(x) mean(x$y))
#  z V1
#1 A  3
#2 B  8
</code></pre>

<p>do a couple of models:</p>

<pre><code>m1 &lt;- glm(y ~ x , data = ds)
m2 &lt;- glm(y ~ z , data = ds)
</code></pre>

<p>In <code>m1</code> the intercept is zero and the estimate for x is 1 as we would expect form the created data. Intercept meaning the value of <code>y</code> for <code>x=0</code>.</p>

<p>In <code>m2</code> the intercept is <code>3</code> and the estimate for <code>B</code> is <code>5</code>. This is because of the contrasts between the intercept with the estimate i.e. the estimate for <code>B</code> is 8. These results make sense in terms of the means of my data as calculated separately above.</p>

<p>so for <code>m1</code> I interpret intercept as <code>y where x=0</code> and <code>slope</code> and for <code>m2</code> as this is an anova, <code>intercept</code> as mean of first level and the rest are differences between that level and the first.</p>

<p>However in the following models <code>m3</code> and <code>m4</code></p>

<pre><code>m3 &lt;- glm(y ~ x + z , data = ds)
m4 &lt;- glm(y ~ x * z , data = ds)

# m3
#Coefficients:
#              Estimate Std. Error    t value Pr(&gt;|t|)    
#(Intercept)  1.123e-15  6.201e-16  1.812e+00    0.113    
#x            1.000e+00  1.720e-16  5.814e+15   &lt;2e-16 ***
#zB          -7.628e-17  9.880e-16 -7.700e-02    0.941    

# m4
#Coefficients:
#              Estimate Std. Error    t value Pr(&gt;|t|)    
#(Intercept)  1.123e-15  8.714e-16  1.289e+00    0.245    
#x            1.000e+00  2.627e-16  3.806e+15   &lt;2e-16 ***
#zB          -9.349e-17  2.305e-15 -4.100e-02    0.969    
#x:zB         3.130e-18  3.716e-16  8.000e-03    0.994  
</code></pre>

<p>The coefficients for intercept and <code>x</code> are the same as <code>m1</code> but how do I interpret the estimate for <code>B</code> and <code>A</code> as the estimate for <code>B</code> is <code>-7.628e-17</code>. </p>

<p>My question is how do I interpret the categorical estimates in <code>m3</code> and from the output extract the estimated means, which I would report? and for <code>m4</code> is the slope for <code>x:zB</code>  <code>1.000e+00 + 3.130e-18</code> and for <code>A</code> <code>1.000e+00 - 3.130e-18</code>?</p>
"
"0.0895377892669139","0.0811107105653813"," 79077","<p>I have 2 categorical independent variables (industry &amp; location) and 1 continuous dependent variable (performance metrics). I need to find significantly different industries by mean performance metrics in each location separately. Sounds like a task for ANOVA, but running one-way ANOVA for each location separately in my understanding inflates the type I error. Running two-way ANOVA will result in either comparison of mean performance metrics by location, or same by industry, or comparing all possible combinations of industries and locations, however I'm not interested in comparing industry performance across different locations. E.g. I am interested in comparing Canada:Energy to Canada:Basic Materials , but not interested in comparing Mexico:Energy to Canada:Basic Materials. Also sample sizes of each location are different, however share of observations from each industry is the same in each location, so not sure how suitable is the data for two-way ANOVA.</p>

<p>Sample dataset (contingency table of the counts):</p>

<pre><code>         Basic Materials Energy Financials
  Canada              10     10         20
  Mexico              15     15         30
  USA                  5      5         10
</code></pre>

<p>Sample R code:</p>

<pre><code>DATA &lt;- data.frame(performance=rnorm(120),
               location=c(rep('USA',20),rep('Canada',40),rep('Mexico',60)),
               industry=rep(c('Basic Materials','Energy','Financials','Financials'),30))
table(DATA[,-1])
TukeyHSD(aov(performance~location*industry,data=DATA))
</code></pre>

<p>Any suggestions (preferably accompanied by some R code)?</p>
"
"0.0361787302646211","0.0382359556450936"," 80172","<p>I performed a multivariate linear regression such that:</p>

<pre><code>fit&lt;-lm(as.matrix(y)~mwtkg+mbmi+mage,data=x)
</code></pre>

<p>where $y$ is a $500 \times 26$ multivariate outcomes. Then, I am wondering how to explain the <code>anova(fit)</code>:</p>

<pre><code>&gt; anova(fit)
Analysis of Variance Table

             Df  Pillai approx F num Df den Df    Pr(&gt;F)    
(Intercept)   1 0.99959    63064     25    651 &lt; 2.2e-16 ***
mwtkg         1 0.03506        1     25    651    0.5403    
mbmi          1 0.20862        7     25    651 &lt; 2.2e-16 ***
mage          1 0.09016        3     25    651 4.567e-05 ***
Residuals   675                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the three Dfs, Pillai, and P values mean for the model?</p>
"
"0.10232890201933","0.0811107105653813"," 80190","<p>I would appreciate another opinion on this simulation approach to estimating power for a within-subjects (or repeated-measures) ANOVA; in this case a 2x2 factorial design. In other words, is there anything WRONG with this method that I've missed? The code below is R.</p>

<hr>

<pre><code>library(ez)

nsub = 30
nconds = 4

nsims = 1000

#create an empty matrix that will be populated with p-values 
p = matrix(NA, nrow=nsims, ncol=3)

#subject vector
sub = sort(rep(1:nsub, nconds))

#2x2 factorial design
cond = data.frame(x1=c('a','a','b','b'), x2=c('c','d','c','d'))

# fixed effects 
intercept = rep(-1, nconds)
true.effect.x1 = c(0, 0, .5, .5)
true.effect.x2 = c(0, .5, .5, .5)
X = rep((intercept + true.effect.x1 + true.effect.x2),nsub)

#simulation loop
for (x in 1:nsims){

  #random effects

  #relatively large subject-specific variance in intercepts
  sub.intercept = rep(rnorm(nsub, mean=0, sd=2), times=1, each=nconds) 

  #relatively small by-subject adjustments to the effects
  sub.effect = rep(rnorm(nsub, mean=0, sd=0.05), times=1, each=nconds) 

  #unexplained error
  error = rnorm(nsub*nconds, mean=0, sd=1)

  #simulated dependent variable
  observed.y = X + (sub.intercept + sub.effect + error)

  #place everything in a data frame
  df = cbind(sub, cond, observed.y)
  names(df) = c('sub','x1','x2','y')
  df$sub = as.factor(df$sub)

  #extract the p-values for each effect from a repeated measure ANOVA (ezANOVA from 'ez' package)
  p[x,1] = ezANOVA(data=df, dv=.(y), wid=.(sub), within=.(x1, x2))$ANOVA[1,5]
  p[x,2] = ezANOVA(data=df, dv=.(y), wid=.(sub), within=.(x1, x2))$ANOVA[2,5]
  p[x,3] = ezANOVA(data=df, dv=.(y), wid=.(sub), within=.(x1, x2))$ANOVA[3,5]
}

###### p-values &lt; .05 ? ######
sig.x1 = ifelse(p[,1] &lt;= .05, 1, 0)
sig.x2 = ifelse(p[,2] &lt;= .05, 1, 0)
sig.int = ifelse(p[,3] &lt;= .05, 1, 0)

###### Histograms ######
par(mfrow=c(3,1))
hist(p[,1], 20, xaxp=c(0, 1, 20), col=2, main = paste('power X1:', mean(sig.x1 * 100), '%  with ', nsub, 'subjects'))
hist(p[,2], 20, xaxp=c(0, 1, 20), col=2, main = paste('power X2:', mean(sig.x2 * 100), '%  with ', nsub, 'subjects'))
hist(p[,3], 20, xaxp=c(0, 1, 20), col=2, main = paste('power interaction:', mean(sig.int * 100), '%  with ', nsub, 'subjects'))
</code></pre>
"
"0.114882898114698","0.121415466064296"," 81430","<p>I have a mixed model and the data looks like this:</p>

<pre><code>&gt; head(pce.ddply)
  subject Condition errorType     errors
1    j202         G         O 0.00000000
2    j202         G         P 0.00000000
3    j203         G         O 0.08333333
4    j203         G         P 0.00000000
5    j205         G         O 0.16666667
6    j205         G         P 0.00000000
</code></pre>

<p>Each subject provides two datapoints for errorType (O or P) and each subject is in either Condition G (N=30) or N (N=33).  errorType is a repeated variable and Condition is a between variable.  I'm interested in both main effects and the interactions.  So, first an anova:</p>

<pre><code>&gt; summary(aov(errors ~ Condition * errorType + Error(subject/(errorType)),
                 data = pce.ddply))

Error: subject
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)
Condition  1 0.00507 0.005065   2.465  0.122
Residuals 61 0.12534 0.002055               

Error: subject:errorType
                    Df  Sum Sq Mean Sq F value   Pr(&gt;F)    
errorType            1 0.03199 0.03199   10.52 0.001919 ** 
Condition:errorType  1 0.04010 0.04010   13.19 0.000579 ***
Residuals           61 0.18552 0.00304                     
</code></pre>

<p>Condition is not significant, but errorType is, as well as the interaction.</p>

<p>However, when I use lmer, I get a totally different set of results:</p>

<pre><code>&gt; lmer(errors ~ Condition * errorType + (1 | subject),
                    data = pce.ddply)
Linear mixed model fit by REML 
Formula: errors ~ Condition * errorType + (1 | subject) 
   Data: pce.ddply 
    AIC    BIC logLik deviance REMLdev
 -356.6 -339.6  184.3     -399  -368.6
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 0.000000 0.000000
 Residual             0.002548 0.050477
Number of obs: 126, groups: subject, 63

Fixed effects:
                       Estimate Std. Error t value
(Intercept)            0.028030   0.009216   3.042
ConditionN             0.048416   0.012734   3.802
errorTypeP             0.005556   0.013033   0.426
ConditionN:errorTypeP -0.071442   0.018008  -3.967

Correlation of Fixed Effects:
            (Intr) CndtnN errrTP
ConditionN  -0.724              
errorTypeP  -0.707  0.512       
CndtnN:rrTP  0.512 -0.707 -0.724
</code></pre>

<p>So for lmer, Condition and the interaction are significant, but errorType is not.</p>

<p>Also, the lmer result is exactly the same as a glm result, leading me to believe something is wrong.</p>

<p>Can someone please help me understand why they are so different?  I suspect I am using lmer incorrectly (though I've tried many other versions like (errorType | subject) with similar results.</p>

<p>(I have seen researchers use both approaches in the literature with similar data.)</p>
"
"0.108536190793863","0.101962548386916"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.0511644510096651","0.0540738070435875"," 82391","<p>I got a warning message when I was trying to do anova for two nlme::gls objects. Here is an example:</p>

<pre><code>require(nlme)
set.seed(123)
y&lt;-rnorm(100,10,2)
x1&lt;-rnorm(100)
x2&lt;-sample(1:5,100,T)
x3&lt;-rt(100,20)
x4&lt;-rbinom(100,1,0.3)
fit1&lt;-gls(y~x1+x2+x3+x4,correlation=corAR1(form=~1|x4))
fit2&lt;-gls(y~x3+x4,correlation=corAR1(form=~1|x4))
anova(fit1,fit2)
     Model df      AIC      BIC    logLik   Test  L.Ratio p-value
fit1     1  7 421.5587 439.4359 -203.7794                        
fit2     2  5 413.9855 426.8590 -201.9928 1 vs 2 3.573242  0.1675
Warning message:
In nlme::anova.lme(object = fit1, fit2) :
  fitted objects with different fixed effects. REML comparisons are not meaningful.
</code></pre>

<p>Dose anybody know what the message means? Should I do the following rather than above?</p>

<pre><code>&gt; anova(fit1, L=c(-1, -1, 1, 1))
Denom. DF: 95 
 F-test for linear combination(s)
(Intercept)          x1          x2          x3 
         -1          -1           1           1 
  numDF  F-value p-value
1     1 281.9844  &lt;.0001
</code></pre>

<p>Why the numDF is one here? It seems 2?</p>
"
"0.0361787302646211","0.0382359556450936"," 82438","<p>I have the data:</p>

<pre><code>numbers &lt;- c(0.176, 0.005, 0.022, 0.016, 0.036, 0.095, 0.069 )
Inds &lt;- as.factor(c(""P06"", ""P07"", ""P08"", ""P09"", ""P10"", ""P12"", ""P13"") )
</code></pre>

<p>and am trying to test for differences in <code>numbers</code> as a function of <code>Inds</code>.  The numbers are proportions of an events success for each individual.  With <code>Inds</code> specified as a factor, I am trying conduct an ANOVA using <code>aov()</code> (below)  </p>

<pre><code>anova(aov(numbers ~ Inds))
</code></pre>

<p>which results in the warning (below)</p>

<pre><code>Analysis of Variance Table
Response: numbers
          Df   Sum Sq   Mean Sq F value Pr(&gt;F)
Inds       6 0.021743 0.0036238               
Residuals  0 0.000000                         
Warning message:
In anova.lm(aov(numbers ~ Inds)) :
  ANOVA F-tests on an essentially perfect fit are unreliable
</code></pre>

<p>Any suggestions (changes in code or theoretical mistakes) would be appreciated.</p>
"
"0.0886194286901087","0.0936585811581694"," 83752","<p>First of all, my data:</p>

<p><a href=""http://www.pastebin.ca/2599202"" rel=""nofollow"">http://www.pastebin.ca/2599202</a> 
(I hope this is not too inconvenient, because I fail creating good fitting example data)</p>

<p>What I basically need, is a plot like:
<img src=""http://i.stack.imgur.com/6GO8L.png"" alt=""enter image description here""></p>

<p>I plotted the repeated measures factor <code>time</code>(x-axis, 3 levels) against <code>ias</code> (continuous dependent variable) for my 3 experimental groups. I did this 4 times (for each quantile of my trait-measure MIHT, <code>miht.binned</code>, .25 - 1.00).</p>

<p>I have to admit I am not really an R professional and the <code>ggplot2</code> manual simply is an overkill for me. I created the plot with <code>ezPlot</code> (from <code>ezANOVA</code>) and only managed to do a bit layout tweaking with <code>ggplot2</code>:</p>

<pre><code>   PlotIAS = ezPlot(                               
      data = MyData
      , dv = .(ias)
      , wid = .(id)
      , between = .(GROUP, miht.binned)
      , within = .(time)
      , x = .(time)
      , split = .(GROUP)
      , col   = .(miht.binned)
      , x_lab = 'time of measurement'
      , y_lab = 'IAS Score (Mean)'
      #, do_bars = FALSE
      , type = 3
    )


    PlotIAS = PlotIAS +  
      theme(
        panel.grid.major.y = element_line(colour = ""gray80"", size = NULL, linetype = NULL,  
                                          lineend = NULL)
        ,panel.grid.minor.y = element_line(colour = ""gray90"", size = NULL, linetype = NULL,
                                           lineend = NULL)
        ,panel.grid.major.x = element_blank()           
        ,panel.grid.minor.x = element_blank()
        ,legend.background = element_rect(fill = NULL, colour = ""black"") 

        ,panel.background = element_rect(fill = ""white"", colour = ""white"", size = NULL, 
                                         linetype = NULL)
      )

print(PlotIAS)
</code></pre>

<p>I did'nt find any information about these error bars ezPlot creates. They seem to be all the same. I just need to have error bars with SE or CI. I don't know if it is possible to add these in my ezPlot-based code (and how?) or if one has to create a complete new ggplot object for that (which is an overcharge for me...). Help is highly appreciated. </p>
"
"0.0957199230302734","0.101162829777814"," 84310","<p>I would like to find a way to compare variance within and between site with differing numbers of observations.  I have seen other posts on this, but they seem to focus on factor levels and I'm not sure that is appropriate with the number of sites (90) I have.  Can someone suggest the best way to compare within and between site variances in R? Or point me to a relevent post? </p>

<p>Here is an example dataset for 10 sites with 7:10 observations at each site</p>

<pre><code>clust&lt;-rep(1:10, sample(c(7,10), 10, replace=T))  
variable&lt;-abs(rnorm(length(clust)))

data&lt;-data.frame(cbind(clust, variable))
</code></pre>

<p>I could (and have) used glm and anova to look at within and between cluster variance, but not sure that is appropriate here.  I have also used var() to look at variance for the whole sample and of the clust mean.  However, I'm unsure this is what I want to be doing, as there are differing numbers of observations per site/cluster.  Should I use a weighted mean/variance function?  are there other ways of going about describing the variance in this type of situation, especially where the number of sites is larger (90)?</p>
"
"0.0957199230302734","0.086710996952412"," 85375","<p>I have carried out this ANOVA:</p>

<pre><code>summary(aov(drat ~ cyl, mtcars))

            Df Sum Sq Mean Sq F value   Pr(&gt;F)    
cyl          1  4.342   4.342   28.81 8.24e-06 ***
Residuals   30  4.521   0.151                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>According to the output, the sum of squares between groups is 4.342. I am trying to calculate the sum of squares using R code</p>

<p>First I have taken substracted the means of each group from the values and squared the result:</p>

<pre><code>cyl_4_minus_mean &lt;- (mtcars$drat[mtcars$cyl == 4] - mean(mtcars$drat[mtcars$cyl ==4]))^2
cyl_6_minus_mean &lt;- (mtcars$drat[mtcars$cyl == 6] - mean(mtcars$drat[mtcars$cyl ==6]))^2
cyl_8_minus_mean &lt;- (mtcars$drat[mtcars$cyl == 8] - mean(mtcars$drat[mtcars$cyl ==8]))^2
</code></pre>

<p>I then tried to multiply the results by the numbers of values within each group and the sum the result:</p>

<pre><code>sum((11*cyl_4_minus_mean), (7*cyl_6_minus_mean), (14*cyl_8_minus_mean))
</code></pre>

<p>This gives 49.4459, which is different to 4.342 given by <code>summary(aov(drat ~ cyl, mtcars))</code>. </p>

<p>Where am I going wrong here?</p>
"
"0.114407190489069","0.108821437516502"," 85664","<h1>Big Picture:</h1>

<p>How can I implement partitioned Chi Square in R?  I understand how to perform the overall Chi square, and then how to get individual parameters (observed counts, expected counts, residuals, etc.).  However, I don't understand how to get p values for each individual comparison.</p>

<h1>Details:</h1>

<pre><code>set.seed(200)
alpha = 0.05
</code></pre>

<p>I have three groups of patients and I have both categorical and continuous data collected on those patients.</p>

<pre><code>Group &lt;- c(rep('A', 'B', 'C'), 10))
Mass &lt;- c(rnorm(10, mean = 60, sd = 1),
          rnorm(10, mean = 70, sd = 1),
          rnorm(10, mean = 80, sd = 1))
Sex &lt;- rep(c('Male', 'Female'), 15)
data &lt;- data.frame(Group, Mass, Sex)
</code></pre>

<p>I'd like to construct your basic ""Table 1"" showing whether these groups differ from one another with respect to these variables.</p>

<p>For the continuous variables, I would do an anova test.  If p were less than alpha, I would do individual t.tests and interpret my p values using a Bonferroni correction.</p>

<pre><code>print('Anova p-value:')
print(anova(lm(Mass ~ Group, data))$'Pr(&gt;F)'[1])

print(t.test(Mass ~ Group, data[data$Group != 'A',])$p.value)
print(t.test(Mass ~ Group, data[data$Group != 'B',])$p.value)
print(t.test(Mass ~ Group, data[data$Group != 'C',])$p.value)                    
</code></pre>

<p>I'd like an analogous output for my discrete variables.  I know that I can do a Chi square test on the data as a whole...</p>

<pre><code>print(chisq.test(x = data$Group, y = data$Sex)[['p.value']])
</code></pre>

<p>...but I'm confused about the correct way to get and interpret individual p values.  I found this question, which links to a powerpoint about ""partitioning"" the chi square test, but I'm having trouble following it.</p>

<p><a href=""http://stats.stackexchange.com/questions/1133/multiple-chi-squared-tests/1210#1210"">Multiple Chi-Squared Tests</a></p>
"
"0.191439846060547","0.202325659555628"," 86032","<p>I'm currently working with a data set that has numerous samples collected over time at different sites in a study area, and I'm interested in detecting a trend over time for that area.  I know that in an ideal experimental or balanced situation, using a random slope and intercept model is a great way to get at the overall trend within the study area.  With our data, however, many of the sites are missing samples and a handful of the sites only have one data point.</p>

<p>I'm curious if there's a way to intuitively understand how the sample imbalance will affect the estimate of the overall slope?  To put it differently, are there ways to know if sample imbalances are causing problems,  or are there things I can look for in my model output that would indicate I shouldn't trust what the model is estimating?</p>

<p>I created a contrived example with 20 data points to look at this. I put 10 data points with a slope of 1 into one site (a), and put the other 10 data points with a slope of -1 into unique sites (b through l).  I had assumed that when I looked at both a random intercept and random slope and intercept model that they would be somewhat similar, or that at least the latter would give more weight to the site with good data over time.</p>

<pre><code>&gt; library(lme4)
&gt; set.seed(9999)

&gt; x = c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9) + rnorm(20,mean=0,sd=0.1)
&gt; y = c(0,1,2,3,4,5,6,7,8,9,9,8,7,6,5,4,3,2,1,0) + rnorm(20,mean=0,sd=0.1)
&gt; z = c(rep('a',10),'b','c','d','e','f','h','i','j','k','l')
&gt; z = factor(z)

&gt; m0 = lm(y~x)
&gt; m1 = lmer(y~x+(1|z))
&gt; m2 = lmer(y~x+(1+x|z))

&gt; summary(m0)
&gt; summary(m1)
&gt; summary(m2)
&gt; anova(m1,m2)
</code></pre>

<p>As expected, the slope of the linear model was near zero, but the results for the two mixed effects models were nearly opposite.  Even though sites b through l only have one data point, it seems like they contribute more towards the slope because the trend is occurring over so many sites.  The random slope and intercept model was also preferred to using model selection criteria.</p>

<pre><code> &gt; summary(m0)$coefficients
                Estimate Std. Error    t value    Pr(&gt;|t|)
 (Intercept)  4.53784796  1.2586990  3.6051890 0.002023703
 x           -0.01178748  0.2335094 -0.0504797 0.960296079

 &gt; summary(m1)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 | z) 

 REML criterion at convergence: 62.0877 

 Random effects:
  Groups   Name        Variance Std.Dev.
  z        (Intercept) 33.30788 5.7713  
  Residual              0.01583 0.1258  
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept) -0.03597    1.74163   -0.02
 x            0.99332    0.01386   71.66

 Correlation of Fixed Effects:
   (Intr)
 x -0.036

 &gt; summary(m2)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 + x | z) 

 REML criterion at convergence: 31.0386 

 Random effects:
  Groups   Name        Variance Std.Dev. Corr 
  z        (Intercept) 7.78818  2.7907        
      x           0.37691  0.6139   -1.00
  Residual             0.01524  0.1234        
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept)   8.2121     0.8566   9.587
 x            -0.8201     0.1882  -4.358

 Correlation of Fixed Effects:
   (Intr)
 x -0.999

 &gt; anova(m1,m2)
 Data: 
 Models:
 m1: y ~ x + (1 | z)
 m2: y ~ x + (1 + x | z)
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
 m1  4 66.206 70.189 -29.103   58.206                             
 m2  6 36.745 42.719 -12.372   24.745 33.462      2  5.419e-08 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I see that under this extreme example, the random slope and intercept have an almost perfect correlation.  Is what I can pull from this is that, in a sense, the model gives more value to the sites with only one data point because the overall trend is so strong but across multiple sites, but that I should view the slope estimate this model produces as suspect with such a high correlation?  Is there anything else that should look for?  For my specific study, I could also set some sort of criteria for what level of replication I thought was necessary to make proper inferences, e.g. eliminate all the sites that less than five samples.</p>

<p>Many thanks for your thoughts.</p>
"
"0.149168726281763","0.148377302419909"," 87487","<p><strong>Short version</strong></p>

<p>Is there a difference <strong>per treatment</strong> given time and this dataset?</p>

<p><strong>Or</strong> if the difference we're trying to demonstrate is important, what's the best method we have for teasing this out?</p>

<p><strong>Long version</strong></p>

<p>Ok, sorry if a bit <em>biology 101</em> but this appears to be an edge case where the data and the model need to line up in the right way in order to draw some conclusions. </p>

<p>Seems like a common issue... Would be nice to demonstrate an intuition rather than repeating this experiment with larger sample sizes. </p>

<p>Let's say I have this graph, showing mean +- std. error:</p>

<p><img src=""http://i.stack.imgur.com/eIKeF.png"" alt=""p1""></p>

<p>Now, it looks like there's a difference here. Can this be justified (avoiding Bayesian approaches)?</p>

<p>The simpleminded man's  approach would be to take Day 4 and apply a <em>t-test</em> (as usual: 2-sided, unpaired, unequal variance), but this doesn't work in this case. It appears the variance is too high as we only had 3x measurements per time-point (err.. mostly my design, p = 0.22).</p>

<p><strong>Edit</strong> On reflection the next obvious approach would be ANOVA on a linear regression. Overlooked this on first draft. This also doesn't seem like the right approach as the usual linear model is impaired from heteroskedasticity (<em>exaggerated variance over time</em>). <strong>End Edit</strong></p>

<p>I'm guessing there's a way to include <strong>all</strong> the data which would fit a simple (1-2 parameter) model of growth over time per predictor variable then compare these models using some formal test. </p>

<p>This method should be justifiable yet accessible to a relatively unsophisticated audience.</p>

<p>I have looked at <code>compareGrowthCurves</code> in <a href=""http://cran.r-project.org/web/packages/statmod/statmod.pdf"" rel=""nofollow"">statmod</a>, read about <a href=""http://www.jstatsoft.org/v33/i07/paper"" rel=""nofollow"">grofit</a> and tried a linear mixed-effects model adapted from <a href=""http://stats.stackexchange.com/questions/61153/nlme-regression-curve-comparison-in-r-anova-p-value"">this question on SE</a>. This latter is closest to the bill, although in my case the measurements are not from the <strong>same subject</strong> over time so I'm not sure mixed-effects/multilevel models are appropriate. </p>

<p>One sensible approach would be to model the rate of growth per time as linear and fixed and have the random effect be <strong>Tx</strong> then <a href=""http://www.statistik.uni-dortmund.de/useR-2008/slides/Scheipl+Greven+Kuechenhoff.pdf"" rel=""nofollow"">test it's significance</a>, although I gather there's <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">some debate</a> about the merits of such an approach.</p>

<p>(Also this method specifies a linear model which would not appear to be the best way to model a comparison of growth which in the case of one predictor has not yet hit an upper boundary and in the other appears basically static. I'm guessing there's a generalized mixed-effects model approach to this difficulty which would be more appropriate.)</p>

<p>Now the code:</p>

<pre><code>df1 &lt;- data.frame(Day = rep(rep(0:4, each=3), 2),
              Tx = rep(c(""Control"", ""BHB""), each=15),
              y = c(rep(16e3, 3),
              32e3, 56e3, 6e3,
              36e3, 14e3, 24e3,
              90e3, 22e3, 18e3,
              246e3, 38e3, 82e3,
              rep(16e3, 3),
              16e3, 34e3, 16e3,
              20e3, 20e3, 24e3,
              4e3, 12e3, 16e3,
              20e3, 5e3, 12e3))
### standard error
stdErr &lt;- function(x) sqrt(var(x)) / sqrt(length(x))
library(plyr)
### summarise as mean and standard error to allow for plotting
df2 &lt;- ddply(df1, c(""Day"", ""Tx""), summarise,
             m1 = mean(y),
             se = stdErr(y) )
library(ggplot2)
### plot with position dodge
pd &lt;- position_dodge(.1)
ggplot(df2, aes(x=Day, y=m1, color=Tx)) +
 geom_errorbar(aes(ymin=m1-se, ymax=m1+se), width=.1, position=pd) +
 geom_line(position=pd) +
 geom_point(position=pd, size=3) +
 ylab(""No. cells / ml"")
</code></pre>

<p>Some formal tests:</p>

<pre><code>### t-test day 4
with(df1[df1$Day==4, ], t.test(y ~ Tx))
### anova
anova(lm(y ~ Tx + Day, df1))
### mixed effects model
library(nlme)
f1 &lt;- lme(y ~ Day, random = ~1|Tx, data=df1[df1$Day!=0, ])
library(RLRsim)
exactRLRT(f1)
</code></pre>

<p>this last giving</p>

<pre><code>    simulated finite sample distribution of RLRT.  (p-value based on 10000
    simulated values)

data:  
RLRT = 1.6722, p-value = 0.0465
</code></pre>

<p>By which I conclude that the probability of this data (or something more extreme), <em>given the null hypothesis that there is no influence of <strong>treatment</strong> on <strong>change over time</em></strong> is close to the elusive 0.05. </p>

<p>Again, sorry if this appears a bit basic but I feel a case like this could be used to illustrate the importance of modelling in avoiding further needless experimental repetition. </p>
"
"0.108536190793863","0.114707866935281"," 90511","<p>My data has a binary response (correct/incorrect), one continuous predictor <code>score</code>, three categorical predictors (<code>race</code>, <code>sex</code>, <code>emotion</code>) and a random intercept for the random factor <code>subj</code>. All predictors are within-subject. One of the categorical factor has 3 levels, the other have two. </p>

<p>I need advice on obtaining ""global"" p-values for each categorical factor (in an ""ANOVA like"" way)</p>

<hr>

<p>Here is how I proceed :</p>

<p>I fitted a binomial GLMM using 'glmer' from the lme4 package (because 'glmmML' doesn't compute on my data and glmmPQL does not provide AIC) and did model selection using <code>drop1</code> repeatedly until no more terms can be dropped. Here is the final model (let's assume it has been validated):</p>

<pre><code>library(lme4)
M5 &lt;- glmer(acc ~ race + sex + emotion + sex:emotion + race:emotion + score +(1|subj), 
        family=binomial, data=subset)
# apparently using family with lmer is deprecated 
drop1(M5, test=""Chisq"")
summary(M5)
</code></pre>

<p><code>drop1</code> gives p-values for the higher level terms only (the two 2-way interactions + <code>score</code>). 
<code>summary</code>gives p-values for every term, but separates the different levels of each categorical factor.</p>

<p>How can I get ""global"" p-values for each factor? I need to report them even if they are not the most relevant or meaningful estimates of signifiance here. How should I proceed? I tried searching on the web and ended up reading about likelihood ratios or the ""Wald test"" but I am not sure if or how this would apply here.</p>

<p>(PS: This is a duplicate from my ""anonymous"" post here that needed editing: <a href=""http://stats.stackexchange.com/questions/90487/binomial-mixed-model-with-categorical-predictors-model-selection-and-getting-p"">Binomial mixed model with categorical predictors: model selection and getting p-values</a> Sorry about that.)</p>
"
"0.0511644510096651","0.0540738070435875"," 90760","<p>suppose to have sample from 3 groups A,B,C.
The hypothesis <code>H0: the mean of the 3 groups is the same</code> can be tested using 3 independent t test. </p>

<pre><code>test1: mean(A)=mean(B) level 0.05
test2: mean(B)=mean(C) level 0.05
test3: mean(A)=mean(C) level 0.05
</code></pre>

<p>It's known that we should prefer an ANOVA test because with the previous method there is an increasing risk of type 1 error. </p>

<p>I would like to have an example with simulated data where the first procedure lead us to an error while the   ANOVA return the good result. </p>

<p>The best would be an R code to simulate the experiment.</p>

<p>Thanks!</p>
"
"0.10232890201933","0.108147614087175"," 90904","<p>I'm new to using CART trees, but have been asked to do so for a project I'm working on. I've had success running the scripts (from both RPART and PARTY packages) but I can't seem to get exactly what I'm looking for. I'm working with spectral data (Red, NIR, NDVI...) for 80 trees in four categories (Mesic-control, Mesic-fertilized, Xeric-control and Xeric-fertilized). There are significant differences in the mean values for spectral bands among the four categories and I'd like to use those differences to develop an algorithm for assigning category to unknown trees. </p>

<p>Here's a dummy tree I made using the RPART package:
<img src=""http://i.stack.imgur.com/7XUQ3.jpg"" alt=""RPART tree""></p>

<pre><code>fit &lt;- rpart(Category ~ red.top + NIR.top + R.NIR.top, method=""anova"", data=CCA)
plot(fit, uniform=T,main=""Classification Tree for Kyphosis"")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
</code></pre>

<p>And here's another tree I made with PARTY:
<img src=""http://i.stack.imgur.com/ourPe.jpg"" alt=""PARTY tree""></p>

<pre><code>library(party)
fit &lt;- ctree(Category ~ red.top + NIR.top + R.NIR.top, data=CCA)
plot(fit, main=""Conditional Inference Tree for Kyphosis"")
gtree &lt;- ctree(Category ~ ., data = CART)
plot(gtree)
</code></pre>

<p>Both look fine, except they don't really do what I want. The RPART one looks good, but I can't figure out how to determine the category identity of the trees in each 'leaf' and the PARTY one is what I want, except the tree is way simplified compared to the regression tree in the first example. My ultimate goal is to essentially combine the two and create a larger regression tree that uses more of the 'rules' from the data and gets me to output 'leaves' with categorical information and some predictive power. I'm not really too hung up on whether I use regression or categorization--as long as it has utilitarian value.</p>

<p>So, I guess what I'm really looking for is better scripts for either package that give me a more detailed tree with visual output (bar graphs on the leaves) or a way to determine the identity of the groups created by the RPART tree.</p>
"
"0.114407190489069","0.108821437516502"," 91134","<p>consider the following example data:</p>

<pre><code>df1 &lt;- data.frame(customer=c(rep(""customer1"",5),rep(""customer2"",10),rep(""customer3"",7)),
                  money_spent=sample(22))

df2 &lt;- data.frame(customer=c(""customer1"",""customer2"",""customer3""),
                  origin=c(""US"",""US"",""UK""),
                  industry_sector=c(""IS1"",""IS2"",""IS3""),
                  currency=c(""USD"",""USD"",""GBP""))
</code></pre>

<p>My actual data consists of about 200000 rows and I would like to examine it in terms of, for instance, do customers from the US spent more money compared to customers from other countries. I would also like to see whether the amount of money spent depends on the industry sector and so on. I have some more explanatory variables apart from origin, industry sector and currency which I would like to look into. Also the number of records for the customers differ so that it might make sense to average the money spent for each customer.</p>

<p>I am not sure about how to best analyse this data. I first thought of cluster analysis, in particular, hierarchical clustering but am not sure whether it can be applied to such data and, in particular, how to structure the data to be put into the function. The R function <code>hclust</code> takes a matrix as the input but how would I structure such a matrix in terms of my data? Could k-means clustering be a better alternative? </p>

<p>Another approach would probably to analyse this data using boxplots and an one-way ANOVA approach to see whether ""money spent"" differs between different countries or industry sectors. However, this approach does not test whether the variables are dependend on each other. To look into this, I have been advised to apply a decision tree first and then do some statistical significance analysis. However, from what I have read so far I cannot see how decision trees can help me to detect variable dependencies.</p>

<p>So, I am wondering whether there any other/better techniques/functions out there which are more suitable for such data? Maybe a time series analysis is more appropriate since we have also recorded the dates when customers spent money.</p>
"
"0.10232890201933","0.0946291623262781"," 91351","<p>I am working on a very large dataset of patients who belong to one of five groups. I want to test if groups are not so much different than each other in terms of age, race, sex, smoking level, weight, and some other factors. Some factors are numerical and some are categorical. I know I can use ANOVA to compare mean differences between groups for numerical factors. However, I do not know how I can test mean differences for categorical factors. Some references say that Chi-Squared test can be used for this purpose, but it seems that <code>chisq.test</code> in <code>R</code> is only for testing either independency or goodness of fit. </p>

<p>Could someone write a sample code in <code>R</code> showing how the mean differences between more than two groups can be evaluated for a categorical factor? Is any pre-evaluation required to check if a specific test is valid to be used? Thanks in advance.</p>
"
"0.144714921058484","0.152943822580375"," 91445","<p>I like to keep analyses all in SAS or all in R when I can help it and lately have been using R more and more, but there's one analysis that I do somewhat routinely that has given me trouble in R.</p>

<p>I have repeated measures data where I would like to fit the following model: $$Delta = Day + Group + Day\times Group$$ where $Delta$ is the change from baseline, $Day$ is the number of days from the beginning of the study, and $Group$ is the experimental group.  I fit a variance-covariance matrix to account for repeated measures (for this example I'm using compound symmetry, but the difference is the same using other variance-covariance matrices).  I have the data at the end of the post.</p>

<p>If I don't include the interaction, I can get the analysis to run as I want it to in both SAS and R.  In SAS:</p>

<pre><code>proc mixed data=df;
  class group day id;
  model delta = day group;
  repeated day / subject=id type=cs;
  lsmeans group / diff=all;
run;
</code></pre>

<p>In R:</p>

<pre><code>library(nlme)
library(lsmeans)
fit.cs &lt;- gls(Delta~Day+Group,
              data=df,
              corr=corCompSymm(,form=~1|ID))
anova(fit.cs,type=""marginal"")
lsmeans(fit.cs,pairwise~Group)
</code></pre>

<p>Obviously the results differ in terms of denominator DF, but I don't intend to start that discussion (unless that difference is causing the problem).  When I add interaction in SAS, everything is fine:</p>

<pre><code>proc mixed data=df;
  class group day id;
  model delta = day | group;
  repeated day / subject=id type=cs;
run;
</code></pre>

<p>But when I do the same from R...</p>

<pre><code>fit.cs &lt;- gls(Delta~Day*Group,
              data=df,
              corr=corCompSymm(,form=~1|ID))
# Error in glsEstimate(object, control = control) : 
#   computed ""gls"" fit is singular, rank 19
</code></pre>

<p>Why does R complain about the fit being singular but SAS doesn't? </p>

<p>Here are some fake data that are representative of data that I work with (from R):</p>

<pre><code>df &lt;- structure(list(Delta = c(-1.27, -0.34, 1.92, 0.45, 1.21, 0.43, -0.41, 0.16, -0.35,
1.49, -0.85, -0.86, 1.04, 0.49, 2.32, 0.13, -0.32, 0.5, 0.48, 1.21, -0.82, 0.93,
-0.58, 2.3, -0.9, 0.21, -0.72, 0.11, -0.28, -0.33, -0.7, -1.16, -0.23, -0.88, 0.97,
0.25, 0.8, 0.16, 0.63, -0.49, -0.63, -0.9, 1.1, -1.45, 0.38, -0.93, 0.4, 0.45, 0.48,
0.14, 1.02, -0.01, -1.98, 2.19, -1.53, -0.49, -1.57, -1.02, 1.09, 1.74, 0.54, -1.57,
-1.5, -0.48, 0.26, 0.2, -0.36, -1.05, -1.73, -0.77, -0.65, -1.07, -0.45, -0.14,
-0.56, 0.84, -2.66, -0.52, 1.44, 0.45, 0.24, -0.92), Day = structure(c(1L, 2L, 3L,
1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L,
1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L,
3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 1L, 2L, 3L, 6L,
7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L), .Label = c(""4"",
""7"", ""10"", ""12"", ""14"", ""16"", ""28""), class = ""factor""), Group = structure(c(1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c(""1"", ""2"",
""3"", ""4""), class = ""factor""), ID = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L,
4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 8L,
8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L, 11L, 11L,
12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 15L, 15L,
15L, 15L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L),
.Label = c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"",
""15"", ""16"", ""17"", ""18""), class = ""factor"")), .Names = c(""Delta"", ""Day"", ""Group"", ""ID""),
class = ""data.frame"", row.names = c(NA, -82L))
</code></pre>

<p>Here's the same data for SAS:</p>

<pre><code>DATA  df;
INPUT Delta Day Group $ ID $ @@;
CARDS;
-1.27 4 1 1 -0.34 7 1 1 1.92 10 1 1 0.45 4 1 2 1.21 7 1 2 0.43 10 1 2
-0.41 4 1 3 0.16 7 1 3 -0.35 10 1 3 1.49 4 2 4 -0.85 7 2 4 -0.86 10 2
4 1.04 16 2 4 0.49 28 2 4 2.32 4 2 5 0.13 7 2 5 -0.32 10 2 5 0.5 16 2 5
0.48 28 2 5 1.21 4 2 6 -0.82 7 2 6 0.93 10 2 6 -0.58 16 2 6 2.3 28 2 6
-0.9 4 2 7 0.21 7 2 7 -0.72 10 2 7 0.11 16 2 7 -0.28 28 2 7 -0.33 4 2 8
-0.7 7 2 8 -1.16 10 2 8 -0.23 16 2 8 -0.88 4 3 9 0.97 7 3 9 0.25 10 3 9
0.8 16 3 9 0.16 28 3 9 0.63 4 3 10 -0.49 7 3 10 -0.63 10 3 10 -0.9 16 3 10
1.1 28 3 10 -1.45 4 3 11 0.38 7 3 11 -0.93 10 3 11 0.4 16 3 11 0.45 28 3 11
0.48 4 3 12 0.14 7 3 12 1.02 10 3 12 -0.01 16 3 12 -1.98 28 3 12 2.19 4 3 13
-1.53 7 3 13 -0.49 10 3 13 -1.57 16 3 13 -1.02 28 3 13 1.09 4 4 14 1.74 7 4 14
0.54 10 4 14 -1.57 16 4 14 -1.5 4 4 15 -0.48 7 4 15 0.26 10 4 15 0.2 16 4 15
-0.36 28 4 15 -1.05 4 4 16 -1.73 7 4 16 -0.77 10 4 16 -0.65 16 4 16 -1.07 28 4 16
-0.45 4 4 17 -0.14 7 4 17 -0.56 10 4 17 0.84 16 4 17 -2.66 28 4 17 -0.52 4 4 18
1.44 7 4 18 0.45 10 4 18 0.24 16 4 18 -0.92 28 4 18
;
RUN;
</code></pre>
"
"0.0964766140389895","0.101962548386916"," 91848","<p>I need a little bit of help and confirmation that I have the right idea. 
I have some fake data of 8 tribes; within each tribe members work hard to gain food for their own tribe. No one can speak to these tribes, but people suspect that each tribe has one of the two strategies presented below for gaining food:</p>

<ol>
<li><p>Members of a tribe who travel farther away from the tribe's main location are given more food, so they face less of a chance of starving before coming back.</p></li>
<li><p>Members of a tribe who travel far are given less food; that way if they are lost, there is less of a food loss to the tribe as a whole.</p></li>
</ol>

<p>The variables (columns) I am working with are <code>Tribe number</code>, <code>Distance from the tribe location</code> when sample was taken (10, 20, 30, or 40 miles), <code>Weight of each member</code> that we are studying (related to the amount of food is given), <code>height</code> (taller people use energy more efficiently, and there is a strong positive correlation between weight and height in arbitrary units and inches), finally I have each observation categorized by height (group <code>1</code>: 56â€“62in, group <code>2</code>: 62â€“64...).</p>

<p>I want to find out if the tribes use different strategies, and also if there is a difference among the classes <code>pf</code> height. In addition I want to find out the strategies that are in use. I am having a hard time with how to classify each tribe as using either strategy <code>1</code> or <code>2</code>. I was thinking of doing a one-way ANOVA to check if there is a difference in <code>mean</code> within each group based on <code>distance</code>. (In a particular tribe is there a difference in the mean of weight between those who were 10, 20 , 30, or 40 miles?) I don't know how to figure out if each colony uses a different strategy.</p>

<p>Finally, I want to build a linear model of mass on colony, distance, and height. I know how to build a model and run diagnostics. My concern here is, can I use distance as a categorical variable since its values are 10, 20, 30, or 40 miles? </p>
"
"0.153493353028995","0.153209119956831"," 92284","<p>I have asked a similar question here: <a href=""http://stackoverflow.com/questions/22648335/interpretation-of-interaction-term-in-r-lm-l-q"">stackoverflow</a> I am puzzled by the interpretation for an interaction term. In my data my Y is an interval variable with the health outcome of an experiment. I have used an interaction term in which I have interacted the condition with the predisposition of the subject considering health status at base level. They are both categorical variables (factor variables in R).</p>

<p>Now it gets complicated because the Condition was two treatments: in treatment A subjects got the placebo first and then the real medicine whereas in treatment B they go the real medicine first and the placebo second. All it changes is the order. </p>

<pre><code>Health outcome = a + Condition * Health.Base
</code></pre>

<p>I have the worst state of health at the base level as my reference category I find that interaction with the Condition is statistically significant but I am not sure how to interpret this.</p>

<p>I use the <code>lm()</code> function of R (although my design looks more like an ANOVA) and in the output I get the b coefficient in an output that looks like this:</p>

<pre><code>ConditionB:Health.Base.So.and.So         (Beta and p-value)
ConditionB:Health.Base.Excellent         (Beta and p-value)
</code></pre>

<p>A statistically significant interaction term for those in Excellent health at baseline would mean that they are affected by the Condition B more than Condition A compared to the reference category people (Poor health at baseline). Is this right? What does the beta-coefficient represent? </p>

<p>If I would like to examine for each Health category at the base line separately without comparing to a reference category I would have to code each category as a dummy variable. However, in this case I would compare whether membership to a specific health status at the base line significantly changes between conditions compared to those who belong to the other health statuses at the base line. Is this right? Again, what does the beta-coefficient represent?</p>

<p>Would it be right to assume that the choice of the interaction between the Condition and the dummy variables is easier to interpret?</p>

<p>--- EDIT ---
R output:</p>

<pre><code>Call:
lm(formula = HealthOutcome ~ Condition * HealthStatus, 
    data = datA)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.5957 -0.5942 -0.2640  0.4423  2.4423 

Coefficients:
                                       Estimate Std. Error t value     Pr(&gt;|t|)    
(Intercept)                            2.595652   0.053044  48.934  &lt; 2e-16 ***
ConditionCondB                        -0.001449   0.077071  -0.019    0.985    
HealthStatusSo.and.So                 -0.331693   0.078094  -4.247  2.35e-05 ***
HealthStatusExcellent                 -0.836724   0.092692  -9.027  &lt; 2e-16 ***
ConditionCondB:HealthStatusSo.and.So   0.137490   0.110612   1.243    0.214    
ConditionCondB:HealthStatusExcellent  -0.199787   0.133943  -1.492    0.136    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.8045 on 1059 degrees of freedom
  (68 observations deleted due to missingness)
Multiple R-squared:   0.16, Adjusted R-squared:  0.156 
F-statistic: 40.33 on 5 and 1059 DF,  p-value: &lt; 2.2e-16
</code></pre>
"
"0.0361787302646211","0.0382359556450936"," 92616","<p>What's going on here?</p>

<pre><code>data.2
         subj phon f.amp
    1     1    V   100
    2     2    V    60
    3     3    V   124
    4     4    V    42
    5     5    V   210
    6     6    V   104
    7     7    V   150
    8     1    Ê”    92
    9     2    Ê”    33
    10    3    Ê”    92
    11    4    Ê”    32
    12    5    Ê”    90
    13    6    Ê”    65
    14    7    Ê”   105
    15    1    h   142
    16    2    h    72
    17    3    h   141
    18    4    h    60
    19    5    h   268
    20    6    h   134
    21    7    h   145
</code></pre>

<p>Pairwise comparison of levels PHON<sub>V</sub> and PHON<sub>h</sub> by running ANOVA on the pertinent subset of data:</p>

<pre><code>library(lme4)
anova(lmer(f.amp~phon+(1|subj),data.2[which(data.2[,2]!=""Ê”""),]))
   Analysis of Variance Table
        Df Sum Sq Mean Sq F value
   phon  1 2113.1  2113.1  9.8144
</code></pre>

<p>Pairwise comparison of the same levels by direct definition of contrast coefficients; different resulting <em>F</em>-ratio:</p>

<pre><code>contrasts(data.2[,2],1)=matrix(c(-1,0,1),nrow=3)
contrasts(data.2[,2])
    [,1]
  V   -1
  Ê”    0
  h    1
anova(lmer(f.amp~phon+(1|subj),data.2))
   Analysis of Variance Table
        Df Sum Sq Mean Sq F value
   phon  1 2113.1  2113.1  1.2566
</code></pre>

<p>Since <em>df</em><sub>PHON</sub>, <em>SS</em><sub>PHON</sub> and <em>MS</em><sub>PHON</sub> are the same for both analyses; and since <em>F</em><sub>PHON</sub> = <em>MS</em><sub>PHON</sub> / <em>MS</em><sub>PHON x <em>S</em></sub>, I deduce that the analyses differ regarding the handling of <em>S</em>.</p>

<p>Any ideas as to how and why precisely?</p>
"
"0.0361787302646211","0.0382359556450936"," 93378","<p>I got the following outcome. I am confused with the interpretation. When I consider RSS then model 2 is better than model 1. What does that high p value mean? Does in influence my conclusion?? </p>

<blockquote>
  <p>anova(reg4.3,reg4.4, test=""Chisq"")</p>
</blockquote>

<pre><code>Analysis of Variance Table

Model 1: Gas ~ Flow * CODin + A + B + C
Model 2: Gas ~ Flow * CODin + A + B + C + Blue + Green

  Res.Df     RSS Df Sum of Sq Pr(&gt;Chi)
1     60 10034.5     
2     58  9851.3  2    183.21   0.5831
</code></pre>
"
"0.194828424987849","0.198806683993076"," 94057","<p>I have an agricultural field experiment (testing a plant protection agent):</p>

<p><strong>Split plot design</strong> with: </p>

<pre><code>2 whole plot treatments ""infestation"": ""high"" &amp; ""low"" 
8 split-plot treatments (""treat""): 

1. Untreated Control (""Ctrl1"")
2. Reference Product (""Ctrl2"")
3. 1 x Test-Product 1
4. 2 x Test-Product 1
5. 3 x Test-Product 1
6. 1 x Test-Product 2
7. 2 x Test-Product 2
8. 3 x Test-Product 2

and 4 replicates (""block""):
</code></pre>

<p>The parameter of interest in this example is grain (<strong>yield</strong>):</p>

<p>First, I could model this:</p>

<pre><code>lme(yield ~ infestation * treat, random = ~ 1 | block/infestation, data)
</code></pre>

<p>or</p>

<pre><code> lmer(yield ~ infestation * treat + (1 | block/infestation), data)
</code></pre>

<p>But as can be seen treatments 3-8 can and have to be recoded as 2 products (""prod"") being tested 1-3 times (""times""), so I have a 2x3 subdesign.</p>

<p>One possibility would be subsetting the data: </p>

<pre><code>  data2 &lt;- subset(data, !data$treat == ""Ctrl1"" &amp;  !data$treat == ""Ctrl2"")
</code></pre>

<p>and recode the resting treatments to ""prod"" = 1,2 and ""times"" = 1:3
then run:</p>

<pre><code>lme(yield ~ infestation * form * times, random = ~ 1 | block/infestation, data)
</code></pre>

<p>Afterwards I could still do contrasts to compare the control treatments with the treated ones. </p>

<p>But (here my actual problem starts): I read an article of</p>

<p><strong>H.P. Piepho</strong>: ""<em>A Note on the Analysis of Designed Experiments with Complex Treatment Structure</em>"", 
HortScience 41(2):446--452. 2006</p>

<p>The author wants to show ""<em>how a meaningful analysis can be obtained based on a linear model by appropiate coidng of factors. (...) Our main objective is to demonstrate that the introduction of dummy variables can conveniently solve a wide variety of inferential problems that would otherwise either require ... multiple linear contrasts... or not make fully eficient use of the data, e.g when only data from orthogonal subdesigns are analysed.</em>""  </p>

<p>A very similar example (Example 1 in the article) is discussed within, and an alternative analysis in SAS is proposed - which I wanted to try to realise in R. </p>

<p>The author adds a dummy variable (<strong>ctrl_vs_trt</strong>) to the data and codes it: ""control"", ""trt"" (in my case <strong>trt</strong>, <strong>Ctrl1</strong>, <strong>Ctrl2</strong>"". </p>

<p>The he uses: 
(in his case <strong>prod</strong> is <strong>form</strong> ulation, and <strong>times</strong> is <strong>conc</strong> entration)</p>

<pre><code>PROC GLM;
CLASS block contr_vs_trt form conc;   ## 
MODEL set = block contr_vs_trt
        contr_vs_trt * form
    contr_vs_trt * conc
    contr_vs_trt * form * conc;
RUN.                    
</code></pre>

<p>I cite a further paragraph: 
""<em>Of course, a test for contr_vs_trt is not produced with this model, and one cannot compute simple means or marginal means. Also, the Type I SS for <strong>form</strong>, <strong>conc</strong>, and <strong>form x conc</strong> are not the same as with Type III SS. With Type III SS, the test for form is adjusted for <strong>conc</strong>, as fitting <strong>conc</strong> takes out the control when coding factors as in Table ""xy"" (as I did here). Similarly, the test for <strong>conc</strong> is adjusted for <strong>form</strong>, because fitting of <strong>form</strong> takes out the control. As a result, the Type III ANOVA for the model <strong>form x conc</strong> turns out to be that for the 3x2 factorial subdesign. (...)
It seems much more stringent and transparent to use the nested model <strong>contr_vs_trt/(form x conc)</strong>, as this properly reflects all nesting and crossing features of the design.</em>""</p>

<p>Now, how to do that in <code>lme</code> or <code>lmer</code>?</p>

<p>lme does not run at all, even if I simplify to: </p>

<pre><code> lme(yield ~ prod * times, random = ~1|block, data), 
 I get
 Error in MEEM(object, conLin, control$niterEM) : 
 Singularity in backsolve at level 0, block 1
</code></pre>

<p>The term <strong>prod * times</strong> cannot be run (<strong>prod + times</strong> logically can). Eliminating both controls from the data set resolves this problem. </p>

<p><code>lmer</code> runs with <strong>prod * times</strong>, but always given the message:</p>

<pre><code> fixed-effect model matrix is rank deficient so dropping ""x"" columns / coefficients
</code></pre>

<p>I understand that the subdesign is not orthogonal and therefor dropping is occuring, but I cannot say if the analysis after dropping can still be right. </p>

<p>Also, I do not know how to specify the full model (leaving out the ""infestation"" whole plot for a second):</p>

<pre><code>lmer(yield ~ prod * times + (1|block/ctr_vs_trt), data)
</code></pre>

<p><strong>prod * times</strong> is nested inside <strong>ctr_vs_trt</strong> but both are nested inside the same block (or whole plot).
Is nesting of fixed effects possible in <code>lme</code> or <code>lmer</code> - does it work as I proposed?
Does it even make sense to run the full model?</p>

<p>With <code>aov()</code> I get the model running, even the partitioning of Df's is right. But due to strong non-orthogonality it is not possible to assume that the results are right.</p>

<p>I can get meaningful results subsetting and using contrasts, but I found the authors approach interesting and it would help in the analysis of some of my other trials. 
Thanks in advance for any help; I hope this question is not too long...</p>
"
"0.0886194286901087","0.0780488176318078"," 94078","<p>Here's the data:</p>

<pre><code>&gt; tires &lt;- data.frame(Wear  = c(17, 14, 12, 13, 14, 14, 12, 11,
                                13, 13, 10, 11, 13, 8, 9, 9),
                      Brand = rep(LETTERS[1:4], 4),
                      Car   = as.character(as.roman(rep(1:4, each = 4))))
&gt; tires
   Wear Brand Car
1    17     A   I
2    14     B   I
3    12     C   I
4    13     D   I
5    14     A  II
6    14     B  II
7    12     C  II
8    11     D  II
9    13     A III
10   13     B III
11   10     C III
12   11     D III
13   13     A  IV
14    8     B  IV
15    9     C  IV
16    9     D  IV
</code></pre>

<p>Now I fit a two-way ANOVA with interaction:</p>

<pre><code>two.way &lt;- aov(Wear ~ Brand + Car + Brand:Car, data = tires)
</code></pre>

<p>Finally, no p-values: </p>

<pre><code>&gt; summary(two.way)
            Df Sum Sq Mean Sq
Brand        3  30.69  10.229
Car          3  38.69  12.896
Brand:Car    9  11.56   1.285
</code></pre>

<p>A regular two-way ANOVA (i.e., <code>Wear ~ Brand + Car</code>) gives me p-values:</p>

<pre><code>&gt; summary(aov(Wear ~ Brand + Car, data = tires))
            Df Sum Sq Mean Sq F value  Pr(&gt;F)   
Brand        3  30.69  10.229   7.962 0.00668 **
Car          3  38.69  12.896  10.038 0.00313 **
Residuals    9  11.56   1.285                   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Is there a way to interpret this? The interaction plot shows me that there is definitely interaction between <code>Brand</code> and <code>Car</code> so I am hoping to incorporate this into my model.</p>
"
"0.169853904989375","0.156097635263616"," 94302","<p>I am performing a parametric bootstrap to test whether I need a specific fixed effect in my model or not. I have mainly done this for exercise and I am interested if my procedure so far is correct.</p>

<p>First, I fit the two models to be compared. One of them includes the effect to be tested for and the other one does not. As I am testing for fixed effects I set <code>REML=FALSE</code>:</p>

<pre><code>    mod8 &lt;- lmer(log(value) 
                 ~ matching 
                 + (sentence_type | subject) 
                 + (sentence_type | context), 
                 data = wort12_lmm2_melt,
                 REML = FALSE)
    mod_min &lt;- lmer(log(value) 
                    ~ 1 
                    + (sentence_type | subject)
                    + (sentence_type | context),
                    data = wort12_lmm2_melt,
                    REML = FALSE)
</code></pre>

<p>Both models are fit on a balanced data set which includes few missing values. There are slightly above 11000 observations for 70 subjects. Every subject saw every item only one time. The dependent variable are reading times; sentence_type and matching are two-level factors. Context and subject are treated as random effects. Context has 40 levels.</p>

<p>I call anova():</p>

<pre><code>    anova(mod_min, mod8)
</code></pre>

<p>and get the output:</p>

<pre><code>    Data: wort12_lmm2_melt
    Models:
    mod_min: log(value) ~ 1 + (sentence_type |  subject) + (sentence_type | context)
    mod8: log(value) ~ matching + (sentence_type |  subject) + (sentence_type |   
    mod8:     context)
            Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
    mod_min  8 3317.6 3375.8 -1650.8   3301.6                            
    mod8     9 3310.9 3376.4 -1646.4   3292.9 8.6849      1   0.003209 **
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Mistrusting the almighty p I set up a parametric bootstrap by hand:</p>

<pre><code>    mod &lt;- mod8
    modnull &lt;- mod_min
    lrt.obs &lt;- anova(mod, modnull)$Chisq[2] # save the observed likelihood ratio test statistic
        n.sim &lt;- 10000  
        lrt.sim &lt;- numeric(n.sim)
        dattemp &lt;- mod@frame
        # pb &lt;- txtProgressBar(min = 0, max = n.sim, style = 3) # set up progress bar to satisfy need for control
        for(i in 1:n.sim) {
        # Sys.sleep(0.1) # progress bar related stuff
        ysim       &lt;- unlist(simulate(modnull) # simulate new observations from the null-model  
        modnullsim &lt;- lmer(ysim 
                           ~ 1
                           + (sentence_type | subject)
                           + (sentence_type | context),
                           data = dattemp,
                           REML = FALSE) # fit the null-model
        modaltsim  &lt;- lmer(ysim
                           ~ matching
                           + (sentence_type | subject)
                           + (sentence_type | context),
                           data = dattemp,
                           REML = FALSE) # fit the alternative model
        lrt.sim[i] &lt;- anova(modnullsim, modaltsim)$Chisq[2] # save the likelihood ratio test statistic
    # setTxtProgressBar(pb, i)
    }

    # assuming chi-squared distribution for comparison

    pchisq((2*(logLik(mod8)-logLik(mod_min))),
           df    = 1,
           lower = FALSE)
</code></pre>

<p>with the output:</p>

<pre><code>    'log Lik.' 0.003208543 (df=9)
</code></pre>

<p>compare to parametric bootstrap p-value</p>

<pre><code>    p_mod8_mod_min &lt;- (sum(lrt.sim&gt;=lrt.obs)+1)/(n.sim+1)  # p-value. alternative: mean(lrt.sim&gt;lrt.obs)
</code></pre>

<p>with the output:</p>

<pre><code>    [1] 0.00319968
</code></pre>

<p>Plot the whole thing:</p>

<pre><code>    xx &lt;- seq(0, 20, 0.1)
    hist(lrt.sim,
         xlim     = c(0, max(c(lrt.sim, lrt.obs))),
         col      = ""blue"", 
         xlab     = ""likelihood ratio test statistic"",
         ylab     = ""density"", 
         cex.lab  = 1.5, 
         cex.axis = 1.2, 
         freq     = FALSE)
    abline(v   = lrt.obs,
           col = ""orange"",
           lwd = 3)
    lines(density(lrt.sim),
          col = ""blue"")
    lines(xx,
          dchisq(xx, df = 1),
          col = ""red"")
    box()
</code></pre>

<p>which yields:</p>

<p><img src=""http://i.stack.imgur.com/bh4YO.png"" alt=""enter image description here""></p>

<p>I do have some questions though:</p>

<p>(1) Is the procedure correct or did I make a mistake?</p>

<p>(2) How is the histogram to be interpreted?</p>

<p>(3) Is the form of the histogram normal or extreme?</p>

<p>Thanks for any help!</p>
"
"0.140394095324012","0.148377302419909"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"0.153493353028995","0.153209119956831"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.0626633989716535","0.0441510785688348"," 95949","<p>I want to statistically compare the mean values from eight different groups.  I do not have the raw data, but have the sample size, mean and associated standard deviation and standard error.  I was wondering if there was any code in R to do the ANOVA test on summary data, as well as the subsequent post-hoc, pair-wise comparisons.  </p>

<pre><code>n &lt;- c(796,95,245,1645,701,1157,211,144)
means &lt;- c(648,680,569,602,594,596,606,623)
sd &lt;- c(146.3,90.5,61.2,98.9,113.6,106.8,87.8,76.9)


#Assigning the standard deviation and the sample size for each sample 
s.x &lt;-146.3
n.x &lt;-796
s.y &lt;-90.5
n.y &lt;-95
#Conducting the t-test with summary statistics
tsum.test(mean(648), s.x, n.x, mean(680), s.y, n.y)
</code></pre>
"
"0.109082976072021","0.126814318375447"," 96195","<p>I have run a split splot model in SPSS (via repeated measures function) and I would like to reproduce my results using R. To do so I used ezANOVA function from ez package to obtain sphericity tests and correction and type III SS. I have read that aov function does not give type III SS. So far the results are identical with those from SPSS but I cannot tell so when I run tests of within subjects contrasts. I use polynomial contrasts and my factor has six levels. What I manage to get is t-values instead of f-values or f-values but not the same as in SPSS. Here is my data and code
<code>id    subj  treat  m1    m2    m3    m4    m5   m6
1       1     1   455   460   510   504   436   466
2       2     1   467   565   610   596   542   587
3       3     1   445   530   580   597   582   619
4       4     1   485   542   594   583   611   612
5       5     1   480   500   550   528   562   576
6       6     2   514   560   565   524   552   597
7       7     2   440   480   536   484   567   569
8       8     2   495   570   569   585   576   677
9       9     2   520   590   610   637   671   702
10     10     2   503   555   591   605   649   675
11     11     3   496   560   622   622   632   670
12     12     3   498   540   589   557   568   609
13     13     3   478   510   568   555   576   605
14     14     3   545   565   580   601   633   649
15     15     3   472   498   540   524   532   583</code></p>

<p>m1-m6 represent measurements in 6 different time points. My code for the contrasts.</p>

<p><code>long.df&lt;- melt(data, id=c('subj','treat'))
 long.df&lt;- long.df[order(long.df$subj)]
 names(long.df)&lt;- c('subj','treat','time','meas')
 mod&lt;- lm(meas~time + time^2 + time^3 + time^4 + time^5, long.df)
 summary(mod) # this is how I obtain t-values for the polynomial contrasts #
 mod1&lt;- aov(meas~time + time^2 + time^3 + time^4 + time^5, long.df)
 summary(mod1, split=list(time=list(""Linear""=1, ""Quad""=2,'q'=3,'f'=4,'fif'=5))) # this is how i obtain f-values #</code></p>

<p>I think that the F-values correspond to type 1 SS (as I use aov function). How am I wrong? Is there a way to conduct the Tests of within subjects contrasts and obtain type III SS so I will have identical results to SPSS? Here are the SPSS results. Many thanks in advance!</p>

<p>Source  time    Type III SS/    df/      Mean Square/       F/   Sig.<br>
time    Linear  123662.881/ 1/  123662.881/ 83.591/ .000<br>
     Quadratic  5928.007/   1/  5928.007/   18.968/ .001<br>
    Cubic   10462.676/  1/  10462.676/  28.075/ .000<br>
    Order 4 798.193  /       1/       798.193/  4.010/  .068<br>
    Order 5 1702.743/   1/  1702.743/   4.878/  .047    </p>
"
"0.114882898114698","0.132453235706504"," 96393","<p>I have run a split splot model in SPSS (via repeated measures function) and I would like to reproduce my results using R. To do so I used ezANOVA function from ez package to obtain sphericity tests and correction and type III SS. I have read that aov function does not give type III SS. So far the results are identical with those from SPSS but I cannot tell so when I run tests of within subjects contrasts. I use polynomial contrasts and my factor has six levels. What I manage to get is t-values instead of f-values or f-values but not the same as in SPSS. Here is my data and code</p>

<pre><code>id    subj  treat  m1    m2    m3    m4    m5   m6
1       1     1   455   460   510   504   436   466
2       2     1   467   565   610   596   542   587
3       3     1   445   530   580   597   582   619
4       4     1   485   542   594   583   611   612
5       5     1   480   500   550   528   562   576
6       6     2   514   560   565   524   552   597
7       7     2   440   480   536   484   567   569
8       8     2   495   570   569   585   576   677
9       9     2   520   590   610   637   671   702
10     10     2   503   555   591   605   649   675
11     11     3   496   560   622   622   632   670
12     12     3   498   540   589   557   568   609
13     13     3   478   510   568   555   576   605
14     14     3   545   565   580   601   633   649
15     15     3   472   498   540   524   532   583
</code></pre>

<p>m1-m6 represent measurements in 6 different time points. My code for the contrasts.</p>

<pre><code>long.df&lt;- melt(data, id=c('subj','treat'))
long.df&lt;- long.df[order(long.df$subj)]
names(long.df)&lt;- c('subj','treat','time','meas')
mod&lt;- lm(meas~time + time^2 + time^3 + time^4 + time^5, long.df)
summary(mod) # this is how I obtain t-values for the polynomial contrasts #
mod1&lt;- aov(meas~time + time^2 + time^3 + time^4 + time^5, long.df)
summary(mod1, split=list(time=list(""Linear""=1, ""Quad""=2,'q'=3,'f'=4,'fif'=5))) 
# this is how i obtain f-values #
</code></pre>

<p>I think that the F-values correspond to type 1 SS (as I use aov function). How am I wrong? Is there a way to conduct the Tests of within subjects contrasts and obtain type III SS so I will have identical results to SPSS? Here are the SPSS results. Many thanks in advance!</p>

<pre><code>Source     time    Type III SS/   df/    Mean Square/        F/   Sig.  
time     Linear     123662.881/    1/     123662.881/   83.591/   .000  
      Quadratic       5928.007/    1/       5928.007/   18.968/   .001  
          Cubic      10462.676/    1/      10462.676/   28.075/   .000  
        Order 4        798.193/    1/        798.193/    4.010/   .068  
        Order 5       1702.743/    1/       1702.743/    4.878/   .047
</code></pre>
"
"0.109082976072021","0.126814318375447"," 97486","<p>This is my first question, please should I write something wrong correct me. </p>

<p>I have a question when comparing two GLMs after applying stepwise selection. What I've always heard is that stepwise selection is not robust enough for the selected variables. Therefore, I use ANOVA to compare the most parsimonious model (lowest AIC) and the model without the least significant variable. Accordingly, if the ANOVA outcome is nonsignificant I remove the variable from the stepwise selection. It is said that the simple models (few predictors) dominated over difficult model (a lot of predictors).</p>

<p>My problem is that when I try to do it with R. The p-value from the ANOVA is the same as the p-value of the stepwise selection. Do you think that I'm doing something wrong?</p>

<p>Here is my code: </p>

<p>I first run the command <code>stepAIC</code> and obtain the most parsimonious model: </p>

<pre><code>Fullmodel&lt;-glm(Geommean~log_CPUEig+log_CPUE+max_depth+amp_temperature+TotalP+Elevation+max_temperature+lake_area)

stepwise &lt;- stepAIC(fullmodel)           
# Step:  AIC=1280.2 Geommean ~ log_CPUE + max_depth + max_temperature
#
#                   Df Deviance    AIC
# &lt;none&gt;                 1474.2 1280.2
# - max_depth        1   1485.6 1280.4
# - max_temperature  1   1495.8 1282.3
# - log_CPUE         1   1937.1 1355.5
</code></pre>

<p>Then, I look the p-value for each selected variables from the  stepwise procedure:</p>

<pre><code>summary(stepwise)
# Coefficients:
# Estimate Std. Error t value Pr(&gt;|t|) 
# (Intercept)      25.3323     3.5104   7.216 5.06e-12 ***
# log_CPUE         -3.5670     0.3811  -9.359  &lt; 2e-16 ***
# max_depth        -0.5818     0.3959  -1.469   0.1428
# max_temperature  -6.1182     3.0290  -2.020   0.0443 *
</code></pre>

<p>I choose the <code>max_depth</code> as the least significant variable.</p>

<p>I run ANOVA to compare the most parsimonious model and the  model without the least significant variable.</p>

<pre><code># Most parsimonious model:
parsimonious &lt;- glm(Geommean ~ log_CPUE+max_temperature+max_depth) 

# Model without the least significant variable:
variable &lt;- glm(Geommean ~ log_CPUE+max_temperature) 
</code></pre>

<p>I compare these two models with ANOVA: </p>

<pre><code>compare &lt;- anova(parsimonious,variable, test=""F"")
compare
# Resid. Df Resid. Dev Df Deviance      F Pr(&gt;F)
# 1       279     1474.2                          
#         280     1485.6 -1   -11.41 2.1593 **0.1428**
</code></pre>

<p>After doing  ANOVA, I obtain the same p-value of the least significant variable (0.1428). In that case, I removed max_depth from the most parsimonoius model because the p-values is greater than 0.05. Do you think that this outcome is normal?</p>

<p>Thank you very much for your help. </p>
"
"0.114407190489069","0.0967301666813349"," 97929","<p>My data is described here <a href=""http://stats.stackexchange.com/questions/97165/what-can-cause-a-error-model-is-singular-error-in-aov-when-fitting-a-repeate"">What can cause a &quot;Error() model is singular error&quot; in aov when fitting a repeated measures ANOVA?</a></p>

<p>I am trying to see the effect of an interaction using <code>lmer</code> so my base case is:</p>

<pre><code>my_null.model &lt;- lmer(value ~ Condition+Scenario+ 
                             (1|Player)+(1|Trial), data = my, REML=FALSE)

my.model &lt;- lmer(value ~ Condition*Scenario+ 
                             (1|Player)+(1|Trial), data = my, REML=FALSE)
</code></pre>

<p>Running the <code>anova</code> gives me significant results, but when I try to account for random slope (<code>(1+Scenario|Player)</code>) the model fails with this error: </p>

<pre><code>  Warning messages:
 1: In commonArgs(par, fn, control, environment()) :
   maxfun &lt; 10 * length(par)^2 is not recommended.
 2: In optwrap(optimizer, devfun, getStart(start, rho$lower, rho$pp),  :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
 3: In commonArgs(par, fn, control, environment()) :
  maxfun &lt; 10 * length(par)^2 is not recommended.
 4: In optwrap(optimizer, devfun, opt$par, lower = rho$lower, control = control,  :
   convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
 5: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
   Model failed to converge with max|grad| = 36.9306 (tol = 0.002)
 6: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
</code></pre>

<p>Alternatively if it fails to converge after a lot of iterations (I set it to <code>100 000</code>) and I am getting the same results after <code>50k</code> and <code>100k</code> it means that it is very close to the actual value, just it does not reach it. So can I report my results like this?</p>

<p>Note that when I set the iterations so high I get only these warnings:</p>

<pre><code> Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
 Model failed to converge with max|grad| = 43.4951 (tol = 0.002)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
 Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
</code></pre>
"
"0.0964766140389895","0.114707866935281"," 99765","<p>I'm trying to run a tree-way repeated measures ANOVA on the following data: I have a completely balanced design with three within-subject factors (type, form and ch - channel) and one dependent variable amp (amplitude).</p>

<p>I'm inclined to believe the results I got using <code>aov</code> function:</p>

<pre><code>res &lt;- aov(amp ~ type*form*ch + Error(sbj/(type*form*ch)), data = p3vals)
</code></pre>

<p>Here is the anova table I have:</p>

<pre><code>Error: Within
               Df Sum Sq Mean Sq F value   Pr(&gt;F)    
type            1   25.0  24.950  12.315 0.000462 ***
form            1   12.9  12.910   6.372 0.011693 *  
ch              1    0.9   0.875   0.432 0.511113    
type:form       1    3.1   3.123   1.542 0.214581    
type:ch         1    0.9   0.938   0.463 0.496404    
form:ch         1    1.3   1.256   0.620 0.431239    
type:form:ch    1    3.0   2.960   1.461 0.226974    
Residuals    1514 3067.3   2.026                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, after reading several pages of similar examples(including <a href=""http://stats.stackexchange.com/questions/14088/why-do-lme-and-aov-return-different-results-for-repeated-measures-anova-in-r"">Why do lme and aov return different results for repeated measures ANOVA in R?</a> ) I decided to try <code>lme</code> and <code>lmer</code> functions from name and lme4 packages for further pairwise multiple comparisons using <code>glht</code> from multcomp package.
In the example mentioned above F values are at least closet those obtained using <code>aov</code>, but I cannot figure out how to get any meaningful results.</p>

<pre><code>lme_p3amp = lmer(amp ~ type*form*ch + (1|sbj) + (1|type:sbj) + (1|form:sbj) + (1|ch:sbj), data = p3vals)
&gt; anova(lme_p3amp)
Analysis of Variance Table
             Df  Sum Sq Mean Sq F value
type          1  0.0266  0.0266  0.0433
form          1  2.0782  2.0782  3.3863
ch            1 28.5513 28.5513 46.5227
type:form     1  2.1980  2.1980  3.5815
type:ch       1  2.9789  2.9789  4.8539
form:ch       1  0.9278  0.9278  1.5118
type:form:ch  1  6.6072  6.6072 10.7661
</code></pre>

<p>and lme produces the following result:</p>

<pre><code> anova(lme(amp ~ type*form*ch, random=list(sbj=pdBlocked(list(~1, pdIdent(~type-1), pdIdent(~form-1), pdIdent(~ch-1)))), data=p3vals))
             numDF denDF  F-value p-value
(Intercept)      1  1508 32.56485  &lt;.0001
type             1  1508  0.02920  0.8643
form             1  1508  3.54422  0.0599
ch               1  1508  8.05747  0.0046
type:form        1  1508  2.79623  0.0947
type:ch          1  1508  3.78969  0.0518
form:ch          1  1508  1.18037  0.2775
type:form:ch     1  1508  8.40553  0.0038
</code></pre>

<p>I'd appreciate if you tell me what is wrong with my code and how can I perform a valid post hoc analysis.</p>
"
"0.181391981885239","0.170405729136922"," 99952","<p><strong>Please read edit 3 first</strong></p>

<p>I am trying to find out the significant factors in a dataset of percentages, a sample of which are below. The difficulty is that the data violates the assumptions of ANOVA, and most of the data are fairly close to 100%.</p>

<p>Please note that glm + binary would not work: the samples used to calculate each proportion are not independent. I do have access to the denominator, if that helps.</p>

<p>Any direction where to start? I've read quite a few things here and elsewhere (notably trying to use some transformation such as arcsin, etc...) and also some other methods I never heard of (""contingency table approaches""). As in a ""textbook ANOVA"" I would like to know which factors are significant, and how much of the variability they explain.</p>

<pre><code>data = c(0.79,0.98,0.95,0.95,1,0.98,0.99,0.97,0.99,0.99,0.98,0.99,0.99,0.94,0.94,  
0.86,0.84,0.86,0.97,0.96,0.53,0.87,0.97,0.81,0.99,1,0.99,0.87,0.98,0.97,0.93,0.8,  
0.7,0.94,0.89,0.98,0.89,0.98,0.96,0.98)
</code></pre>

<p><strong>Edit:</strong> sorry for the lack of clarity. Here's how my percentages are calculated: I basically throw a number of particles (known only a posteriori) in a funnel and count how many make it through/how many remain stuck in the funnel. The percentage is the ratio of the particles having made it through divided by the total number of particles.If a particle which comes at the beginning of the trial remains stuck, the odds of a subsequent particle to remain stuck are higher. As such, I don't think I can apply a generalized linear model, specifying binomial as familly (in R I mean). But again, my statistical insights may be utterly wrong.</p>

<p><strong>Edit2:</strong> regarding independance, I guess my comment was misleading. Each sample in the vector above is independant from the others. However, as I explain in the edit above, the samples used to calculate each percentage are not. </p>

<p><strong>Edit3:</strong>
Well, I reckon I probably made a mess of my question, think it may help if I rephrase the problem and show some data (a fraction of my whole dataset). Besides, I have progressed a bit, hopefully in the right direction. I do not know if I should do a full edit of my original question, or even abandon and start anew, I'll add this as an edit for the interest of history (let me know if I should do differently).</p>

<p>My response variable is the percentage of particles having made it through a funnel (the total number of particles is different for each percentage). If a particle at the beginning of the trial remains stuck, the odds of a subsequent particle to remain stuck are higher.</p>

<p>The (potential) explanatory variables are 1) the type of particles, 2) the funnel type, 3) the funnel position and 4) the total number of particles. In a first stage, I want to find which of these actually impact the response variable and by how much.
In a second stage, I would like to use the current dataset as a reference for the analysis of future samples.  Precisely, I would like to know if the percentage of particles having made through the funnel is  significantly different from the reference dataset and by how much. </p>

<p>Plotting the data indicates me that each population may have a different mean and a different variance.</p>

<pre><code># Libraries ####
library(ggplot2)
library(betareg)
library(lmtest)

#Create data####
df5 = structure(list(type = c(""Type1"", ""Type1"", ""Type1"", ""Type2"", ""Type2"", 
                              ""Type2"", ""Type2"", ""Type2"", ""Type2"", ""Type3"", ""Type3"", ""Type3"", 
                              ""Type1"", ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", 
                              ""Type2"", ""Type2"", ""Type1"", ""Type1"", ""Type1"", ""Type1"", ""Type1"", 
                              ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", ""Type2"", 
                              ""Type1"", ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type1"", 
                              ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", 
                              ""Type2"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", ""Type2""), 
                     funnelType = c(""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", ""fType2"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2""), 
                     position = c(""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", 
                                  ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", 
                                  ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", 
                                  ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", 
                                  ""a"", ""b"", ""c"", ""a"", ""b"", ""c""), 
                     total = c(420L, 293L, 324L, 549L, 
                               527L, 603L, 533L, 571L, 438L, 496L, 534L, 604L, 489L, 360L, 383L, 
                               524L, 560L, 606L, 493L, 513L, 572L, 530L, 527L, 543L, 616L, 471L, 
                               554L, 392L, 530L, 443L, 561L, 529L, 599L, 529L, 481L, 521L, 621L, 
                               567L, 609L, 447L, 398L, 462L, 528L, 574L, 522L, 654L, 531L, 556L, 
                               642L, 569L, 684L, 372L, 540L, 345L), 
                     percentage = c(0.98808, 0.95569, 0.9784, 0.57741, 0.81017, 0.82919, 0.93809, 0.94041, 
                                    0.94744, 0.93352, 0.98129, 0.99006, 0.97339, 0.94728, 0.98695, 
                                    0.9542, 0.84989, 0.92574, 0.79116, 0.92782, 0.98077, 0.96605, 
                                    0.97155, 0.96503, 0.97076, 0.95756, 0.99097, 0.96429, 0.9434, 
                                    0.99097, 0.78253, 0.94518, 0.97664, 0.99056, 0.96261, 0.98273, 
                                    0.88402, 0.96824, 0.92118, 0.95524, 0.97991, 0.9762, 0.83144, 
                                    0.95643, 0.98085, 0.95107, 0.94162, 0.98741, 0.83635, 0.94372, 
                                    0.98099, 0.88447, 0.94817, 0.95365)), 
                .Names = c(""type"", ""funnelType"",""position"", ""total"", ""percentage""), class = ""data.frame"", 
                row.names = c(2L, 
                              3L, 4L, 18L, 19L, 20L, 50L, 51L, 52L, 66L, 67L, 68L, 98L, 99L, 
                              100L, 114L, 115L, 116L, 130L, 131L, 132L, 146L, 147L, 148L, 162L, 
                              163L, 164L, 194L, 195L, 196L, 210L, 211L, 212L, 226L, 227L, 228L, 
                              258L, 259L, 260L, 306L, 307L, 308L, 322L, 323L, 324L, 354L, 355L, 
                              356L, 370L, 371L, 372L, 386L, 387L, 388L))

ggplot(df5,aes(x=total,y=percentage))+geom_boxplot(outlier.shape = NA)+geom_jitter(aes(col=position))+facet_grid(type~funnelType)
</code></pre>

<p>It does seem that I am in a situation comparable the dyslexic example in <a href=""http://psycnet.apa.org/journals/met/11/1/54/"" rel=""nofollow"">http://psycnet.apa.org/journals/met/11/1/54/</a>. I consequently think that I should be able to apply the same method to analyse my data. To do so I use the package betareg in R (no interactions for the sake of the example).</p>

<pre><code>full &lt;- betareg(percentage ~ type+funnelType+position+total|type+funnelType+position+total,data = df5)
summary(full)
</code></pre>

<p>This yields the results that all explanatory variables but ""total"" have a significant effect on the mean. From what I know explanatory variables are likely to be significant in large data sets (I have a relatively large number of observastions in my full dataset). How do I judge the magnitude of the effect of each factor? What should I do if two of the explanatory variables are correlated?</p>

<p>In my second stage I want to compare a new sample to the subpopulation in my dataset that has the same predictor values. How should I perform this? Unequal variance t-test?</p>
"
"0.0808981002113217","0.0512989176042577"," 99962","<p>I am trying to catch changes in customers sales from week to week using R.</p>

<pre><code>Historical.stats &lt;- ddply(Historical, .(Company,Customer), summarise, mean= mean(Sales), lowerlimit = quantile(Sales,.05) ,higherlimit = quantile(Sales,.95), pval=t.test(Sales,data=Historical)$p.value)
</code></pre>

<p>My first attempt at doing this was to use the Quantile and then merge by Current Week data by the Company and Customer. I then would check if the sales this week were above or below the lower and higher quantiles.</p>

<pre><code>dat &lt;- merge(CurrentWeek, Historical.stats, by = c(""Company"",""Customer""))
dat$plot &lt;- ifelse((dat$Sales&lt; dat$lowerlimit | dat$Sales&gt; dat$higherlimit) ,1 ,0)
</code></pre>

<p>After doing more research would it be best to use something like the Levine test, a standard t.test or some type of ANOVA method to do this type of anomaly detection. I understand I won't be able to take into account seasonality or holidays.</p>

<pre><code>Example Data
Company   Customer   Week   Sales
Company A Customer 1 05/07/2014 100
Company A Customer 2 05/07/2014 50
Company B Customer 1 05/07/2014 200
Company B Customer 2 05/07/2014 150
Company A Customer 1 05/14/2014 20
Company A Customer 2 05/14/2014 20
Company B Customer 1 05/14/2014 100
Company B Customer 2 05/14/2014 50
</code></pre>
"
"0.0808981002113217","0.0512989176042577","100857","<p>I have a 2x2 between-subject design with (slightly) unequal cell sizes. Hence I chose to use the <code>anova(lm(...))</code> function so that Type 1 SS are used:</p>

<p><code>anova(lm(prob ~ utility * imagination, data = e2data))</code></p>

<p><code>prob</code>is the numerical DV and <code>utility</code> and <code>imagination</code> are factors with 2 levels each. The ANOVA yields <strong>no significant main or interaction effects</strong>. </p>

<p>Then, I went on to visualise the cell means including errorbars: </p>

<pre><code>library(reshape)
mystats &lt;- function(x) round(c(M = mean(x), SD = sd(x), n = length(x), SE = sd(x)/sqrt(length(x))), 2)
prob.means1 &lt;- data.frame(cast(e2data, utility + imagination ~ ., value=""prob"", mystats))
</code></pre>

<p><code>mystats</code> calculates the mean, standard deviation, number of observations and standard error of the mean. This results in the following data.frame, which is the basis for the plot below: </p>

<pre><code>&gt; prob.means1
  utility imagination    M   SD   n   SE
1    gain      absent 7.18 1.23 100 0.12
2    gain     present 7.49 1.34  95 0.14
3    loss      absent 7.10 1.47 103 0.14
4    loss     present 7.19 1.49  95 0.15
</code></pre>

<p>The plot:</p>

<pre><code>(plot1 &lt;- ggplot(prob.means1, aes(x = utility, y = M,
                                  group = imagination,
                                  color = imagination)) + 
  geom_point(size = 3.5) + 
  geom_line() + 
  geom_errorbar(aes(ymax=M+SE, ymin=M-SE, width=0.05)) +
  scale_y_continuous(name=""Probability estimate"", limits=c(6,8))
)
</code></pre>

<p>Note that I set the errorbars to range from M-SE to M+SE. </p>

<p><img src=""http://i.stack.imgur.com/sSybJ.png"" alt=""2x2 plot""></p>

<p><strong>In the gain condition (left part of the plot) the error bars do not overlap. Does this allow the conclusion that the difference between these two cells is significant? How do I run this post-hoc comparison appropriately (between-subject, unequal sample sizes)?</strong> I tried <code>Tukey.HSD()</code> but it doesn't work on a <code>lm</code> object. Also, I tried to use <code>pairwise.t.test</code> but I couldn't figure out how to use it with more than 1 factor.</p>
"
"0.0808981002113217","0.0854981960070962","101020","<p>I am well aware how to read the model summary in R for a regression model when a factor is included. The ""first"" level, in terms of ABC, is regarded as the base level to which all further levels of that factor are compared to. In an ANOVA-style model the baseline value is found in the intercept (equals the mean response value for base-level class).</p>

<p>However, if one or several factors are mixed with continuous predictors, then how can I see what the base-level values are at all? to what would I compare? </p>

<p>In the model output below, there are two factors:</p>

<ol>
<li>LandUse (4 Levels)</li>
<li>Type_LU (4 Levels)</li>
</ol>

<p>I see now for example that <code>LandUseLow</code> is 0.35 units higher than the base-line <code>LandUseHigh</code>. Would one now simply look at the mean response for the class <code>LandUseHigh</code> and compare? Is it that simple? </p>

<pre><code>My_model: 
                  Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)
(Intercept)      4.6086772  1.7754606   1.7773711   2.593  0.00951 **
DiTempRange     -0.1409464  0.0764872   0.0765969   1.840  0.06575 .
LandUseLow       0.3520743  0.5989777   0.5997903   0.587  0.55721
LandUseMedium    0.2741413  0.3668149   0.3675811   0.746  0.45579
LandUseNone     -1.0128945  0.5735342   0.5744652   1.763  0.07787 .
MAP              0.0048810  0.0009128   0.0009142   5.339    1e-07 ***
Rivier           0.3502782  0.2252743   0.2257546   1.552  0.12076
TempRange        0.0823410  0.0606546   0.0607942   1.354  0.17560
Tmean           -0.1762862  0.0994486   0.0996353   1.769  0.07684 .
TYPE_LUconserva -0.9312487  0.4770681   0.4781244   1.948  0.05145 .
TYPE_LUprivate  -0.4839229  0.3289011   0.3296201   1.468  0.14207
TYPE_LUstate     0.0004062  0.4079678   0.4089744   0.001  0.99921
logVRM           0.1370973  0.1140166   0.1142342   1.200  0.23008
logTWI          -0.0735540  0.4195267   0.4202589   0.175  0.86106
logDAH           1.7132823  3.5937000   3.6028996   0.476  0.63441
</code></pre>
"
"0.0511644510096651","0.0540738070435875","101541","<p>I've been trying to figure out how to properly report a two way ANOVA with two between-subject variables and their interaction. Most references I found suggest to report it in the following way: <code>F(df between-subject, df within-subject)=f-value, p=p-value</code>, e.g. <a href=""http://osil.psy.ua.edu/~Rosanna/PY602/Writing%20up%20your%20results%20%E2%80%93%20APA%20Style%20guidelines.pdf"" rel=""nofollow"">this</a> site. But at <a href=""http://staff.bath.ac.uk/pssiw/stats2/page2/page3/page3.html"" rel=""nofollow"">another place</a> I found a different way to report: <code>F(df effect, df error) = F-value, MSE = mean-square error, p-value</code>.</p>

<p>Now I am confused, because I don't have any within-subject measurements so in case the first approach is the right one I have no idea how to report my results. Here is my output from R:</p>

<pre><code>                                     Df Sum Sq Mean Sq F value Pr(&gt;F)    
 happinessLevel1                      1      5    4.73   3.449 0.0633 .  
 happinessLevel2                      1      0    0.13   0.096 0.7562    
 happinessLevel1:happinessLevel2      1    138  137.93 100.542 &lt;2e-16 ***
 Residuals                       131810 180827    1.37
</code></pre>

<p>In case I report using the second version I'd have <code>F(1,131810)=100.542, p&lt;0.001</code> right?</p>
"
"0.0361787302646211","0.0382359556450936","102703","<p>I want to perform an ANOVA using R. I have three populations, represented by their respective means and SD:
Pop.1: 5.5 +- 0.4 (n=100)
Pop.2: 5.9 +- 0.3 (n=150)
Pop.3: 6.2 +- 0.5 (n=200)</p>

<p>Which is the exact code using R to perform the ANOVA using exclusively these data?</p>

<p>Moreover, how can I perfom subsequently a post-hoc analysis?</p>

<p>Thanks in advance</p>
"
"0.0808981002113217","0.0683985568056769","103705","<p>I have a 2x2 between-subject design with unequal cell sizes. I ran an ANOVA with type I sums of squares to account for the unequal cell sizes. </p>

<p>Neither of the two main effects nor the interaction effects were significant (all ps > 0.1).</p>

<p><img src=""http://i.stack.imgur.com/WgMCm.png"" alt=""Cell means that were subjected to ANOVA""></p>

<p>With an overall of 2x2 = 4 cells/groups there are obviously 6 possible pairwise comparisons, i.e. 6 individual differences between cell means that might be statistically significant. <strong>Can I conclude from the non-significant main and interaction effects that none of these pairwise comparisons are statistically significant?</strong> </p>

<p>As the plot suggests it would be particularly interesting to examine the difference between the two means on the left-hand side, i.e gain-absent vs. gain-present.</p>

<p>Would it be legitimate/necessary to run a Tukey-Kramer test (due to unequal cell sizes) to test this? If so, how do I do this in R? (Here I'm assuming that the standard <code>TukeyHSD()</code> function would be invalid due to unequal cell sizes)</p>
"
"0.0886194286901087","0.0780488176318078","104040","<p>I am trying to understand difference between different <a href=""http://userwww.sfsu.edu/efc/classes/biol710/boots/rs-boots.htm"">resampling methods</a> (Monte Carlo simulation, parametric bootstrapping, non-parametric bootstrapping, jackknifing, cross-validation, randomization tests, and permutation tests) and their implementation in my own context using R.</p>

<p>Say I have the following situation â€“ I want to perform ANOVA with a <em>Y</em> variable (<code>Yvar</code>) and <em>X</em> variable (<code>Xvar</code>). <code>Xvar</code> is categorical. I am interested in the following things:</p>

<p>(1) Significance of p-values â€“ false discovery rate</p>

<p>(2) effect size of <code>Xvar</code> levels  </p>

<pre><code>Yvar &lt;- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)
Xvar &lt;- c(rep(""A"", 5),  rep(""B"", 5),    rep(""C"", 5))
mydf &lt;- data.frame (Yvar, Xvar)
</code></pre>

<p>Could you gel me to explain the  sampling differences with explicit worked examples how these resampling method work?</p>

<p><strong>Edits:</strong>
Here are my attempts:</p>

<p><strong>Bootstrap</strong> 
10 bootstrap samples, sample number of samples with replacement, means that samples can be repeated   </p>

<pre><code>boot.samples &lt;- list()
for(i in 1:10) {
   t.xvar &lt;- Xvar[ sample(length(Xvar), length(Xvar), replace=TRUE) ]
   t.yvar &lt;- Yvar[ sample(length(Yvar), length(Yvar), replace=TRUE) ]
   b.df &lt;- data.frame (t.xvar, t.yvar) 
   boot.samples[[i]] &lt;- b.df 
}
str(boot.samples)
 boot.samples[1]
</code></pre>

<p><strong>Permutation:</strong>
10 permutation samples, sample number of samples without replacement</p>

<pre><code> permt.samples &lt;- list()
    for(i in 1:10) {
       t.xvar &lt;- Xvar[ sample(length(Xvar), length(Xvar), replace=FALSE) ]
       t.yvar &lt;- Yvar[ sample(length(Yvar), length(Yvar), replace=FALSE) ]
       b.df &lt;- data.frame (t.xvar, t.yvar) 
       permt.samples[[i]] &lt;- b.df 
    }
    str(permt.samples)
    permt.samples[1]
</code></pre>

<p><strong>Monte Caro Simulation</strong> </p>

<p><a href=""http://ww2.coastal.edu/kingw/statistics/R-tutorials/resample.html"">Although the term ""resampling"" is often used to refer to any repeated random or pseudorandom sampling simulation, when the ""resampling"" is done from a known theoretical distribution, the correct term is ""Monte Carlo"" simulation.</a></p>

<p>I am not sure about all above terms and whether my above edits are correct. I did find some information on <a href=""http://www.math.ntu.edu.tw/~hchen/teaching/LargeSample/references/R-bootstrap.pdf"">jacknife</a> but I could not tame it to my situation. </p>
"
"0.0626633989716535","0.0662266178532522","104548","<p>I followed <a href=""http://rtutorialseries.blogspot.hk/2010/01/r-tutorial-series-basic-hierarchical.html"" rel=""nofollow"">this tutorial</a> to learn Hierarchical Linear Regression (HLR) in R, but couldn't understand how to interpret its sample output of <code>&gt;anova(model1,model2,model3)</code></p>

<p><img src=""http://i.stack.imgur.com/MxXIM.png"" alt=""enter image description here""></p>

<p>The tutorial simply says </p>

<blockquote>
  <p>each predictor added along the way is making an important contribution to the overall model.</p>
</blockquote>

<p>But I would like some more details to <strong>quantify</strong> the contribution of each explanatory variable, like:</p>

<ol>
<li><p>""UNEM"" explains <code>X</code> (or <code>X%</code>) variance</p></li>
<li><p>Adding the ""HGRAD"" variable explains <code>Y</code> (or <code>Y%</code>) more variance</p></li>
<li><p>Adding the ""INC"" variable further explains <code>Z</code> (or <code>Z%</code>) more variance</p></li>
</ol>

<p>So, can I get the value of <code>X</code>, <code>Y</code>, and <code>Z</code> using the above ANOVA table? How? Specifically, what do <code>Res.Df</code>, <code>RSS</code>, <code>Sum of Sq</code> mean in this ANOVA table?</p>
"
"0.173507095076498","0.175400453519478","105906","<blockquote>
  <p>The bounty I placed on this question expires in the next 24 hours.</p>
</blockquote>

<p>I have a psychological data set which, traditionally, would be analysed using a paired samples t test.
The design of the experiment is $39 (subjects) \times 7 (targets) \times 2 (conditions)$, and I'm interested in the difference in a given variable between the conditions.</p>

<p>The traditional approach has been to average across targets so that I have 2 observations per participant, and then compare these averages using a paired t test.</p>

<p>I wanted to use a mixed models approach, as has become increasingly popular in this field (i.e. <a href=""http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf"" rel=""nofollow"">Baayen, Davidson &amp; Bates, 2008</a>), and so the first model I fit, which I thought should approximate the results of the t test, was one with $condition$ as a fixed effect, and random intercepts for $subjects$ (i.e. $var = \alpha + \beta*condition + Intercept(subject) + \epsilon$. Obviously, the full model would also include random intercepts for $targets$.</p>

<p>However, I'm struggling to understand why I achieve pretty divergent results between the two approaches.
Can anyone explain what's going on here?
I've also seen (what I understand to be) a similar question asked <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">here</a>, with an answer about correlation structure which I'm not equipped to understand. If this is also what's at issue here, I would appreciate if anyone could suggest some resources to read up on this.</p>

<p><strong>Edit:</strong> I've posted <a href=""https://gist.github.com/EoinTravers/ce86c93fb42fba284464"" rel=""nofollow"">the example data, and R script, here</a>.</p>

<p><strong>Edit #2 - Bounty added</strong></p>

<p>Some additional points:</p>

<ul>
<li>I'm only analysing the correct responses (think of it as analogous to reaction time), so there are <strong>missing cases</strong> - not every participant provides 7 data points per condition.
<ul>
<li>When I analyse all responsees, rather than just the correct ones, the difference between the two results is reduced, but not eliminated. This suggests to me that the missing cases are a factor here.</li>
</ul></li>
<li>The variable isn't normally distributed. In my final model, I scale it using a Box-Cox transformation, but I omit that here for consistency with the t test.</li>
<li>As pointed out by @PeterFlom, the $df$s differ hugely between the two approaches, but I assume this to be because the t test is being applied to the aggregate data (2 observations per participant, 1 per condition), while the mixed model is applied to raw scores ($&lt;14$ observations per participant, $&lt;7$ per condition).</li>
<li>@BenBolker notes that the t values also differ pretty considerably.</li>
</ul>

<p>My analysis code is below.</p>

<pre><code>&gt;library(dplyr)
&gt;subject_means = group_by(data, subject, condition) %&gt;% summarise(var=mean(var))
&gt;t.test(var ~ condition, data=subject_means, paired=T)

    Paired t-test

data:  var by condition
t = -1.3394, df = 37, p-value = 0.1886
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.14596388  0.02978745
sample estimates:
mean of the differences 
            -0.05808822 

&gt;library(lme4)
&gt;lm.0 = lmer(var ~ (1|subject), data=data)
&gt;lm.1 = lmer(var ~ condition + (1|subject), data=data)
&gt;anova(lm.0, lm.1)

Data: data
Models:
object: var ~ (1 | subject)
..1: var ~ condition + (1 | subject)
       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
object  3 489.09 501.23 -241.55   483.09                           
..1     4 485.81 502.00 -238.90   477.81 5.2859      1     0.0215 *

&gt;library(lmerTest)
&gt;summary(lm.1)$coef

              Estimate Std. Error        df  t value     Pr(&gt;|t|)
(Intercept) 0.11862462 0.02878027  98.60659 4.121734 7.842075e-05
condition   0.09580546 0.04161237 400.27441 2.302331 2.182890e-02
</code></pre>

<p>Notice, specifically, the jump in the p value from $p = .188$ in the t test, to $p = .021$ from either <code>lmer</code> method.</p>

<hr>

<p>I've tried, and failed to provide a reproducible example of this, using the <code>anorexia</code> dataset in the <code>MASS</code> package, so I would assume the problem is something idiosyncratic to my data, but I don't understand what.</p>

<pre><code># Borrowing from http://ww2.coastal.edu/kingw/statistics/R-tutorials/dependent-t.html
&gt;data(anorexia, package=""MASS"")
&gt;ft = subset(anorexia, subset=(Treat==""FT""))
&gt;wgt = c(ft$Prewt, ft$Postwt)
&gt;pre.post = rep(c(""pre"",""post""),c(17,17))
&gt;subject = rep(LETTERS[1:17],2)
&gt;t.test(wgt~pre.post, data=ft.new, paired=T)

    Paired t-test

data:  wgt by pre.post
t = 4.1849, df = 16, p-value = 0.0007003
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3.58470 10.94471
sample estimates:
mean of the differences 
               7.264706 

&gt;m = lmer(wgt ~ pre.post + (1|subject), data=ft.new)
&gt;summary(m)$coef

             Estimate Std. Error       df   t value     Pr(&gt;|t|)
(Intercept) 90.494118   1.689013 26.17129 53.578096 0.0000000000
pre.postpre -7.264706   1.735930 15.99968 -4.184908 0.0007002806
</code></pre>
"
"0.140119619801808","0.148087219439773","106227","<p>I have the following data with factor 1 (A, B and C) and factor 2 (D and E):
$$
\begin{array}{ccc}
\hline
 &amp; D &amp; E\\
\hline
A &amp; 68 &amp; 59\\
 &amp; 65 &amp; 57\\
 &amp; 63 &amp; 54\\
 &amp; 59 &amp; 56\\
 &amp; 67 &amp; \\
\hline
B &amp; 59 &amp; 51\\
 &amp; 50 &amp; 45\\
 &amp; 51 &amp; 46\\
 &amp;    &amp; 48\\
 &amp;    &amp; 49\\
\hline
C &amp; 40 &amp; 47\\
 &amp; 39 &amp; 39\\
 &amp; 35 &amp; 40\\
 &amp; 36 &amp; 40
\end{array}
$$</p>

<p>Now I would like to make two-way ANOVAs of type I, II and III with R and interpret them.</p>

<p>I made the following:</p>

<p><code>
rm(list=ls(all=T))
data &lt;- c(68,65,63,59,67,59,50,51,40,39,35,36,59,57,54,56,51,45,46,48,49,47,39,40,40)
f1 &lt;- c(0,0,0,0,0,1,1,1,2,2,2,2,0,0,0,0,1,1,1,1,1,2,2,2,2) ### A=0,B=1,C=2
f2 &lt;- c(3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4) ### D=3,E=4
summary(aov(data ~ f1*f2)) ### type 1
install.packages(""car"")
library(car)
Anova(lm(data~f1*f2),type=c(""II"")) ### type 2
Anova(lm(data~f1*f2),type=c(""III"")) ### type 3
</code></p>

<hr>

<p>For type 1 I get
<code>
Df Sum Sq Mean Sq F value   Pr(>F)<br>
f1           1 1941.9  1941.9 193.420 4.58e-12 <strong>*
f2           1   65.5    65.5   6.527  0.01845 *<br>
f1:f2        1  145.9   145.9  14.537  0.00102 ** 
Residuals   21  210.8    10.0<br>
Signif. codes:  0 â€˜<em></strong>â€™ 0.001 â€˜*</em>â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></p>

<hr>

<p>For type 2 I get
<code>
Response: data
           Sum Sq Df F value    Pr(>F)<br>
f1        1901.11  1 189.355 5.607e-12 <strong>*
f2          65.53  1   6.527  0.018450 *<br>
f1:f2      145.95  1  14.537  0.001016 ** 
Residuals  210.84 21<br>
Signif. codes:  0 â€˜<em></strong>â€™ 0.001 â€˜*</em>â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></p>

<hr>

<p>For type 3 I finally get
<code>
Response: data
             Sum Sq Df F value    Pr(>F)<br>
(Intercept) 1802.06  1 179.489 9.308e-12 <strong>*
f1           329.87  1  32.856 1.089e-05 <em></strong>
f2           208.54  1  20.771 0.0001714 <strong></em>
f1:f2        145.95  1  14.537 0.0010157 ** 
Residuals    210.84 21<br>
Signif. codes:  0 â€˜<em></strong>â€™ 0.001 â€˜*</em>â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></p>

<p>Is that allright? 
How would you interpret these results?</p>
"
"0.177238857380217","0.179512280553158","108280","<p>I know that there are already a host of questions about nested designs but many of them haven't been answered or come from biological domains which I sometimes find hard to transfer to my domain.</p>

<p>I am currently trying to analyse the data from a psychological study. Participants read four statements and indicated how appropriate they found each on a 5-point Likert-scale. But I'll just call the outcome variable ""out"" and so the content will not distract from the statistical problem at hand.</p>

<p>There are two IVs, one that was varied on three levels between subjects (let's call it ""between"" and the levels 1, 2 and 3) and one on two levels within subjects (""within"" with its levels a and b). The within subjects factor is realised in such a manner that two of the four statements correspond to one of the levels and the other two to the other. </p>

<p>Correspondingly, for every participant, I now have four ""out"" data points, two for the a statements and two for the b statements, like this (in long format):</p>

<pre><code>subj    between within  item    out
123     1       a       a1      3
123     1       a       a2      4
123     1       b       b1      1
123     1       b       b2      2
124     2       a       a1      5
124     2       a       a2      4
124     2       b       b1      2
124     2       b       b2      3
125     3       a       a1      1
125     3       a       a2      1
125     3       b       b1      2
125     3       b       b2      3
</code></pre>

<p>and so on
What is the right way to analyse the effect and interaction of between and within? I assume that the two different a and the two different b statements do not differ systematically but are just two instances of the same thing. Or do I need an additional factor that tells R which of the four statements it is in order to allow for that error? Anyway, I have tried these options:</p>

<pre><code>m1 &lt;- aov(out~between*within+Error(subj/within))
summary(m1)

m2 &lt;- lm(out~between*within+within/subj)
anova(m2)

m3 &lt;- lmer(out~between*within+(between|subj))
Anova(m3)
</code></pre>

<p>They produce different results but I'm not entirely sure about which one is right or what the differences are. Could anyone enlighten me on this? I assume this is about fixed and random effects which always gets my head in a twist. I have read other posts about nested designs and I think <a href=""https://stats.stackexchange.com/questions/94882/repeated-measures-mixed-model-using-lmer-in-r"">this</a> one comes nearest. But unfortunately the question ID being nested groups wasn't answered. Any help would be very much appreciated!</p>

<p><strong>Edit:</strong><br>
I greatly appreciate the answers so far! The comments prompted me to read some more tutorials on mixed effects models as well as answers to similar questions. This enables me to clarify my question: I know that I will need to specify random intercepts at least for subjects and probably also the four different items representing the two levels of the within factors. However, I am unsure about the specific fixed/random effects structure I would have to model, and specifically what the maximal model in my case would look like.
Right now, I have tried some code which roughly looks like this:</p>

<pre><code>m1 &lt;- lmer(out ~ between * within + (between|subj) + (within|subj) + (between|within/item))
</code></pre>

<p>This is probably wrong on several levels so any feedback would be greatly appreciated! Just for clarity: The item factor has no intrinsic meaning, but both levels of the within factor is realised by two items each (which every participant sees) and I am unsure whether this is relevant to the model.</p>

<p>Apart from the model specification, I am still getting confused about the concepts nested and crossed and which one applies to my setup. Lastly, most of my models fail to converge but that is probably a matter for another question (and has been discussed here in length.)</p>
"
"0.0511644510096651","0.0540738070435875","108899","<p>Can anyone explain the theory (or the formula) about computing Sum Sq (bold highligh below) related to regression items?  The Wikipedia <a href=""http://en.wikipedia.org/wiki/Partition_of_sums_of_squares"" rel=""nofollow"">link</a> gives an introduction on how to calculate the total, model, and regression sum of squares. Is it similar to the Sum Sq computation? Is the regression sum of squares equal to (0.000437+ 0.002545+ 0.060984+ 0.062330+ 0.060480)?</p>

<pre><code>TraingData &lt;- data.frame(x1 = c(3.532,2.868,2.868,3.532,2.868,2.536,3.864),
                         x2 = c(1.992,1.992,1.328,1.328,1.328,1.66,1.66),
                         y  = c(9.040330254,8.900894412,8.701929163,9.057944749,
                                8.701929163,8.74317832,9.10859913)
                         )
lm.sol &lt;- lm(y~1+x1+x2+I(x1^2)+I(x2^2)+I(x1*x2), data=TraingData)
anova(lm.sol)

Analysis of Variance Table

Response: y
            Df   **Sum Sq**     Mean       Sq F    value Pr(&gt;F)
x1          1   0.000437  0.000437    0.1055    0.8001
x2          1   0.002545  0.002545    0.6141    0.5768
I(x1^2)     1   0.060984  0.060984   14.7162    0.1623
I(x2^2)     1   0.062330  0.062330   15.0409    0.1607
I(x1 * x2)  1   0.060480  0.060480   14.5945    0.1630
Residuals   1   0.004144  0.004144  
</code></pre>
"
"0.0808981002113217","0.0854981960070962","110531","<p>Non-parametric ANOVA â€“ a hot topic that is unanswered. </p>

<p>There are many questions on this topic online. However, they all seem to end in a debate and no definite answer or clear explanation (that I can relate to my data set). The only good answers I can find seem to relate to one treatment, that was repeated over time. How do I handle two treatments? More importantly, should this be handled as a linear model? </p>

<p>I cannot meet assumptions of homogeneity, normality, equal sample size, and transformations were not helpful.</p>

<p>Data setup: </p>

<ul>
<li>Soil cores growing plants.</li>
<li>Subject â€“ about 33 samples. A core will consist of a treatment combo (<em>n</em> = 5 to 6; repeats)</li>
<li><code>Treatment01</code> - clay, sand, loamy sand</li>
<li><code>Treatment02</code> - water, fertiliser</li>
<li>Time - an sample is analysed from each core sample, every week, for 3 months.</li>
<li>Variable - Nutrients (mg/kg soil), moisture, pH, mass of soil etc. For simplicity, let's do carbon.</li>
</ul>

<p>Here are my failed attempts on R. This problem rests in the statistical theory. </p>

<p><code>Anova(lm(Carbon ~ Treatment01 * Treatment02, data), type = ""3"")</code><br>
or
<code>model1 &lt;- lm(Carbon ~ Treatment01 * Treatment02)</code></p>

<p>Then something like...</p>

<pre><code>print(lsmeans(model1, list(pairwise ~ Treatment01)), adjust = c(""tukey""))
print(lsmeans(model1, list(pairwise ~ Treatment02)), adjust = c(""tukey""))
print(lsmeans(model1, list(pairwise ~ Treatment01 | Treatment02)), adjust = c(""tukey""))
print(lsmeans(model1, list(pairwise ~ Treatment02 | Treatment01)), adjust = c(""tukey""))
</code></pre>

<p>Any suggestions? What are the disadvantages in pulling the data set apart to do a series of Friedman or Kruskalâ€“Wallis tests (depending on how much I pull this data set apart?)</p>
"
"0.0886194286901087","0.0936585811581694","110632","<p>If there are 2 nlme models with same non-linear mean function, model 1 and model 2, how do you compare them ? Which R function does this for us ?</p>

<p>And when there are random effects or fixed effects, I don't know how a nested model is defined ? </p>

<p>For example model 1, </p>

<pre><code>`model1  &lt;- nlme(df_measures ~ meanfunc(w, time, b0,b1,b2) , 
            fixed = list(b0 ~1, b1 ~ 1, b2 ~ 1),
            random = b0 + b1 + b2 ~ 1,
            groups = ~ subject ,
            data = dataa,
            start = list(fixed = c(b0 = 3,1, b1 = 5,1,b2 = 1,1)),
            verbose = T
            )`
</code></pre>

<p>and model 2: </p>

<pre><code>`model2 &lt;- nlme(df_measures ~ meanfunc(w, time, b0,b1,b2) , 
            fixed = list(b0 ~w, b1 ~ w, b2 ~ w),
            random = b0 + b1 + b2 ~ 1,
            groups = ~ subject ,
            data = dataa,
            start = list(fixed = c(b0 = 3,1, b1 = 5,1,b2 = 1,1)),
            verbose = T
            )`
</code></pre>

<p>Is model2 a nested model of model 1 ? This is confusing me and how do I compare them ? </p>

<p>If model 2 is a nested model of model 1, can I still use <code>ANOVA</code>  function in R to compare them ? 
What if model 2 is not a nested model of model 1 ? How should I compare between these 2 models? Based on what criteria ? </p>

<p>Thanks.</p>
"
"0.108536190793863","0.114707866935281","110917","<p>Suppose I create a dummy scenario as such:</p>

<pre><code>&gt; A &lt;- rnorm(10000) 
&gt; B &lt;- rnorm(10000) 
&gt; C &lt;- rnorm(10000) 
&gt; Y &lt;- A*B + rnorm(10000,sd=0.1)
</code></pre>

<p>Doing a simple ANOVA correctly identifies that none of the variables are significantly predictive of the outcome:</p>

<pre><code>&gt; anova(lm(Y~A+B+C))
Analysis of Variance Table

Response: Y
            Df  Sum Sq Mean Sq F value Pr(&gt;F)
A            1     1.5 1.54411  1.4209 0.2333
B            1     0.3 0.28909  0.2660 0.6060
C            1     1.6 1.62425  1.4946 0.2215
Residuals 9996 10862.8 1.08672    
</code></pre>

<p>But not let's say I decide to include the interaction terms:</p>

<pre><code>&gt; anova(lm(Y~A*B*C))
Analysis of Variance Table

Response: Y
           Df  Sum Sq Mean Sq    F value    Pr(&gt;F)    
A            1     1.5     1.5 1.5281e+02 &lt; 2.2e-16 ***
B            1     0.3     0.3 2.8610e+01  9.05e-08 ***
C            1     1.6     1.6 1.6074e+02 &lt; 2.2e-16 ***
A:B          1 10761.8 10761.8 1.0650e+06 &lt; 2.2e-16 ***
A:C          1     0.0     0.0 9.8700e-02    0.7534    
B:C          1     0.0     0.0 1.5062e+00    0.2197    
A:B:C        1     0.0     0.0 1.6790e-01    0.6820    
Residuals 9992   101.0     0.0                         
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>It has correctly identified the interaction between A and B as being the most significant, but now for some reason the individual terms A and B have also gained significance... and C which had nothing at all to do with creating the model is significant as well?  Either I have not written the test correctly or I am completely misunderstanding how a Two-Way ANOVA with interaction terms works</p>

<p>Using a simple linear model gives expected results:</p>

<pre><code>&gt; summary(lm(Y~A*B*C))

Call:
lm(formula = Y ~ A * B * C)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.29566 -0.06667 -0.00092  0.06665  0.33620 

Coefficients:
              Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept) -0.0003212  0.0009707   -0.331    0.741    
A            0.0003483  0.0009613    0.362    0.717    
B            0.0003184  0.0009619    0.331    0.741    
C           -0.0003213  0.0009702   -0.331    0.741    
A:B          1.0008711  0.0009370 1068.214   &lt;2e-16 ***
A:C         -0.0014855  0.0009588   -1.549    0.121    
B:C          0.0008860  0.0009561    0.927    0.354    
A:B:C       -0.0002489  0.0009085   -0.274    0.784    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.09705 on 9992 degrees of freedom
Multiple R-squared:  0.9913,    Adjusted R-squared:  0.9913 
F-statistic: 1.634e+05 on 7 and 9992 DF,  p-value: &lt; 2.2e-16
</code></pre>
"
"0.0511644510096651","0.0540738070435875","111189","<p>I have a rather simple question (I hope) but for some reason I can't find the right/ a working answer anywhere on the web.</p>

<p>For my research I have measured mood on a scale 1-10, 6 times a day for 5 weeks. (thus a repeated measures design if i am not mistaken). The participants were assigned to three conditions based on anxiety scores (n= 15, 15, 5 respectively). What I want to do is comparing mood between the three groups to see if they differ.</p>

<p>I know I can't use a normal anova because the mood measurements aren't independable. and because I have over 6000 entries even the smallest difference is significant. I also tried calculating the mean mood of each particpant and then conduct the anova using just the mean mood of the participants, but then I lose so much.</p>

<p>What would be the correct way to test for group difference.</p>

<p>By the way: I am using R and the data frame is in a long format (rows = individual mood entries, column = mood rating and group)</p>

<p>I hope someone can help me out.</p>
"
"0.0808981002113217","0.0683985568056769","111902","<p>I am conducting a two-sample test (1-way ANOVA with 2 treatments), and the goal is to estimate the ratio of cell means assuming that the data are lognormal. A simple approach is to log the response and fit a model </p>

<p>$\log Y = b_0 + b_1 * X$</p>

<p>and then estimate the ratio as</p>

<p>$R = e^{b_1}$</p>

<p>However, that gives the ratio of geometric cell means rather than arithmetic cell means. </p>

<p>I assumed that if I fit a ""proper"" lognormal model using either <code>gamlss</code> in R or <code>PROC GLIMMIX</code> in SAS, I will get the ratio of arithmetic means, but for some reason both procedures generate the same slope as the $\log Y$ regression.</p>

<p>This is odd because when I use this approach with Poisson or Negative Binomial regression, I do get the ratio of arithmetic means. What am I missing?</p>

<hr>

<p>P.S.</p>

<p>I think I identified the source of confusion, but I don't have an explanation for it. A lognormal setup with the identity link function is:</p>

<p>$\log Y_1 \sim N(b_0, \sigma^2)$</p>

<p>$\log Y_2 \sim N(b_0 + b_1, \sigma^2)$</p>

<p>which implies </p>

<p>$\frac{E[Y_2]}{E[Y_1]} = \frac{e^{b_0 + b_1 +\sigma^2/2}}{e^{b_0 + \sigma^2/2}} = e^{b_1}$</p>

<p>To me, it means that $e^{b_1}$ should have a point estimate equal to the ratio of arithmetic means for the original response.</p>

<p>On the other hand,</p>

<p>$E[\log Y_1] = b_0$</p>

<p>$E[\log Y_2] =  b_0 + b_1$</p>

<p>$b_0$ is estimated as arithmetic mean of $\log Y_1$, $b_0 + b_1$ is estimated as arithmetic mean of $\log Y_2$. Hence, $e^{b_1}$ should have
a point estimate equal to the ratio of geometric means for the original response, and it does, given the output from those two packages. Where did I make a mistake?</p>
"
"0.0361787302646211","0.0382359556450936","112552","<p>I'm running mixed design ANOVA using R.<br>
Somehow, I got incorrect values for the degrees of freedom (Df). There are two factors: <code>temperature</code> (2 levels) and <code>identity</code> (7 levels) and when I perform this command :  </p>

<p><code>aov4&lt;- aov(NO3.means~temperature*identity,data=mydata)</code></p>

<p>I get:</p>

<pre><code>                     Df  Sum Sq Mean Sq F value   Pr(&gt;F)    
temperature           1 3936158 3936158  24.519 1.54e-05 ***
identity              1   71125   71125   0.443  0.50967    
temperature:identity  1 1363345 1363345   8.493  0.00595 ** 
Residuals            38 6100236  160533                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>the degrees of freedom for identity should be 6 rather than 1. What is the problem?</p>
"
"0.0626633989716535","0.0441510785688348","113002","<p>Is there a statistical method to compare these density plots other than ANOVA (MANOVA)? I would like to compare the densities among year within each plot and report which of those distributions are ""significantly different"". The reason is that I am more interested in the spread and where on the distribution of the dataset rather than solely on the mean and variance. Thanks</p>

<p>Here is the code and plot</p>

<pre><code>ggplot(NMPSCFAM, aes(Length, colour= Year)) + 
  geom_density(aes(y = ..count..), alpha=0.3) + xlim(0, 700) +  
  geom_density(aes(colour = Year)) +
  xlab("""") +
  ylab(expression(paste(""Mean density ( "", m^2, "")"", sep = """"))) +
facet_wrap( ~ Family + Sector2, ncol=3, scales = ""free"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/vI6pI.png"" alt=""enter image description here""></p>

<hr>

<p>Edit: Ok so no answers so far. If I did these density plots in a histogram would there be a way to do a Chi-square test on each of the plots? Could any one help me out on how to do this in R?</p>
"
"0.0511644510096651","0.0540738070435875","113690","<p>I have used ANOVA function so that I can get the overall p value of significant factors:</p>

<pre><code>&gt; anova(lmer81)
Analysis of Variance Table
                    Df Sum Sq Mean Sq F value
sex                  1   0.12   0.118  0.0195
vowel3               2 399.96 199.982 33.0859
Language             2 120.41  60.204  9.9604
sex:vowel3           2  89.73  44.865  7.4227
sex:Language         2 166.93  83.463 13.8084
vowel3:Language      4  48.27  12.067  1.9964
sex:vowel3:Language  4  52.76  13.189  2.1821
</code></pre>

<p>However, this does not give me p value. Can I ask how I can get p value from this?</p>
"
"0.0886194286901087","0.0780488176318078","113756","<p>I'd like to test the <em>anova rbf kernel</em> included in the <strong>kernlab</strong> package in <strong>caret</strong>. Following excelent tutorial (<a href=""https://topepo.github.io/caret/custom_models.html"" rel=""nofollow"">https://topepo.github.io/caret/custom_models.html</a>) I've come up with the following code:</p>

<pre><code>SVManova &lt;- list(type = ""Regression"", library = ""kernlab"", loop = NULL)
prmanova &lt;- data.frame(parameter = c(""C"", ""sigma"", ""degree"", ""eps""),
                     class = rep(""numeric"", 4),
                     label = c(""Cost"", ""Sigma"", ""Degree"", ""Eps""))
SVManova$parameters &lt;- prmanova
    svmGridanova &lt;- function(x, y, len = NULL) {
    library(kernlab)
    sigmas &lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)
    expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,
                C = 2 ^(-5:len), degree = 1:2) # len = tuneLength in train
    }
    SVManova$grid &lt;- svmGridanova
svmFitanova &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  ksvm(x = as.matrix(x), y = y,
       kernel = ""anovadot"",
       kpar = list(sigma = param$sigma, degree = param$degree),
       C = param$C, epsilon = param$epsilon,
       prob.model = classProbs,
       ...) #default type = ""eps-svr""
}
SVManova$fit &lt;- svmFitanova
    svmPredanova &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
      predict(modelFit, newdata)
    SVManova$predict &lt;- svmPredanova
svmProb &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type=""probabilities"")
SVManova$prob &lt;- svmProb
    svmSortanova &lt;- function(x) x[order(x$C), ]
SVManova$sort &lt;- svmSortanova
</code></pre>

<p>I then asked for the model to train some dataset:</p>

<pre><code>set.seed(100) #use the same seed to train different models
svrFitanova &lt;- train(R ~ .,
                data = trainSet,
                method = SVManova,
                preProc = c(""center"", ""scale""),
                trControl = ctrl, tuneLength = 20,
                allowParallel = TRUE) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = ""ROC""
#Print the results
svrFitanova
</code></pre>

<p>But I get the following error:</p>

<pre><code>Error in train.default(x, y, weights = w, ...) : 
  The tuning parameter grid should have columns C, sigma, degree, eps
</code></pre>

<p>I don't see why this error occurs.... tune grid has four columns as requested... Any ideas? Thanks</p>
"
"0.0964766140389895","0.0764719112901873","114390","<p>I am looking for help on post-hoc tests of my group data (treatment and stage and interaction) after running a 2 way ANOVA in R. The data shown below is an example only. Actually my data shows significant result after ANOVA test. When I tried post hoc with TukeyHSD it gives pair wise comparisons which is a long list. But I am looking for other test where I can compare treatment mean alone, stage mean alone as well as stage*treatment with letter after mean showing significant or non-significant in group. Could there be other post-hoc test which fulfils my need?</p>

<p>My data looks like:</p>

<pre><code>Treatment   Stage   Chlorophyll
Salt        Green   0.2
Salt        Green   0.3
Salt        Green   0.4
Salt        Pink    0.5
Salt        Pink    0.3
Salt        Pink    0.2
Salt        Red     0.5
Salt        Red     0.6
Salt        Red     0.7
Nitrogen    Green   0.4
Nitrogen    Green   0.6
Nitrogen    Green   0.9
Nitrogen    Pink    0.2
Nitrogen    Pink    0.3
Nitrogen    Pink    0.5
Nitrogen    Red     0.4
Nitrogen    Red     0.2
Nitrogen    Red     0.3
Control     Green   0.5
Control     Green   0.6
Control     Green   0.8
Control     Pink    0.5
Control     Pink    0.4
Control     Pink    0.6
Control     Red     0.2
Control     Red     0.3
Control     Red     0.1
</code></pre>
"
"0.207830982477317","0.21964884255349","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.125326797943307","0.110377696422087","115154","<p>Relatively new to stats. I use linear regression  and get R^2, which is quite low.</p>

<p><strong>MODEL 1</strong></p>

<pre><code>    lmoutar=lm(formula = ts_y ~ ts_y_lag + ts_x)
</code></pre>

<p>So switched to arima with external regressor. Using ""auto.arima"", I formulate arimax model</p>

<p><strong>MODEL 2</strong></p>

<pre><code>    fitarima &lt;- auto.arima(ts_y, xreg=ts_x)
    arimaout&lt;-arima(ts_y,order=c(2,0,5),xreg=ts_x)
</code></pre>

<p>How can I compare the explanability of AR model with arima model. From the thread <a href=""http://stats.stackexchange.com/questions/8750/how-can-i-calculate-the-r-squared-of-a-regression-with-arima-errors-using-r"">How can I calculate the R-squared of a regression with arima errors using R?</a>, I understand R^2 is not an option for ARIMA.</p>

<p>From the thread <a href=""http://stats.stackexchange.com/questions/11850/model-comparison-between-an-arima-model-and-a-regression-model"">Model comparison between an ARIMA model and a regression model</a>, AIC/BIC is not the right criteria and MSE from forcast/predict can be possible criteria for comparison across AR and ARIMA model. Is MSE the best option for model comparison, if so how would I generate MSE for AR and ARIMA?</p>

<p>I tried to compare the above ar and arima model using anova, but I get following error message</p>

<pre><code>anova.lm(lmoutar,arimaout)
   Warning message:
    In anova.lmlist(object, ...) :
            models with response â€˜""NULL""â€™ removed because response differs from model 1
</code></pre>

<p>What does this error message mean? </p>

<p><strong><em>EDIT</em></strong></p>

<p>Thanks for the response so far and insight that AR is nested within ARIMA. How would one answer this question, if I rephrase  as ""How to compare AR, ARIMA and General Linear Models?"". The first model I listed has AR(1) and independent variable; it is a general linear model. So how would I compare a GLM versus ARIMAX model? Any thing else besides MSE that I could use to judge between GLM and ARIMAX</p>
"
"0.12041009266179","0.137861698644752","115304","<p>I am learning about building linear regression models by looking over someone elses R code.  Here is the example data I am using:</p>

<pre><code>v1  v2  v3  response
0.417655013 -0.012026453    -0.528416414    48.55555556
-0.018445979    -0.460809371    0.054017873 47.76666667
-0.246110341    0.092230159 0.057435968 49.14444444
-0.521980295    -0.428499038    0.119640369 51.08888889
0.633310578 -0.224215856    -0.153917427    48.97777778
0.41522316  0.050609412 -0.642394965    48.5
-0.07349941 0.547128578 -0.539018121    53.95555556
-0.313950353    0.207853678 0.713903994 48.16666667
0.404643796 -0.326782199    -0.785848428    47.7
0.028246796 -0.424323318    0.289313911 49.34444444
0.720822953 -0.166712488    0.323246062 50.78888889
-0.430825851    -0.308119827    0.543823856 52.65555556
-0.964175294    0.661700584 -0.11905972 51.03333333
-0.178955757    -0.11148414 -0.151179885    48.28888889
0.488388035 0.515903257 -0.087738159    48.68888889
-0.097527627    0.188292773 0.207321867 49.86666667
0.481853599 0.21142728  -0.226700254    48.38888889
1.139561277 -0.293574756    0.574855693 54.55555556
0.104077762 0.16075114  -0.131124443    48.61111111
</code></pre>

<p>I read in the data and use a call to <code>lm()</code> to build a model:</p>

<pre><code>&gt; my_data&lt;- read.table(""data.csv"", header = T, sep = "","")
&gt; my_lm &lt;- lm(response~v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, data=my_data)
&gt; summary(my_lm)

Call:
lm(formula = response ~ v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, 
data = my_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0603 -0.6615 -0.1891  1.0395  1.8280 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.33944    0.42089 117.226  &lt; 2e-16 ***
v1            0.06611    0.82320   0.080  0.93732    
v2           -0.36725    1.06359  -0.345  0.73585    
v3            0.72741    1.00973   0.720  0.48508    
v1:v2        -2.54544    2.21663  -1.148  0.27321    
v1:v3         0.80641    2.77603   0.290  0.77640    
v2:v3       -12.16017    3.62473  -3.355  0.00573 ** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.375 on 12 degrees of freedom
Multiple R-squared:  0.697, Adjusted R-squared:  0.5455 
F-statistic:   4.6 on 6 and 12 DF,  p-value: 0.01191
</code></pre>

<p>Following along with their code I then use a call to <code>anova()</code>:</p>

<pre><code>&gt; my_lm_anova &lt;- anova(my_lm)
&gt; my_lm_anova
Analysis of Variance Table

Response: response
          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
v1         1  0.0010  0.0010  0.0005 0.982400   
v2         1  0.2842  0.2842  0.1503 0.705036   
v3         1  9.8059  9.8059  5.1856 0.041891 * 
v1:v2      1  4.3653  4.3653  2.3084 0.154573   
v1:v3      1 16.4582 16.4582  8.7034 0.012141 * 
v2:v3      1 21.2824 21.2824 11.2545 0.005729 **
Residuals 12 22.6921  1.8910                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, I am not sure:</p>

<ol>
<li>Why I would use the call to ANOVA in this situation, and</li>
<li>What the ANOVA table is telling me about the predictor variables.</li>
</ol>

<p>From the code they appear to use the ANOVA table as follows.  For predictor variable v1, the result of </p>

<ul>
<li>Adding the 'Sum Sq' entry for v1 together with half of the 'Sum Sq' entry for v1:v2 and half of the 'Sum Sq' entry for v1:v3, </li>
<li>Dividing by the sum of the entire 'Sum Sq' column, and</li>
<li>Multiplying by 100</li>
</ul>

<p>gives the percent of variance of the response variable that is explained by predictor variable v1 in the <code>lm()</code> model.  I don't see why this is nor why half of the 'Sum Sq' entry for v1:v2 is attributed to v1 and half to v2.  Is this just convenience?</p>
"
"0.0808981002113217","0.0683985568056769","116196","<p>My goal is to investigate a dependent variable which is metric (time in hours). The independent variables include 3 metric, 2 binary (factors), and one factor variable, which consists of 11 districts of a city.</p>

<p>I tried to conduct a GLM.</p>

<p>Can I put all this together in one model? It seems to be difficult to interpret the output!
Should I rather use different/various models, with only one factor per regression?
If I use the GLM which kind of family and link function should I use?</p>

<p>The idle time seems to be a right skewed distribution and thefore I chose a gamma family with a inverse link function. </p>

<p>The output of a model wich contains all the independent variables as decribed above: </p>

<p><img src=""http://i.stack.imgur.com/N1hEi.jpg"" alt=""enter image description here""></p>

<p>How can I eleminate the NAs? Or what do they actually mean? </p>

<p>Moreover the ANOVA test was conducted to get a closer look on the district variable called Bezirk, which shows massive differences in the mean value! Is this consistent with small coefficients in the GLM regression? (The means vary between from 3,7 in T.Mitte and 15 in T.Treptow)</p>

<p>Best regards</p>
"
"0.0511644510096651","0.0540738070435875","116398","<p>I want to test if the means of different groups (tissue types) are different using one-way ANOVA. Most of the groups have replicates but there is one group that only has one sample. Like this:</p>

<p>heart, heart, heart, skin, skin, skin,skin, bone, bone, bone, brain</p>

<p>In the above example, only one sample/data point for brain and the other three tissues have replicates. </p>

<p>So can ANOVA work for this data or it has to require all groups have replicates?</p>
"
"0.0626633989716535","0.0662266178532522","116487","<p>I need to predict payment day of the month (1-31) for each client (I have at most 9 month of payments and on average is 5). I have both categorical variables and numerical. I tried to use rpart to do a regression tree (method='anova') but I'm not sure if it's using the nominal variables. </p>

<p>I also tried a regression (linear to start) and doesn't work good either, but it's better then the regression tree.</p>

<p>If I use a Weibull for this, will it mean that each client is going to have a parameter of shape and scale? what about the other variables? How can I insert them into the distribution?</p>

<p>So, what model would you recommend?</p>

<p>Thanks</p>
"
"0.0895377892669139","0.108147614087175","116761","<p>This is my first question (previously the search function has been enough), so please bear with me.  I have a very simple experimental design with one outcome variable and 5 groups.  My typical strategy in this case would be to run a simple ANOVA and then use something like a Tukey's test to calculate significance between groups.  </p>

<p>In this case, one of the groups has a mean that is way above the other 4.  If I exclude the group with the very high mean there are many significant pairs in the data.  Including the very high group gives significance only in comparisons with that group.  The groups don't have equal variance which I know is a problem, but I'm not sure how to deal with it.  I've tried a ""robust"" anova using the package robustbase, but I haven't been able to figure out a suitable post-hoc test.  Any help you can offer on how to analyze something like this would be greatly appreciated.</p>

<p>Here's a simplified version of the code:</p>

<pre><code>#Baseline condition (this is what the Test conditions need to be compared with)
Baseline = c(450,400,200,250)

#Negative control
Control = c(13,22,17,20)

#Test conditions
Test1 = c(200,400,450,300) 
Test2 = c(120,140,90,80) 
VeryHighTest = c(2700,2500,1800,1750)

#Constructing a data frame for ANOVA including all data########
Labels.all =     c(rep('Baseline',4),rep('Control',4),rep('Test1',4),rep('Test2',4),rep('VeryHighTest',4))
data.all = c(Baseline,Control,Test1,Test2,VeryHighTest)
df.allValues = data.frame(Labels=Labels.all, Values=data.all)

#Constructing data frame for ANOVA excluding the VeryHigh group#######
Labels.low = Labels.all[1:16]
data.low = data.all[1:16]
df.lowValues = data.frame(Labels=Labels.low, Values=data.low)

############ANOVAs##############
anova.all = aov(Values ~ Labels, data = df.allValues)
summary(anova.all) #P value on the order of 10^-9
anova.low = aov(Values ~ Labels, data = df.lowValues)
summary(anova.low) #P value &lt; 0.0001

##########Post-hocs##############
phoc.low = TukeyHSD(anova.low)         #Many comparisons are significant  
phoc.low
phoc.all = TukeyHSD(anova.all)         #!!!!Only comparisons with VERYHIGHTEST are significant!!!!#
phoc.all
</code></pre>

<p>Would I be justified in excluding the VeryHighGroup from my analysis because the variance is so high and then maybe do a single T-test between VeryHighGroup and all the other groups combined?  Clearly, I'm out of my statistical depth.</p>

<p>Here's the residual plot for each group.</p>

<p><img src=""http://i.stack.imgur.com/pzADY.jpg"" alt=""Residual Plot""></p>
"
"0.145616580380379","0.153896752812773","117332","<p>I want to check whether the addition of the new predictor <code>x3</code> improves the predictive information of a Cox model significantly or not. </p>

<p>So far I have:</p>

<pre><code>&gt; m1 &lt;- coxph(Surv(time, y) ~  x1+x2,    data=a)
&gt; m2 &lt;- coxph(Surv(time, y) ~  x1+x2+x3, data=a)
&gt; anova(m1, m2)
Analysis of Deviance Table
 Cox model: response is  Surv(time, y)
 Model 1: ~ x1 + x2
 Model 2: ~ x1 + x2 + x3
   loglik  Chisq Df P(&gt;|Chi|)  
1 -319.85                      
2 -317.17 5.3526  1   0.02069 *
</code></pre>

<p>What about c-statistics? How can I calculate and compare it? Somewhere I read ""C-statistic results were compared by the nonparametric method described by DeLong"". What about other methods? What are the pros and cons of each method? And how do I all this using R?</p>

<p><strong>UPDATE #1</strong></p>

<p>Even after reading the manual, I do not understand how to use <code>rcorrp.cens</code> in my example. The following code</p>

<pre><code>m1 &lt;- coxph(Surv(time, y) ~  x1+x2,    data=a)
m2 &lt;- coxph(Surv(time, y) ~  x1+x2+x3, data=a)
rcorrp.cens(m1, m2, Surv(a$time, a$y)) 
</code></pre>

<p>does not work. Which I have not seriously thought after reading the manual. However, I have no idea what to do. And which method-value in <code>rcorrp.cens</code> would be the best for me?</p>

<ol>
<li><p>Would you/or someone else please give me a straightforward example; as possible derived from my example code?</p></li>
<li><p>Do you know ad hoc a published medical paper which used this statistical method in its analysis?</p></li>
</ol>

<p><strong>UPDATE #2</strong></p>

<p>I am sorry, but unfortunately, I do not get it.  I wrote a executable example.</p>

<pre><code>&gt; library(survival)
&gt; library(Hmisc)
&gt; data(colon)
&gt; d &lt;- colon
&gt; surv &lt;- y &lt;- Surv(d$time, 1-(d$status))
&gt; m1 &lt;- coxph(surv ~ rx+sex, data=d)
&gt; m2 &lt;- coxph(surv ~ rx+sex+age, data=d)
&gt; anova(m1, m2)
Analysis of Deviance Table
 Cox model: response is  surv
 Model 1: ~ rx + sex
 Model 2: ~ rx + sex + age
   loglik  Chisq Df P(&gt;|Chi|)  
1 -5522.6                      
2 -5519.3 6.4838  1   0.01089 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; rcorrp.cens(fitted(m1), fitted(m2), surv)
Error in rcorrp.cens(fitted(m1), fitted(m2), surv) : 
  y must have same length as x
&gt; fitted(m1)
NULL
&gt; fitted(m2)
NULL
</code></pre>

<p>The manual on fitted says ""an object for which the extraction of model fitted values is meaningful"". But what is that? Apparently <code>coxph</code> is not such a model.</p>

<p><strong>UPDATE #3</strong></p>

<pre><code>Following I show a working example incorporating the help from Harrell:

&gt; library(survival)
&gt; library(Hmisc)
&gt; data(colon)
&gt; d &lt;- colon
&gt; surv &lt;- y &lt;- Surv(d$time, 1-(d$status))
&gt; m1 &lt;- coxph(surv ~ rx+sex, data=d)
&gt; m2 &lt;- coxph(surv ~ rx+sex+age, data=d)
&gt; anova(m1, m2)
Analysis of Deviance Table
 Cox model: response is  surv
 Model 1: ~ rx + sex
 Model 2: ~ rx + sex + age
   loglik  Chisq Df P(&gt;|Chi|)  
1 -5522.6                      
2 -5519.3 6.4838  1   0.01089 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; (r &lt;- rcorrp.cens(predict(m1), predict(m2), surv))
               Dxy               S.D. x1 more concordant x2 more concordant                  n 
      6.069800e-02       2.345218e-02       5.282221e-01       4.675241e-01       1.858000e+03 
           missing         uncensored     Relevant Pairs          Uncertain               C X1 
      0.000000e+00       9.380000e+02       9.379880e+05       2.510688e+06       4.733685e-01 
              C X2             Dxy X1             Dxy X2 
      4.614846e-01      -5.326294e-02      -7.703084e-02 
&gt; 
&gt; (conc.m1 &lt;- round((1 - r[['x1 more concordant']])*100, digits=1)) # smaller model with 2 predictors
[1] 47.2
&gt; (conc.m2 &lt;- round((1 - r[['x2 more concordant']])*100, digits=1)) # larger model with 3 predictors
[1] 53.2
&gt; 
&gt; (p.value &lt;- round(2*(1 - pnorm(r[['Dxy']] / r[['S.D.']])), digits=4))
[1] 0.0096
</code></pre>
"
"0.0886194286901087","0.0936585811581694","117637","<p>I have problem with the diagnostic of the one way analysis of variance model (fitted in R).
I've checked all the assumptions of the analysis of variance</p>

<p>1) ""For each level of the within-subjects factor, the dependent variable must have a normal distribution."" (shapiro-wilk test) <a href=""http://en.wikipedia.org/wiki/Repeated_measures_design#Repeated_measures_ANOVA"" rel=""nofollow"">http://en.wikipedia.org/wiki/Repeated_measures_design#Repeated_measures_ANOVA</a>
Ok, so after there shapiro-wilk failure to reject, I bury my head in the sand and silently assume that this assumption is met.</p>

<p>2) there is homogeneity of the variance of the dependent variable in groups, there are only 2 groups (bartlett test)</p>

<p>3) all observations are independent</p>

<p>Then I fitted model with <code>aov()</code> function and after checking whether the p-value is greater or less than 0.05 (doesn't really matter now) I would like to check the <strong>quality of the fit</strong>.</p>

<p>So after I check if residuals have normal distribution and the answer is no, is there any solution? Because that means the quality is weak. For example box-cox transformation for dependent variable?</p>

<p>Should I also check the homogeneity of the variance for residuals ( motivation for this question is that ANOVA is some kind of a linear model )? Can I perform bartlett test again? And what if there is heterogeneity? Should then I use the weighted least squares method <strong>WLSM</strong> (the same as I would be a linear model)?</p>

<p>Are diagnostic plots for <code>aov()</code> function, which is similar to <code>lm()</code> (as I would perform <code>plot(lm(  formula ) )</code> are valid? Or only they are proper for linear model where explanatory variable are continuous?</p>

<p>Thanks for help! </p>
"
"0.0723574605292422","0.0573539334676404","117660","<p>My question is based on <a href=""http://stats.stackexchange.com/a/13816/442"">this response</a> which showed which <code>lme4::lmer</code> model corresponds to a two-way repeated measures ANOVA:</p>

<pre><code>require(lme4)
set.seed(1234)
d &lt;- data.frame(
    y = rnorm(96),
    subject = factor(rep(1:12, 4)),
    a = factor(rep(1:2, each=24)),
    b = factor(rep(rep(1:2, each=12))),
    c = factor(rep(rep(1:2, each=48))))

# standard two-way repeated measures ANOVA:
summary(aov(y~a*b+Error(subject/(a*b)), d[d$c == ""1"",]))

# corresponding lmer call:
anova(lmer(y ~ a*b+(1|subject) + (1|a:subject) + (1|b:subject), d[d$c == ""1"",]))
</code></pre>

<p>My question now is on how to extend this to the case of a three-way ANOVA:</p>

<pre><code>summary(aov(y~a*b*c+Error(subject/(a*b*c)), d))
## [...]
## Error: subject:a:b:c
##           Df Sum Sq Mean Sq F value Pr(&gt;F)
## a:b:c      1  0.101  0.1014   0.115  0.741
## Residuals 11  9.705  0.8822 
</code></pre>

<p>The natural extension as well as versions thereof do not match the ANOVA results:</p>

<pre><code>anova(lmer(y ~ a*b*c +(1|subject) + (1|a:subject) + (1|b:subject) + (1|c:subject), d))
## [...]
## a:b:c  1 0.1014  0.1014  0.1500

anova(lmer(y ~ a*b*c +(1|subject) + (1|a:subject) + (1|b:subject) + (1|c:subject) + 
               (1|a:b:subject) + (1|a:c:subject) + (1|b:c:subject), d))
## [...]
## a:b:c  1 0.1014  0.1014  0.1539
</code></pre>

<p>Note that a very similar question has been asked <a href=""http://stats.stackexchange.com/q/99765/442"">before</a>. However, it was missing example data (which is provided here).</p>
"
"0.0723574605292422","0.0764719112901873","118391","<p>I have a data set where I measured the number of molecules (M) present in cells as a function of drug (with or without) and days of treatment (5 timepoints). I repeated the experiment 3 times, with cells from a separate donor each time. I am currently trying to compare the means between groups. However, the data are not normal and heteroskedastic and I'm in the process of figuring out how to best deal with this.</p>

<p>Transforming the data makes the dataset normal, but the heteroskedacity remains. I'm a stats novice, but my reading over the past several days suggests that a linear mixed model should be able to deal with this. Based primarily on the Pinhiero/Bates book, I have cobbled together the following models using lme in R; they are the same except for the varPower statement. </p>

<pre><code>model1&lt;-lme(sqrt(M) ~ drug + Days + drug*Days, 
            random = ~ 1+drug+Days+drug*Days|Donor, data=D)

model2&lt;-lme(sqrt(M) ~ drug + Days + drug*Days,
            random = ~ 1+drug+Days+drug*Days|Donor, data=D, varPower(form = ~fitted(.)) )
</code></pre>

<p>When I compare these two models using anova(), model2 has a significantly increased log likelihood. However, when I examine the standardized residuals plotted against either fitted values or the independent variables, the graphs for the two models have identical shape. Note that the magnitude of the residuals is slightly greater with varPower() included:</p>

<pre><code>plot(model1, resid(., type=""p"") ~ fitted(.), abline=0)
plot(model1, resid(., type=""p"") ~ Days|drug, abline=0)
</code></pre>

<p><img src=""http://i.imgur.com/vcFwduT.png"" alt=""residual plots""></p>

<p>Does the similarity between these plots mean that I have failed to sufficiently account for the unequal variances between groups?</p>

<p>If so, what approaches might yield sufficient correction?  Additionally, if you have any general comments about the suitability of lme here, or the structure of the model, those would be welcome as well!</p>
"
"0.114407190489069","0.108821437516502","118475","<p>I have an unbalanced linear mixed effects model with three fixed factors of various levels and one random factor for my repeated measures data (<a href=""http://stats.stackexchange.com/questions/99742/how-to-analyze-interdependent-interaction-terms-of-lmer-model"">for details see here</a>).
Thanks to your help I managed to do post-hoc tests on the significant interaction terms using <code>lsm</code> from the <strong>lsmeans</strong> package. However, I need to report the F statistic (F value and degrees of freedom) for these post-hoc tests and wonder how???</p>

<p>Here is what I do:</p>

<ol>
<li><p>Model comparison using <code>anova()</code> resulting into the final model
<code>model_final</code>, which reads:</p>

<p><code>sc ~ time + cond + place + time:cond + cond:place + (1|ID), data)</code> . </p></li>
<li><p>I analyze the significant interaction time:cond using <code>lsmeans</code>:</p>

<p><code>posthoc_1 &lt;- glht(model_final, lsm(pairwise ~ cond|time)</code></p>

<p><code>summary(posthoc_1)</code></p></li>
</ol>

<p>and get sth like below for each level of <code>time</code>, here is the example for <code>time1</code>.</p>

<pre><code>&gt; Note: df set to 268 
&gt;
&gt; $`time = time1`
&gt; 
&gt;    Simultaneous Tests for General Linear Hypotheses
&gt; 
&gt; Fit: lme4::lmer(formula = sc ~ time + cond + place + time:cond + cond:place + (1|ID), data)
&gt; 
&gt; Linear Hypotheses:
&gt;                    Estimate Std. Error t value Pr(&gt;|t|) 
&gt; cond1 - cond2 == 0   3.1867     0.6797   4.688 4.39e-06 ***
</code></pre>

<p>This gives me t-values for the various levels of the interaction terms and their corresponding p-value, but no F stats!</p>

<p>My questions: </p>

<ol>
<li>Is there any way of obtaining the F stats? (F value and degree of
freedom) </li>
<li>Or am I stuck with the t-values? If so, is t(0.095;268) =
4.588, p &lt; 0.001 reporting the correct degrees of freedom?</li>
</ol>
"
"0.0886194286901087","0.0780488176318078","119790","<p>Is there a difference between chi-squared (from <code>coxph</code> -> <code>anova</code>) and Wald chi-squared (from <code>cph</code> -> <code>anova</code>)?</p>

<p>And how do I have to interpret these chi-squared values? What does a P&lt;0.05 mean in this case? Why does the sum of chi-squared values of each variable not equal TOTAL? My idea was that each chi-squared indicates the predictive information of each variable and TOTAL that of the entire model.</p>

<pre><code>&gt; library(survival)
&gt; library(rms)
&gt;
&gt; data(colon)
&gt; d &lt;- colon
&gt; m1 &lt;- cph(Surv(time, status) ~ age + sex + nodes, data=d)
&gt; anova(m1)
                Wald Statistics          Response: Surv(time, status) 

 Factor     Chi-Square d.f. P     
 age          0.03     1    0.8612
 sex          0.93     1    0.3349
 nodes      189.79     1    &lt;.0001
 TOTAL      192.01     3    &lt;.0001
&gt; 0.03+0.93+189.79 # = 190.75
[1] 190.75
&gt; m2 &lt;- coxph(Surv(time, status) ~ age + sex + nodes, data=d)
&gt; anova(m2)
Analysis of Deviance Table
 Cox model: response is Surv(time, status)
Terms added sequentially (first to last)

       loglik    Chisq Df Pr(&gt;|Chi|)    
NULL  -6424.0                           
age   -6423.6   0.7147  1     0.3979    
sex   -6423.4   0.5019  1     0.4787    
nodes -6356.9 132.8685  1     &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.114882898114698","0.121415466064296","121517","<p>I have two models:</p>

<pre><code>frm.mE &lt;- glm(frm ~ age + education + socialrole + countedmembers +
            offset(log(words)), family=quasipoisson, data=daten.alle.kom)
frm.oE &lt;- glm(frm ~ age + socialrole + countedmembers +
                 offset(log(words)), family=quasipoisson, data=daten.alle.kom)
</code></pre>

<p>now I want to know which model is the better one, but because of quasipoisson, AIC don't work</p>

<pre><code>summary(frm.mE)
Call:
glm(formula = frm ~ age + education + socialrole + countedmembers + 
offset(log(words)), family = quasipoisson, data = daten.alle.kom)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-6.7040  -1.6727  -0.2329   1.0003   7.4897  

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    -3.95362    0.21432 -18.448  &lt; 2e-16 ***
age             0.01293    0.07041   0.184  0.85454    
education1      0.11532    0.11647   0.990  0.32367    
socialrole1    -0.28367    0.23685  -1.198  0.23287    
socialrole2    -0.80474    0.29054  -2.770  0.00629 ** 
countedmembers -0.03716    0.06120  -0.607  0.54461    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 5.792638)

Null deviance: 909.51  on 160  degrees of freedom
Residual deviance: 841.35  on 155  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
</code></pre>

<p>and the second model:</p>

<pre><code>Call:
glm(formula = frm ~ age + socialrole + countedmembers + offset(log(words)), 
family = quasipoisson, data = daten.alle.kom)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-6.4844  -1.6613  -0.3583   1.1036   7.1557  

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    -3.89079    0.20350 -19.119  &lt; 2e-16 ***
age             0.00540    0.06966   0.078  0.93832    
socialrole1    -0.33991    0.22947  -1.481  0.14054    
socialrole2    -0.75470    0.28553  -2.643  0.00905 ** 
countedmembers -0.02634    0.05996  -0.439  0.66104    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 5.761264)

Null deviance: 909.51  on 160  degrees of freedom
Residual deviance: 847.08  on 156  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
</code></pre>

<p>is there another way to compare them? or to know if I should keep the variable ""education""?
thanks for any help!</p>

<p>I tried a F test, but not sure if it makes sense:</p>

<pre><code> anova(frm.mE, frm.oE, test=""F"")
Analysis of Deviance Table

Model 1: frm ~ age + education + socialrole + countedmembers + offset(log(words))
Model 2: frm ~ age + socialrole + countedmembers + offset(log(words))
Resid. Df Resid. Dev Df Deviance      F Pr(&gt;F)
1       155     841.35                          
2       156     847.08 -1  -5.7368 0.9904 0.3212
</code></pre>

<p>but I'm not sure how to understand it, does it mean that I should keep ""education"" because model 2 has a too big p-value?</p>
"
"NaN","NaN","121661","<p>I've got two models (all variable are count variables):</p>

<pre><code>frm.ct &lt;- glmer(frm ~ age + education + socialrole +
              offset(log(words)) + (1|subkorpus), family=negative.binomial(1), 
data=daten.alle.kom)

frm.oage &lt;- glmer(frm ~ education + socialrole +
              offset(log(words)) + (1|subkorpus), family=negative.binomial(1), 
data=daten.alle.kom)
</code></pre>

<p>I used this to compare them:</p>

<pre><code>anova(frm.ct, frm.oage)
</code></pre>

<p><img src=""http://i.stack.imgur.com/xb6Jm.png"" alt=""enter image description here""></p>

<p>AIC values tell me that <code>frm.oage</code> is the better model, right? but what do 0.0452 and 0.8315 mean?</p>
"
"0.176341439677724","0.179712689361946","122717","<p>I have some trouble obtaining equivalent results between an <code>aov</code> between-within repeated measures model and an <code>lmer</code> mixed model.</p>

<p>My data and script look as follows</p>

<pre><code>data=read.csv(""https://www.dropbox.com/s/zgle45tpyv5t781/fitness.csv?dl=1"")
data$id=factor(data$id)
data
   id  FITNESS      TEST PULSE
1   1  pilates   CYCLING    91
2   2  pilates   CYCLING    82
3   3  pilates   CYCLING    65
4   4  pilates   CYCLING    90
5   5  pilates   CYCLING    79
6   6  pilates   CYCLING    84
7   7 aerobics   CYCLING    84
8   8 aerobics   CYCLING    77
9   9 aerobics   CYCLING    71
10 10 aerobics   CYCLING    91
11 11 aerobics   CYCLING    72
12 12 aerobics   CYCLING    93
13 13    zumba   CYCLING    63
14 14    zumba   CYCLING    87
15 15    zumba   CYCLING    67
16 16    zumba   CYCLING    98
17 17    zumba   CYCLING    63
18 18    zumba   CYCLING    72
19  1  pilates   JOGGING   136
20  2  pilates   JOGGING   119
21  3  pilates   JOGGING   126
22  4  pilates   JOGGING   108
23  5  pilates   JOGGING   122
24  6  pilates   JOGGING   101
25  7 aerobics   JOGGING   116
26  8 aerobics   JOGGING   142
27  9 aerobics   JOGGING   137
28 10 aerobics   JOGGING   134
29 11 aerobics   JOGGING   131
30 12 aerobics   JOGGING   120
31 13    zumba   JOGGING    99
32 14    zumba   JOGGING    99
33 15    zumba   JOGGING    98
34 16    zumba   JOGGING    99
35 17    zumba   JOGGING    87
36 18    zumba   JOGGING    89
37  1  pilates SPRINTING   179
38  2  pilates SPRINTING   195
39  3  pilates SPRINTING   188
40  4  pilates SPRINTING   189
41  5  pilates SPRINTING   173
42  6  pilates SPRINTING   193
43  7 aerobics SPRINTING   184
44  8 aerobics SPRINTING   179
45  9 aerobics SPRINTING   179
46 10 aerobics SPRINTING   174
47 11 aerobics SPRINTING   164
48 12 aerobics SPRINTING   182
49 13    zumba SPRINTING   111
50 14    zumba SPRINTING   103
51 15    zumba SPRINTING   113
52 16    zumba SPRINTING   118
53 17    zumba SPRINTING   127
54 18    zumba SPRINTING   113
</code></pre>

<p>Basically, 3 x 6 subjects (<code>id</code>) were subjected to three different <code>FITNESS</code> workout schemes each and their <code>PULSE</code> was measured after carrying out three different types of endurance <code>TEST</code>s.</p>

<p>I then fitted the following <code>aov</code> model :</p>

<pre><code>library(afex)
library(car)
set_sum_contrasts()
fit1 = aov(PULSE ~ FITNESS*TEST + Error(id/TEST),data=data)
summary(fit1)
Error: id
          Df Sum Sq Mean Sq F value   Pr(&gt;F)    
FITNESS    2  14194    7097   115.1 7.92e-10 ***
Residuals 15    925      62                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: id:TEST
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
TEST          2  57459   28729   253.7  &lt; 2e-16 ***
FITNESS:TEST  4   8200    2050    18.1 1.16e-07 ***
Residuals    30   3397     113                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The result I obtain using</p>

<pre><code>set_sum_contrasts()
fit2=aov.car(PULSE ~ FITNESS*TEST+Error(id/TEST),data=data,type=3,return=""Anova"")
summary(fit2)
</code></pre>

<p>is identical to this.</p>

<p>A mixed model run using <code>nlme</code> gives a directly equivalent result, e.g. using <code>lme</code> :</p>

<pre><code>library(lmerTest)    
lme1=lme(PULSE ~ FITNESS*TEST, random=~1|id, correlation=corCompSymm(form=~1|id),data=data)
anova(lme1)
             numDF denDF   F-value p-value
(Intercept)      1    30 12136.126  &lt;.0001
FITNESS          2    15   115.127  &lt;.0001
TEST             2    30   253.694  &lt;.0001
FITNESS:TEST     4    30    18.103  &lt;.0001


summary(lme1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC    logLik
  371.5375 393.2175 -173.7688

Random effects:
 Formula: ~1 | id
        (Intercept) Residual
StdDev:    1.699959 9.651662

Correlation Structure: Compound symmetry
 Formula: ~1 | id 
 Parameter estimate(s):
       Rho 
-0.2156615 
Fixed effects: PULSE ~ FITNESS * TEST 
                                 Value Std.Error DF   t-value p-value
(Intercept)                   81.33333  4.000926 30 20.328628  0.0000
FITNESSpilates                 0.50000  5.658164 15  0.088368  0.9308
FITNESSzumba                  -6.33333  5.658164 15 -1.119327  0.2806
TESTJOGGING                   48.66667  6.143952 30  7.921069  0.0000
TESTSPRINTING                 95.66667  6.143952 30 15.570868  0.0000
FITNESSpilates:TESTJOGGING   -11.83333  8.688861 30 -1.361897  0.1834
FITNESSzumba:TESTJOGGING     -28.50000  8.688861 30 -3.280062  0.0026
FITNESSpilates:TESTSPRINTING   8.66667  8.688861 30  0.997446  0.3265
FITNESSzumba:TESTSPRINTING   -56.50000  8.688861 30 -6.502579  0.0000
</code></pre>

<p>Or using <code>gls</code> :</p>

<pre><code>library(lmerTest)    
gls1=gls(PULSE ~ FITNESS*TEST, correlation=corCompSymm(form=~1|id),data=data)
anova(gls1)
</code></pre>

<p>However, the result I obtain using <code>lme4</code>'s <code>lmer</code> is different :</p>

<pre><code>set_sum_contrasts()
fit3=lmer(PULSE ~ FITNESS*TEST+(1|id),data=data)
summary(fit3)
Linear mixed model fit by REML ['lmerMod']
Formula: PULSE ~ FITNESS * TEST + (1 | id)
   Data: data

REML criterion at convergence: 362.4

Random effects:
 Groups   Name        Variance Std.Dev.
 id       (Intercept)  0.00    0.0     
 Residual             96.04    9.8     
...

Anova(fit3,test.statistic=""F"",type=3)
Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)

Response: PULSE
                    F Df Df.res    Pr(&gt;F)    
(Intercept)  7789.360  1     15 &lt; 2.2e-16 ***
FITNESS        73.892  2     15 1.712e-08 ***
TEST          299.127  2     30 &lt; 2.2e-16 ***
FITNESS:TEST   21.345  4     30 2.030e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Anybody any thoughts what I am doing wrong with the <code>lmer</code> model? Or where the difference comes from? Could it have to do anything with <code>lmer</code> not allowing negative intraclass corellations or something like that? Given that <code>nlme</code>'s <code>gls</code> and <code>lme</code> do return the correct result, though, I am wondering how this is different in <code>gls</code> and <code>lme</code>? Is it that the option <code>correlation=corCompSymm(form=~1|id)</code> causes them to  directly estimate the intraclass correlation, which can be either positive or negative, whereas <code>lmer</code> estimates a variance component, which cannot be negative (and ends up being estimated as zero in this case)?</p>
"
"0.0511644510096651","0.0540738070435875","122935","<p>I have a design involving 1 between-subjects categorical factor, 1 between-subjects continuous factor and 2 within-subjects categorical factors.</p>

<p>This is theoretically a case of multiple regression, but as in this question <a href=""http://stats.stackexchange.com/questions/48455/anova-or-regression-1-continuous-factor-1-categorical-factor-with-continuous"">here</a>, I was able to use <code>lm()</code> in <code>R</code> to specify my model (actually, using the <code>aov.car()</code> function in the <code>afex</code> package).</p>

<p>This means that my output was in the form of an ANOVA table, with F and p-values. Can I report my analysis as though it was an ANOVA, or should I find a way to force the output of regression coefficients and report the analysis as though it were a multiple regression? I know that I shouldn't use an ANCOVA in this case, because my continuous predictor variable is certainly not a nuisance variable!</p>
"
"0.135368413338721","0.122627867896993","125787","<p>I would like to learn what is the correct way to approach analysis of this data. I have done some reading on the subject, but I still feel uncertain. Perhaps many approaches are valid, but simply  that some are more conservative than others?</p>

<p>My study: </p>

<p>I have 5 grasslands, and in each grassland I have 30 spiders. For each spider I have an estimate of what proportion of herbivores it consumes ""Diet"" (so 5 x 30, n = 150). For each grassland I also have an estimate of the overall biomass of herbivores that exist there ""Biomass"". Thus I have 5 values of ""Biomass"" (one for each grassland) and 150 of ""Diet"" (30 spiders per grassland). Both Diet and Biomass are continous variables. </p>

<p>I would like to run an anlysis that tests how Diet changes across Biomass and derive a slope value, thus keeping Biomass as a continous variable:</p>

<p>Diet ~ Biomass</p>

<p>As I understand it, if I use raw data for Diet (n=150) then using anova is more approrpiate, and grassland becomes a factor with 5 levels.</p>

<p>Or I could run it as a linear regression and thus keep Biomass as a continuous variable and derive a slope value. However, as a linear regression, should I use the raw data (n=150) or mean values (so 5 means - one for each grassland based on 30 samples). Which of the 2 linear regression approaches is correct? (means or raw data). </p>

<p>While I am familiar with the notion that both anova and regression have the same underlying mathematics and are now regarded as general linear modelling, I still don't know how this affects the data that I should be using when running a linear model of the form:  Diet ~ Biomass</p>

<p>Using raw data seems better because it captures the variability in the dataset, but if i use it with Biomass as a continous variable to get a slope value (i.e regression analysis) I am concerned that it inflates the degrees of freedom (df=1,149) and is psuedo-replicated, so inaccurately increases my chances of a significant result? Therefore, is it incorrect to model the raw data (n=150) against only 5 values of ""Biomass"" in a linear form (and not as factors as required in an anova)?</p>
"
"0.0255822255048325","0.0270369035217938","126510","<p>How do we do two-way ANOVA (one observation per mean), as testing H_A in Section 8.5 in Seber and Lee's Linear Regression Analysis, in R?
Note that the linear model for this case doesn't have interaction between the row and column factors.</p>

<p>For example, I want to test in the following 3 x 2 table, if the mean of each row is the same. </p>

<p>5 | 4<br>
7 | 6<br>
4 | 7  </p>

<p>Note that I used <code>lm</code> for one-way ANOVA, but couldn't find out which function and arguments to do two-way ANOVA (one observation per mean). I am not trying to implement it in R.</p>

<p>Thanks.</p>
"
"0.0626633989716535","0.0220755392844174","128694","<p>I'd like to bootstrap a two sample t-test. My DV is some psychological variable. I have two groups (women and men), unequal sizes and I do not assume equal variances. I'm not sure if my code or/and my thinking is correct, 'cause in the end I got 0 t-statistics greater than t-statistic from original data.</p>

<pre><code>group_k  # women: N=377
group_m  # men:   N=306
t.est &lt;- t.test(group_k, group_m, var.equal=FALSE)$stat
#        t 
# 5.659757

nullA &lt;- group_k - mean(group_k, na.rm=T)
nullB &lt;- group_m - mean(group_m, na.rm=T)
set.seed(1)
b &lt;- function(){
  A &lt;- sample(nullA, 200, replace=T)  # is 200-element from 377-element sample ok? 
  B &lt;- sample(nullB, 200, replace=T) 
  stud_test &lt;- t.test(A, B, var.equal=FALSE)
  stud_test$stat
}
t.stat.vect = vector(length=10000)
t.vect &lt;- replicate(10000, b())

1 - mean(t.est&gt;t.vect)
# [1] 0 :(
</code></pre>

<p>I have some additional questions:</p>

<ol>
<li>Why not bootstrapping simply differences between women and men? </li>
<li>How to choose bootstrap sample size? In other words, is 200-elements from 377- and 306-element groups OK? Should they be 377 and 306, respectively, as <a href=""http://stats.stackexchange.com/questions/20952/choosing-n-when-doing-t-tests-on-bootstrapped-samples"">this</a> post recommends?</li>
</ol>

<p>The idea behind subtracting means was <a href=""http://stats.stackexchange.com/questions/91872/alternatives-to-one-way-anova-for-heteroskedastic-data"">here</a> - gung's reply. I thought that it can be directly taken from ANOVA case to Student's t test.</p>

<p><strong>[UPDATE 13XII]</strong>
I corrected my code, but results are still awkward to me:</p>

<pre><code>t.est &lt;- t.test(group_k, group_m, var.equal=FALSE)$stat
# t = 5.6598, df = 255.185, p-value = 4.066e-08

b &lt;- function(){
  A &lt;- sample(group_k, 377, replace=T)  
  B &lt;- sample(group_m, 306, replace=T) 
  stud_test &lt;- t.test(A, B, var.equal=FALSE)
  stud_test$stat
}
t.stat.vect = vector(length=10000)
t.vect &lt;- replicate(10000, b())

1 - mean(t.est&gt;t.vect)
[1] 0.5042
</code></pre>

<p>Is it possible that using original samples the difference between means is ""so significant"" (p-value = 4.066e-08), but the bootstrap samples shows that actually it's not (0.5042) ??  </p>
"
"NaN","NaN","129470","<p>I have many plant populations of the same species sampled (around 100).
What is measured is an allele on one gene, that can be A,B or C. This is what I would like to use as Y variable.
Then I have the yearly rain, the altitude and the the mean temperature.</p>

<p>I was hopping to do something similar of an ANOVA, with allele = rain + altitude + temperature but I did not find I way to do it. Ideas?</p>

<p>Thanks for suggetsions</p>
"
"0.0808981002113217","0.0683985568056769","130104","<p>I have a weekly number of items sold from 2012 to 2014. 2014 not being complete. So bassically two periods. I am looking at seasonality and stationarity of the response variable (# of items sold)</p>

<p>I have first plotted them against time</p>

<p>![response in time][1]</p>

<p>I then had did stl decomposition in suspicion of trend and seasonality</p>

<p>![stl decomposition][2]</p>

<p>However my data is stationary without any transformation</p>

<p>![enter image description here][3]
![enter image description here][4]</p>

<p>All my testing attests to stationarity of course...
so I did </p>

<pre><code>boxplot(Dealer_Traffic~month, data=total, xlab= ""Month"", ylab = ""Traffic to Dealer"")
</code></pre>

<p>to see differences in mean of sales through month. Month was created using dates I had to create the month a given observation belonged to. </p>

<p>![Boxplot][5]</p>

<p>and difference in mean observed are not significant. WHAT????</p>

<p>![ANOVA][6]</p>

<p>and here is the acf and pacf</p>

<p>![acf and pacf][7]</p>

<p>So could I conclude that my raw data is stationary hence needs no transformation?</p>

<p>Can I conclude that an ARIMA would be suitable? Which ARIMA model would you recommend?</p>

<p>It seems like I cannot have photos here. So basically all my tests reject nonstationarity but I observe seasonality and trend in stl decomposition. No difference between monthly means. I tried 
 pp test Box.test kpss and adf.test for first and pairwise.t.test and aov for means tuckey. </p>

<p>Can I conclude that the series is stationary?</p>
"
"0.125326797943307","0.121415466064296","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.0957199230302734","0.086710996952412","133650","<p>With some co-authors, I coded 200 video game reviews based on whether they mentioned any of nine different game elements. We now have the total number of times that each game element showed up in these reviews, and weâ€™d like to run some kind of test to demonstrate that some elements showed up significantly more times than others. Weâ€™ve looked into two ways of testing this and hit obstacles with both ways: </p>

<p>Weâ€™d hoped to use a chi-square goodness of fit test to determine whether each element showed up an equal number of times in the data; however, the nine different game element categories arenâ€™t mutually exclusive (i.e., one game review might have several game elements in it), which violates the assumptions of a chi-square test. </p>

<p>We explored using an ANOVA test (or equivalent) to determine whether the mean instances of a game element per review differed significantly for the nine different game elements. The data don't have a normal distribution, so I'm looking into a Kruskal-Wallis test, but the data also violate the assumption of homogeneity of variance; it's not clear to me whether Kruskal-Wallis can also handle this, or if there's another route we should take. </p>
"
"0.114407190489069","0.108821437516502","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.125326797943307","0.121415466064296","136448","<p>Given is the following problem setting:</p>

<p>We split our subjects into two groups, where each group undergoes another kind of treatment (blocked or random).
There are three measurements (Stages) for each subject of the dependent variable called ""RE"", at pretest, posttest and at retention.
The treatment is given to the subjects after pretest and before posttest.</p>

<p>So some part of the data would look like:</p>

<pre><code>Stage    Treatment   Radial Error
1   1   192.5538007
1   2   179.3135849
3   2   163.4928798
1   2   172.1848789
3   2   186.4486723
1   2   190.9450916
2   1   296.8551904
2   2   216.1953759
1   1   294.7580782
</code></pre>

<p>Now we want to test the following two hypothesis:</p>

<ol>
<li><p>H1: After the treatment, the RE is bigger for the â€œrandomizedâ€ group than for the â€œblockedâ€ group</p></li>
<li><p>In the retentiontest the RE is smaller for the â€œrandomizedâ€ group then for the â€œblockedâ€ group</p></li>
</ol>

<p>For 1. we first did the following: using anova to check that the RE values for pretest for the both groups are not significantly different.Then we simply used anova to compare the posttest RE values of the two groups and got a p-value of 0.043.</p>

<p>We are not sure if this approach is correct, also the supervisor has suggested a 2x3 factorial design (blocked, random)x(pretest,posttest, retentiontest) which we coded now as follows:</p>

<pre><code>ret = aov(Radial.Error ~ Treatment*Stage, entire.data)

output:

              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
Treatment         1  10290   10290   3.699   0.0567 .  
Stage             2  63363   31682  11.390 2.86e-05 ***
Treatment:Stage   2   1658     829   0.298   0.7428    
Residuals       125 347705    2782                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
[1] ""P Value: 0.0567154315196806""
&gt; #post_ret = perform_aov(Avg.RE ~ TR, posttest)
&gt; #combined = perform_aov(Avg.RE ~ TR, merged_data)
&gt; 
&gt; TukeyHSD(anova_ret, which=c('Treatment', 'Stage'), conf.level=.95)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = formula, data = data_)

$Treatment
         diff       lwr       upr     p adj
2-1 -17.84265 -36.20311 0.5178059 0.0567154

$Stage
           diff       lwr       upr     p adj
2-1 -43.6189874 -70.14635 -17.09163 0.0004557
3-1 -44.1817458 -70.70911 -17.65439 0.0003789
3-2  -0.5627584 -30.46738  29.34186 0.9989021
</code></pre>

<p>However, we are not sure if this suited for answering H1 or H2. Also, when we solely check for aov(Radial.Error ~ Treatment:Stage, entire.data), the result is very different than in the line above. Why is that so?</p>

<pre><code> Df Sum Sq Mean Sq F value   Pr(&gt;F)    
Treatment:Stage   5  75311   15062   5.415 0.000153 ***
Residuals       125 347705    2782                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
[1] ""P Value: 0.000152626028849265""
</code></pre>
"
"0.223094021453684","0.217642875033004","136495","<p><b>Background:</b><br>
I am using linear mixed-effects models (LMMs) in order to determine how the interaction between two fixed effects influences measures of a response variable.  Since I am working with a dataset in which there are multiple samples from multiple individuals that could violate the assumption of independence of data points, I am treating ""individual"" as a random effect.  Thus, the generic model I am working with is:  </p>

<pre><code>lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind), dataset, REML=T)
</code></pre>

<p>Note: for my actual dataset, I used a likelihood ratio test to determine whether I needed to also nest the multiple trials within individual [i.e., lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind/Trial), dataset)], and failed to reject the null hypothesis that this ""fuller"" model contributed significantly to accounting for additional variation in the data.   </p>

<p><b>Problem to solve:</b><br>
Determine whether the results from my Tukey's post-hoc comparisons are reliable, given the interactions included in my LMM model.</p>

<p><b>Loading data and libraries:</b><br>
library(car) # for Soils dataset<br>
data(Soils)<br>
library(lme4) # for lmer()<br>
library(lsmeans) # for remaining functions  </p>

<p><b>Example code:</b><br>
     ## Create the LMM<br>
     ## ""Na"" is a numeric continuous response variable<br>
     ## ""Contour"" is a factor, with character categories, and is treated as a fixed effect<br>
     ## ""P"" is an integer variable, is treated as a fixed effect, and differs across the Contour groups<br>
     ## ""Group"" is a a numerical factor and is treated as a random effect  </p>

<pre><code>Na.LMER &lt;- lmer(Na ~ Contour*P + (1|Group), Soils, REML=T)
Na.LMER  

Linear mixed model fit by REML ['lmerMod']
Formula: Na ~ Contour * P + (1 | Group)
   Data: Soils
REML criterion at convergence: 190.4919
Random effects:
 Groups   Name        Std.Dev.
 Group    (Intercept) 2.514   
 Residual             1.063   
Number of obs: 48, groups:  Group, 12
Fixed Effects:
   (Intercept)    ContourSlope      ContourTop               P  ContourSlope:P    ContourTop:P  
    7.104951        4.381251       -0.260527       -0.006811       -0.026952       -0.006258  

### Conduct Tukey's post-hoc comparisons
Na.Tukey &lt;- lsmeans(Na.LMER, pairwise~Contour, adjust=""tukey"")
</code></pre>

<blockquote>
  <p>NOTE: Results may be misleading due to involvement in interactions  </p>
</blockquote>

<pre><code>Na.Tukey  

$lsmeans
 Contour      lsmean       SE   df lower.CL upper.CL
 Depression 5.973118 1.289466 8.15 3.008857 8.937380
 Slope      5.875929 1.286895 8.08 2.913697 8.838160
 Top        4.672639 1.294933 8.24 1.701416 7.643863

Confidence level used: 0.95 

$contrasts
 contrast             estimate       SE   df t.ratio p.value
 Depression - Slope 0.09718976 1.821763 8.11   0.053  0.9984
 Depression - Top   1.30047917 1.827450 8.19   0.712  0.7635
 Slope - Top        1.20328941 1.825636 8.16   0.659  0.7925

P value adjustment: tukey method for a family of 3 means 
</code></pre>

<p><b>So this is where the question comes in.</b><br>
Since I received the warning message (""NOTE: Results may be misleading due to involvement in interactions""), I want to verify whether I can reliably use the p-values output from lsmeans() to determine which contrasts were different from each other.  So how can I tell whether the interactions from this particular dataset could be problematic for interpreting the results from the Tukey's post-hoc comparisons.  </p>

<p><b>Here is what I have tried to investigate this issue.</b><br>
Based on the recommendations by Professor Russell Lenth (developer of the lsmeans R package), I used additional functions from the lsmeans R package to investigate what's going on with the data.</p>

<pre><code>### First, here are the F-tests of the fixed effects of the LMM.
anova(Na.LMER)   

Analysis of Variance Table
          Df  Sum Sq Mean Sq F value
Contour    2  0.5696  0.2848  0.2520
P          1 10.4083 10.4083  9.2093
Contour:P  2  6.7070  3.3535  2.9672  
</code></pre>

<p>Does the Contour:P interaction seem relatively strong?  </p>

<p>Next, I'm going to evaluate whether this interaction is important by determining to what extent the values of P varies across the Contour groups, using lsmip().    </p>

<pre><code>Na.lsm &lt;- lsmeans(Na.LMER, ~Contour|P, at=list(P = c(75, 100, 200, 300, 400)))  
Na.lsm    

P =  75:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.594094 1.399580 10.70  3.5029413  9.685246
 Slope       8.953983 1.562754 13.53  5.5913341 12.316631
 Top         5.864180 1.511863 12.76  2.5917619  9.136598

P = 100:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.423808 1.355688  9.64  3.3876590  9.459957
 Slope       8.109909 1.429365 10.79  4.9562943 11.263524
 Top         5.537432 1.391548 10.16  2.4433848  8.631479

P = 200:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.742665 1.286244  8.08  2.7814923  8.703838
 Slope       4.733616 1.354120  9.32  1.6863856  7.780847
 Top         4.230440 1.384598 10.01  1.1459415  7.314939

P = 300:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.061522 1.396923 10.63  1.9738402  8.149204
 Slope       1.357323 1.960472 21.77 -2.7109112  5.425557
 Top         2.923449 2.025495 24.22 -1.2549312  7.101829

P = 400:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  4.380379 1.651907 17.57  0.9037052  7.857053
 Slope      -2.018970 2.841216 33.67 -7.7950921  3.757152
 Top         1.616457 2.914268 36.01 -4.2938885  7.526803

Confidence level used: 0.95  
</code></pre>

<blockquote>
  <h3>Plotting the interactions</h3>
  
  <p>Na.lsmip &lt;- lsmip(Na.lsm, Contour~P)</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/9uDul.jpg"" alt=""Interaction of Contour and P""></p>

<blockquote>
  <h3>It seems like the levels of Contour vary at different values of P (especially for Slope), but I'm going to use pairs() to verify this using pairwise comparison at each value of P.</h3>
</blockquote>

<pre><code>pairs(Na.lsm)  
P =  75:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -2.3598888 2.097862 12.15  -1.125  0.5175
 Depression - Top    0.7299139 2.060232 11.74   0.354  0.9335
 Slope - Top         3.0898026 2.174381 13.15   1.421  0.3589

P = 100:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -1.6861012 1.970019 10.22  -0.856  0.6784
 Depression - Top    0.8863760 1.942755  9.90   0.456  0.8928
 Slope - Top         2.5724773 1.994865 10.47   1.290  0.4308

P = 200:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  1.0090489 1.867637  8.70   0.540  0.8539
 Depression - Top    1.5122246 1.889851  9.04   0.800  0.7122
 Slope - Top         0.5031757 1.936686  9.67   0.260  0.9636

P = 300:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  3.7041990 2.407248 16.78   1.539  0.2988
 Depression - Top    2.1380732 2.460493 18.21   0.869  0.6660
 Slope - Top        -1.5661258 2.818879 23.01  -0.556  0.8447

P = 400:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  6.3993492 3.286534 28.79   1.947  0.1439
 Depression - Top    2.7639218 3.349889 30.81   0.825  0.6905
 Slope - Top        -3.6354273 4.070070 34.89  -0.893  0.6481

P value adjustment: tukey method for a family of 3 means  
</code></pre>

<blockquote>
  <h3>Based on the pairs() output, it doesn't seem like Contour groups vary at these incremental values of P.</h3>
  
  <p><b>Since the Contour groups do not seem to vary at different levels of P, does that mean that the interaction strength is not that strong?  and thus, I am okay to ignore the warning message that ""NOTE: Results may be misleading due to involvement in interactions""?</b>  </p>
</blockquote>

<p>I would appreciate any feedback about interpreting these results, and whether there are additional analyses that I should be conducting in order to address my concern.  If there is any additional information that would be helpful in tackling this problem, please let me know.  </p>

<p>Thank you for your time!</p>

<p>UPDATE (2/6/15): I had a minor typo at the beginning, in which the first line of code read ""Dens.LMER &lt;- lmer(...)"".  The lmer product should have been named ""Na.LMER"", which was used in the remaining code.  Thus, the Dens.LMER product that rvl mentions is equivalent to Na.LMER.  I apologize for the inconvenience.  </p>
"
"0.220356567825383","0.203029403105005","136661","<p>Suppose that I have a data with two independent groups:</p>

<pre><code>    g1.lengths &lt;- c (112.64, 97.10, 84.18, 106.96, 98.42, 101.66)

    g2.lengths &lt;- c (84.44, 82.10, 83.26, 81.02, 81.86, 86.80, 
                     85.84, 97.08, 79.64, 83.32, 91.04, 85.92,
                     73.52, 85.58, 97.70, 89.72, 88.92, 103.72,
                     105.02, 99.48, 89.50, 81.74)

   group = rep (c (""g1"", ""g2""), c (length (g1.lengths), length (g2.lengths)))

   lengths = data.frame( lengths = c(g1.lengths, g2.lengths), group)
</code></pre>

<p>It is evident that sample size per group is biased where <strong>g1 has 6</strong> observations and <strong>g2 has 22</strong>. Traditional ANOVA suggests that groups have different means when critical value is set to 0.05 (p value is <strong>0.0044</strong>).</p>

<pre><code>   summary (aov (lengths~group, data = lengths))  
</code></pre>

<p>Given that my aim is to compare mean difference, such unbalanced and small sampled data might give inappropriate results with traditional approach. Therefore, I want to perform permutation test and bootstrap.</p>

<p><strong>PERMUTATION TEST</strong></p>

<p>Null hypothesis (H0) states that group's means are the same. This assumption in permutation test is justify by pooling groups into one sample. This ensures that the samples for two groups were drawn from the identical distribution. By repeated sampling (or more precisely - reshuffling) from the pooled data, the observations are reallocated (shuffled) to the samples in new way, and test statistic is calculated. Performing this n times, will give sampling distribution of the test statistics under the assumption where H0 is TRUE. At the end, under the H0, p value is probability that the test statistic equals or exceeds the observed value.</p>

<pre><code>s.size.g1 &lt;- length (g1.lengths)
s.size.g2 &lt;- length (g2.lengths)

pool &lt;- lengths$lengths

obs.diff.p &lt;- mean (g1.lengths) - mean (g2.lengths)

iterations &lt;- 10000

sampl.dist.p &lt;- NULL

set.seed (5)
for (i in 1 : iterations) {

        resample &lt;- sample (c(1:length (pool)), length(pool))

        g1.perm = pool[resample][1 : s.size.g1]
        g2.perm = pool[resample][(s.size.g1+1) : length(pool)]
        sampl.dist.p[i] = mean (g1.perm) - mean (g2.perm) 

}

p.permute &lt;- (sum (abs (sampl.dist.p) &gt;= obs.diff.p) + 1)/ (iterations+1)
</code></pre>

<p>Reported p value of permutation test is <strong>0.0053</strong>. OK, if I did it correctly, permutations and parametric ANOVA give nearly identical results.</p>

<p><strong>BOOTSTRAP</strong></p>

<p>First of all, I am aware that bootstrap can not help when sample sizes is to small. <a href=""http://stats.stackexchange.com/questions/112147/can-bootstrap-be-seen-as-a-cure-for-the-small-sample-size/112681#112681"">This post showed that it can be even worse and misleading</a>. Also, <a href=""http://stats.stackexchange.com/questions/20217/bootstrapping-vs-permutation-tests"">second one highlighted that permutation test is generally better than bootstrap</a> when the hypothesis testing is the main aim. Nonetheless, <a href=""http://stats.stackexchange.com/questions/104040/resampling-simulation-methods-monte-carlo-bootstrapping-jackknifing-cross"">this great post</a> addresses important differences among computer-intensive methods. However, here I want to raise (I believe) a different question. </p>

<p>Let me introduce most common bootstrap approach first (Bootstrap1: <strong>resampling within the pooled sample</strong>):</p>

<pre><code>s.size.g1 &lt;- length (g1.lengths)
s.size.g2 &lt;- length (g2.lengths)

pool &lt;- lengths$lengths

obs.diff.b1 &lt;- mean (g1.lengths) - mean (g2.lengths)

iterations &lt;- 10000

sampl.dist.b1 &lt;- NULL

set.seed (5)
for (i in 1 : iterations) {

        resample &lt;- sample (c(1:length (pool)), length(pool), replace = TRUE) # ""replace = TRUE"" is the only difference between bootstrap and permutations

        g1.perm = pool[resample][1 : s.size.g1]
        g2.perm = pool[resample][(s.size.g1+1) : length(pool)]
        sampl.dist.b1[i] = mean (g1.perm) - mean (g2.perm) 

}

p.boot1 &lt;- (sum (abs (sampl.dist.b1) &gt;= obs.diff.b1) + 1)/ (iterations+1)
</code></pre>

<p>P value of bootstrap performed in this way is <strong>0.005</strong>. Even that this sounds reasonable and almost identical to parametric ANOVA and permutation test, is it appropriate to justify H0 in this bootstrap on the basis that we <strong>just pooled samples</strong> from which we drawn subsequent samples?</p>

<p>Different approach I found in several scientific papers. Specifically, I saw that researchers modify the data in order to meet H0 prior to bootstrap. Searching around, I have found <a href=""http://stats.stackexchange.com/questions/20701/computing-p-value-using-bootstrap-with-r"">very interesting post in CV</a> where <a href=""http://stats.stackexchange.com/users/11403/jan-s"">@jan.s</a> explained unusual results of bootstrap in the post question where the aim was to compare two means. However, in that post it is not covered how to perform bootstrap when data are modified prior to bootstrap. Approach where the data is modified prior to bootstrap looks like this:</p>

<ol>
<li>H0 states that the means of two groups are the same</li>
<li>H0 holds true if we subtract individual observations from the mean of pooled sample</li>
</ol>

<p>In this case, modification of the data should affects the groups means, and therefore its difference, but not variation within (and between) groups.</p>

<ol start=""3"">
<li>Modified data will be basis for further bootstrap, with caveats that sampling is carried out <strong><em>within each group separately</em></strong>.</li>
<li>Difference between bootstrapped mean of g1 and g2 is calculated and compared with observed (non-modified) difference between groups.</li>
<li>Proportion of equal or more extreme values than observed one divided by number of iteration will give p value.    </li>
</ol>

<p>Here is the code (Bootstrap2: <strong>resampling within the groups after modification that H0 is TRUE</strong>):</p>

<pre><code>s.size.g1 &lt;- length (g1.lengths)
s.size.g2 &lt;- length (g2.lengths)

pool &lt;- lengths$lengths

obs.diff.b2 &lt;- mean (g1.lengths) - mean (g2.lengths)

# make H0 to be true (no difference between means of two groups)
     H0 &lt;- pool - mean (pool)

# g1 from H0 
     g1.H0 &lt;- H0[1:s.size.g1] 

# g2 from H0
     g2.H0 &lt;- H0[(s.size.g1+1):length(pool)]

iterations &lt;- 10000

sampl.dist.b2 &lt;- NULL

set.seed (5)
for (i in 1 : iterations) {

        # Sample with replacement in g1
              g1.boot = sample (g1.H0, replace = T)

        # Sample with replacement in g2
              g2.boot = sample (g2.H0, replace = T)

        # bootstrapped difference
              sampl.dist.b2[i] &lt;- mean (g1.boot) - mean (g2.boot)  
}

p.boot2 &lt;- (sum (abs (sampl.dist.b2) &gt;= obs.diff.b2) + 1)/ (iterations+1)
</code></pre>

<p>Such performed bootstrap will give p value of <strong>0.514</strong> which is tremendously different compared to previous tests. I believe that this has to deal with <a href=""http://stats.stackexchange.com/users/11403/jan-s"">@jan.s'</a> explanation, but I can not figure out where is the key...    </p>

<p><strong>So, here are the questions:</strong></p>

<p><strong>Q1:</strong> Why modified data in the bootstrap 2 yielded to such differences and is it reasonable to modify data prior to bootstrap in such way?  </p>

<p><strong>Q2:</strong> If it isn't reasonable at minor points, what is the solution that modified data give accurate result if it is anyhow possible?   </p>

<p><strong>Q3a:</strong> How does it is important to ensure that resampling with replacement is under TRUE H0? </p>

<p><strong>Q3b:</strong> Does resampling from the pooled data (bootstrap 1 example) justify assumption that ensures that H0 is TRUE?       </p>

<p><strong>Q4:</strong> Therefore, what is main difference between bootstraps where resampling is within the pooled sample and one where resampling is within groups?  </p>

<p><strong>Q5:</strong> Does the second bootstrap procedure take into account <strong><em>within group</em></strong> variation in contrasts to bootstrap 1, at least in case when sample sizes are biased?</p>

<p>Please, keep in mind that I am not neither mathematicians nor statisticians and I learned all those stuff trough self-learning, so please provide as simple as possible (if possible) any explanation.</p>

<p>Thank you for the time to consider the thread.</p>
"
"0.207830982477317","0.212992817021566","136899","<p>My <strong>question</strong> is simple: </p>

<p>How do you determine the overall significance of an interaction (i.e. the marginal effect of $X$ on $Y$ for different values of $Z$)? </p>

<hr>

<p>But the background is a bit long-winded, so please bear with me. Consider this example (<a href=""http://homepages.nyu.edu/~mrg217/marginal_effect_plot.zip"" rel=""nofollow"">data available here</a> along with some <a href=""http://homepages.nyu.edu/~mrg217/marginal_effect_plot.pdf"" rel=""nofollow"">details</a>, taken from <a href=""https://files.nyu.edu/mrg217/public/jop2.pdf"" rel=""nofollow"">Berry, Golder and Milton 2012</a>): </p>

<pre><code>library(""foreign"")
library(""lmtest"")
dta &lt;- read.dta(""alexseev.dta"")
slvote.mod  &lt;- lm(xenovote ~ slavicshare * changenonslav  +
    inc9903 + eduhi02 + unemp02 + apt9200 + vsall03 + brdcont, data=dta)
coeftest(slvote.mod)
</code></pre>

<p>Which will output: </p>

<pre><code>&gt; coeftest(slvote.mod)  ##same as in Berry et al. 2012, but without clustered SEs

t test of coefficients:

                             Estimate  Std. Error t value  Pr(&gt;|t|)    
(Intercept)                8.94287844  2.71084994  3.2989 0.0016110 ** 
slavicshare                0.03148617  0.02089537  1.5068 0.1369279    
changenonslav             -0.85110769  0.35210573 -2.4172 0.0185955 *  
[..]
slavicshare:changenonslav  0.00822591  0.00527380  1.5598 0.1239045    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However <a href=""https://files.nyu.edu/mrg217/public/pa_final.pdf"" rel=""nofollow"">Brambor, Clark and Golder (2006)</a> (which comes with an <a href=""https://files.nyu.edu/mrg217/public/interaction.html"" rel=""nofollow"">internet appendix</a>) caution that: </p>

<blockquote>
  <p>Although we still want to know about the marginal effect of some independent variable X on Y in an interaction model ($\beta_1 + \beta_3Z$), typical results tables will report only the marginal effect of X when the conditioning variable is zero, i.e., $\beta_1$. Similarly, these tables report only the standard error for this particular effect. As a result, the only inference that can be drawn in this situation is whether $X$ has a significant effect on $Y$ for the unique case in which $Z = 0$. [..] There is simply no way of knowing from the typical results table if $X$ has a significant effect on $Y$ when the conditioning variable is not zero.</p>
  
  <p>The analyst cannot even infer whether $X$ has a meaningful conditional effect on $Y$ from the magnitude and significance of the coefficient on the interaction term either. As we showed earlier, it is perfectly possible for the marginal effect of $X$ on $Y$ to be significant for substantively relevant values of the modifying variable $Z$ even if the coefficient on the interaction term is insignificant. [..] It means that one cannot determine whether a model should include an interaction term simply by looking at the significance of the coefficient on the interaction term. [..]</p>
  
  <p>The point here is that the typical results table often conveys very little information of interest because the analyst is not concerned with model parameters per se; he or she is primarily interested in the marginal effect of $X$ on $Y$ ($\beta_1 + \beta_3Z$) for substantively meaningful values of the conditioning variable $Z$.</p>
</blockquote>

<p>So what they propose is a <strong>marginal-effect plot</strong>. Following <a href=""http://www.ats.ucla.edu/stat/r/faq/concon.htm"" rel=""nofollow"">How can I explain a continuous by continuous interaction</a>, I can roughly replicate it in R as follows: </p>

<pre><code>##marginal effect plot
at.changenonslav &lt;- seq(min(dta$changenonslav, na.rm=T), 
        max(dta$changenonslav, na.rm=T), 1)
sl_slopes &lt;- coef(slvote.mod)['slavicshare'] + 
    coef(slvote.mod)['slavicshare:changenonslav'] * at.changenonslav

library(msm)
sl_estmean&lt;-coef(slvote.mod)
sl_var&lt;-vcov(slvote.mod)
SEs &lt;- rep(NA, length(at.changenonslav))
for (i in 1:length(at.changenonslav)){
    j &lt;- at.changenonslav[i]
    SEs[i] &lt;- deltamethod(~ (x2) + (x10)*j, sl_estmean, sl_var)
}
sl_upper &lt;- sl_slopes + 1.96*SEs
sl_lower &lt;- sl_slopes - 1.96*SEs

plot(at.changenonslav, sl_slopes, ylim=c(-.05, .25), type='l', 
     ylab='Marginal effect of X (""slavicshare"") on Y', 
     xlab='Level of Z (""changenonslav"")')
points(at.changenonslav, sl_upper, type = ""l"", lty = 2)
points(at.changenonslav, sl_lower, type = ""l"", lty = 2)
abline(h=0, col=""grey"")
</code></pre>

<p>Which outputs this (not unlike the <a href=""http://homepages.nyu.edu/~mrg217/fig_jop2.pdf"" rel=""nofollow"">graph in Stata</a> by the authors of the paper): </p>

<p><img src=""http://i.stack.imgur.com/WgzqM.png"" alt=""enter image description here""></p>

<p>Brambor, Clark and Golder (2006) go on to suggest that (slightly paraphrased): </p>

<blockquote>
  <p>95% confidence intervals around the line allow us to determine the conditions under which $X$ has a statistically significant effect on $Y$â€”they have a statistically significant effect whenever the upper and lower bounds of the confidence interval are both above (or below) the zero line.</p>
</blockquote>

<p>There are also the <strong>effect displays</strong> from <code>effects</code> package in R: </p>

<pre><code>##effects display
require(effects)
plot(effect(""slavicshare:changenonslav"", slvote.mod, xlevels=4))
</code></pre>

<p>Which outputs the following graph, although it seems to me that this is a different (more detailed) presentation of the marginal-effect plot above, and that it doesn't help directly wrt statistical significance (i.e. the interpretation is less intuitive and more subjective): </p>

<p><img src=""http://i.stack.imgur.com/fMdQx.png"" alt=""enter image description here""></p>

<p>Another way to determine significance of interactions seems to be suggested (although not explicitly) in <a href=""http://www.jstatsoft.org/v08/i15"" rel=""nofollow"">Fox 2003</a>. There each linear model is followed by an <strong>Anova</strong> table which, if my understanding is correct, indicates if the interaction effect is statistically significant overall. </p>

<pre><code>##Anova
require(car)
Anova(slvote.mod)
</code></pre>

<p>Which outputs: </p>

<pre><code>&gt; Anova(slvote.mod)
Anova Table (Type II tests)

Response: xenovote
                           Sum Sq Df F value    Pr(&gt;F)    
slavicshare                41.374  1  8.8471 0.0041787 ** 
changenonslav              34.098  1  7.2913 0.0089209 ** 
[..]
slavicshare:changenonslav  11.377  1  2.4329 0.1239045    
Residuals                 289.945 62                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>But the two methods seemingly contradict each other in this case, the marginal-effect plot indicating significance, while the Anova indicating lack of significance for the interaction...</p>

<hr>

<p>So to return to my original <strong>question</strong>, how can I determine if the marginal effect of $X$ on $Y$ (for different values of $Z$) is overall statistically significant? </p>

<p>Are both the <em>marginal-effect plot</em> and the <em>Anova</em> (as presented above) valid tools for determining the significance of an interaction? Is one preferred over the other? Are there other statistical tools that can assist in this case? I'm interested in statistical approaches more generally, although solutions in R are very much appreciated...</p>
"
"0.0626633989716535","0.0441510785688348","136927","<p>I want to compare the following two linear models:</p>

<pre><code>model 1: y = mean + A + B  
model 2: y = mean + A + A*B
</code></pre>

<p>Is model 2 equivalent to y = mean + A + B + A*B? Can I use <code>anova(model1, model2)</code> in R to compare the two nested models? </p>

<p>If not, how can I compare them in R?</p>
"
"0.0723574605292422","0.0764719112901873","136983","<p>I am trying to figure out what is the estimated variance (i.e. the estimated ""error"") of residuals around a fitted line. </p>

<pre><code>&gt; summary(model)

Call:
lm(formula = fecundity ~ Organic)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.2909 -1.6439 -0.4606  1.5121  3.7273 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  47.6667     1.4907   31.98 9.97e-10 ***
Organic      -8.6788     0.4805  -18.06 9.06e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.182 on 8 degrees of freedom
Multiple R-squared:  0.9761,    Adjusted R-squared:  0.9731 
F-statistic: 326.2 on 1 and 8 DF,  p-value: 9.063e-08

&gt; anova(model)
Analysis of Variance Table

Response: fecundity
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
Organic    1 1553.5 1553.50  326.22 9.063e-08 ***
Residuals  8   38.1    4.76                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0886194286901087","0.0936585811581694","137766","<p>I did the model comparison using these three models:</p>

<pre><code>lmer1 &lt;- lmer(peak_Mid ~ (1|item) + (1+vowel|speaker) + gender*vowel*language, data=data.frame, REML=FALSE, na.action=na.omit)
lmer2 &lt;- lmer(peak_Mid ~ (1|item) + (1+vowel|speaker) + gender + vowel + Language + language:vowel + Language:gender + vowel:gender, data=data.frame, REML=FALSE, na.action=na.omit)
anova(lmer1,lmer2)
lmer3 &lt;- lmer(peak_Mid ~ (1|item) + (1+vowel|speaker) + gender + vowel + language, data=data.frame, REML=FALSE, na.action=na.omit)
anova(lmer2,lmer3)
</code></pre>

<p>Then I run Tukey's HSD Post-hoc test based on the optimal model without changing 'REML=FALSE' to 'REML=TRUE'. The question is whether I need to change this before running the Tukey's HSD Post-hoc test? In my opinion, I don't need to change it as if I change, it means I didn't get the Tukey's HSD Post-hoc test based on the optimal model from model comparison as I changed the formula. What do you think?</p>
"
"NaN","NaN","138576","<p>I am trying to run a one way ANOVA on some data. One factor, 4 levels. I only have mean, SD, and sample sizes to work with. Is there a way to do this in R?</p>
"
"NaN","NaN","139289","<p>I'm new to statistics, and so far I'm not grasping the significance of the concept of ""degrees of freedom"". So far, I've only had tangential exposure to it by way of calculating Student's t.</p>

<p>When we run anova() in R, in this example, I don't understand why the degrees of freedom are returned (the ""Df"" column) in this context, and what it contributes:</p>

<pre><code>&gt; tomato &lt;-data.frame(
+ weight=c(1.5, 1.9, 1.3, 1.5, 2.4, 1.5, # water
+  1.5, 1.2, 1.2, 2.1, 2.9, 1.6, # Nutrient 
+ 1.9, 1.6, 0.8, 1.15, 0.9, 1.6), # Nutrient+24D
+ trt = rep(c(""water"", ""Nutrient"", ""Nutrient+24D""),
+ c(6, 6, 6)))
&gt; tomato.aov &lt;- aov(weight ~ trt, data=tomato)
&gt; anova(tomato.aov)
Analysis of Variance Table

Response: weight
          Df Sum Sq Mean Sq F value Pr(&gt;F)
trt        2 0.6269 0.31347  1.2019  0.328
Residuals 15 3.9121 0.26081   
</code></pre>

<p>Some clarification would be greatly appreciated.</p>
"
"0.0981746784648192","0.126814318375447","139996","<p>I'm trying to extract a bunch of effect sizes from a couple of different studies in for which the authors have performed ANOVAs on their data. More specifically, I'm interested in the main effects of a certain dichotomous variable.</p>

<p>Now, if I only have the F-values and the sample sizes for my two groups that I'm interested in, can I really deduct something meaningful from this? The <code>compute.es</code> package for <code>R</code> has a function called <code>fes()</code> (see page 45 of the manual <a href=""http://cran.r-project.org/web/packages/compute.es/compute.es.pdf"" rel=""nofollow"">here</a>), for which you input the F-value and the sample sizes and get an effect size. The formula that it uses is:</p>

<p>$$
d=\sqrt{F*\frac{n_1+n_2}{n_1n_2}}
$$</p>

<p>However, I did some tests with different ANOVAs. I held the data constant for the two groups that I'm interested in, and added/removed other data in order to create a couple of different analyses: A One-way ANOVA, a 2x2 ANOVAs, a 2x3 ANOVA, and a 2x2x2 ANOVA. All of them gave me different F-values for the main effect of the variable I'm interested in, and subsequently <code>fes()</code> gave me different estimations of the effect size.</p>

<p>I'm not quite sure what I'm doing here. Is it ever possible to get some kind of ""true"" effect size (that is, the same you acquire get if you had the means and standard deviations from the two groups) from an F-value and the sample sizes? In that case, for what type of ANOVA?</p>

<p>In <a href=""http://stats.stackexchange.com/questions/6824/calculating-and-interpreting-effect-sizes-for-interaction-terms/6829#6829"">this answer</a>, it's suggested that it's not possible, but what's the deal with the <code>fes()</code> function then?</p>
"
"0.10232890201933","0.108147614087175","140055","<p>I have a dataset with thousands of observations pre-assigned to 18 groups and with measures for 8 different variables. I am using canonical discriminant analysis to see how separable my 18 groups are. What I am actually most interested in is which individual variable separates the groups most (and least). </p>

<p>I have tried running canonical discriminant analysis in R using the ldm() function from the MASS library. </p>

<pre><code>mydata.lda &lt;- lda(group ~ x1 + x2 + x3 .... + x8, data=mydata)
</code></pre>

<p>If I understand correctly, the output has coefficients of linear discriminant which indicates how strongly each variable is associated with each individual discriminant function, and I could standardize the coefficients to help interpret the meanings of the resultant discriminant functions. </p>

<p>I think what I want however is the partial F-square of each individual variable, or the relative ability of each variable to separate groups across all discriminant functions, not one at a time. 
In SPSS, the discriminant analysis function allows one to ask for ""univariate ANOVAs"" which seem to produce what I want: a table showing the Wilks' Lambda statistic and F statistics for each of my 8 variables. How would I get this kind of output in R? Do I need to run a (M)ANOVA based on the output of my lda()? </p>
"
"NaN","NaN","140605","<p>I am trying to compare a linear model and other non linear models(Asymptotic, Logistic and Ricker) by means of an F test or a likelihood ratio test. I have tried anova(Linear, Logistic,Ricker, Asymptote) but this generates an error. Is there a way to do this in R?</p>

<p>I used the following models:</p>

<pre><code>Linear&lt;-lm(mean~age,Lmaxl)
Logistic&lt;- nlsLM(mean ~ k/(1+((k- Bo)/Bo)*exp(-r*age)), 
    data=Lmaxl, start=list(k=50,Bo=20,r=0.1), 
    control=liâ€Œst(maxiter=200),
    na.action=""na.exclude"")
Asymptote&lt;-nlsLM(mean ~k+(Bo-)*exp(-r*age), 
    data=Lmaxl,
    start=list(k=50,Bo=20,r=0.1),
    control=list(maxitâ€Œâ€‹er=200),
    na.action=""na.exclude"")
 Ricker&lt;- nlsLM( mean~ Bo+(a*age)*exp(-b*age), 
    data=Lmaxl, 
    start=list(Bo=10, a=5, b=0.01),
    control=list(maxiter=200),
    na.action=""na.exclude"")
</code></pre>
"
"0.0738495239084239","0.0936585811581694","140690","<p>In a balanced repeated measures one-way ANOVA, we compare the true averages of a response variable across time points. Are the results about the time effects equivalent to those from a classic two-way ANOVA with ""time"" and ""subject id"" as factors? If yes, do you know a good reference about it?</p>

<p>Example in R (20 subject ids with one value for each of the three time points):</p>

<pre><code># Data generation
set.seed(2)
t0 &lt;- rnorm(20)
t1 &lt;- rexp(20) + t0 / 4
t2 &lt;- runif(20) + t1 / 2
response &lt;- c(t0, t1, t2)
time &lt;- factor(rep(1:3, each = 20))
id &lt;- factor(rep(1:20, times = 3))
#___________________________________________________________

# Two-way between-subject ANOVA
drop1(lm(response ~ time + id), test = ""F"")

# Output -&gt; p-value of any time effect is 0.0013734 

       Df Sum of Sq    RSS      AIC F value    Pr(&gt;F)    
&lt;none&gt;              21.038 -18.8811                      
time    2     8.723 29.761  -2.0690  7.8780 0.0013734 ** 
id     19    39.129 60.167   6.1669  3.7199 0.0002787 ***
#___________________________________________________________

# Now the repeated-measures ANOVA
summary(aov(response ~ time + Error(id)))

# Output -&gt; p-value also 0.00137
Error: id
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 19  39.13   2.059               

Error: Within
          Df Sum Sq Mean Sq F value  Pr(&gt;F)   
time       2  8.723   4.361   7.878 0.00137 **
Residuals 38 21.038   0.554                   
</code></pre>

<p>As far as I know, the methods provide also similar (wrong) p-values in the unbalanced case.</p>
"
"0.0966917238133724","0.102189889914161","140991","<p>I've been having some trouble in attempting to compare sets of data. I can't seem to analyse whether two models describe the same set of data, or if they describe different sets.</p>

<p>Here is my a portion of my basic data:</p>

<pre><code> ZT     WT_PAL Line_37_PAL  WT_PhPRR5 Line_37_PhPRR5    WT_EOBI Line_37_EOBI   WT_EOBII Line_37_EOBII     WT_CM1 Line_37_CM1     WT_ADT
1   0 0.08017366 0.000959987 0.26035363     0.03264146 1.46476869  0.009786237 4.16477772   0.000742414 0.07395887 0.000456353 0.06000000
2   0 0.05930462 0.021197691 0.26147552     0.22926780 1.57837816  0.926847383 1.15031587   0.461807744 0.03682062 0.101097795 0.05322561
3   0 0.14389513 0.756356081 0.63035752     0.72129878 1.76452175  0.640368308 2.42348584   1.364089162 0.12954215 0.892205209 0.13821109
4   4 0.12194367 0.297290671 0.13444482     0.14225469 0.99144104  1.131902963 0.91522009   0.910081812 0.29664680 0.505630813 0.51706760
5   4 0.06025697 0.164053161 0.15448683     0.26627386 1.31917230  1.519721821 0.62925084   2.483566296 0.12296628 0.364813045 0.35061055
6   4 0.20896743 0.249435523 1.23052341     0.61818565 1.77819303  1.284683192 1.41398975   1.523446689 0.30023862 0.282538740 0.56811626
7   8 2.38864472 0.042225180 1.54472331     0.04236890 1.04169534  0.860432687 0.26977645   2.001020769 2.93724542 1.340914776 3.00230489
8   8 2.27484249 0.108464160 1.27963226     0.21218338 0.92997042  0.999347054 0.24756421   0.878011535 2.36280758 0.564269963 2.05923549
9   8 1.72728498 0.284489142 1.17311707     0.63301025 0.73380469  0.863829602 0.20109633   0.831139775 2.37338677 1.046991612 2.24797092
10 12 1.13821434 0.462596491 2.22919520     0.15287139 0.34310114  0.817010999 0.29965738   0.236064056 1.18592546 0.725928756 1.01932917
11 12 1.10145755 0.368458720 2.13568842     0.39531534 0.33147292  1.107039633 0.32343745   0.888220142 0.98362898 0.663785645 0.93808648
12 12 1.91985246 0.219754262 1.44412345     0.66775319 0.22753689  0.513590231 0.07657606   1.100251286 1.75011191 0.251849690 1.61130028
13 16 0.68005324 0.396014538 0.31868826     0.14759449 0.38865638  0.778205100 1.09767555   0.627603654 0.55060102 0.784160371 0.60319061
14 16 0.83616544 0.514261850 0.21921500     0.19384070 0.22801491  1.029590354 0.12193953   0.494258870 0.62367453 0.868126888 0.59068953
15 16 0.59058070 0.758966630 0.56687274     0.80844039 0.12417071  0.698339222 0.12503996   1.321782313 0.50518054 1.127351763 0.90570233
16 20 0.30896858 0.376021422 0.18652112     0.16757942 0.50239187  0.823056297 0.30242397   0.549940528 0.32069459 0.464616256 0.33701357
17 20 0.04854291 0.231663315 0.07268395     0.10814706 0.07590502  0.620767904 0.03008203   0.491554754 0.04180077 0.374756383 0.04942141
18 20 0.81359279 0.833815983 0.58218634     0.32892256 0.35501741  0.381413660 0.34660498   0.558786138 0.43100429 0.645363500 0.99771479
</code></pre>

<p>What I would like to do, is to see if the expression profile over time of Line_37_PAL is significantly different to that of WT_PAL</p>

<p>First thing I did was try to fit the model:</p>

<pre><code>fitWT_PAL_1 &lt;- lm(data1$WT_PAL ~ data1$ZT)
fitWT_PAL_2 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2))
    fitWT_PAL_3 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3))
    fitWT_PAL_4 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3) + I(data1$ZT^4))
fitWT_PAL_5 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3) + I(data1$ZT^4) + I(data1$ZT^5))
</code></pre>

<p>Which determined that fitWT_PAL_4 fit the data best.</p>

<p>I then did the same for the Line_37_PAL, fit37_PAL_5 proved to be the best fit.</p>

<p>I wanted to see here whether or not the two models adequately described the same data, or if the data they described were different (and that the models were in fact describing different expression profiles).</p>

<p>But when entering the anova I get:</p>

<pre><code>&gt; anova(fit37_PAL_4, fitWT_PAL_1)
Analysis of Variance Table

Response: data1$Line_37_PAL
                  Df  Sum Sq  Mean Sq F value Pr(&gt;F)
    data1$ZT       1 0.10797 0.107974  1.7944 0.1901
I(data1$ZT^2)  1 0.00342 0.003422  0.0569 0.8131
    I(data1$ZT^3)  1 0.12717 0.127171  2.1134 0.1561
I(data1$ZT^4)  1 0.04095 0.040949  0.6805 0.4157
    Residuals     31 1.86536 0.060173               
    Warning message:
    In anova.lmlist(object, ...) :
      models with response â€˜""data1$WT_PAL""â€™ removed because response differs from model 1
</code></pre>

<p>I'm assuming this is because my Y-values come from two different sets of data? Please correct me if I'm wrong, and I would be thankful for any advice you might be able to give.</p>

<p>I ran the predicted values of a model against the actual values using t.test(x,y, paired = TRUE), but that only describes the differences in means of the two populations, not the possible differences in expression patterns. Advice on how to proceed?</p>
"
"0.102754387854659","0.108597294255834","141011","<p>I've been having some trouble in attempting to compare sets of data. I can't seem to analyse whether two models describe the same set of data, or if they describe different sets. </p>

<p>Here is my a portion of my basic data:</p>

<pre><code>   ZT     WT_PAL Line_37_PAL  WT_PhPRR5 Line_37_PhPRR5    WT_EOBI Line_37_EOBI   WT_EOBII Line_37_EOBII     WT_CM1 Line_37_CM1     WT_ADT
1   0 0.08017366 0.000959987 0.26035363     0.03264146 1.46476869  0.009786237 4.16477772   0.000742414 0.07395887 0.000456353 0.06000000
2   0 0.05930462 0.021197691 0.26147552     0.22926780 1.57837816  0.926847383 1.15031587   0.461807744 0.03682062 0.101097795 0.05322561
3   0 0.14389513 0.756356081 0.63035752     0.72129878 1.76452175  0.640368308 2.42348584   1.364089162 0.12954215 0.892205209 0.13821109
4   4 0.12194367 0.297290671 0.13444482     0.14225469 0.99144104  1.131902963 0.91522009   0.910081812 0.29664680 0.505630813 0.51706760
5   4 0.06025697 0.164053161 0.15448683     0.26627386 1.31917230  1.519721821 0.62925084   2.483566296 0.12296628 0.364813045 0.35061055
6   4 0.20896743 0.249435523 1.23052341     0.61818565 1.77819303  1.284683192 1.41398975   1.523446689 0.30023862 0.282538740 0.56811626
7   8 2.38864472 0.042225180 1.54472331     0.04236890 1.04169534  0.860432687 0.26977645   2.001020769 2.93724542 1.340914776 3.00230489
8   8 2.27484249 0.108464160 1.27963226     0.21218338 0.92997042  0.999347054 0.24756421   0.878011535 2.36280758 0.564269963 2.05923549
9   8 1.72728498 0.284489142 1.17311707     0.63301025 0.73380469  0.863829602 0.20109633   0.831139775 2.37338677 1.046991612 2.24797092
10 12 1.13821434 0.462596491 2.22919520     0.15287139 0.34310114  0.817010999 0.29965738   0.236064056 1.18592546 0.725928756 1.01932917
11 12 1.10145755 0.368458720 2.13568842     0.39531534 0.33147292  1.107039633 0.32343745   0.888220142 0.98362898 0.663785645 0.93808648
12 12 1.91985246 0.219754262 1.44412345     0.66775319 0.22753689  0.513590231 0.07657606   1.100251286 1.75011191 0.251849690 1.61130028
13 16 0.68005324 0.396014538 0.31868826     0.14759449 0.38865638  0.778205100 1.09767555   0.627603654 0.55060102 0.784160371 0.60319061
14 16 0.83616544 0.514261850 0.21921500     0.19384070 0.22801491  1.029590354 0.12193953   0.494258870 0.62367453 0.868126888 0.59068953
15 16 0.59058070 0.758966630 0.56687274     0.80844039 0.12417071  0.698339222 0.12503996   1.321782313 0.50518054 1.127351763 0.90570233
16 20 0.30896858 0.376021422 0.18652112     0.16757942 0.50239187  0.823056297 0.30242397   0.549940528 0.32069459 0.464616256 0.33701357
17 20 0.04854291 0.231663315 0.07268395     0.10814706 0.07590502  0.620767904 0.03008203   0.491554754 0.04180077 0.374756383 0.04942141
18 20 0.81359279 0.833815983 0.58218634     0.32892256 0.35501741  0.381413660 0.34660498   0.558786138 0.43100429 0.645363500 0.99771479
</code></pre>

<p>What I would like to do, is to see if the expression profile over time of Line_37_PAL is significantly different to that of WT_PAL</p>

<p>First thing I did was try to fit the model: </p>

<pre><code>fitWT_PAL_1 &lt;- lm(data1$WT_PAL ~ data1$ZT)
fitWT_PAL_2 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2))
fitWT_PAL_3 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3))
fitWT_PAL_4 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3) + I(data1$ZT^4))
fitWT_PAL_5 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3) + I(data1$ZT^4) + I(data1$ZT^5))
</code></pre>

<p>Which determined that fitWT_PAL_4 fit the data best. </p>

<p>I then did the same for the Line_37_PAL, fit37_PAL_5 proved to be the best fit. </p>

<p>I wanted to see here whether or not the two models adequately described the same data, or if the data they described were different (and that the models were in fact describing different expression profiles). </p>

<p>But when entering the anova I get: </p>

<pre><code>&gt; anova(fit37_PAL_4, fitWT_PAL_1)
Analysis of Variance Table

Response: data1$Line_37_PAL
              Df  Sum Sq  Mean Sq F value Pr(&gt;F)
data1$ZT       1 0.10797 0.107974  1.7944 0.1901
I(data1$ZT^2)  1 0.00342 0.003422  0.0569 0.8131
I(data1$ZT^3)  1 0.12717 0.127171  2.1134 0.1561
I(data1$ZT^4)  1 0.04095 0.040949  0.6805 0.4157
Residuals     31 1.86536 0.060173               
Warning message:
In anova.lmlist(object, ...) :
  models with response â€˜""data1$WT_PAL""â€™ removed because response differs from model 1
</code></pre>

<p>I'm assuming this is because my Y-values come from two different sets of data? Please correct me if I'm wrong, and I would be thankful for any advice you might be able to give. </p>

<p>I ran the predicted values of a model against the actual values using t.test(x,y, paired = TRUE), but that only describes the differences in means of the two populations, not the possible differences in expression patterns. Advice on how to proceed? </p>
"
"0.0808981002113217","0.0854981960070962","141279","<p>For a <code>MANOVA</code> with $n$ variables, I would like to do pairwise comparisons between $k$ levels for one of the variables. </p>

<p>What is the suitable method to adopt for this while adjusting $\alpha$ for the $k(k-1)$ multiple comparisons?</p>

<ol>
<li><p>Is multiple <code>Hotelling</code> $T^2$ tests along with <code>Bonferroni</code> correction or FDR/pFDR appropriate? FDR/pFDR q values would be preferable as the $\beta$ value is important here.</p></li>
<li><p>Any suggestions for <code>R</code> packages to do the same? (Particularly for MANOVA post-hoc multiple comparisons}</p></li>
<li><p>How to test the null hypothesis $H_0^j:|\mu_1^j-\mu_2^j|\ge\delta$ instead of  $H_0^j:\mu_1^j=\mu_2^j$ as in an equivalence test for the multiple comparisons?</p></li>
</ol>

<h1>Edit</h1>

<p>Based on the answer and further comment by <a href=""http://stats.stackexchange.com/users/52554/rvl"">rvl</a>, I was able to explore and come up with the following.</p>

<pre><code>library(lsmeans)
# Use the `oranges` dataset in `lsmeans` package.
# multivariate linear model
oranges.mlm &lt;- lm(cbind(sales1,sales2) ~ price1 + price2 + day + store,
                  data = oranges)
# Get the least square means
oranges.Vlsm &lt;- lsmeans(oranges.mlm, ""store"")
# Multiple comparisons with fdr p value adjustment
test(contrast(oranges.Vlsm, ""pairwise""), side = ""="",  adjust = ""fdr"")
# With threshold spcified
test(contrast(oranges.Vlsm, ""pairwise""), side = ""="",  adjust = ""fdr"", delta = 0.25)
</code></pre>
"
"0.1576994291291","0.149122807017544","141820","<p>I want to find which soil variables better explain plant productivity, using a database that contains information for about 100 forests plots across Europe.
These plots have only one species per plot, but overall there are 4 different species in the dataset. These plots also have different climate conditions (temperature, precipitation,...). My final goal is finding out which combination of the more than 20 different soil variables better explain plant productivity. However, both climate and species may confound the analysis because both affect plant growth (some species grow more than others, and plants grown in warmer climates may grow more). I am only interested in plant growth due to soil characteristics, so I need to get rid of the species and climate effects on plant productivity that may confound the analysis. According to what I have read I could just include all variables in the model: soil, climate and species (factor of 4 levels), like this:</p>

<pre><code>fit &lt;- lm(scale(IVMean)~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+
                        scale(EXCHCA)+scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+
                        scale(EXCHNA)+scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+
                        scale(S_SO4)+scale(N_NH4)+scale(BS)+scale(CN)+scale(Temp)+
                        scale(Precip)+scale(Rad)+scale(PET)+species)
</code></pre>

<p>IVMean = mean stem volume increment (productivity). Note climate variables (temperature, precipitation, radiation and potential evapotranspiration -PET-) and species at the end, and the standardisation of all variables with <code>scale()</code>.</p>

<p>After this, I could run a stepwise regression analysis to preliminarily find which variables are the most important explaining plant productivity.</p>

<pre><code>library(MASS)
step &lt;- stepAIC(fit, direction=""backward"")
step$anova # display results
</code></pre>

<p>Which renders the following best minimal model:</p>

<pre><code>Final Model:
scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
    scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species

&gt; model &lt;- lm(scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
+               scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species, 
+             data = icp)
&gt; summary(model)

Call:
lm(formula = scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + 
    scale(EXCHMG) + scale(EXCHMN) + scale(BS) + scale(Temp) + 
    scale(PET) + species, data = icp)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.13836 -0.41522 -0.02816  0.35094  1.65587 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -0.37587    0.16967  -2.215 0.030897 *  
scale(PHCACL2)      0.58776    0.20617   2.851 0.006128 ** 
scale(EXCHCA)      -0.38061    0.19025  -2.001 0.050381 .  
scale(EXCHMG)      -0.37374    0.14686  -2.545 0.013769 *  
scale(EXCHMN)       0.13102    0.09970   1.314 0.194241    
scale(BS)           0.39502    0.19428   2.033 0.046871 *  
scale(Temp)         1.34654    0.32033   4.204 9.74e-05 ***
scale(PET)         -0.62177    0.29749  -2.090 0.041250 *  
speciesoak         -1.24553    0.34788  -3.580 0.000726 ***
speciespicea_abies  1.38679    0.25031   5.540 8.79e-07 ***
speciesscots_pine   0.02627    0.25960   0.101 0.919769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.6411 on 55 degrees of freedom
Multiple R-squared:  0.6522,    Adjusted R-squared:  0.5889 
F-statistic: 10.31 on 10 and 55 DF,  p-value: 1.602e-09
</code></pre>

<p>The final model includes 5 soil variables, 2 out of 4 climate variables, and species. So far so good?</p>

<p>However, this seems to be not good enough for my supervisor. Rather, he asked me to do an analysis of the residuals to â€œget rid of climate and species effectsâ€! To be honest, I have no idea what he is talking about, and I was afraid to ask because he sounded like something I should know since my childhood. Perhaps he meant I should study which SOIL variables can explain the residuals of productivity ~ climate * species? Please, help me find out which type of analysis of the residuals would make sense to focus on soil effects eliminating climate and species effects.</p>

<p>This is the only thing I can think of:  </p>

<pre><code># Study the importance of confounding effects:
confounding     &lt;- IVMean ~ (Temp + Precip + PET + Rad) * species 
confounding.res &lt;- residuals(confounding)
lm(confounding.res ~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+scale(EXCHCA)+
                    scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+scale(EXCHNA)+
                    scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+scale(S_SO4)+
                    scale(N_NH4)+scale(BS)+scale(CN))
</code></pre>

<p>This way maybe I could study which soil variables explain what climate and species effects could not explain? I donâ€™t know if it makes any sense. I am open to suggestions and alternatives. </p>
"
"0.0361787302646211","0.0382359556450936","141894","<p>Apologies if this topic seems to have been beaten to death, but I couldn't find an exact duplicate. Take this data in R:</p>

<pre><code>test &lt;- data.frame(Y = c(41, 43, 50, 51, 43, 53, 54, 46, 45, 55, 56, 60, 58, 62, 62,
56, 47, 45, 46, 49, 58, 54, 49, 61, 52, 62, 59, 55, 68, 63, 43, 56, 48, 46, 47, 59,
46, 58, 54, 55, 69, 63, 56, 62, 67),
A = factor(rep(1:3,each=15),labels=c(""A1"", ""A2"", ""A3"")),
B = factor(rep(rep(1:3,3), c(3,5,7,5,6,4,5,4,6)), labels=c(""B1"", ""B2"", ""B3"")) )
</code></pre>

<p>Now if I use <code>drop1</code> to get a Type III, SPSS/SAS style anova result, I get:</p>

<pre><code>z &lt;- lm(Y ~ A + B, data=test, contrasts = c(""contr.sum"", ""contr.poly""))
drop1(z, .~., test = ""F"")

#Single term deletions
#       Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)    
#A       2    238.48 1258.1 155.88  4.6779  0.01494 *    # A after B 
#B       2   1253.19 2272.8 182.50 24.5817 1.09e-07 ***  # B after A
</code></pre>

<p>If I fit a model predicting the residuals left over after fitting <code>B</code>, by <code>A</code>, I figured I would get the same result as above, but it is marginally different.</p>

<pre><code>anova(lm(resid(lm(Y ~ B)) ~ A, data=test))

#          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
#A          2  232.33 116.164  4.7563 0.01374 *
#Residuals 42 1025.77  24.423  
</code></pre>

<p>I gather I'm missing something elementary, but why is this?</p>
"
"0.201533073917398","0.206336791489642","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.104438998286089","0.110377696422087","144096","<p>I have data from gene expression arrays and I have clinical data associated with the samples used. I am using gene expression (discrete), age at diagnosis (discrete) and ethnicity (categorical) to build a regression model. I'd like to predict gene expression by age at diagnosis and ethnicity so the model would be something like:</p>

<p>$$y = b_0 + b_1x_1 + b_2x_2 + \epsilon$$</p>

<p>in R it looks something like:</p>

<pre><code>y &lt;- as.numeric(gene_expression_myFavoriteGenes)
age &lt;- age_diagnosis 
ethnicity &lt;- ethnicity 
l &lt;- lm(y ~ age + ethnicity)
</code></pre>

<p>now...when I look at the coefficients I get something like:</p>

<pre><code>Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -0.442785   0.151175  -2.929 0.008299 ** 
age_diagnosis    -0.005616   0.002422  -2.319 0.031090 *  
Caucasian        -0.910633   0.115870  -7.859 1.53e-07 ***
Hispanic         -0.801088   0.125429  -6.387 3.13e-06 ***
Honduran         -0.682405   0.210694  -3.239 0.004114 ** 
Peurto Rican     -0.679251   0.209620  -3.240 0.004100 ** 
South Asian      -1.237134   0.213569  -5.793 1.14e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1815 on 20 degrees of freedom
Multiple R-squared:  0.7795,    Adjusted R-squared:  0.7023 
F-statistic:  10.1 on 7 and 20 DF,  p-value: 2.21e-05
</code></pre>

<p>I'm a lil confused on how to interpret the data.
I first built the model without correcting for ethnicity, nothing was significant. 
then I added ethnicity, checked with <code>anova(fist_lm, second_lm)</code> and the difference is significant.</p>

<p>so now I have all ethnicity associated with a significant p-value and and adjusted R^2 of 0.7 ...so, my questions, sorry if they sound silly, are:</p>

<ul>
<li>in this specific case, can age + ANY ethnicity significantly predict expression of myFavoriteGene?</li>
<li>models with other genes only have one or two significant ethnicity, does that mean that those are the only ones that can reliably predict expression of myFavoriteGene?</li>
</ul>

<p>------- edit ---------</p>

<p>here an example of how my data look:</p>

<pre><code>&gt; data[1:10,1:6]
           skin.AA_2  skin.AA_3  skin.AA_4  skin.AA_5  skin.AA_6   skin.AA_7
100_g_at   5.5526731  4.7001569  5.3724104  5.3700587  5.7571421  5.76974711
1000_at    7.7757596  5.4761710  7.3896019  5.5514442  8.2559761  7.14107706
1001_at    1.4554496  0.3315354  1.3311387  1.4979105  2.0579317  1.50734217
1002_f_at -0.4427732 -0.7381431 -0.3714425 -0.3159300 -0.2270056  0.31245288
1003_s_at  1.7908548  1.2590320  1.4839795  1.6727171  1.8550568  1.99870500
1004_at    1.8082815  0.9940647  1.4085169  1.8658939  1.9275267  2.25192977
1005_at    3.1792907 10.2456153  6.1170771  9.9058017  8.3695269  5.02225258
1006_at   -0.3059731 -0.8761517 -0.7151807 -0.4620902 -0.5923052  0.02495093
1007_s_at  9.9911387 10.2839949 10.1105075  9.9944011 10.3866696 10.31211765
1008_f_at  7.9190579  4.5957139  4.0043624  4.6297893  4.2067368  7.62499810

&gt; clinicalData[1:10,1:5]  
patient_ID        ethnicity age_diagnosis age colname_data_matrix_SB
1      AA1005 African American            39  47              skin.AA_1
2      AA1007        Caucasian            32  50              skin.AA_3
3      AA1008        Caucasian            50  50              skin.AA_4
4      AA1009        Caucasian            18  68              skin.AA_5
5      AA1010        Caucasian            31  40              skin.AA_6
6      AA1015        Caucasian            41  43                       
7      AA3001         Hispanic            49  49              skin.AA_7
8      AA3006         Hispanic            48  51              skin.AA_8
9      AA3007         Hispanic            51  51              skin.AA_9
10     AA3008         Hispanic            49  50             skin.AA_10
</code></pre>
"
"0.0626633989716535","0.0662266178532522","144837","<p>What is the difference between <code>Anova(data)</code>and <code>anova(data)</code>?</p>

<pre><code>&gt; anova(res.full)
Analysis of Variance Table

Response: Sales
                     Df Sum Sq Mean Sq   F value    Pr(&gt;F)    
Promotional.Accounts  1   1070    1070   35.0800 0.0001465 ***
Active.Accounts       1  44337   44337 1453.2067 3.680e-12 ***
Competing.Brands      1  43331   43331 1420.2314 4.124e-12 ***
Potential             1     16      16    0.5146 0.4895772    
Residuals            10    305      31                        
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; Anova(res.full)
Anova Table (Type II tests)

Response: Sales
                     Sum Sq Df   F value    Pr(&gt;F)    
Promotional.Accounts    275  1    9.0193   0.01327 *  
Active.Accounts       27371  1  897.1238 4.025e-11 ***
Competing.Brands      42877  1 1405.3398 4.346e-12 ***
Potential                16  1    0.5146   0.48958    
Residuals               305 10                        
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ enter code here
</code></pre>

<p>I don't understand why in each table the F statistic for each factor is different.</p>
"
"0.102966471440162","0.120912708351669","145657","<p>Basically I'm attempting to recreate the results of an example from class in R. What I'm trying to do is decide whether it's best to use a single regression line for an entire data set or two lines based on a categorical variable. The teacher indicates there are three steps to this:</p>

<ol>
<li>Determine if two different lines are required</li>
<li>If yes, determine if they differ in slope</li>
<li>If yes, determine if they differ in intercept</li>
</ol>

<p>Here is my data:</p>

<pre><code>&gt; example
   Predictor Response Group
1         21       11     A
2         24       21     A
3         26       23     A
4         29       29     A
5         35       34     A
6         45       51     A
7         51       59     A
8         68       73     A
9         72       83     A
10        76       95     A
11        17       11     B
12        21       55     B
13        26       34     B
14        28       44     B
15        32       26     B
16        36       34     B
17        40       15     B
18        45       21     B
19        51       16     B
20        68       21     B
</code></pre>

<p>I've realized that if I add the interaction and group terms to the model:</p>

<pre><code>ex_mod &lt;- lm(Response ~ Predictor,data = example)
ex_mod2 &lt;- lm(Response ~ Predictor + Group + Predictor:Group,data = example)
</code></pre>

<p>And then perform ANOVA on this. I get the right answer for step 1:</p>

<pre><code>&gt; anova(ex_mod,ex_mod2)
Analysis of Variance Table

Model 1: Response ~ Predictor
Model 2: Response ~ Predictor + Group + Predictor:Group
  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    
1     18 6616.4                                 
2     16 1583.8  2    5032.6 25.42 1.078e-05 ***   
</code></pre>

<p>Which means I need different lines, but now I need to know if they differ in slope or y-intercept or both. And here is where I'm stuck. I cant seem to get the right answer (F = 293.17 for slope, and F = 170.77 for intercept). </p>

<p>The teacher indicates that the next steps are: 1) to generate RSS in which the slope is fixed,but the y-intercepts are allowed to vary; and 2) generate RSS in which the y-intercept is fixed, but the slopes are allowed to vary.</p>

<p>I apologize if the question is confusing or simplistic, but I dont know how to proceed from here.</p>

<p>Thanks</p>
"
"0.0808981002113217","0.0683985568056769","145790","<p>I'm trying to figure out how to produce an ANOVA Table in R for a multiple regression model. So far I can only produce it for each regressor, and the Mean Square is calculating as the same as Sum Of Squares.</p>

<pre><code>&gt; anova(nflwin.lm)
Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
pass_yard     1  76.193  76.193  26.172 3.100e-05 ***
percent_rush  1 139.501 139.501  47.918 3.698e-07 ***
oppo_rush     1  41.400  41.400  14.221 0.0009378 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I'm trying to produce something like</p>

<pre><code>Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
Model         3  76.193  76.193  26.172 3.100e-05 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"NaN","NaN","146775","<p>I have only just started using R in my first year at university and I am completely stuck. I have done an anova test as the data is normally distributed and parametric. However, this is my result. I'm unsure if the missing values are important as its only using R basically and also if I reject or accept the null hypothesis. Following the correct anova would I then carry out another test? I'm trying to find variation of a variable between different species of fish.</p>

<pre><code>Df  Sum Sq   Mean Sq F value Pr(&gt;F)
fishdata$Species   2 0.00024 0.0001214   0.765  0.467
Residuals        228 0.03619 0.0001587               
4 observations deleted due to missingness 
</code></pre>
"
"0.0886194286901087","0.0936585811581694","147033","<p>I am trying to calculate standard errors of group means for a two-way-anova. I found two ways to do this (<code>predict.lm(, se = T)</code> and <code>summary.lm()</code>:</p>

<pre><code>set.seed(42234)
exmpl &lt;- data.frame(DV = rnorm(40) + rep(3:6 * 10, each = 10), #  Dependent Variable
                    IV1 = factor(rep(LETTERS[1:2], each = 20)), # Independent Variable (Treatment) 1 
                    IV2 = factor(rep(rep(LETTERS[3:4], each = 10), 2))) #  Independent Variable (Treatment) 2

exmpl.lm &lt;- lm(DV ~ IV1 + IV2, data = exmpl) #  Example data was generated without interactions

summary(exmpl.lm)
as.data.frame(predict(exmpl.lm, data.frame(IV1 = c('A', 'B', 'A', 'B'),
                                           IV2 = c('C', 'C', 'D', 'D')), se = T))
</code></pre>

<p>The standard errors of some group means differ. I managed to recalculate the standard errors given by <code>predict()</code> with the explanations from <a href=""http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf"" rel=""nofollow"">Practical Regression and Anova using R</a> from Faraway (section 3.5). I couldn't find any information about the algorithms used by the <code>summary()</code> function. Any ideas?
 What I find really confusing is that you can change the output by relabelling one factor:</p>

<pre><code>exmpl2 &lt;- exmpl
exmpl2$IV1 &lt;- factor(exmpl2$IV1, levels = LETTERS[2:1])

exmpl2.lm &lt;- lm(DV ~ IV1 + IV2, data = exmpl2)
summary(exmpl2.lm)
summary(exmpl.lm)
</code></pre>

<p>In the first example group A-C has a standard error of 0.2013. In the second example the standard error is given as 0.2324. The data of both examples is the same, only the order of the labels of a categorial (not ordinal) variable were changed. How does this influence the statistical model?</p>
"
"0.0511644510096651","0.0270369035217938","149521","<p>I have 2 fixed effects and 2 continuous predictors.  Nothing interacts so I believe I want a model of the form:</p>

<p>$$Y_{ij} = \mu + \tau_i + \beta_j + \gamma_1(X_{1ij}-\bar{X_1}) + \gamma_2(X_{1ij}-\bar{X_2}) + \epsilon_{ij} $$</p>

<p>I gather that this would be expressed be something like:</p>

<pre><code>mod = lm(y ~ facA + facB + contX1 + contX2, data = set1)
mod$coefficients
anova(mod)
</code></pre>

<p>However, I'm confused on 2 points.  The first is in my case the fitted intercept does not equal the ""grand mean"" (sample mean) of all Y responses as is typically the case in fixed-effect ANOVA.  Should this be the case?</p>

<p>If I manually center the continuous predictors the intercept <em>still</em> does not the grand mean.  This gives rise to the second point of confusion - does R automatically center the continuous responses?  I can't find info on this in the documentation.</p>
"
"0.135368413338721","0.143065845879825","151200","<p>I have been trying to figure out how to do a fairly basic repeated measures analysis using linear mixed effects in R, and then analysing it using post-hoc tests. The problem is that I'm not sure whether the output I get is statistically sound?</p>

<p>The response variable: <code>weighted</code>- an index of habitat preference (prop. individuals on habitatA / prop. of total habitat that is A). A value above 1 indicates the habitat is being used more than what you would expect from its availability. this was repeatedly  measured on the same colony through time over several weeks</p>

<p>Fixed variables: <code>Type</code> - habitat type (live/dead), <code>weeks</code> - the time variable</p>

<p>Random variables: <code>colony</code> - because each measurement of colony violates independence assumption.</p>

<p>Here's what the data loss like plotted over time (orange=live habitat, blue=dead habitat):</p>

<p><img src=""http://i.stack.imgur.com/atUVm.jpg"" alt=""enter image description here""> </p>

<p>i run the analysis using the <code>lmer()</code> function from the <code>lme4</code> package:</p>

<pre><code>results_full=lmer(weighted~type*weeks+(weeks|colony), data=Pos, REML=F)
</code></pre>

<p>My reasoning is that i have no reason to expect a random intercept, they should all start on 1 at time 0, and then individuals will start avoiding the dead habitat and favouring the live habitat. The <code>(weeks|colony)</code> term allows the slope of each colony to be random across time?</p>

<p>So to my question:</p>

<p>I compare the likelihood of two models with each other, in a likelihood ratio test to get p-values of the fixed effects using a reduced model:</p>

<pre><code>results_null=lmer(weighted~type+weeks+ (weeks|colony), data=Pos, REML=F)
anova(results_null, results_full)
</code></pre>

<p>But what I'm really interested in is at what time point (week) do the individuals start avoiding the dead habitat. as you can see from the figure this happens at week 1 so comparing live-dead habitat week by week ""should"" generate a n/s result at week 0 and sig result from then on (I'm not trying to force a statistically significant result, but the fig is pretty clear...)</p>

<p>I tried converting the weeks into a factor, and then performing </p>

<pre><code>lsmeans(results_full, pairwise~type+weeks)
</code></pre>

<p>But it didn't generate anything that seemed meaningful, the output didn't make sense in relation to the data. </p>

<p>Does anyone have any thoughts on A) whether my model and test is appropriate to this data, and B) how I can perform a post hoc test to compare habitat type over time?</p>

<p>Would it be appropriate to use a Dunetts post hoc test to compare preferences to a reference value (=1) rather than to each other?</p>

<p>Grateful for any ideas or pointers!</p>
"
"0.0723574605292422","0.0573539334676404","152474","<p>I have a continuous outcome variable and several different <code>lasso</code> models to predict the outcome. Something like</p>

<pre><code>outcome ~ explain1 + explain2 + confounders
outcome ~ explain3 + explain4 + confounders
outcome ~ explain1 + explain2 + explain3 + explain4 + confounders
</code></pre>

<p>I know that I can calculate several goodness of fit measures like <a href=""http://en.wikipedia.org/wiki/PRESS_statistic"" rel=""nofollow"" title=""PRESS"">PRESS</a> or just the <a href=""http://en.wikipedia.org/wiki/Mean_squared_error"" rel=""nofollow"" title=""mean square error"">mean square error</a>. And models with lower PRESS/MSE predict the outcome more accurately. But how can I quantify this difference, ideally with a p-value, which indicates if one model is significantly better than the other?</p>

<p>For ""normal"" regression models I could use anova, but this doesn't seem to work for lasso models (at least in <code>R</code>).</p>
"
"0.0957199230302734","0.086710996952412","152774","<p>I have read ""Design and Analysis of Experiments"" 8th Edition by D.C. Montagomery. In Chapter 14, there is a nested experiment with two factor A and B. Both factors are random. Then the ANOVA test should be performed as: </p>

<p>For A, MS_A / MS_B(A)</p>

<p>For B, MS_B(A) / MS_E</p>

<p>I want to know is it possible to perform this kind of test using classical ANOVA by <code>aov()</code> in R.</p>

<p>I try to include Error term in the formula, but for this simple question, I have to run <code>aov()</code> twice. Something like:</p>

<pre><code>&gt; summary(aov(water ~ A + Error(B), abc))

Error: B
          Df Sum Sq Mean Sq F value Pr(&gt;F)
A          2  416.8  208.39   22.68 0.0155 *
Residuals  3   27.6    9.19
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: Within
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals  6   0.31 0.05167
&gt; summary(aov(water ~ B + Error(A), abc))

Error: A
  Df Sum Sq Mean Sq
B  2  416.8   208.4

Error: Within
          Df Sum Sq Mean Sq F value   Pr(&gt;F)
B          3  27.57   9.190   177.9 2.99e-06 ***
Residuals  6   0.31   0.052
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt;
</code></pre>

<p>My question is whether it is possible to combine the above two steps into on formula in R?</p>
"
"0.102966471440162","0.120912708351669","153122","<p>As we know, if we are doing many tests or multiple comparison, we don't use the same $\alpha$ value and use some $\alpha$ correction methods like Bonferroni. This is done because when we do multiple tests, we have higher chance of getting something as significant compared to doing for fewer numbers of tests. </p>

<p>But my main question is this: </p>

<p>1) It is said if you are comparing multiple sample means using ANOVA and once you find there is some significant difference then you can do a <em>post hoc</em> analysis by doing pairwise comparison. But now you don't have to actually do a Bonferroni correction. Why is that? Isn't this <em>post hoc</em> analysis same as other pairwise t test where we use Bonferroni correction?</p>

<p>2) If Bonferroni correction is required because more tests leads to more chances of getting something significant then why we don't use the same thing, where we are doing something like regression where we are testing significance of $\beta$ estimates, or whether a variable is significant or not for feature selection using p value/F score? In that case also we are doing multiple comparison in checking whether each variable is significant or not. Then why don't we use Bonferroni correction on critical $\alpha$ there?</p>

<p>Please advise.</p>
"
"0.0511644510096651","0.0540738070435875","153944","<p>I have a dataframe ""all.st.icc"", where 27 subjects have measures of social behavior for each of 8 different years.</p>

<p>First four rows of data are as follows:</p>

<pre><code>all.st.icc

      X0607     X0708     X0809    X0910     X1011    X1112   X1213  X1314

sub1  1.99      8.1992    5.070    14.5032   10.311   13.424  4.63   5.034
sub2  6.672     5.6972    5.909    8.5209    3.405    4.166   3.792  3.25
sub3  10.820    8.9275    7.266    41.180    6.8357   7.078   3.96   4.72
sub4  11.70     9.133     7.799    8.149     3.010    4.068   2.235  4.889 
...
</code></pre>

<p>I run the <code>ICC</code> function in the R package <code>psych</code> and view the ANOVA summary table. Am I correct to interpret the output below as saying that there are significant differences in social measures between subjects (subs) AND between years (ind)? </p>

<p>This would imply that subject behavior is repeatable, but there are still significant differences in behavior by year. I have read Shrout and Fleiss 1979 but have not found an example discussing such a situation.</p>

<pre><code>ast&lt;-ICC(all.st.icc)
ast$summary
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
subs         26   1172   45.10   3.106 4.66e-06 ***
ind           7   1291  184.45  12.703 3.09e-13 ***
Residuals   182   2643   14.52                    
</code></pre>
"
"0.149168726281763","0.157650883821153","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.130778311815021","0.148087219439773","155572","<p>I have a statistical analysis / data analysis problem: </p>

<p>I am analyzing data using a factorial three-way ANOVA with a-priori contrasts and type III sums of squares.  (Please don't speak about type I SS vs. type III SS.  That's not the point of my question.)  I get the contrasts like I need using <code>summary.aov()</code>, however that uses type I SS.  When I use the <code>Anova()</code> function from <code>library(car)</code> to get type III SS, I don't get the contrasts.  Why don't I get contrasts?</p>

<p>I have also tried using <code>drop1()</code> with the <code>lm()</code> model, but I get the same results as <code>Anova()</code> (without the contrasts).  I have also tried the following, all without a resolution to my issue: <code>ezANOVA()</code> from <code>library(ez)</code>, <code>glht()</code> from <code>library(multcomp)</code> which returned the error <em>Error in glht.matrix(EpiLM, linfct = con) :   â€˜ncol(linfct)â€™ is not equal to â€˜length(coef(model))â€™</em>, and <code>C()</code>.   Why do I get this error?  How do I resolve this error?  I have searched extensively online for answers to this error message as well as this issue generally, but without finding a solution.  </p>

<p>How do I get the results of a factorial ANOVA with a-priori contrasts <strong>and</strong> type III SS as shown in my example code below?  </p>

<p>Sample data:</p>

<pre><code>DF &lt;- structure(list(Code = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L,  
3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 
9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L), .Label = c(""A"", 
""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""I"", ""J"", ""K"", ""L""), class = 
""factor""), GzrTreat = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), contrasts = structure(c(1, 
-2, 1, 1, 0, -1), .Dim = c(3L, 2L), .Dimnames = list(c(""I"", 
""N"", ""R""), NULL)), .Label = c(""I"", ""N"", ""R""), class = ""factor""), 
BugTreat = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = 
c(""Immigration"", ""Initial"", ""None""), class = ""factor""), TempTreat =   
structure(c(2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L), .Label = c(""Not Warm"", ""Warmed""), class = 
""factor""), ShadeTreat = structure(c(2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 
2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 
1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L), .Label = c(""Light"", 
""Shaded""), class = ""factor""), EpiChla = c(0.268482353, 0.423119608, 
0.579507843, 0.738839216, 0.727856863, 0.523960784, 0.405801961, 
0.335964706, 0.584441176, 0.557543137, 0.436456863, 0.563909804, 
0.432398039, 0.344956863, 0.340309804, 0.992884314, 0.938390196, 
0.663270588, 0.239833333, 0.62875098, 0.466011765, 0.536182353, 
0.340309804, 0.721172549, 0.752082353, 0.269372549, 0.198180392, 
1.298882353, 0.298354902, 0.913139216, 0.846129412, 0.922317647, 
0.727033333, 1.187662745, 0.35622549, 0.073547059), log_EpiChla = 
c(0.10328443, 0.153241402, 0.198521787, 0.240259426, 0.237507762, 
0.182973791, 0.147924145, 0.125794985, 0.19987612, 0.192440084, 
0.157292589, 0.194211702, 0.156063718, 0.128708355, 0.127205194, 
0.299482089, 0.287441205, 0.220962908, 0.093363308, 0.21185469, 
0.166137456, 0.186442772, 0.127205194, 0.235824411, 0.243554515, 
0.103589102, 0.078522208, 0.361516746, 0.113393422, 0.281746574, 
0.266262141, 0.283825153, 0.23730072, 0.339980371, 0.132331903, 
0.030821087), MeanZGrowthAFDM_g = c(0.00665, 0.003966667, 0.004466667, 
0.01705, 0.0139, 0.0129, 0.0081, 0.003833333, 0.00575, 0.011266667, 
0.0103, 0.009, 0.0052, 0.00595, 0.0105, 0.0091, 0.00905, 0.0045, 0.0031, 
0.006466667, 0.0053, 0.009766667, 0.0181, 0.00725, 0, 0.0012, 5e-04, 
0.0076, 0.00615, 0.0814, NA, 0.0038, 0.00165, 0.0046, 0, 0.0015)), 
.Names = c(""Code"", ""GzrTreat"", ""BugTreat"", ""TempTreat"", ""ShadeTreat"", 
""EpiChla"", ""log_EpiChla"", ""MeanZGrowthAFDM_g""), class = ""data.frame"", 
row.names = c(NA, -36L))
</code></pre>

<p>Code:</p>

<pre><code>## a-priori contrasts
library(stats)
contrasts(DF$GzrTreat) &lt;- cbind(c(1,-2,1), c(1,0,-1))
    round(crossprod(contrasts(DF$GzrTreat)))
c_labels &lt;- list(GzrTreat=list('presence'=1, 'immigration'=2))

## model  
library(car)
EpiLM &lt;- lm(log_EpiChla~TempTreat*GzrTreat*ShadeTreat, DF)
summary.aov(EpiLM, split=c_labels) ### MUST USE summary.aov(), to get 
#contrast results, but sadly this uses Type I SS
Anova(EpiLM, split=c_labels, type=""III"") # Uses Type III SS, but NO     
#CONTRASTS!!!!!

# I need contrast results like from summary.aov(), AND Type III SS 
# like from Anova()
</code></pre>
"
"0.0626633989716535","0.0662266178532522","157121","<p>I'm working with a dataset of legislative voting results in the brazilian Chamber of Deputies. I have a list of votes (mapped as 1 and 0 for Yes and No), like:</p>

<p><code>
| deputy_name | rollcall_1 | rollcall_2 | ... | rollcall_n |
| ----------- | ---------- | ---------- | ... | ---------- |
| John        | 0          | 1          | ... | 1          |
| Mary        | 0          | 0          | ... | 1          |
| Paul        | 1          | 1          | ... | 0          |
| ...         | ...        | ...        | ... | ...        |
</code></p>

<p>Based on this, I splitted the rollcalls in 2 arbitrary groups (before and after), and used a metric (Optimal Classification) that maps all deputies in a single axis, with similar deputies close together. I also have a column (<code>changed_position</code>) that says if a deputy entered/left the government's coalition (i.e. she was opposition and became government or vice-versa). The data looks like:</p>

<p><code>
| deputy_name | before | after | changed_position |
| ----------- | ------ | ----- | ---------------- |
| John        | 0.3    | 0.8   | TRUE             |
| Mary        | 0.7    | 0.6   | FALSE            |
| Paul        | 1      | 1     | FALSE            |
| ...         | ...    | ...   | ...              |
</code></p>

<p>I've been trying to build a model to predict <code>changed_position</code> using <code>before</code> and <code>after</code> (and a few variations, like <code>after - before</code>, etc.), but I've been unable to find a good one. My hypothesis is that it's because there's not enough information on just these variables to be able to predict well (i.e. my model has high bias).</p>

<p>It seems that ANOVA would be a good way to test how much variation in <code>changed_position</code> can be attributed to <code>before</code> and <code>after</code>, but unfortunately I need a non-parametric test.</p>

<p>Andrew Ng suggests plotting the learning curve. If I get something like:</p>

<p><img src=""http://i.stack.imgur.com/tRPyk.png"" alt=""High bias learning curve""></p>

<p>That would mean that I have a high bias problem. Unfortunately, that requires (as far as I know) that I should define what the ""desired performance"" is, which I would like to avoid.</p>

<p>How can I determine how much of the variation in <code>changed_position</code> can be explained by <code>before</code> and <code>after</code>?</p>
"
"0.0417755993144357","0.0662266178532522","157426","<p>I would like to perform an ANOVA analysis to determine which factors can explain most of the variance for a response variable</p>

<pre><code>y ~ A + B + A:B + C 
</code></pre>

<p>My data are not balanced and I am totally confused on which type of anova should I use.</p>

<p>1) If I use Type I (aov) I get different results depending on the order (as expected)</p>

<p>2) I have been reading that Type II do not include the interaction.. However, I used the Anova (car) function with the type II specification and one of the analysed factor is A:B. What then does ""not including the interaction"" mean?</p>

<p>3) I have been reading that Type III shouldn't be used but I haven't really understood why.. </p>

<p>Many thanks</p>
"
"0.0361787302646211","0.0382359556450936","157938","<p>I used the <code>anova</code> function in R to get an ANOVA table for my model.</p>

<pre><code>fit &lt;- lm(open_time ~ sent_time + email_id + day, data=mydata)
anova(fit)
</code></pre>

<p>I got the following output:</p>

<pre><code>Analysis of Variance Table

Response: open_time 
             Df    Sum Sq Mean Sq F value Pr(&gt;F)
sent_time    1    222321  222321  2.2673 0.1323
email_id     1     15229   15229  0.1553 0.6936
day          1       798     798  0.0081 0.9281
Residuals 1996 195721653   98057  
</code></pre>

<p>Can someone explain in basic terms as to how can I use this output to get useful information about my model (as in which independent variables are more significant in the prediction model, etc)?</p>
"
"0.102966471440162","0.120912708351669","159673","<p>I'm new to using R, only started in the last 2 weeks so I apologize if I'm missing something very obvious. 
I am analyzing data from a repeated measures experiment with three factors, diversity (2 levels), temperature(2 levels) and par (3 levels), with ecosystem respiration as my variables. 
So far I've been successful in running a mixed model with the code  </p>

<pre><code>lmermod.e&lt;- lmer(er ~ div:par+div:tem+tem:par + (1 | week), data=mm)
</code></pre>

<p>I now need to do a pairwise comparison, 
I've tried ...</p>

<pre><code>summary(glht(ermod.e,linfct=mcp(div=""Tukey"")))
</code></pre>

<p>but get this error message...</p>

<pre><code>Error in summary(glht(ermod.e, linfct = mcp(div = ""Tukey""))) : 
  error in evaluating the argument 'object' in selecting a method for function 'summary': Error in mcp2matrix(model, linfct = linfct) : 
  Variable(s) â€˜divâ€™ of class â€˜integerâ€™ is/are not contained as a factor in â€˜modelâ€™.
</code></pre>

<p>I am aware that wouldn't be appropriate to test the interactions I have. I have also tried the aov, with</p>

<pre><code>anovaer&lt;-aov(er ~ div:tem + tem:par + par:div + (1 | week), data=mm)
summary(anovaer)
anova(anovaer)
TukeyHSD(anovaer)
</code></pre>

<p>but I just get this </p>

<pre><code>Error in TukeyHSD.aov(anovaer) : no factors in the fitted model
In addition: Warning messages:
1: In replications(paste(""~"", xx), data = mf) :
  non-factors ignored: div, tem
2: In replications(paste(""~"", xx), data = mf) :
  non-factors ignored: tem, par
3: In replications(paste(""~"", xx), data = mf) :
  non-factors ignored: div, par
</code></pre>

<p>when I try <code>lsmeans</code></p>

<pre><code>lsmeans(ermod.i, pairwise~tem*par*div, adjust=""tukey"")
</code></pre>

<p>I get...</p>

<pre><code>Error in solve.default(L %*% V0 %*% t(L), L) : 
  Lapack routine dgesv: system is exactly singular: U[1,1] = 
</code></pre>

<p>Any help would be much appreciated</p>
"
"0.0542680953969316","0.0764719112901873","159711","<p>I'm trying to figure out why the <code>anova</code> function in R gives me the same results (for the p-value) regardless of the order of the models.</p>

<pre><code>&gt; anova(lm.fit ,lm.fit2)
Analysis of Variance Table

Model 1: medv ~ lstat
Model 2: medv ~ lstat + I(lstat^2)
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    504 19472                                 
2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt; anova(lm.fit2,lm.fit)
Analysis of Variance Table

Model 1: medv ~ lstat + I(lstat^2)
Model 2: medv ~ lstat
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    503 15347                                 
2    504 19472 -1   -4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I can't understand why the p-value is so low in both cases. The way I'm understanding I should interpret the result of the <code>anova</code> is that the model 2 is better than model 1 if the p-value is very low, but in this case I'm getting exactly the same no matter the order.</p>

<p>I'm trying to read <code>?anova</code> to check what this all means, but the help page is very succinct, is there another help where it states what the <code>Df</code> parameter means for instance?</p>
"
"0.191439846060547","0.202325659555628","159745","<p>I want to test a regression model with neuroticism as focal predictor, agreeableness as moderator and RT variability as dependent measure (covariates: attentional control and mean RT). Previously, I have used the modprobe macro in SPSS by Andrew Hayes for this (see: <a href=""http://link.springer.com/article/10.3758%2FBRM.41.3.924"" rel=""nofollow"">http://link.springer.com/article/10.3758%2FBRM.41.3.924</a>). I am in the process of transitioning to R, however, and would like to learn how to run a similar routine there. I have set up my regression model as follows:</p>

<pre><code>m3&lt;-lm(data=stp2_sub2, all_SD~Neuroticism*Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # full interaction model
m33&lt;-lm(data=stp2_sub2, all_SD~Neuroticism+Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # reduced model
</code></pre>

<p>I know that I can obtain F-change and p-change, using:</p>

<pre><code>anova(m3, m33) # provides F-change and p-change
</code></pre>

<p>What I still donâ€™t know yet is how to obtain the R squared change value, which gives me the effect size of the interaction effect. I have already posted a similar question at one of the sister websites (<a href=""http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#"">http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#</a>), although it seems that my question was slightly off-topic there. The users there have been very helpful (especially @gung), but I still have some remaining questions. Hence me posting here.</p>

<p>Basically, the recommendations have been so far to compute (a) semi-partial r-squared or (b) partial eta squared. For (a) semi-partial r-squared a custom-written function already exists (see: <a href=""http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615"">http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615</a>). Unfortunately I am a bit at a loss as to how to exactly adapt it to my own case, particularly the following bit: y = 4 + .5*x1 - .3*x2 + rnorm(10, mean=0, sd=1). Moreover, even if I did succeed at adapting y to my own needs, I would still need to know how to compute the r change value. The function for semi-partial r-squared yields a single r value (i.e. it doesnâ€™t provide a change value, yet). In my case, would I need to, in a first step, run this function for (1) the full model (here: m3) as well as for (2) the reduced model (here: m33), thereby providing me with two semi-partial r-squared values (full vs. reduced)? In a second step, would I then subtract semi-partial r-squared (reduced) from semi-partial r-squared (full), with the outcome being the r-change value that I need?</p>

<p>As for b, I have been told to use the following Â« formula Â» ( SSE(reduced)-SSE(full) ) / SSE(reduced) to compute partial eta squared for the interaction effect. I have found code for computing the sum of squared errors (see: <a href=""http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq"">http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq</a>) and have applied this to my case as follows:</p>

<pre><code>SSEfull&lt;-sum(m3$residuals^2) # sum of squared errors/residual sum of squares for the full regression model
SSEred&lt;-sum(m33$residuals^2) # sum of squared errors/residual sum of squares for the reduced regression model

pes&lt;-(SSEred-SSEfull)/SSEred # computing partial eta-squared
</code></pre>

<p>I would like to know whether I have correctly implemented the code to compute partial eta squared for the interaction effect. Moreover, given that I need the r squared change value, I was wondering how I might be able to convert the partial eta squared value to the r squared change value that I need. That said, as mentioned above, I just need to know how to compute the r squared change value for my interaction model. Ultimately, I donâ€™t mind whether I do this following suggestions (a) or (b) (but note that I still have some questions in this regard) or use a completely different way. Indeed, perhaps thereâ€™s already a function/package in R that calculates the r change value (I havenâ€™t found one yet)? I feel like being very close to finding what I need, but just need a final few pointers. Apologies for the long-winded way of writing this simple question, but I thought it could be of use to show what thought (even if not yet conclusive) has already gone into this. Any guidance on how to best go about this would be much appreciated.</p>
"
"0.0895377892669139","0.108147614087175","160087","<p>My colleagues and I conducted a study of the effects of an experimental translocation on the movement and activity patterns of common brushtail possums in New Zealand. This involved first capturing 12 individuals (6 males and 6 females), fitting them with GPS collars, and releasing each study animal within its home range. After seven days, we re-captured the animals, fitted them with new GPS collars, and then moved them to a common release site that was well outside of all of their home ranges. For both deployments the GPS collars recorded location data at 5-min intervals for an 11-h period during the night when possums are active. Our response variables were duration of nightly active periods, total distance moved per night, mean nightly speed and several other metrics that were descriptive of variation in movement behaviour. 
Our data are a bit messy (unbalanced) because we didnâ€™t get observations from all animals each night of the two 7-d sampling periods (for various reasons), but basically look like this for each animal, with multiple response variables:</p>

<ol>
<li>Up to seven nights of movement data prior to translocation;</li>
<li>Translocation event; </li>
<li>Up to seven nights of movement data after translocation.</li>
</ol>

<p>Our major research questions were:
1.  Do males and females differ in the responses to translocation?
2.  How do responses to translocation (as measured by activity, movement, etc) change with respect to the time (day) since the translocation event? 
3.  The interaction between 1 and 2 above.  </p>

<p>As far as I can tell, these data are best analysed with a mixed-effects model with REML, because of their repeated-measures nature (both before and after translocation), missing values, and combination of fixed (sex) and random factors. From what Iâ€™ve read (Zuurâ€™s book), classical repeated-measures ANOVA is inappropriate for several reasons.</p>

<p>My question is thus: what is an appropriate mixed-effects model formulation for these data in R using the â€˜lme4â€™ package? There is a before/after effect (with respect to translocation), a repeated-measures effect (seven sequential days of data for each GPS collar deployment), and again the fixed effect of sex. I am confused on how to nest the data properly to incorporate the two different levels of temporal autocorrelation (i.e., before/after translocation and then the time series of each GPS deployment). </p>

<p>Any help with what is the correct model code for analysis in R would be hugely appreciated!!</p>
"
"0.108536190793863","0.114707866935281","160985","<p>I'm fitting multi-level models in R using both <code>lme</code> from <code>nlme</code> package and <code>lmer</code> from <code>lme4</code>. It is a very simple model. </p>

<p>Using <code>lme</code>: <code>lmeModel &lt;- lme(Flourish ~ Week * condition, random = ~ Week|id, na.action = na.exclude)</code></p>

<p>Using <code>lmer</code>: <code>lmerModel &lt;- lmer(Flourish ~ Week * condition + (Week | id), na.action = na.exclude)</code></p>

<p>For each fixed effect, I get the same t-values, regardless of whether I use <code>lme</code> or <code>lmer</code>. Here's one of the fixed effects:</p>

<pre><code>summary(lmeModel) #using nlme
Fixed effects: Flourish ~ Week * condition 
       Value Std.Error  DF  t-value p-value
Week 0.00570 0.1728345 569  0.03297  0.9737

summary(lmerModel) #using lme4
Fixed effects:
       Estimate Std. Error t value
Week 0.005698   0.172834    0.03
</code></pre>

<p>Someone has suggested that I use the <code>pamer.fnc</code> function from the <code>LMERConvenienceFunctions</code> package to run an ANOVA on my <code>lmerModel</code> to get the p values. Here is a section of the output for the same fixed effect:</p>

<pre><code>pamer.fnc(lmerModel)
       Df  Sum Sq Mean Sq F value upper.p.val lower.p.val expl.dev.(%)
Week    1 49.7262 49.7262  5.0849      0.0244      0.0246       0.1043
</code></pre>

<p>Can anyone explain why the t-value of the fixed effect 0.03 (<strong>p = 0.97</strong>), which is not significant at all, but the F-value of the same fixed effect is 5.08 (<strong>p â‰ˆ 0.02</strong>), which is significant? I really appreciate any help anyone can provide. Thanks a lot!</p>
"
"0.0957199230302734","0.101162829777814","161997","<p>I need to know what type of test to use to compare species diversity between 2 years.</p>

<p>I have been analyzing a species survey data set collected in 2012 and 2014. The data include stem count (abundance), the type of invasive management treatment applied, and the soil type. I have successfully wrote a script in R to run ANOVAs to compare the means of the 1) treatment types and 2) soil types for both of the individual years. These tests have been straightforward because each of the 32 plots has been assigned a treatment type and soil type. In my current question, however, I have plot data for each year, so I am struggling with how to set up a shannon species diversity measurement (using the vegan plug-in in R), and whether or not an ANOVA is the right type of test.</p>

<p>Has anyone had any experience with running a test like this? What is the best way to set up the statistical analyses in general in R?</p>
"
"NaN","NaN","162562","<ol>
<li><p>What does the P value in the following example mean?</p>

<pre><code>library(rms)

data(pbc)
d &lt;- pbc
rm(pbc)
d$status &lt;- ifelse(d$status != 0, 1, 0)

ddist &lt;- datadist(d)
options(datadist='ddist')

fit &lt;- lrm(status ~ rcs(age, 4), data=d)
(an &lt;- anova(fit))
plot(Predict(fit), anova=an, pval=TRUE)
</code></pre>

<p><a href=""http://i.stack.imgur.com/WGfjA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WGfjA.png"" alt=""P value in the top center of the figure""></a></p></li>
<li><p>How can I interpret the following R output in the upper example (<code>an &lt;- anova(fit)</code>) in the upper example)?</p>

<pre><code>                Wald Statistics          Response: status 

 Factor     Chi-Square d.f. P     
 age        9.18       3    0.0269
 Nonlinear  2.52       2    0.2832
 TOTAL      9.18       3    0.0269
</code></pre></li>
</ol>
"
"0.189574968445683","0.187428627462958","163161","<p>I try to figure out how to describe my continuous variable. Unfortunately, I did not understand all of the statistics. I would really appreciate I you guys would help me out here. To better illustrate my problem, I wrote the following example.</p>

<pre><code>library(rms)
library(survival)

data(pbc)
d &lt;- pbc
rm(pbc, pbcseq)
d$status &lt;- ifelse(d$status != 0, 1, 0)

dd = datadist(d)
options(datadist='dd')

# linear model
f1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
p1 &lt;- Predict(f1, fun=exp)
(a1 &lt;- anova(f1))
Function(f1)
plot(p1, anova=a1, pval=TRUE, ylab=""Hazard Ratio"")

# rcs model
f2 &lt;- cph(Surv(time, status) ~  rcs(albumin, 4), data=d)
p2 &lt;- Predict(f2, fun=exp)
(a2 &lt;- anova(f2))
Function(f2)
plot(p2, anova=a2, pval=TRUE, ylab=""Hazard Ratio"")

# minimal CI width
p1$diff &lt;- p1$upper-p1$lower
    min(p1$diff) # = 0.002321521
p1[which(p1$diff==min(p1$diff)),]$albumin # = 3.494002
    describe(d$albumin) # mean = 3.497

p2$diff &lt;- p2$upper-p2$lower
    min(p2$diff) # = 0.2039817
p2[which(p2$diff==min(p2$diff)),]$albumin # = 3.502447
    describe(d$albumin) # mean = 3.497

# both models in a single figure
p &lt;- rbind(linear.model=p1, rcs.model=p2)
library(ggplot2)
df &lt;- data.frame(albumin=p$albumin, yhat=p$yhat, lower=p$lower, upper=p$upper, predictor=p$.set.)
(g &lt;- ggplot(data=df, aes(x=albumin, y=yhat, group=predictor, color=predictor)) + geom_line(size=1))
(g &lt;- g + geom_ribbon(data=df, aes(ymin=lower, ymax=upper), alpha=0.2, linetype=0))
(g &lt;- g + theme_bw())
(g &lt;- g + xlab(""Albumin""))
(g &lt;- g + ylab(""Hazard Ratio""))
(g &lt;- g + theme(axis.line = element_line(color='black', size=1)))
(g &lt;- g + theme(axis.ticks = element_line(color='black', size=1)))
(g &lt;- g + theme( plot.background = element_blank() ))
(g &lt;- g + theme( panel.grid.minor = element_blank() ))
(g &lt;- g + theme( panel.border = element_blank() ))
</code></pre>

<ol>
<li>Why shows the plot of the linear model (p1) not a straight line? </li>
<li>How can I plot the models f1 and f2 in the same figure? </li>
<li>How can I compare the models f1 and f2 to investigate which models fits the data better? ... like anova() for coxph in the survival package</li>
<li>Why is the minimal CI width near the mean of albumin more pronounce in the
linear (f1) model? </li>
<li>What does the P value in the plots mean? How do I have to interpret the output of anova(...)</li>
</ol>

<p><a href=""http://i.stack.imgur.com/kbGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kbGuv.png"" alt=""pkot of f1""></a></p>

<p><a href=""http://i.stack.imgur.com/1UVJd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1UVJd.png"" alt=""plot of f2""></a></p>

<p><a href=""http://i.stack.imgur.com/WG3ui.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WG3ui.png"" alt=""combined plot""></a>  </p>

<p><strong>Update #1</strong>
Following the answer from Harrell, I updated the code above showing how to combine spline plots of two predictors in a single figure. One last question: How can I compare the two rms models like <code>anova(m1, m2)</code> of the survival package as shown below?</p>

<pre><code>&gt; m1 &lt;- coxph(Surv(time, status) ~ albumin, data=d)
&gt; m2 &lt;- coxph(Surv(time, status) ~ pspline(albumin), data=d)
&gt; anova(m1, m2) # compare models
Analysis of Deviance Table
 Cox model: response is  Surv(time, status)
 Model 1: ~ albumin
 Model 2: ~ pspline(albumin)
   loglik  Chisq Df P(&gt;|Chi|)
1 -975.61                    
2 -973.26 4.6983 11    0.9449
&gt; summary(m1)
Call:
coxph(formula = Surv(time, status) ~ albumin, data = d)

  n= 418, number of events= 186 

           coef exp(coef) se(coef)      z Pr(&gt;|z|)    
albumin -1.4695    0.2300   0.1714 -8.574   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

        exp(coef) exp(-coef) lower .95 upper .95
albumin      0.23      4.347    0.1644    0.3219

Concordance= 0.688  (se = 0.023 )
Rsquare= 0.147   (max possible= 0.992 )
Likelihood ratio test= 66.6  on 1 df,   p=3.331e-16
Wald test            = 73.51  on 1 df,   p=0
Score (logrank) test = 72.38  on 1 df,   p=0
</code></pre>

<p><strong>UPDATE #2</strong>
I think I just answered my ""one last question"" by myself (see below). I hope this does not show correct accidentally. I would think that I can compare models from <code>cph</code> and <code>coxph</code> that way, can't I? Is the way of calculating the degrees of freedom <code>df</code> correct?</p>

<pre><code>&gt; # using coxph from survival
&gt; m1 &lt;- coxph(Surv(time, status) ~  albumin, data=d)
&gt; m2 &lt;- coxph(Surv(time, status) ~  albumin + age, data=d)
&gt; # loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
&gt; m1$loglik[2]
    [1] -975.6126
    &gt; m2$loglik[2]
[1] -973.2272
&gt; (df &lt;- abs(length(m1$coefficients) - length(m2$coefficients)))
[1] 1
&gt; (LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2]))
[1] 4.770787
&gt; pchisq(LR, df, lower=FALSE)
[1] 0.02894659
&gt; anova(m2, m1)
Analysis of Deviance Table
 Cox model: response is  Surv(time, status)
 Model 1: ~ albumin + age
 Model 2: ~ albumin
   loglik  Chisq Df P(&gt;|Chi|)  
1 -973.23                      
2 -975.61 4.7708  1   0.02895 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; m1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
&gt; m2 &lt;- cph(Surv(time, status) ~  albumin + age, data=d)
&gt; # loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
&gt; m1$loglik[2]
    [1] -975.6126
    &gt; m2$loglik[2]
[1] -973.2272
&gt; (df &lt;- abs(length(m1$coefficients) - length(m2$coefficients)))
[1] 1
&gt; (LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2]))
[1] 4.770787
&gt; pchisq(LR, df, lower=FALSE)
[1] 0.02894659
</code></pre>

<p><strong>UPDATE #3</strong>
I changed the example following the kind answer from DWin as follows. This way the degrees of freedom should be calculated properly:</p>

<pre><code>library(Hmisc)
library(rms)
library(ggplot2)
library(gridExtra)

data(pbc)
d &lt;- pbc
rm(pbc, pbcseq)
d$status &lt;- ifelse(d$status != 0, 1, 0)

### log likelihood test using a coxph model
m1 &lt;- coxph(Surv(time, status) ~  albumin, data=d)
m2 &lt;- coxph(Surv(time, status) ~  albumin + age, data=d)
# loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
m1$loglik[2]
    m2$loglik[2]
(df &lt;- abs(sum(anova(m1)$Df, na.rm=TRUE) - sum(anova(m2)$Df, na.rm=TRUE)))
(LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2])) # the most parsimonious models have to be first
pchisq(LR, df, lower=FALSE)
anova(m2, m1)

### log likelihood test using a cph model
dd = datadist(d)
options(datadist='dd')
m3 &lt;- cph(Surv(time, status) ~  albumin, data=d)
m4 &lt;- cph(Surv(time, status) ~  albumin + age, data=d)
# loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
m3$loglik[2]
    m4$loglik[2]
(df &lt;- abs(print(anova(m3)[, ""d.f.""])[['TOTAL']] - print(anova(m4)[, ""d.f.""])[['TOTAL']]))
(LR &lt;- 2 * (m4$loglik[2] - m3$loglik[2])) # the most parsimonious models have to be first
pchisq(LR, df, lower=FALSE)
</code></pre>
"
"0.0511644510096651","0.0540738070435875","164314","<p>I have a data set with performance and training data that looks something like (this is not the exact data, but gives a general idea):</p>

<pre><code>&gt;dat
Performance Training
1           1
0           1
1           2
0           2
1           3
1           3
</code></pre>

<p>I want to find if there is are any significant differences between performance means for the respective levels of performance in R.  I have tried linear regression and anova, such as: <code>summary(lm(performance~training))</code> or <code>summary(aov(performance~training))</code> both of which yield non-significant results.  However, when I do a T-test to compare some of the means manually it is telling be significant differences exist.  Any thoughts on how to code what I am looking for or what might be going on here?</p>
"
"0.0723574605292422","0.0764719112901873","165073","<p>Testing for the interaction of Gender on Type (grouping variable, group A &amp; B) for a 'Test' score -</p>

<pre><code>model.lm &lt;- lm(formula= Test~ Type + Gender + Type*Gender,
           data=describeDF3,na.action=na.omit)
Anova(model.lm,type='II',white.adjust='hc3')
</code></pre>

<p>White adjust as unequal variance.</p>

<p>Results came up as Type significant and Gender significant but no interaction. I used both a oneway.test ANOVA and t-test to check the Gender effect but in these tests it is coming up as not significant. Why is this?</p>

<p>Is it because the two way test shows whether the variable creates a reduction in the deviance of the lm and the oneway tests look at differences in means?</p>

<p>Any help understanding this would be great!</p>

<p>Rachel</p>
"
"0.0511644510096651","0.0540738070435875","167109","<p>I'm new to R and trying to run some two-way ANOVAs to test treatment effects in an ecology study.
I've just tested my data using the Levene function:</p>

<pre><code>&gt; leveneTest(ant.richness~Treatment, data= PFants)
Levene's Test for Homogeneity of Variance (center = median)
       Df F value   Pr(&gt;F)   
group   1  8.6603 0.003917 **
      118                    
---
</code></pre>

<p>And then went on the Log transform the data because it fails the assumptions of ANOVA:</p>

<blockquote>
  <p>antlog&lt;- log10(ant.richness)</p>
</blockquote>

<p>However I am getting a lot of
 -Inf
in the resulting dataset, which means I can't run an ANOVA....</p>

<p>Any ideas? (Probably a dumb question! I'm not very good with stats/ R)
Thanks very much!!!</p>
"
"0.119991273679223","0.126814318375447","167946","<p><a href=""http://i.stack.imgur.com/p1woC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1woC.png"" alt=""residue normality and data boxplots""></a>  </p>

<p>I have a data set from 7 groups, with 20 fish in each group. Measurement of a parameter is made on 25 cells from each fish (so each observation in the data-set is completely independent, right?). One of the groups functions as the control group while other 6 are treatment groups. So we have a total of 25*20*7 measurements. This is how the data looks like (a boxplot of all 7 groups is attached):</p>

<pre><code>samples subjects groups response
    1        1      1     4.85
    2        1      1     3.77 ..
    25       1      1     4.71
    26       2      1     4.51 ..
    500      20     1     4.21
    501      1      2     4.11 ..
    3500     20     7     4.19
</code></pre>

<p>I wish to run an ANOVA and the expectation is that a couple of groups should differ from the control group in regards to the parameter under observation. Here are a few questions:</p>

<ol>
<li><p>Is the following R code appropriate? (It shows there is no significant difference between groups.)</p>

<pre><code>n = 20
k = 25
g = 7
subjects = gl(n,   k, n*k*g)
groups   = gl(g, n*k, n*k*g)

study1 = data.frame(c(1:(n*k*g)), subjects, groups, r11)
colnames(study1) = c(""samples"", ""subjects"", ""groups"", ""response"")

fit = lm(response~groups + samples*subjects, data=study1) # or aov?
anova(fit)

Analysis of Variance Table

Response: response
                   Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
groups              6  846.8 141.134 122.2864 &lt; 2.2e-16 ***
samples             1   13.1  13.055  11.3114 0.0007787 ***
subjects           19  119.5   6.289   5.4493 2.078e-13 ***
samples:subjects   19  149.6   7.872   6.8206 &lt; 2.2e-16 ***
Residuals        3454 3986.4   1.154                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre></li>
<li><p>The attached qqplot shows normal residues, however the Shapiro-Wilk test always fails on all groups. (Is my sample size of 3500 too big and problematic?)</p>

<pre><code>shapiro.test(study1$response[study1$groups==1])

data:  study1$response[study1$groups == 1] W = 0.9818, p-value =
6.648e-06
</code></pre>

<p>And so does the Levene for equality of variance:</p>

<pre><code>leveneTest(lm(response ~ groups, data=study1))
Levene's Test for Homogeneity of Variance (center = median)
        Df F value    Pr(&gt;F)    
group    6   19.37 &lt; 2.2e-16 ***
      3493    
</code></pre></li>
</ol>

<p>Please guide me as to how should I proceed. Should I keep on using ANOVA and disregard the fact that the normality and equality of variance assumptions are being violated? Should I remove the outliers from my data? Should I transform data somehow to be 'more' normal? Should I switch to non parametric or rank based tests? The end goal is to identify groups that differ significantly from the control group. </p>
"
"0.108536190793863","0.114707866935281","169543","<p>I'm doing a two-factor ANOVA using the <code>lmerTest</code> package. Each factor has multiple levels. When one (or more) of the effects are significant, I would like to do a post-hoc test to determine which of the levels differ from each other. Here, I set up the model as:</p>

<pre><code>library('lmerTest')
model = lmer('measure~factor*experiment+(1|subject_id)', data=data)
print(anova(model))
</code></pre>

<p>The output appears as follows:</p>

<pre><code>Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
                  Df  Sum Sq Mean Sq F value  Denom    Pr(&gt;F)    
factor             3 2388.82  796.27 16.3140  9.999 0.0003527 ***
experiment         3  254.11   84.70  2.7689 30.000 0.0588323 .  
factor:experiment  9 1301.40  144.60  2.9626 30.000 0.0121071 *  
</code></pre>

<p>At this point, I can inspect the factors and see which factors are significant. However, when I look at the summary of the model:</p>

<pre><code>summary(model)
</code></pre>

<p>I get a much more detailed output (truncated to the relevant portion for clarity):</p>

<pre><code>                                t value Pr(&gt;|t|)    
(Intercept)                      -5.600 8.99e-06 ***
factorLevel1                      0.289  0.77522    
factorLevel2                      2.855  0.00871 ** 
factorLevel3                     -6.535 9.00e-07 ***
experimentSession1               -0.747  0.46086    
experimentSession2               -0.825  0.41596    
experimentSession3                0.317  0.75354    
factorLevel1:experimentSession1  -1.297  0.20454    
factorLevel2:experimentSession1  -0.903  0.37376    
factorLevel3:experimentSession1   3.025  0.00506 ** 
factorLevel1:experimentSession2   0.591  0.55917    
factorLevel2:experimentSession2  -0.777  0.44341    
factorLevel3:experimentSession2   3.027  0.00504 ** 
factorLevel1:experimentSession3  -0.123  0.90269    
factorLevel2:experimentSession3  -1.060  0.29770    
</code></pre>

<p>How do I interpret these values? Is this telling me that coefficient for <code>factorLevel2</code> is significantly different from 0? If I then do a Multiple Comparison of Means:</p>

<pre><code>print(summary(glht(m, linfct=mcp(experiment=""Tukey"", factor=""Tukey""))))
</code></pre>

<p>I get the following output:</p>

<pre><code>experiment: Session1 - Session0 == 0   -0.747   0.9829    
experiment: Session2 - Session0 == 0   -0.825   0.9718    
experiment: Session3 - Session0 == 0    0.317   0.9999    
experiment: Session2 - Session1 == 0   -0.078   1.0000    
experiment: Session3 - Session1 == 0    1.064   0.9079    
experiment: Session3 - Session2 == 0    1.142   0.8759    
factor: Level2 - Level1 == 0            0.289   0.9999    
factor: Level3 - Level1 == 0            2.855   0.0425 *  
factor: Level4 - Level1 == 0           -6.535   &lt;0.001 ***
factor: Level3 - Level2 == 0            1.517   0.6542    
factor: Level4 - Level2 == 0           -5.395   &lt;0.001 ***
factor: Level4 - Level3 == 0           -8.341   &lt;0.001 ***
</code></pre>

<p>This is more understandable as it is telling me which pairwise factors are significantly different. But, I'm unsure how to interpret the summary table produced by <code>summary()</code> and how the numbers compare to the table produced by <code>glht()</code>.</p>
"
"0.140119619801808","0.148087219439773","169671","<p>I would like to compare the means of three groups using the Kruskal Wallis test in R. </p>

<p>My data looks like this: </p>

<pre><code>bosaov
    category count_genus proportion sampleID combination
1        Bos        1081     0.0892    C1_01          C1
9        Bos        7277     0.6637    C1_02          C1
17       Bos        4815     0.5736    C1_03          C1
.
.
.
161      Bos         914     0.0824    C2_01          C2
169      Bos        6935     0.6584    C2_02          C2
177      Bos        7703     0.6582    C2_03          C2
.
.
.
321      Bos           0     0.0000    C3_01          C3
329      Bos           0     0.0000    C3_02          C3
337      Bos           0     0.0000    C3_05          C3
.
.
.
</code></pre>

<p>I would like to compare the means of proportion (continous variable) for the three levels (C1, C2, C3) of combination. </p>

<p>As my data is not normally distributed I decided to use a Kruskal Wallis Test instead of one way ANOVA. </p>

<p>I use the R package PMCMR and the following lines of code:</p>

<pre><code>kruskal.test(proportion ~ combination, data = Bosaov) 

    Kruskal-Wallis rank sum test

data:  proportion by combination
Kruskal-Wallis chi-squared = 32.958, df = 2, p-value = 6.97e-08
</code></pre>

<p>And for the post-hoc Test: </p>

<pre><code>posthoc.kruskal.nemenyi.test(x=Bosaov$proportion, g=Bosaov$combination, method=""Tukey"")

    Pairwise comparisons using Tukey and Kramer (Nemenyi) test  
                   with Tukey-Dist approximation for independent samples 

data:  Bosaov$proportion and Bosaov$combination 

   C1      C2     
C2 0.6     -      
C3 3.3e-07 3.8e-05

P value adjustment method: none 
Warning message:
In posthoc.kruskal.nemenyi.test.default(x = Bosaov$proportion, g = Bosaov$combination,  :
  Ties are present, p-values are not corrected.
</code></pre>

<p>I get the warning that ties are present, because in my data some values occur more than once (f.e. 0).</p>

<p>I tried to solve this problem by adjustig ties using </p>

<pre><code>Bosaov$proportion &lt;- rank(Bosaov$proportion, ties.method=""random"")
</code></pre>

<p>I donÂ´t get the warning anymore but the problem with randomly adjusted ties is that I logically get not reproducible results. So I decided to use ties.method=""average"" instead:</p>

<pre><code>Bosaov$proportion &lt;- rank(Bosaov$proportion, ties.method=""average"")
</code></pre>

<p>But if I use ties.method=""average"" I still get the warning regarding persent ties.</p>

<pre><code>posthoc.kruskal.nemenyi.test(x=Bosaov$proportion, g=Bosaov$combination, method=""Tukey"").

    Pairwise comparisons using Tukey and Kramer (Nemenyi) test  
                   with Tukey-Dist approximation for independent samples 

data:  Bosaov$proportion and Bosaov$combination 

   C1      C2     
C2 0.6     -      
C3 3.3e-07 3.8e-05

P value adjustment method: none 
Warning message:
In posthoc.kruskal.nemenyi.test.default(x = Bosaov$proportion, g = Bosaov$combination,  :
  Ties are present, p-values are not corrected.
</code></pre>

<p>This is how my adjusted ties look like</p>

<pre><code>rank(Bosaov$proportion, ties.method=""average"")
 [1] 31.0 50.0 45.0 56.0 47.0 44.0 28.0 34.0 43.0 38.5 52.0 22.0 46.0 38.5 54.0 58.0 10.5 55.0 25.0 29.0 30.0 49.0 48.0 57.0 40.0
[26] 51.0 24.0 32.0 35.0 33.0 53.0 21.0 26.0 36.0 41.0 37.0 42.0 10.5 10.5 27.0 10.5 10.5 10.5 10.5 10.5 10.5 10.5 10.5 10.5 10.5
[51] 10.5 10.5 10.5 23.0 10.5 10.5 10.5 10.5
</code></pre>

<p>I really donÂ´t know what causes the warning message. IÂ´m happy if anybody can help and hope I was clear about the problem. Sorry! That was my first post in this forum which is great I think! </p>
"
"0.109082976072021","0.126814318375447","172782","<p>Newbie question using R's mtcars dataset with anova() function. My question is how to use anova() to select the best (nested) model. Here's some example data:</p>

<pre><code>&gt; anova(lm(mpg~disp,mtcars),lm(mpg~disp+wt,mtcars),lm(mpg~disp+wt+am,mtcars))
Analysis of Variance Table

Model 1: mpg ~ disp
Model 2: mpg ~ disp + wt
Model 3: mpg ~ disp + wt + am
  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   
1     30 317.16                                
2     29 246.68  1    70.476 8.0036 0.008535 **
3     28 246.56  1     0.126 0.0143 0.905548   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; anova(lm(mpg~disp,mtcars),lm(mpg~disp+wt,mtcars),lm(mpg~disp+wt+hp,mtcars))
Analysis of Variance Table

Model 1: mpg ~ disp
Model 2: mpg ~ disp + wt
Model 3: mpg ~ disp + wt + hp
  Res.Df    RSS Df Sum of Sq       F   Pr(&gt;F)   
1     30 317.16                                 
2     29 246.68  1    70.476 10.1201 0.003571 **
3     28 194.99  1    51.692  7.4228 0.010971 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My understanding is anova() compares the reduction in the residual sum of squares to report a corresponding p-value for each nested model, where lower p-values means that nested model is more significantly different from the first model. </p>

<p>Question 1: Why is it that changing the 3rd regressor variable effects results from the 2nd nest model? That is, the p-value for <code>disp+wt</code> model changes from 0.008535 to 0.003571 going from the first to the second example. (does anova's model 2 analysis use data from model 3???)</p>

<p>Question 2: Since the 3rd model's <code>Sum of Sq</code> value is much lower in the first example (e.g. 0.126 versus 51.692), I'd expect the p-value to be lower as well, but it in fact increases (e.g. 0.905548 versus 0.010971). Why?</p>

<p>Question 3: Ultimately I'm trying to understand, given a dataset with a lot of regressors, how to use anova() to find the best model. Any general rules of thumb are appreciated. </p>
"
"0.130778311815021","0.138214738143788","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.102966471440162","0.108821437516502","173026","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>

<p><strong>EDIT</strong> The result of the features reversed as commented by @Michael M:</p>

<pre><code>&gt; model_All2 &lt;- lm(y ~ x2 + x1, data=df)
&gt; anova(model_All2)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x2         1 17.468  17.468  22.907 0.0001718 ***
x1         1 53.612  53.612  70.304 1.914e-07 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0895377892669139","0.0946291623262781","173047","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>
"
"0.0808981002113217","0.0854981960070962","173207","<p>I have a dataset gpa2 ddata that can be found here <a href=""https://www.dropbox.com/s/7rphi1k9pxert1a/ddata.csv?dl=1"" rel=""nofollow"">https://www.dropbox.com/s/7rphi1k9pxert1a/ddata.csv?dl=1</a></p>

<p>I estimate the model colgpa = athelte.</p>

<pre><code>gpa2 &lt;- read.csv(~""/path/ddata.csv"")
model1 &lt;- lm(formula = colgpa ~ athlete, data = gpa2)
summary(model1)
</code></pre>

<p>And now I want to see if I can get <strong>Std.Error by this formula</strong>
$$se(\beta_j) = \frac{\hat \sigma_u}{SST_j(1âˆ’R_j^2)}$$
where $$SST_j = \sum_{i=1}^n (x_{ij} - \bar x_j)^2$$ is the total sample variation in $x_j$
and $R_j^2$ is the $R^2$ from regressing $x_j$ on all the other independent variables.</p>

<p>From this answer 
<a href=""http://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression"">How are the standard errors of coefficients calculated in a regression?</a>
we know that the standard error of the estimated slope, $se(\beta_1)$ in our case, is
$$\sqrt{\widehat{\textrm{Var}}(\hat{b})} = \sqrt{[\hat{\sigma}^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1}]_{22}} = \sqrt{\frac{n \hat{\sigma}^2}{n\sum x_i^2 - (\sum x_i)^2}}.$$</p>

<p>I do this with an anova-table, and try to </p>

<pre><code>anova(model1) # the anova table
# now I take out elements of the anova
model1_hatsigmau &lt;- anova(model1)[[3]][2] #takes row 3 column 2 in the anova-table.
model1_MSathlete &lt;- anova(model1)[[3]][1]
model1_SSathlete &lt;- anova(model1)[[2]][1]
numerator &lt;- n*model1_hatsigmau
meanathlete &lt;- mean(gpa2$athlete)
denominator &lt;- n*model1_SSathlete - (n*meanathlete)^2 
sqrt(numerator /  denominator) # should be se(beta_1) for model1. 
</code></pre>

<p>But I get </p>

<pre><code>&gt; sqrt(numerator /  denominator) # should be se(beta_1) for model1.
[1] 0.270643
</code></pre>

<p>And not $0.04824$ as in the summary-output (see below). So the problem is that $$0.270643 \neq 0.04824$$</p>

<pre><code>&gt; summary(model1)

Call:
lm(formula = colgpa ~ athlete, data = gpa2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.66603 -0.43603  0.00397  0.46397  1.61851 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.66603    0.01045 255.212  &lt; 2e-16 ***
athlete     -0.28453    0.04824  -5.898 3.97e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.656 on 4135 degrees of freedom
Multiple R-squared:  0.008343,  Adjusted R-squared:  0.008104 
F-statistic: 34.79 on 1 and 4135 DF,  p-value: 3.966e-09
</code></pre>
"
"0.0723574605292422","0.0764719112901873","173500","<p>I have two variables that I have observed across different units of observation. The only way I can relate the two variables is across three categories.  I've taken the mean of each variable for each category, and then run an OLS with n=3.  However, the has to be another method that takes into account variance and is more powerful.</p>

<p>It may help to put it into specifics: I have tree diversity data at the plot level across three villages, with about 15 plots for each village.  I also have about 75 observations of childhood malnutrition for each village.  I would like to see if there is a relationship between tree diversity and malnutrition. Both variables are continuous. However, the only was to relate the tree diversity variable with the malnutrition variable is at the village level.</p>

<p>I've looked at different ANOVA/ANCOVA/MANOVA but nothing seems to be specific to this situation. I'd be grateful for anyone's suggestions for what to do here. I'm using R.</p>
"
"0.135670238492329","0.152943822580375","173976","<p>I need to understand why, in this simple simulation, does repeated measures anova's false discovery rate is kept at 5% when that of mixed model is about 10 % ?</p>

<p>I simulated 1000 sampling and measurement of 10 individuals from a given infinite population with true mean = 0 and true standard deviation = 1.</p>

<p>Each individual was measured 10 times with error following N(0, 0.1).</p>

<p>the first five individuals were assigned to group ""A"", and the 5 others to ""B"". This grouping is completely artificial so that, in the long run, no more that 5 % of the experiments should show ""significant"" group effect at alpha = 5%...</p>

<p>I test for group effect using repeated measures anova AND mixed model. </p>

<p>repeated measures anova syntax used :
     summary(aov(myvar ~ group + Error(indiv), SIMULATED_DATASET))</p>

<p>While using lmer from R's lme4 package :
     FULL_model &lt;- lmer(
            myvar # Response
            ~ 1 + group +  (1 | indiv)
            , data=SIMULATED_DATASET
            ,REML = FALSE
     )</p>

<pre><code> NULL_model &lt;- lmer(
        myvar # Response
        ~ 1 +          (1 | indiv)
        , data=SIMULATED_DATASET
        ,REML = FALSE
 )
</code></pre>

<p>lmer test (method 1) using Anova() from car package:
     Anova(FULL_model) </p>

<p>lmer test (method 2) using anova():
     anova(NULL_model, FULL_model) </p>

<p>The first problem is that NONE of the above mentioned method for lmer test of group effect keeps FDR at 5% (closer to 10%), while repeated meaures anova keeps FDR at 5%.</p>

<p>The second problem is : why does each method for test with lmer gives different results ?</p>

<p>I provide complete code for reproducing these results :</p>

<p><a href=""http://i.stack.imgur.com/2uxbF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2uxbF.png"" alt=""simulated p-value under null hypothesis for (left to right) repeated measures anova, Anova(lmer) and anova(null,full)""></a></p>

<pre><code>require(lme4)
require(car)

pp1 &lt;- c()
pp2 &lt;- c()
pp3 &lt;- c()
for(i in 1:1000){

    print(i)

    gg &lt;- data.frame(
         group=as.factor(rep(c(""A"",""B""), each=500))
        ,indiv=as.factor(rep(letters[1:10], each=100))
        ,myvar=rep(rnorm(10,0,1),each=100) + rnorm(1000,0,0.1)
    )

    lmer_fit &lt;- lmer(
        myvar # Response
        ~ 1 + group +  (1 | indiv)
        , data=gg
        ,REML = FALSE   # FALSE nÃ©cessaire quand on veut comparer deux modÃ¨les mixtes
    )

    lmer_fit_null &lt;- lmer(
        myvar # Response
        ~ 1 +  (1 | indiv)
        , data=gg
        ,REML = FALSE   # FALSE nÃ©cessaire quand on veut comparer deux modÃ¨les mixtes
    )



    pp1 &lt;- c(pp1,unlist(summary(aov(myvar ~ group + Error(indiv), gg))[[1]])[[9]])

    pp2 &lt;- c(pp2,Anova(lmer_fit)[[3]])

    pp3 &lt;- c(pp3,anova(lmer_fit_null,lmer_fit)$'Pr(&gt;Chisq)'[2])

}


par(mfrow=c(1,3))

# repeated measures anova rate of false positive under H0
hist(pp1, main=""aov(myvar ~ group + Error(indiv), gg)"")
100*length(which(pp1&lt;0.05))/length(pp1  )

# Anova(lmer_fit) rate of false positive under H0
hist(pp2, main=""Anova(lmer_fit)"")
100*length(which(pp2&lt;0.05))/length(pp2  )

# anova(null_lmer,full_lmer) rate of false positive under H0
hist(pp3, main=""anova(lmer_fit_null,lmer_fit)"")
100*length(which(pp3&lt;0.05))/length(pp3  )
</code></pre>
"
"0.0626633989716535","0.0662266178532522","174523","<p>My question is quite straightforward, but I did not find a clear answer anywhere.</p>

<p>I'm computing the Standard Error of the Estimate (SEE) by doing the square root of the Residuals Mean Square output of the anova table:</p>

<pre><code>anovatable&lt;-anova(lm(carb~hp,data=mtcars))

anovatable

Analysis of Variance Table

Response: carb

          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
hp         1 45.469  45.469  38.527 7.828e-07 ***
Residuals 30 35.406   1.180                      

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEE&lt;-sqrt(anovatable$`Mean Sq`[2])
SEE
[1] 1.086363    
</code></pre>

<p>Is it the correct way of doing it?</p>

<p>Is there any already implemented way in R to obtain the SEE? If so, It will be better than accessing the <code>Mean Sq</code> term for Residuals, since its position depends upon the number of predictors.</p>
"
"0.108536190793863","0.114707866935281","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.125699240957384","0.132846856888409","175581","<h2>In short and in general terms:</h2>

<p>While LMMs are often used in longitudinal studies with multiple repeated measures, I never came across an example which models a paired design. However, I also never read one should not use a LMM in case of a paired design. Thus, I am wondering if a LMM might make sense in case of paired design.</p>

<h2>My specific case:</h2>

<p>I performed an experiment with a counterbalanced design. Thus 2 groups with 10 subjects each and two measurements: one with and one without treatment in reverse order. I am interested in the effect of the treatment on two dependent variables. Besides the independent variable of the treatment, I am also analyzing extraneous variables and their effect on the two dependent variables.</p>

<p>For both models the LMM formula would be as follows:</p>

<pre><code>Vd ~ Vtreat + Vextra1 + ... + Vextrak + (1 | subject) 
</code></pre>

<p>When comparing LM and LMM for the <strong>first</strong> dependent variable I get: </p>

<ul>
<li><code>exactRLRT(LMM)</code> does not support the use of a random effect (<code>p-value = 0.5016</code>) </li>
<li>AICc smaller for LM (<code>delta = 2.5</code>)</li>
<li><code>anova.lme(LMM, LM)</code> gives <code>p-value = 0.4596</code></li>
</ul>

<p>And for the models of the <strong>second</strong> dependent variable: </p>

<ul>
<li><code>exactRLRT(LMM)</code> supports the use of a random effect (<code>p-value = 5e-04</code>) </li>
<li>AICc smaller for LMM (<code>delta = 8.2</code>)</li>
<li><code>anova.lme(LMM, LM)</code> gives (<code>p-value = 0.0013</code>)</li>
</ul>

<p>Thus, I would use a LM for dependent variable 1 and a LMM for the dependent variable 2. Would that be an appropriate solution or am I missing anything fundamentally? </p>

<h2>Another specific concern:</h2>

<p>Can I compare LM and LMM by means of AICc (LMM fitted via <code>ML</code>)? Is it also a valid comparison given a different fixed effect structure? Such as:</p>

<pre><code>LM:  Vd ~ Vtreat + Vextra1 + (1 | subject)
LMM: Vd ~ Vtreat   
</code></pre>
"
"0.125326797943307","0.121415466064296","175773","<p>I have a hypothesis testing model in which I would like to know if environmental variables I collected influences the probability a bacteria on a given amphibian kills a pathogen. </p>

<p>Following Crawley's R intro to stats book on binomial models I do this:</p>

<pre><code>y &lt;- cbind(df$Antipathogen, df$Total_isolated - df$Antipathogen)
</code></pre>

<p>I still follow Crawley, but bring in a glmer model (using help from online) as I want to examine this at the Site level and not transect level. So, I make transect a random effect. I sampled three sites. One site has one transect, the second site has two transects, and the third site was sampled along an altitudinal gradient and has seven transects.</p>

<p>This is my model:</p>

<pre><code>model &lt;- glmer(y ~ Site + Species + sex + BodyCon + Leaf_litter + (1|Transect), 
               data = df, family = binomial)
</code></pre>

<p>I use the Anova function in car to see which terms are significant when they are introduced into the model</p>

<pre><code>Anova(model, type = ""III"", test.statistic = ""Chisq"")
</code></pre>

<p>I get this:</p>

<pre><code>Response: y
              Chisq Df Pr(&gt;Chisq)    
(Intercept) 21.3200  1  3.887e-06 ***
Site        12.0107  2   0.002466 **
Species      0.0617  2   0.969644  
sex          0.2313  2   0.890785    
BodyCon      0.7058  1   0.400851    
Leaf_litter  2.8763  1   0.089890 . 
</code></pre>

<p>I am starting to understand the function lsmeans in lsmeans package to look at pairwise comparisons to figure out how each of my sites differ from one another. </p>

<p>This is where my question comes in:</p>

<p>What is the appropriate approach -- use the full model and apply the lsmeans function to the full model </p>

<pre><code>lsmeans(model, pairwise~Site, adjust = ""tukey"")
</code></pre>

<p>OR -- remove non-significant terms in a step-wise manner following Crawleyâ€™s (2007, pg. 325) Principle of Parsimony to simplify the full model. And use </p>

<pre><code>model2 &lt;- update(model, ~.-Species)
anova(model, model2, test = ""Chisq"")
</code></pre>

<p>To make sure removing the terms is valid.</p>

<p>Then, I can do lsmeans on the final model that now only contains the only significant variable -- site. </p>

<pre><code>lsmeans(model5, pairwise~Site, adjust = ""tukey"")
</code></pre>

<p>Regardless of how I do it I still get a significant p-value for Site, and the pairwise comparisons give similar results. The p-value is only much lower when I do lsmeans on the reduced model. </p>

<p>I don't know what the better approach is, and doing some basic searching I could not find a similar question. </p>
"
"0.177380818842608","0.157472444733047","175975","<p>Say, I wanted to compare the effect of $t=3$ (8 hours, 12 hours, 16 hours) lengths of exposures to sun on plant growth. I randomly applied these 3 lengths of exposures to $r=3$ pots but let us say that it is too costly or tedious to measure plant growth for all plants in the entire pot, so I randomly selected $s=4$ plants within each pot for my measurement. The variables would be: treatment (a factor of 3 different hours of exposure to sunlight at which each measurement of growth is taken), pot (a factor of three per treatment), plant (a factor of four plants per pot), growth (dependent variable). The following are the null hypotheses to be tested in order (according to our manual): (1) The mean plant growth within pots are the same. (2) There are no differences in mean plant growth among the different treatments.</p>

<p>The linear model for the experiment is</p>

<p>$$Y_{ijk}=\mu+\tau_i+\delta_{ij}+\varepsilon_{ijk}$$</p>

<p>where $Y_{ijk}$ is the $k$th response on the $j$th pot applied with the $i$th treatment, $\tau_i$ is the effect of the length of exposure to sunlight on plant growth, $\delta_{ij}$ is the error associated to the $j$th pot in the $i$th treatment on the growth of the plant, and $\varepsilon_{ijk}$ is the error attributed to the $k$th plant on the $j$th pot applied with treatment $i$.</p>

<p>In the corresponding ANOVA table, I have $SSTot=SSTrt+SSPE+SSSSE$, where SSTot is Total Sum of Squares, SSTrt is Treatment Sum of Square, SSPE is Sum of Squares for the Pots, and SSSSE is the sum of squares for the subsamples,  with degrees of freedom, $t-1$, $t(r-1)$, $tr(s-1)$, respectively, where $t=$ number of treatments, $r=$ number of experimental units and $s=$ number of sampling units,</p>

<p>Our school manual suggests a sequential tests of hypotheses. For tests of variability of the experimental units, it says to test</p>

<p>$$ \frac{MSPE}{MSSSE}\sim F_{(\alpha,t(r-1),tr(s-1))}$$</p>

<p>Then it suggests to consider the following cases for the test of differences among treatment means</p>

<ul>
<li>When $H_0:\sigma^2_\varepsilon=0$ is rejected</li>
</ul>

<p>$$\frac{MSTrt}{MSPE}\sim F_{(t-1,t(r-1))}$$</p>

<ul>
<li>When $H_0: \sigma^2_{\varepsilon}=0$ is accepted</li>
</ul>

<p>$$\frac{MSTrt}{MSE_{pooled}}\sim F_{(t-1,tr(s-1))}$$</p>

<p>where </p>

<p>$$MSE_{pooled}=\frac{SSPE+SSSSE}{t(rs-1)}$$.</p>

<p>Here, MS stands for mean squares for the corresponding sum of squares previously defined.</p>

<p><strong>Question: How do I perform this sequential tests of hypotheses in R</strong>?</p>

<pre><code>model &lt;- lm(response ~ trt/pot, data)
anova(model)
</code></pre>

<p>I can't get it to display the correct $F$ for the treatment differences in both cases. Below is an example when $H_0:\sigma^2_\varepsilon=0$ is rejected.</p>

<p>I actually don't know if all of these even make sense. I did not come from a stats background and I finally decided to start learning it after a while. My school uses SAS and this procedure seems to be built-in. I know of the University Edition of SAS but I couldn't run it on my old laptop. So I am trying to find a way to get the same output in R.</p>

<p><strong>Example</strong></p>

<p><a href=""https://dl.dropboxusercontent.com/u/28713619/crossvalidated/example.csv"" rel=""nofollow"">Here</a> is a the data set for the following.</p>

<pre><code>example &lt;- read.csv(""example.csv"", header=T)
example$pot &lt;- factor(example$pot)
example$hours &lt;- factor(example$hours)
model &lt;- lm(growth ~ hours/pot, example)
anova(model)
</code></pre>

<p>which gives the following output</p>

<pre><code>## Analysis of Variance Table
## 
## Response: growth
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## hours      2 15.0417  7.5208 15.3255 3.569e-05 ***
## hours:pot  6  8.2083  1.3681  2.7877   0.03054 *  
## Residuals 27 13.2500  0.4907                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>In this particular example where there is significant variance among experimental units, our school manual prescribes that the F value for hours should be <code>anova(model)$""Sum Sq""[1]/anova(model)$""Sum Sq""[2]</code>. Is there a way to compute it automatically? In this case, the $F$ for the test of treatment differences should be 5.4973.</p>

<p>This is based on the example given in our manual. I don't know if the manual is even correct in its prescribed procedures. </p>
"
"0.144714921058484","0.143384833669101","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.0511644510096651","0.0270369035217938","176677","<p>First, I am not great with stats, it was definitely my weakest class in undergrad/honours, so please be patient (and gentle!)</p>

<p>I have a data set with responses to a 4 point survey item, gathered from 12 groups (which differ in size). My goal is to determine whether the groups are sufficiently similar so as to suggest that they belong to the same population (i.e. are a representative sample). </p>

<p>My first step was to run an ANOVA, which tells me that the means are significantly different between groups, but I'd like to see how their distributions compare too before ruling out the hypothesis that the groups' responses are similar between groups, not just the mean. I figured that although the means may vary, if the distributions of each group overlapped sufficiently, I could argue that they were similar.</p>

<p>I've looked at the coefficient of variation, which ""[normalizes] the standard deviation so that it can be compared across various mean scales"", I got the SD of the means of each group, CV = .47, which, it is suggested, indicates relatively low variation between group means.</p>

<p>I've also looked at KS test (which I am very vague on), but that appears to be suited only to 2 groups? </p>

<p>I'd love some guidance (handholding). I've always been afraid of stats since I first struggled, and now really regret not learning more. </p>
"
"0.0738495239084239","0.0780488176318078","176864","<p>I have a dataframe called ""spf""</p>

<pre><code>str(spf)
'data.frame':   120 obs. of  6 variables:
 $ id     : Factor w/ 30 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 2 2 2 2 3 3 ...  
 $ BTW    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...  
 $ WTH1   : Factor w/ 2 levels ""0"",""1"": 1 1 2 2 1 1 2 2 1 1 ...  
 $ WTH2   : Factor w/ 2 levels ""0"",""1"": 1 2 1 2 1 2 1 2 1 2 ...  
 $ Z_score: num  -1.06 -0.678 1.194 1.94 -1.06...
</code></pre>

<p>And I do three-ways anova analysis:</p>

<pre><code>anova &lt;- aov (Z_score ~ BTW*WTH1*WTH2 + Error (id/(WTH1*WTH2)), data = spf)
</code></pre>

<p>Then</p>

<pre><code>summary (anova)
</code></pre>

<p>I have:</p>

<pre><code>Error: id
          Df Sum Sq Mean Sq F value Pr(&gt;F)
BTW        4   0.00   0.000       0      1
Residuals 25  39.19   1.567               

Error: id:WTH1
           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
 WTH1       1 13.632  13.632  18.769 0.00021 ***
 BTW:WTH1   4  2.181   0.545   0.751 0.56686 
 Residuals 25 18.158   0.726  
...
</code></pre>

<p>My question is:</p>

<ul>
<li><p>why the ""Sum Sq"" of BTW variable is 0, it does not make sense for me.</p></li>
<li><p>Why ""Sum Sq"" and ""Mean Sq"" of WTH1 are the same (both are 13.632)? I am a very newbie in ANOVA, but I guess ""sum of something"" and ""mean of something"" should be different?</p></li>
</ul>

<p>Did I do anything wrong?</p>
"
"0.130778311815021","0.108597294255834","177700","<h2>Background</h2>

<p>I'm involved in creating a group presentation and paper for a college class where our hypothesis is that password length is more important than character sets included in a password. Our research into things like the ""Password Haystack"" Calculator (can't link, not enough rep) tell us that length should become more important than complexity (or character sets included) in the password. For example, while ""#dA6,!pd%6"" is strong, ""aaabbbcccaaabbbcccaaabbbcccaaabbbcccaaabbbccc"" is stronger, simply because an adversary does not know any prior knowledge what a user has chosen, besides the websites requirements.</p>

<p>We have written several programs, one that creates random passwords of varying lengths and including various character sets (a-z, 0-9, symbols, A-Z) and another that inputs various passwords and computes the required number of brute force tries to arrive at the correct guess.</p>

<p>Using R I have generated some pretty graphics, for example <a href=""http://i.stack.imgur.com/SNROw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SNROw.png"" alt=""Brute force violin plot for eight character passwords on a log y scale""></a>. Since uppercase is the last set to be searched, all uppercase only passwords having the largest iteration values makes sense.</p>

<p><a href=""http://i.stack.imgur.com/2Sqa7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2Sqa7.png"" alt=""Brute force heat map for various character factors and password length, given on log scale for iterations as color""></a>
This graph also makes sense to me, there is variation depending on the character sets, with those in the upper reaches of the character sets having more calculated iterations, and password length contributing as well to all of them.</p>

<p>Running a two-way ANOVA test on the Rockyou database (32 million real stolen plain text passwords) with a given passwords keyspace location versus both character sets and length returns</p>

<pre><code># Two-way ANOVA
&gt; rockyou17.anova &lt;- aov(location ~ length*charSet,   data=rockyou17)

&gt; summary(rockyou17.anova)

                     Df    Sum Sq   Mean Sq F value Pr(&gt;F)    

length                1 1.209e+68 1.209e+68   50209 &lt;2e-16 ***

charSet              14 1.179e+68 8.419e+66    3498 &lt;2e-16 ***

length:charSet       14 1.341e+69 9.581e+67   39804 &lt;2e-16 ***

Residuals      14245361 3.429e+70 2.407e+63  

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The p-values (Pr>F), if I understand it, means that both character sets and password length are immensely important. This makes sense to me, since the keyspace location is calculated based entirely on character set and password length. However, I'm way out of my league and we did not have time to even discuss Chi-square tests this summer in my Applied Statistics class.</p>

<h2>Questions</h2>

<ol>
<li>How can we prove, or show a stronger relationship for length versus character set/radix?</li>
<li>Is this something that is more adapt at theoretical mathematics?</li>
<li>Given a real world database set of 14 million unique passwords, besides interesting data like password length histograms and percentage of character sets used, what else could I tease out of such a set?</li>
</ol>

<p>I attempted to contact several professors at my college, but the only one that others deemed qualified was sick last week, and this week is behind playing catch up for last week, and so is unable to help.</p>

<p>(I had to pare down the background information considerably to allow this to post. My apologies if it is lacking in the necessary details.)</p>
"
"0.119991273679223","0.126814318375447","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.108536190793863","0.114707866935281","179505","<p>It seems like something is amiss with the Anova() function from the car package.  Using the prodscore data set in the alr4 package, we get:</p>

<pre><code>&gt; Anova(lm(Value~P*Year*County,data=prodscore))
Anova Table (Type II tests)

Response: Value
               Sum Sq  Df  F value    Pr(&gt;F)    
P              413397   4   8.6972 4.271e-06 ***
Year          1464788   1 123.2664 &lt; 2.2e-16 ***
County         546844   6   7.6698 7.805e-07 ***
P:Year            999   1   0.0840    0.7725    
P:County        55189   3   1.5481    0.2065    
Year:County     63417   3   1.7789    0.1558    
P:Year:County    1340   3   0.0376    0.9902    
Residuals     1235843 104                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The effect of P should be tested (using Type II SS, as I understand it) by differencing the residual sums of squares of the models P+Year+County and Year+County.  Since P is continuous, that would mean the sum of squares for effect P would have 1 df, not 4.</p>

<p>In fact, what the function is (evidently) doing is differencing the residual sums of squares of the models P+Year+County+P:County and Year+County.  Doing that results in the 413397 SS and 4 df that the function is reporting.</p>

<p>Indeed, based on my calculations, every main effect here is being tested using P+Year+County+P:County as the ""full model"". Is this a bug in the program?</p>
"
"0.150002077605491","0.141844082724498","180225","<p>I would like to know the way to exclude all the unwanted pairs of POST-HOC tests from the output generated from two-way ANOVA. Details as follows:</p>

<p>I have <code>datTable</code> contain proportion data under two factor <code>site</code> (four levels: A, B, C, D) and <code>treatment</code>(two levels: control and treated). Specifically, I want to do a pair-wise test among all the <code>site</code> under each same <code>treatment</code> (e.g. control-A VS. control-B, control-A VS.control-C, treated-A VS.treated-C, etc.), while excludes comparisons between different <code>sites</code> and different <code>treatments</code>(e.g., pairs such as control-A VS. treated-B, control-B VS. treated-C).</p>

<p>The data looks like this:</p>

<pre><code>&gt; datTable
   site treatment proportion
     A   control  0.5000000
     A   control  0.4444444
     A   treated  0.1000000
     A   treated  0.4000000
     B   control  0.4444444
     B   control  0.4782609
     B   treated  0.0500000
     B   treated  0.3000000
     C   control  0.3214286
     C   control  0.4705882
     C   treated  0.1200000
     C   treated  0.4000000
     D   control  0.3928571
     D   control  0.4782609
     D   treated  0.4000000
     D   treated  0.4100000
</code></pre>

<p>I did a two-way ANOVA (also not sure whether to use within subject <code>site/treatment</code> or between subject <code>site*treatment</code>...), and summarised the results.</p>

<pre><code>  m1 &lt;- aov(proportion~site*treatment,data=datTable) # Or should I use 'site/treatment'?
</code></pre>

<p>Then my <code>summary(m1)</code> gave me the following:</p>

<pre><code>&gt; summary(m1)
               Df  Sum Sq Mean Sq F value Pr(&gt;F)  
site            3 0.02548 0.00849   0.513 0.6845  
treatment       1 0.11395 0.11395   6.886 0.0305 *
site:treatment  3 0.03686 0.01229   0.742 0.5561  
Residuals       8 0.13239 0.01655                 
</code></pre>

<p>Next step is to use <code>TukeyHSD</code> post-hoc test to see actually which pair caused the <code>*</code> significance in <code>site</code> factor.</p>

<pre><code>&gt; TukeyHSD(m1)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = proportion ~ site * treatment, data = datTable)

$site
            diff        lwr       upr     p adj
B-A -0.042934783 -0.3342280 0.2483585 0.9631797
C-A -0.033106909 -0.3244002 0.2581863 0.9823452
D-A  0.059168392 -0.2321249 0.3504616 0.9124774
C-B  0.009827873 -0.2814654 0.3011211 0.9995090
D-B  0.102103175 -0.1891901 0.3933964 0.6869754
D-C  0.092275301 -0.1990179 0.3835685 0.7461309

$treatment
                      diff        lwr         upr     p adj
treated-control -0.1687856 -0.3171079 -0.02046328 0.0304535

$`site:treatment`
                            diff        lwr       upr     p adj
B:control-A:control -0.010869565 -0.5199109 0.4981718 1.0000000
C:control-A:control -0.076213819 -0.5852551 0.4328275 0.9979611
D:control-A:control -0.036663216 -0.5457045 0.4723781 0.9999828
A:treated-A:control -0.222222222 -0.7312635 0.2868191 0.6749021
B:treated-A:control -0.297222222 -0.8062635 0.2118191 0.3863364  # Not wanted
C:treated-A:control -0.212222222 -0.7212635 0.2968191 0.7154690  # Not wanted
D:treated-A:control -0.067222222 -0.5762635 0.4418191 0.9990671  # Not wanted
C:control-B:control -0.065344254 -0.5743856 0.4436971 0.9992203
D:control-B:control -0.025793651 -0.5348350 0.4832477 0.9999985
A:treated-B:control -0.211352657 -0.7203940 0.2976887 0.7189552  # Not wanted
B:treated-B:control -0.286352657 -0.7953940 0.2226887 0.4233804  # Not wanted
C:treated-B:control -0.201352657 -0.7103940 0.3076887 0.7583437  # Not wanted
D:treated-B:control -0.056352657 -0.5653940 0.4526887 0.9996991
D:control-C:control  0.039550603 -0.4694907 0.5485919 0.9999713
A:treated-C:control -0.146008403 -0.6550497 0.3630329 0.9304819  # Not wanted
B:treated-C:control -0.221008403 -0.7300497 0.2880329 0.6798628  # Not wanted
C:treated-C:control -0.136008403 -0.6450497 0.3730329 0.9499131 
D:treated-C:control  0.008991597 -0.5000497 0.5180329 1.0000000  # Not wanted
A:treated-D:control -0.185559006 -0.6946003 0.3234823 0.8168230  # Not wanted
B:treated-D:control -0.260559006 -0.7696003 0.2484823 0.5194129  # Not wanted
C:treated-D:control -0.175559006 -0.6846003 0.3334823 0.8505865  # Not wanted
D:treated-D:control -0.030559006 -0.5396003 0.4784823 0.9999950  
B:treated-A:treated -0.075000000 -0.5840413 0.4340413 0.9981528
C:treated-A:treated  0.010000000 -0.4990413 0.5190413 1.0000000
D:treated-A:treated  0.155000000 -0.3540413 0.6640413 0.9096378
C:treated-B:treated  0.085000000 -0.4240413 0.5940413 0.9960560
D:treated-B:treated  0.230000000 -0.2790413 0.7390413 0.6429921
D:treated-C:treated  0.145000000 -0.3640413 0.6540413 0.9326207
</code></pre>

<p>However, there are some pairs I don't want to be included in the two-way ANOVA which I preformed, specified as <code># not wanted</code>. </p>

<p>Is there any way that I can tweak the <code>aov</code> or <code>TukeyHSD</code> function to exclude those possibilities ('not wanted' ones) I listed above? I could easily select the significant entires that I am interested (with <code>*</code>) from the long list produced from <code>TukeyHSD</code>. But I don't want my result from anova to be biased by those! (It happens in the real data that the significance actually caused by those unwanted pairs!) </p>

<p>NB: You might have noticed that the <code>site:treatment</code> post-hoc tests doesn't show any significance, this is because I only selected a small sample from the original data. </p>
"
"0.127911127524163","0.1441968187829","180288","<p>I am trying to understand the effect of a covariate (COVAR) in a linear mixed effects model with 2 categorical IVs (IV1, IV2). In order to illustrate where I am struggling, I had to paste the rather long <code>dput()</code> here:</p>

<pre><code>df &lt;- structure(list(ID=c(1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L),
IV1=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L),.Label=c(""412A"",""415D"",""512A"",""515A"",""615A""),class=""factor""),
IV2=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L),.Label=c(""24"",""27"",""2403""),class=""factor""),
DV=c(NA,NA,NA,17,19,27,14,21,21,31,34,NA,22,29,32,16,18,NA,NA,NA,39,33,27.5,28,27,NA,18,NA,24,38,27,15,NA,NA,22,27,17,52,NA,19,35,37,38,30,29,44,74,60,31,54,66,61,60,35,49,NA,52,53,30,36.5,46,57,54,59,NA,41,45,53.5,39,48,43,58.5,50,31,46,23,46,44,25,51,49,32.5,51,37,53,34,52,56,50.5,10,33,31,35,39,27,22,36,21,39,26,35,24,NA,28,39,28,35,21,39,34,30,NA,25,13,NA,31,28,29,32,NA,21,18,32,34,33.5,55,46,26.5,57,29,37,NA,23,52,31,32,41,25,29.5,47,37.5,30,NA,NA,NA,NA,43,43,43,29,42,31,NA,36,16,55,11,30,50,49,38,33,42,45,43.5,35,28,NA,44,36.5,34,41,35,17,38.5,24,49,42,40.5,37.5,15,37.5,32,30,44,25,38,39.5,37.5,43,25,28.5,26,32,43,NA,19,35,19,40.5,33,13,39,39,32,39,44,7,39,40,16,35,52,33,NA,54,24,52,37,31,27,24,31,18,50,16,31,NA,43,NA,42,39,NA,NA,51,36,38,28,NA,30,27,30,31,31,19,NA,38,35,38,21,29,31,8,32,19,23,18,NA,22,30,31,44,31,14,NA,28,25,34,32,39,30,27,33,44,47,16,46.5,12,24,17,40,29,21,47,6,19.5,39,32,28,43,51,42,44,36,48,37,32,37,43,41,10,5,37,28,10,35,45.5,51,22,35,38,39,45,44,46,24,41,37.5,30,NA,33,21,24,NA,25,27,18,NA,22,42,19,30,31,36,19,18,42,25,12,30,32,36.5,27,36,39,37,36,43,35,30.5,11,36,15,43,37,38,23,34,NA,14,39,35,42,38,45,31,41,37,36,37,33,12,44,42,45,39.5,36,44.5,38,14,14,36.5,36,32,43,39,35,38,51,43,48,35,25,49,46,26,46,51.5,35,45.5,NA,53,38.5,45,53,34,51,31,13,36,NA,32,37,43,43,19,35.5,45,41,28,42,44,43,44,34,30,46,43,45,37,33.5,47,23,19,36,38.5,26,41,NA,34,35.5,25,11,38,34,47,9,47,16,20,31,9,9,35,32,NA,34.5,31,NA,32,39,NA,NA,NA,NA,32,26,10,11,NA,37,44,25,15,37,25,10,NA,15,32,NA,24,27,NA,25,31,23,41.5,27,40,31,32,11,NA,14,25,29,36,37,31.5,37,27,21,NA,27,38,NA,NA,25,23,25,40,NA,47,35,33,39,35,38,43,27,35.5,33,28,NA,40,30,48,39,11,35,42.5,42.5,42,42,38,48,46,41,NA,32.5,43.5,34,29,35,NA,38,NA,NA,31,36,31,28.5,15,25,34,30,36,26,35,39,19,NA,NA,31,22,NA,NA,35,35,15,23,38.5,38,NA,36,16,18,26,30,28,NA,25,27,26,25,5,41,29,37,28,34,43,38,29,45,NA,41,32,37,50,31,NA,35,40,41,36,25,34,38,32,38,42,33,34,39,34,39,31,46,8,NA,36,48,25,32,37,NA,40,32,17,37,29,NA,37.5,NA,38,39,NA,44,48,40,NA,20,NA,36.5,20,33,31,41,32.5,28,43,39,29,23,37,32,39,26,36,15,37,31,11,38,29,42,38.5,32,30,37,38,32,33),
COVAR=c(5.2,5.2,5.87,5.68,5.49,7.67,6.3,8.34,7.01,5.51,5.8,4.35,3.95,5.23,6.32,4.01,3.16,3.61,4.67,3.44,5.27,4.59,4.18,4.64,3.97,4.11,3.68,7.57,3.97,5.9,6.02,4.79,5.14,5.84,7.61,4.99,4.18,7.25,3.92,6.3,6.04,5.02,8.01,4.14,8.24,6.21,7.44,5.69,6.31,5.9,6.7,4.96,5.08,4.93,6.4,7.2,7.38,9.59,6.37,8.24,5.6,5.87,4.99,3.64,3.44,5.72,4.52,6.5,4.78,5.18,5.92,8.79,7.65,4.5,4.3,5.76,8.53,4.38,4.46,8.7,8.26,8.89,5.85,6.98,6.65,7.27,8.92,7.43,5.91,5.49,7.64,7.15,6.8,5.74,4.63,4.62,7.02,5.43,9.59,5.42,6.13,8.9,4.66,6.87,6.83,8.38,8.96,5.25,5.54,6.95,8.03,4.33,7.76,6.35,4.99,7.41,6.13,4.67,4.1,4.51,4.6,3.71,6.72,5.37,8.21,6.5,5.46,5.6,7.83,5.08,5.42,3.9,4.88,6.63,4.21,5.3,4.57,8.56,3.84,7.07,4.84,6.19,5.15,3.73,5.32,8.32,7.09,6.06,5.42,7,6.65,5.28,6.08,4.84,4.73,5.15,5.44,6.38,7.4,6.28,4.96,5.14,5.53,8.46,6.93,5.34,5.03,4.4,6.68,7.31,6.17,5.5,9.65,4.36,4.64,6.77,6.95,7.56,8.47,4.68,3.9,4.33,4.77,3.65,5.17,4.44,6.37,4.35,4.55,7.09,4.06,7.78,4.49,6.37,9.03,2.67,3.89,4.38,5.56,6.77,4.48,4.69,4.94,6.17,4.32,4.25,8.11,3.79,5.62,3.99,5.19,4.47,7.07,8.32,8.79,4.27,4.55,4.5,4.15,5.12,10.11,7.68,4.01,6.53,5.66,6.52,5.99,6.62,9.44,5.44,11.1,8.62,5.85,3.82,9.46,8.69,10.36,6.95,6.27,8.37,6.35,7.12,3.71,8.21,5.98,5.49,7.62,6.31,7.98,8.26,6.93,7.03,3.4,3.35,4.74,5.84,7.99,5.07,7.35,7.88,7.44,9.32,7.22,6.47,5.32,5.98,6.61,8.26,7.79,8.19,7.05,3.24,6.5,3.94,7.33,4.4,6.22,5.95,3.56,6.13,6.98,5.2,5.67,5.29,3.6,4.71,5.88,4.27,4.52,5.44,5.39,6.07,6.51,3.24,7.55,4.52,4.19,6.41,5.43,5.48,4.08,5.26,6.99,3.66,5.4,6.13,7.24,10.57,5.92,6.78,6.47,7.78,12.14,8.49,8.77,4.74,8.49,8.03,9.02,5.42,8.22,4.95,5.77,7.49,4.52,4.8,4.62,7,9.01,9.36,4.73,5.14,6.63,7.44,6.91,5.47,7.24,7.46,4.52,6.35,9.13,9.56,8.11,8.97,12.03,8.16,10.79,7.8,6.39,5.8,3.97,7.44,5.03,8.35,6.94,8.44,4.04,6.6,6.04,4.61,5.9,7.72,7.57,6.25,6.96,5.55,9.01,7.44,5.09,5.56,9.17,8.97,7.99,10.16,11.04,6.33,6.96,7,5.08,5.37,4.4,5.49,6.17,6.97,7.65,6.48,5.54,7.79,8.42,7,8.11,5.02,3.9,5.09,4.4,4.63,7.92,9.47,7.05,9.63,4.93,8.36,7.83,10.81,11.58,5.68,11.66,8.01,4.35,5.43,9.3,6.01,5.7,7.64,8.03,7.8,5.9,9.05,6.9,6.36,9.57,6.58,7.66,7.14,5.75,3.58,10.36,6.4,6.09,7.46,7.16,8.78,5.12,4.66,4.61,4.48,4.66,8.11,4.18,5.93,5.97,6.36,6.07,7.4,4.78,8.51,5.21,8.44,5.25,4.68,4.1,3.92,3.57,4.7,5.54,4.5,5.88,5.42,4.45,4.86,6.48,4.71,4.67,4.29,4.71,3.71,5.23,5.64,4.67,3.93,4.79,4.21,4.39,3.4,4.41,4.81,3.85,4.72,4.58,3.09,5.58,4.84,5.19,6.39,3.82,3.89,4.04,4.53,5.8,4.6,4.49,4.35,5.85,4.67,5.44,3.83,5.28,4.33,5.14,3.92,4.37,6.03,6.1,6.38,6.04,5.98,5.26,5.44,3.76,5.37,5.36,6.33,5.52,4.56,4.6,5.58,5.1,4.21,5.03,4.85,4.56,5.79,4.22,3.77,3.34,4.03,6.53,6.97,4.49,6.4,4.49,5.98,5.41,5.03,5.28,4.92,6.92,4.91,4.7,6.6,4.98,6.81,4.8,4.1,4.09,4.87,4.83,4.77,4.4,4.89,4.55,4.55,4.65,5.12,4.85,5.78,5.49,4.58,5.25,5.09,4.93,4.9,5.42,5.33,4.81,4.61,6.67,4.46,5.33,8.05,5.99,4.35,5.06,5.31,4.29,4.29,3.48,4.32,3.86,4.64,4.03,4.18,5.39,4.35,3.54,4.22,3.65,4.63,4.61,4.14,3.4,4.28,5.98,3.48,3.68,5.54,4.22,4.78,3.49,5.84,6.52,6.1,3.9,4.77,4.59,5.31,4.45,4.44,3.97,4.24,3.75,3.84,5.66,4.15,4.35,5.62,5.09,5.65,4.57,4.97,3.53,3.64,3.87,5.49,5.33,4.66,5.85,3.69,6.43,4.73,4.67,4.76,4.7,5.05,8.12,4.53,9.82,3.97,5.24,11.78,5.09,4.94,4.33,5,6.49,7.02,5.1,5.98,4.56,4.06,5.76,4.51,6.56,5.41,4.35,3.76,3.91,3.77,4.69,3.97,4.83,4.78,4.75,4.39,3.46,8.21,3.85,3.48,9.49,3.91,5.19,4.52,4.2,4.7,4.95)),.Names=c(""ID"",""IV1"",""IV2"",""DV"",""COVAR""),class=""data.frame"",row.names=c(NA,675L))
</code></pre>

<p>Model fit with the covariate:</p>

<pre><code>require(lmerTest)
require(car)

m1&lt;-lmer(DV ~ COVAR*IV1*IV2 + (1|IV1:ID), data=df)
</code></pre>

<p>Then I wanted to test whether COVAR is significant and whether an interaction between COVAR and the IVs exists. I used the <code>anova()</code> function provided by <code>lmerTest</code>. Here the covariate is significant as well as the interaction between COVAR and IV2:</p>

<pre><code>anova(m1)
Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
          Sum Sq Mean Sq NumDF  DenDF F.value    Pr(&gt;F)    
COVAR         4589.8  4589.8     1 567.74  56.506 2.197e-13
IV1            610.4   152.6     4 548.49   1.879  0.112731    
IV2           1223.2   611.6     2 562.64   7.529  0.000593
COVAR:IV1      208.7    52.2     4 560.52   0.642  0.632594    
COVAR:IV2      703.4   351.7     2 563.35   4.330  0.013613  
IV1:IV2        776.8    97.1     8 561.48   1.195  0.299305    
COVAR:IV1:IV2  680.6    85.1     8 561.47   1.047  0.399018
</code></pre>

<p>However when I use <code>Anova(m1, type=3)</code>, just to check and compare different outputs, it comes out like this:</p>

<pre><code>Analysis of Deviance Table (Type III Wald chisquare tests)

Response: DV
           Chisq Df Pr(&gt;Chisq)   
(Intercept)   7.7308  1   0.005429
COVAR         1.9850  1   0.158866   
IV1           6.5038  4   0.164549   
IV2           2.0069  2   0.366610   
COVAR:IV1     0.3739  4   0.984554   
COVAR:IV2     1.6527  2   0.437654   
IV1:IV2       9.5635  8   0.297007   
COVAR:IV1:IV2 8.3786  8   0.397383 
</code></pre>

<p>When I run the <code>Anova(m1)</code> it looks again closer to what <code>anova(m1)</code> produced, however, IV1 is now ""highly"" significant (which is what I would have expected a priori given the nature of IV1), plus there is also an interaction between IV1 and IV2. That being said and also given the discussions regarding type 2 and type 3 SS, I would opt for going ahead with type 2 SS:</p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: DV
             Chisq Df Pr(&gt;Chisq)    
COVAR          97.1301  1  &lt; 2.2e-16 
IV1           104.2557  4  &lt; 2.2e-16 
IV2            20.0292  2  4.474e-05 
COVAR:IV1       0.2244  4  0.9941594    
COVAR:IV2       9.1881  2  0.0101119   
IV1:IV2        28.5092  8  0.0003865 
COVAR:IV1:IV2   8.3786  8  0.3973834 
</code></pre>

<p><strong>Question 1:</strong> What is the explanation for these substantial variations between these outputs (especially <code>anova(m1)</code> vs. <code>Anova(m1, type=3)</code> which are both <code>type=3</code> calculations)?</p>

<p>Given the fact that COVAR interacts with IV2 and also that <code>m2 &lt;- lmer(COVAR ~ IV1*IV2 + (1|IV1:ID), data=df); Anova(m2)</code> turns out to be significant (again for IV2), I cannot sell this anlysis as ANCOVA since both additional assumptions for ANCOVA are violated. </p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: COVAR
      Chisq Df Pr(&gt;Chisq)    
IV1       3.093  4     0.5424    
IV2     160.317  2  &lt; 2.2e-16
IV1:IV2  34.734  8  2.989e-05
</code></pre>

<p>However, COVAR seems to play an important role and therefore should be kept in the model nonetheless.</p>

<p><strong>Question 2:</strong> Is this reasonable? And if yes, how do I go on and interpret the output of such a model, especially the interaction between COVAR and IV2?</p>

<p>What I would do is plot the interactions for IV1:IV2 and for COVAR:IV2 first:</p>

<pre><code>with(na.omit(df), interaction.plot(IV1,IV2,DV))
require(ggplot2)
ggplot(df,aes(x=COVAR, y=DV))+geom_point(aes(colour=IV2))+
geom_smooth(aes(colour=IV2), method=lm)
</code></pre>

<p>and then start discussing.</p>

<pre><code>&gt; sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
[1] ggplot2_1.0.1   car_2.1-0       lmerTest_2.0-29
[4] lme4_1.1-10     Matrix_1.2-2
</code></pre>
"
"0.144714921058484","0.143384833669101","180531","<p>I am currently running multiple comparisons following ANOVA. For this I want to set my own contrasts in TukeyHSD to exclude comparisons not of interest. So the first thing I did is test whether my manual contrasts give the same results as the automatic contrasts set by <code>multcomp</code>: </p>

<pre><code>library(multcomp)
set.seed(123)

Group &lt;- paste0(rep('R',24), rep(1:6, each= 4))
meanList &lt;- list(n = 24, mean = rep(c(10:16), each=4), sd = 1)
Value &lt;- do.call('rnorm', meanList)
df &lt;- data.frame(Group,Value)

mod1 &lt;- aov(Value ~ Group, data= df)

tuk1 &lt;- glht(mod1, mcp(Group = 'Tukey'))

summary(tuk1)
Linear Hypotheses:
             Estimate Std. Error t value Pr(&gt;|t|)    
R2 - R1 == 0   1.0504     0.6911   1.520  0.65678    
R3 - R1 == 0   1.9032     0.6911   2.754  0.11231    
R4 - R1 == 0   3.2260     0.6911   4.668  0.00223 ** 
R5 - R1 == 0   3.4803     0.6911   5.036  &lt; 0.001 ***
R6 - R1 == 0   4.0302     0.6911   5.831  &lt; 0.001 ***
R3 - R2 == 0   0.8528     0.6911   1.234  0.81492    
R4 - R2 == 0   2.1756     0.6911   3.148  0.05321 .  
R5 - R2 == 0   2.4299     0.6911   3.516  0.02529 *  
R6 - R2 == 0   2.9798     0.6911   4.311  0.00486 ** 
R4 - R3 == 0   1.3228     0.6911   1.914  0.42536    
R5 - R3 == 0   1.5771     0.6911   2.282  0.25116    
R6 - R3 == 0   2.1270     0.6911   3.078  0.06095 .  
R5 - R4 == 0   0.2543     0.6911   0.368  0.99896    
R6 - R4 == 0   0.8042     0.6911   1.164  0.84794    
R6 - R5 == 0   0.5499     0.6911   0.796  0.96465 
</code></pre>

<p>If I now insert the contrast manualy I get a different result: 
I created my contrast matrix using the contrMat function for the example but this could also be done by hand.</p>

<pre><code>cM1 &lt;- contrMat(n=table(df$Group), type='Tukey') 
tuk2 &lt;- glht(mod1, linfct = cM1)

summary(tuk2)
Linear Hypotheses:
             Estimate Std. Error t value Pr(&gt;|t|)    
R2 - R1 == 0  -9.1592     1.0928  -8.382  &lt; 0.001 ***
R3 - R1 == 0  -8.3064     1.0928  -7.601  &lt; 0.001 ***
R4 - R1 == 0  -6.9837     1.0928  -6.391  &lt; 0.001 ***
R5 - R1 == 0  -6.7293     1.0928  -6.158  &lt; 0.001 ***
R6 - R1 == 0  -6.1795     1.0928  -5.655  &lt; 0.001 ***
R3 - R2 == 0   0.8528     0.6911   1.234  0.80911    
R4 - R2 == 0   2.1756     0.6911   3.148  0.05094 .  
R5 - R2 == 0   2.4299     0.6911   3.516  0.02412 *  
R6 - R2 == 0   2.9798     0.6911   4.311  0.00458 ** 
R4 - R3 == 0   1.3228     0.6911   1.914  0.41675    
R5 - R3 == 0   1.5771     0.6911   2.282  0.24407    
R6 - R3 == 0   2.1270     0.6911   3.078  0.05845 .  
R5 - R4 == 0   0.2543     0.6911   0.368  0.99890    
R6 - R4 == 0   0.8042     0.6911   1.164  0.84291    
R6 - R5 == 0   0.5499     0.6911   0.796  0.96318  
</code></pre>

<p>The p-values are very different why is this? I later found that using <code>mcp</code> lead to better results but they are still not exactely the same:</p>

<pre><code>tuk3 &lt;- glht(mod1, linfct = mcp(Group = cM1))  
summary(tuk3)

Linear Hypotheses:
             Estimate Std. Error t value Pr(&gt;|t|)    
R2 - R1 == 0   1.0504     0.6911   1.520  0.65684    
R3 - R1 == 0   1.9032     0.6911   2.754  0.11235    
R4 - R1 == 0   3.2260     0.6911   4.668  0.00218 ** 
R5 - R1 == 0   3.4803     0.6911   5.036  0.00105 ** 
R6 - R1 == 0   4.0302     0.6911   5.831  &lt; 0.001 ***
R3 - R2 == 0   0.8528     0.6911   1.234  0.81497    
R4 - R2 == 0   2.1756     0.6911   3.148  0.05298 .  
R5 - R2 == 0   2.4299     0.6911   3.516  0.02534 *  
R6 - R2 == 0   2.9798     0.6911   4.311  0.00474 ** 
R4 - R3 == 0   1.3228     0.6911   1.914  0.42571    
R5 - R3 == 0   1.5771     0.6911   2.282  0.25134    
R6 - R3 == 0   2.1270     0.6911   3.078  0.06076 .  
R5 - R4 == 0   0.2543     0.6911   0.368  0.99896    
R6 - R4 == 0   0.8042     0.6911   1.164  0.84789    
R6 - R5 == 0   0.5499     0.6911   0.796  0.96468    
</code></pre>

<p>Any help on understanding the particulars of the contrasts in glht would be greatly appreciated. </p>

<p><strong>EDIT</strong> to adress rvl's question:
The contrast matrix between the two tests indeed look differently. So that is the source of the difference. The first matrix is</p>

<pre><code>cM1
Multiple Comparisons of Means: Tukey Contrasts

        R1 R2 R3 R4 R5 R6
R2 - R1 -1  1  0  0  0  0
R3 - R1 -1  0  1  0  0  0
R4 - R1 -1  0  0  1  0  0
R5 - R1 -1  0  0  0  1  0
R6 - R1 -1  0  0  0  0  1
R3 - R2  0 -1  1  0  0  0
R4 - R2  0 -1  0  1  0  0
R5 - R2  0 -1  0  0  1  0
R6 - R2  0 -1  0  0  0  1
R4 - R3  0  0 -1  1  0  0
R5 - R3  0  0 -1  0  1  0
R6 - R3  0  0 -1  0  0  1
R5 - R4  0  0  0 -1  1  0
R6 - R4  0  0  0 -1  0  1
R6 - R5  0  0  0  0 -1  1
</code></pre>

<p>and the one automatically generated by glht is:</p>

<pre><code>tuk1$linfct

           (Intercept) GroupR2 GroupR3 GroupR4 GroupR5 GroupR6
R2 - R1           0       1       0       0       0       0
R3 - R1           0       0       1       0       0       0
R4 - R1           0       0       0       1       0       0
R5 - R1           0       0       0       0       1       0
R6 - R1           0       0       0       0       0       1
R3 - R2           0      -1       1       0       0       0
R4 - R2           0      -1       0       1       0       0
R5 - R2           0      -1       0       0       1       0
R6 - R2           0      -1       0       0       0       1
R4 - R3           0       0      -1       1       0       0
R5 - R3           0       0      -1       0       1       0
R6 - R3           0       0      -1       0       0       1
R5 - R4           0       0       0      -1       1       0
R6 - R4           0       0       0      -1       0       1
R6 - R5           0       0       0       0      -1       1
</code></pre>

<p>To me it seems that the first matrix should be the correct one, because it does only the pairwise comparisons. The first 5 lines of the second matrix compare every group to the mean of all the groups(?). </p>
"
"0.1576994291291","0.166666666666667","181687","<p>I am just dipping my toe into the ocean that is linear effects models and am working through Barr et al.'s 'Keeping it Maximal' paper, trying to figure out the best way to fit a lmem for my experiment. Say you have three groups given three different types of drug over three days: 100mg on first day, 50mg on second day, 10mg on last day. The outcome measure is how they feel that next day on some scale (e.g. mood), before they are given their daily dose (i.e. so we are measuring the effects of the previous day's dose). However participants don't come in at exactly the same time each day, thus as each time of measurement the drug will have had less time to take effect. </p>

<p>I would like to know how best to include that random effect of 'time elapsed since dose' into this model, and just how best to fit the model really.</p>

<p>This is a toy dataset. I have not built any trends into it. </p>

<pre><code>dose100mg &lt;- c(6,2,9,4,6,5,2,4,6,7,3,2)
dose50mg &lt;- c(1,2,4,3,6,1,3,3,2,1,4,1)
dose10mg &lt;- c(8,9,7,9,6,7,8,9,8,7,1,3)
timeD1 &lt;- c(24.2,20.5,26,30,22,26,19,23,29,30,24,16)
timeD2 &lt;- c(24,16,28,20,19,28,30,20,18,15,27,32)
timeD3 &lt;- c(21,28,29,30,29,17,23,18,24,16,28,21)
subject &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12)
group &lt;- factor(c(0,1,0,2,1,2,0,2,1,2,1,0))

df &lt;- data.frame(subject, group, dose100mg, dose50mg, dose10mg, cov)
</code></pre>

<p>Turn it from wide to long</p>

<pre><code>require(tidyr)

df &lt;- gather(df, dose, score, dose100mg:dose50mg:dose10mg)
</code></pre>

<p>Now add the 'hours elapsed since last dose' variable to the dataframe (btw: if anyone knows how to build this into the gather function above I'd appreciate it) </p>

<pre><code>df$hrsElapsed &lt;- c(timeD1, timeD2, timeD3) 
</code></pre>

<p>Now fit a model. First group*dose plus with random intercepts for subject. </p>

<p>require(lme4)</p>

<pre><code># random intercepts

anDf_randomintercepts &lt;- lmer(score ~ group*dose + (1|subject), data = df)

anova(anDf_randomintercepts)
</code></pre>

<p>Next random slopes, and my first question. Is it better to include hrsElasped as a covariate, like this?</p>

<pre><code>anDf_randomSlopes &lt;- lmer(score ~ group*dose + hrsElapsed + (1|subject) + (1+hrsElapsed|subject), data = df)
</code></pre>

<p>Or to include it as a random effect? like this</p>

<pre><code>nDf_randomSlopes &lt;- lmer(score ~ group*dose + (1|subject) + (1+hrsElapsed|subject), data = df)
</code></pre>

<p>I know it's not the latter because I get an error message. </p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0450795 (tol = 0.002, component 1)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>But I don't know WHY this doesn't work. I would have thought time elapsed would be exactly the sort of variable you'd want to assign to random effects.</p>

<p>What am I doing wrong?</p>

<p>An ancillary question pertains to fitting random slopes for the group-by-subject effect </p>

<pre><code>anDf_randomSlopes &lt;- lmer(score ~ group*dose + (1|subject) + (1+group|subject), data = df)
</code></pre>

<p>When I run this i get the error message</p>

<pre><code>Error: number of observations (=36) &lt;= number of random effects (=36) for term (1 + group | subject); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable
</code></pre>

<p>Why doesn't this work? What does it mean?</p>
"
"0.0361787302646211","0.0382359556450936","181691","<p>I am a statistics newbie, so please excuse me if I am asking very basic questions! </p>

<p>For my coursework (using R) , I am analysing three survey questions which all look at attitudes to saving in old age. My IV is the categorical variable ""age"", split into ""Young"" and ""Old"". Two of the survey questions are numeric, and one categorial (yes/no). </p>

<p>I am very confused as to how I should go about analysing the data. I have been given the following methods to use, and only these! : </p>

<p>Correlations,
Differences between means,
ANOVA,
 and OLS</p>

<p>I would really appreciate some help :) </p>
"
"0.0957199230302734","0.101162829777814","182489","<p>Hello I have been struggling for many days with this problem. The situation and data set are following. The data set comes from an experiment that asked subjects (ID) to compare several CVs on different experimental conditions. Each CV had two factors, picture(no picture, modified picture, normal picture) and gender (level: male/female). In order to rate CVs, participants were asked to complete likert scale questionnaire with minimum score of 7 to 49. Higher score indicated they ""liked"" the CV more. Furthermore the participants were sampled in 4 different settings, so the setting as well had a Factor (level: A,B,C,D).</p>

<p>Meaning that setting was between-factor, while picture and gender within-factor.</p>

<p>data.frame looks like this:</p>

<pre><code>      ID  Setting Male_Normal Male_Modified Male_No_Picture Female_Normal....and so on..
        1      A         7          11            16               49    
        2      B        10          16            30               30
        3      C        11          30            20               20
        4      D        30          11            2                10
        5      A        20          15            13               15
        6      B        10          11            11               10
    ..and    ..so       ..on        ..            ..               ..
</code></pre>

<p>Main hypothesis: 
H1:Scores differ based on Setting
H2:Different scores for Gender 
H3:Different scores for Pictures</p>

<p>Question is how to analyze it correctly in SPSS or/and R and how to do post-hoc afterwards, should univariate test be used or discriminant analyses? </p>

<p>.....</p>

<p>I was trying to run repeated measures MANOVA in SPSS and in I tried approach using the car package R. </p>

<p>In R I did not get past formatting data set so it makes sense to R, or in other words I was not able to create factors for Gender and Pictures properly...</p>

<p>In SPSS I did large rep.measures MANOVA, found significance for everything literally and then I am not sure how to break the effects down. For example if Gender<em>Picture</em>Group is significant, what would you do?</p>

<p>I know the question is probably not an easy one, though any leads are highly appreciated.</p>

<p>Thank you very much!</p>
"
"0.0957199230302734","0.101162829777814","182696","<p>As you may know, R has the problem that it uses the wrong MSE in calculating p values in 2-way ANOVA for split plot experiments. </p>

<p>The whole plot factor is Tree.Name, subplot factor is In.Out, the replicates are Tree.ID, and P is the dependent variable as shown below:</p>

<pre><code>structure(list(Tree.Name = structure(c(5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L), .Label = c(""Akun"", ""Jaror"", ""Ku'ch"", ""Petz-kin"", 
""Puuna"", ""Yax bache""), class = ""factor""), Tree.ID = structure(c(10L, 
10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L, 11L), .Label = c(""AK-1"", 
""AK-2"", ""AK-3"", ""AK-4"", ""AK-5"", ""AK-6"", ""AK-7"", ""AK-8"", ""AK-9"", 
""C-1"", ""C-2"", ""C-3"", ""C-4"", ""C-5"", ""C-6"", ""C-6 "", ""C-7"", ""C-8"", 
""C-9"", ""Ce-1"", ""Ce-2"", ""Ce-3"", ""Ce-4"", ""Ce-5"", ""Ce-6"", ""Ce-7"", 
""Ce-8"", ""Ce-9"", ""J-1"", ""J-2"", ""J-3"", ""J-4"", ""J-5"", ""PK-1"", ""PK-3"", 
""PK-4"", ""PK-5"", ""PK-6"", ""PK-7"", ""PK-8"", ""PK-9"", ""Y-1"", ""Y-2"", 
""Y-3"", ""Y-4"", ""Y-5"", ""Y-6"", ""Y-7"", ""Y-8"", ""Y-9""), class = ""factor""), 
    Sample = c(1L, 2L, 3L, 5L, 6L, 7L, 1L, 2L, 3L, 4L), In.Out = structure(c(2L, 
    1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L), .Label = c(""In"", ""Out""
    ), class = ""factor""), P = c(9.18, 10.38, 6.77, 7.37, 7.97, 
    7.37, 7.07, 6.77, 6.47, 9.78), OM = c(14.71, 15.55, 11.19, 
    15.89, 14.21, 18.91, 12.19, 17.9, 9.84, 30.65), N = c(0.73, 
    0.79, 0.57, 0.8, 0.72, 0.96, 0.61, 0.91, 0.5, 1.55)), .Names = c(""Tree.Name"", 
""Tree.ID"", ""Sample"", ""In.Out"", ""P"", ""OM"", ""N""), row.names = c(NA, 
10L), class = ""data.frame"")
</code></pre>

<p>I found this solution while digging online: </p>

<pre><code>split.plot&lt;-aov(P~Tree.Name*In.Out+
                  Error(Tree.ID),
                data=tree.data)
summary(split.plot)
</code></pre>

<p>However, a friend recommended that I try using <code>lmer()</code></p>

<pre><code>library(""lme4"", lib.loc=""~/R/win-library/3.2"")
x &lt;- lmer(
  P~Tree.Name*In.Out+(1|Tree.ID),
  data=tree.data)
anova(x)
</code></pre>

<p>However the (F) values in the results of the analysis using <code>aov()</code></p>

<pre><code>##                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Tree.Name         5 2273.5   454.7  17.935 2.14e-09 ***
## In.Out            1    3.5     3.5   0.137    0.713    
## Tree.Name:In.Out  2  192.0    96.0   3.787    0.031 *  
## Residuals        41 1039.5    25.4   
</code></pre>

<p>are quite different than those from <code>lmer()</code></p>

<pre><code>## Analysis of Variance Table
##                  Df Sum Sq Mean Sq F value
## Tree.Name         5 322.78  64.556 15.7145
## In.Out            1  25.87  25.870  6.2974
## Tree.Name:In.Out  5  26.04   5.208  1.2677
</code></pre>

<p>My question is twofold:</p>

<p>Why aren't I getting the appropriate F values from lmer? and is one method preferred over the other for any particular reason?</p>
"
"0.0886194286901087","0.0936585811581694","183043","<p>Consider the following experiment:</p>

<p>Patients with epileptic seizures have blood samples taken when arriving at the hospital. Additional samples are being taken for a study of a new blood biomarker of epileptic activity. One week after, the patients have baseline blood samples taken. The biomarker level is expected to be increased immediately following seizures.
However, the biomarker is also expected to decrease below the baseline level after 25 minutes or more following the seizures.</p>

<p>I would like to compare baseline to seizure levels while taking the effect of time into account.
I have tried this approach, but I'm not sure if it's correct:</p>

<pre><code>#Sample dataset
subject &lt;- factor(c(""s1"",""s1"", ""s2"",""s2"",""s3"",""s3"",""s4"",""s4"",""s5"",""s5""))
condition&lt;-factor(c(""seizure"",""baseline"",""seizure"",""baseline"",""seizure"",""baseline,""seizure"",""baseline"",""seizure"",""baseline""))
blood &lt;- runif(10)
time &lt;- c(12,12,30,30,25,25,14,14,37,37)
mydata &lt;- data.frame(subject, condition, blood, time)
baseline &lt;- subset(mydata, condition==""baseline"")$blood
    attack &lt;- subset(mydata, condition==""seizure"")$blood
time &lt;- subset(mydata, condition==""baseline"")$time
</code></pre>

<p>I then do an ANCOVA like this:</p>

<pre><code>#ANCOVA
options(contrasts = c(""contr.sum"", ""contr.poly""))
mod.0 &lt;- lm(seizure ~ baseline)
mod.1 &lt;- lm(seizure ~ baseline + time)
a0 &lt;- anova(mod.0)
a1 &lt;- anova(mod.1)
pval.1 &lt;- a$`Pr(&gt;F)`[1]
mse.1 &lt;- a$`Mean Sq`[2]
print(a0)
print(a1)
</code></pre>

<p>Is this correct?</p>
"
"0.108536190793863","0.114707866935281","183248","<p>During an experiment I presented participants with some sound stimuli and I asked them to modify two parameters of the sound (Centroid and Sound_Level_Peak) to reach a given experimental goal. The sounds represented hand interactions with different kinds of materials by means of different kinds of objects, and where created thanks to a parametric synthesizer.</p>

<p>The initial sounds (preset sounds) had a value for those two parameters. What I am interested in is whether participants' modifications of the two parameters of the sound stimuli resulted in values actually different from the initial values of the parameters of the preset sounds.</p>

<p>In more details, the experiment involved 19 subjects, who where subjected to 12 kinds of stimuli, each based on 3 presets of centroid and peak level (so in total there where 36 stimuli). Each stimulus was repeated twice for a total of 72 trials per subject. Accordingly, the number of presets was 72.</p>

<p>To give an idea, some rows of my data set are the following:</p>

<pre><code>&gt; head(scrd) 
Stimulus_Type   Centroid        Sound_Level_Peak    Preset
Stimulus_A      1960.2          -20.963             no
Stimulus_A      5317.2          -42.741             no
.....
Stimulus_B      11256.0          -16.480            no
Stimulus_B      9560.3          -19.682             no
.....
.....
Stimulus_A      1900.2          -18.63             yes
Stimulus_A      5617.6          -44.41             yes
Stimulus_B      12056.0          -17.80            yes
Stimulus_B      8960.5          -21.82             yes
</code></pre>

<p>This is the analysis I performed:</p>

<pre><code>&gt; fit &lt;- manova(cbind(Centroid,Sound_Level_Peak)~ Stimulus_Type*Preset, data=scrd)
&gt; summary(fit, test=""Pillai"")
                       Df  Pillai approx F num Df den Df  Pr(&gt;F)    
Stimulus_Type          11 0.91888  106.629     22   2760 &lt; 2e-16 ***
Preset                  1 0.00343    2.371      2   1379 0.09378 .  
Stimulus_Type:Preset   11 0.01155    0.729     22   2760 0.81348    
Residuals            1380                                           
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
&gt; 
</code></pre>

<p>These results should then say that for each stimulus type there is no difference between the patterns of the two parameters in the preset and modified conditions.</p>

<p>Can anyone please tell me if this analysis is correct or suggest how to perform in R an alternative analysis?</p>
"
"0.10232890201933","0.108147614087175","183460","<p>EDIT: It appears that for this particular problem I made a simple arithmetic error. I am now getting the same results. What prompted me to write this question was that this is the third time that I've had this problem. However, in the past, I was mistakenly using <code>interval=""predict""</code> when I should have been using <code>interval=""confidence""</code>.</p>

<p>I am trying to compute a 95% confidence interval for a mean response on a small dataset, yet when I calculate this manually I get a very different interval. How is R calculating the interval when using <code>predict.lm</code>? Am I using the wrong function call? I verified that my manual calculation is correct with the solutions manual.</p>

<p>Computing a 95% confidence interval for a mean response at $x_0 = 170$ for the following dataset:</p>

<pre><code>      x        y
1   170   162.50
2   140   144.00
3   180   147.50
4   160   163.50
5   170   192.00
6   150   171.75
7   170   162.00
8   110   104.83
9   120   105.67
10  130   117.58
11  120   140.25
12  140   150.17
13  160   165.17
</code></pre>

<p>I calculated the confidence interval using the following equation from my book:
$$
\hat\mu_{y|x_0}\pm t_{\alpha/2, n-2}\sqrt{MS_{\rm res}\left(\frac 1 n + \frac{(x_0-\bar x)^2}{S_{xx}} \right)}
$$
And I got an interval of $168.36\pm23.465$ (or alternatively, $(144.895, 191.825)$). When I did this manual calculation, the MSRes I calculated is the same as the output from calling <code>anova(model)</code>. Additionally, I calculated Sxx in R as follows:</p>

<pre><code># Results are equivalent:
sum((data$x - mean(data$x))^2) # = 6230
(length(data$x)-1)*var(data$x) # = 6230
</code></pre>

<p>However, when I calculate this in R, I get the same fitted value but a different interval:</p>

<pre><code>&gt; predict.lm(data.lm, newdata=data.frame(x=170), interval=""confidence"", level=0.95)
       fit      lwr      upr
1 168.3611 153.9071 182.8152
</code></pre>

<p>Why is R giving me an incorrect interval? Or is my textbook simply wrong? This also applies to a 95% prediction interval on the same $x_0$, using the following equation:
$$
\hat y_0 \pm t_{\alpha/2, n-2}\sqrt{MS_{\rm res}\left(1 + \frac 1 n + \frac{(x_0-\bar x)^2}{S_{xx}} \right)}
$$</p>
"
"0.0886194286901087","0.0936585811581694","184248","<p>For my analysis I am currently running a 1-way within-subjects bootstrapped ANOVA with trimmed means (<code>rmanovab</code> from Rand Wilcox's <code>WRS</code> package). This is a very nice robust method, but it does not report effect size.</p>

<p>I really need to get an estimate of effect size like a generalized eta squared or Cohen's D. Since I know that <code>ezANOVA</code> reports a generalized eta squared effect size I tried using it, but it lacks trimming and the results differ quite a lot from <code>rmanovab</code> due to a single outlier. I even tried naively to manually trim the values but that also does not seem to be an option because that would mess up the 'within subject' calculations that <code>ezANOVA</code> does: I got this error:</p>

<pre><code>Warning: You have removed one or more Ss from the analysis. Refactoring ""Participant"" for ANOVA.
Error in ezANOVA_main(data = data, dv = dv, wid = wid, within = within,  : 
One or more cells is missing data. Try using ezDesign() to check your data. 
</code></pre>

<p>Does anyone have an idea for how to compute a generalized eta-squared or Cohen's d using trimmed means (and optionally using a bootstrapped method like <code>rmanovab</code>)? Any pointers are much appreciated.</p>
"
"0.130444267050272","0.106047460495963","184572","<p>About four years ago I took a graduate course in multivariate statistics. Now I'm faced with a data problem which is ringing a bell about something covered toward the end of the semester but even though I've gone back through my old homeworks etc. I'm having trouble determining what technique I am looking for. Secondarily, whatever technique I'm looking for I'm wanting to implement in R but I'll settle for just being pointed in the right direction.</p>

<p>Here is a simplified statement of the problem.</p>

<p><strong>Suppose I have a data set where each entry describes an object and for each entry there are several variables. I'll arbitrarily name those variables X, A, B, C, and D, some of which may be numerical quantities and some may be categories:</strong></p>

<pre><code> X | A | B | C | D
---|---|---|---|---
4.3|265| Y |red|7.1   
3.2|740| N |grn|9.0
2.2|655| Y |wht|8.2
.
.
.
</code></pre>

<p><strong>Through some mathematical process performed on this data, a function <em>f</em> is established such that X = <em>f</em>(A,B,C,D), with the understanding that here by ""="" I mean ""a value and some confidence interval"" and not strict equality.</strong></p>

<p><strong>Suppose now that the aforementioned data set is a subset of a larger dataset in which the X variable is missing and so are some number of the other variabless (always the same variables for all elements of the dataset), such that e.g. <em>f</em>(B,D) gives me an estimated value for X with what I would expect to be a wider confidence interval than I would have obtained if I also had supplied fields A and C.</strong></p>

<p>In my actual problem, the data set elements are buildings, the X variable is a given year's energy consumption for each building, and I have a great many other variables I can supply to include footprint square footage, number of stories, land use type (roughly 200 mostly-not-hierarchical categories), annual heating and cooling degree days at the building's location, and construction year. My expectation is that some of the variables will prove to have a very high bearing on energy consumption, others will not, and yet others may have a significant bearing even though the connection between them is not at all readily apparent.</p>

<p>Of the entire territory over which I am concerned with the buildings' energy consumption, I only have many data variables for a contiguous area comprising roughly 1/10th of the entire territory and I can only obtain actual annual energy consumption (X) for a subset of that contiguous area's buildings. My hope is that through statistical techniques I can leverage the data where I have a large number of data variables including energy consumption to estimate 1) energy consumption for the rest of that contiguous area's buildings and 2) energy consumption for the other 9/10ths of the entire territory where I don't have energy consumption data and have only a few of the other variables. I may be able to get real energy consumption data from buildings where I only have a few variables instead of the complete (less energy consumption) set of variables if that helps anything (even if just to increase the sample size). </p>

<p>Is what I'm trying to do possible and does it have a name? Is this what ANOVA/MANOVA are for?</p>
"
"0.0808981002113217","0.0854981960070962","185990","<p>I am running a linear multi level model in R. 
The predictor variable is called ""OAI"", and the response variable is called ""Ens"", I am allowing the intercepts and slopes to vary with ""ID"".</p>

<p>Here is a visual plot of the data: <a href=""http://i.stack.imgur.com/nGXgc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nGXgc.png"" alt=""OAI vs Ens, with ID""></a></p>

<p>I have built my model up, to see if including slopes and intercepts significantly improves the model.</p>

<p>First a base line model:</p>

<pre><code>interceptOnly &lt;-gls(Ens~1, data = d, method = ""ML"", na.action=na.exclude)
</code></pre>

<p>Then I allowed intercepts to vary with ID: </p>

<pre><code>randomInterceptOnly &lt;-lme(Ens~1, data = d, random = ~1|ID, method = ""ML"", na.action=na.exclude)
</code></pre>

<p>Next I added OAI as a predictor</p>

<pre><code>randomInterceptOAI &lt;-lme(Ens~OAI, data = d, random = ~1|MusID, method = ""ML"",na.action=na.exclude)
</code></pre>

<p>Next I wanted to compare the ""randomInterceptOnly"" and the ""randomInterceptOAI"" models to see if the fit has improved now I've added the predictor variable:</p>

<pre><code>anova(randomInterceptOnly, randomInterceptOAI)
</code></pre>

<p>Unfortunately, I get this error: 
<a href=""http://i.stack.imgur.com/kX5Rc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kX5Rc.png"" alt=""error in R""></a></p>

<p>This is because I have some missing data points for both ""Ens"" and ""OAI"" - this means there are different numbers of observations for the ""randomInterceptOnly"" and  ""randomInterceptOAI"" models. </p>

<p>Is there a good work around for this issue? </p>
"
"0.122844833408511","0.157650883821153","186426","<p>So, for example, using the Iris data and treating iris species as the predictor variable and sepal length, sepal width, petal length, and petal width as the dependent variables we get MANOVA output that looks like this:</p>

<pre><code>set.seed(2)
# Creating a matrix of the 4 dependent variables (DVs)
Y &lt;- as.matrix(iris[,c(1:4)]) 

# MANOVA looking at the effect of species on DVs
summary(manova(Y ~ iris$Species))
    #                 Df  Pillai    approx F  num Df  den Df    Pr(&gt;F)    
    # iris$Species    2   1.1919    53.466    8       290       &lt; 2.2e-16 ***
</code></pre>

<p>That seems to make sense. Species has a significant effect on our DVs. Now what if we add another predictor variable (a random one which we shouldnâ€™t expect to have an effect on the DVs)?</p>

<pre><code># Creating a random dummy variable to be used as a predictor variable
iris$random.dummy &lt;- sample(x = c(0,1), size = 150, replace = TRUE)

# MANOVA looking at the effect of species + our random dummy on DVs
summary(manova(Y ~ iris$Species + iris$random.dummy))
    #                     Df  Pillai    approx F  num Df  den Df  Pr(&gt;F)    
    # iris$Species        2   1.19339   53.263    8       288     &lt;2e-16 ***
    # iris$random.dummy   1   0.03784   1.406     4       143     0.2349  
</code></pre>

<p>That also seems to make sense. Species is significant still, but our random dummy variable is not. Now what if we simply switch the order of those variables?</p>

<pre><code># Switching the order of our two predictor variables in the formula
summary(manova(Y ~ iris$random.dummy + iris$Species))
    #                     Df  Pillai    approx F  num Df  den Df  Pr(&gt;F)  
    # iris$random.dummy   1   0.13031   5.357     4       143     0.0004764 ***
    # iris$Species        2   1.19526   53.470    8       288     &lt; 2.2e-16 ***
</code></pre>

<p>Now, the Pillaiâ€™s trace and approximate F-values change and our random dummy variable has become significant. </p>

<p>So my questions are these.</p>

<p><strong>Why do the results of a MANOVA change when the order of the predictor variables is changed?</strong></p>

<p>and</p>

<p><strong>What does this mean for those of us trying to use and interpret a MANOVA?</strong></p>
"
"0.0626633989716535","0.0662266178532522","187745","<p>The code is like the following:</p>

<pre><code>&gt; d &lt;- data.frame(a=c(1,1,1,1,2,2,2,2), b=c(1,1,2,2,1,1,2,2),v=1:8)
&gt; anova(lm(v~a*b, data=d))
Analysis of Variance Table

Response: v
          Df Sum Sq Mean Sq F value   Pr(&gt;F)   
a          1     32    32.0      64 0.001324 **
b          1      8     8.0      16 0.016130 * 
a:b        1      0     0.0       0 1.000000   
Residuals  4      2     0.5                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; anova(lm(v~a, data=d))
Analysis of Variance Table

Response: v
          Df Sum Sq Mean Sq F value   Pr(&gt;F)   
a          1     32  32.000    19.2 0.004659 **
Residuals  6     10   1.667                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My question is why ""a"" has different value of F and Pr between lm(v~a<em>b) and lm(v~a)?  As far as I think anova(v~a</em>b) will test avona(v~a), anova(v~b) and anova for a and b together. </p>
"
"0.10232890201933","0.108147614087175","187776","<p>I want to test if it is worth splitting a variable into subgroups. The research question that I have is: <em>does it adds anything to the prognosis knowing if the cancer is located on the forearm or if arm is sufficient?</em></p>

<p>My plan is to use the <code>rms</code>-package and test models with <code>anova()</code>. I initially thought that I would be able to get away with a simple <code>x1 + x1:x2</code> but I don't think the <code>model.matrix</code> looks quite right. Here's an illustration of what I mean:</p>

<pre><code>library(magrittr)
n &lt;- 12
test_data &lt;- data.frame(
  x1 = factor(rep(c(1, 2),
                  each = n/2),
              labels = c(""a"", ""b"")),
  x2 = c(rep(LETTERS[1:2], each = n/4),
             rep(LETTERS[3:4], each = n/4))
)

# Now we add some code for manually generating the interaction variables
test_data %&lt;&gt;% 
  within({
    is_a_and_x2 &lt;- x2
    is_a_and_x2[x1 != ""a""] &lt;- LETTERS[1]
    is_b_and_x2 &lt;- x2 
    is_b_and_x2[x1 == ""a""] &lt;- LETTERS[3]
    is_a_and_x2 &lt;- factor(is_a_and_x2)
    is_b_and_x2 &lt;- factor(is_b_and_x2)
  })
</code></pre>

<p>The data looks as following:</p>

<pre><code>| x1 | x2 | is_b_and_x2 | is_a_and_x2 |
|----+----+-------------+-------------|
| a  | A  | C           | A           |
| a  | A  | C           | A           |
| a  | A  | C           | A           |
| a  | B  | C           | B           |
| a  | B  | C           | B           |
| a  | B  | C           | B           |
| b  | C  | C           | A           |
| b  | C  | C           | A           |
| b  | C  | C           | A           |
| b  | D  | D           | A           |
| b  | D  | D           | A           |
| b  | D  | D           | A           |
</code></pre>

<p>Now if we do a simple <code>model.matrix</code>:</p>

<pre><code>model.matrix( ~ x1 + is_a_and_x2 + is_b_and_x2, data = test_data)
</code></pre>

<p>We get the expected:</p>

<pre><code>|================================================== 
| (Intercept) | x1b | is_a_and_x2B | is_b_and_x2D 
| 1           | 0   | 0            | 0            
| 1           | 0   | 0            | 0            
| 1           | 0   | 0            | 0            
| 1           | 0   | 1            | 0            
| 1           | 0   | 1            | 0            
| 1           | 0   | 1            | 0            
| 1           | 1   | 0            | 0            
| 1           | 1   | 0            | 0            
| 1           | 1   | 0            | 0            
| 1           | 1   | 0            | 1            
| 1           | 1   | 0            | 1            
| 1           | 1   | 0            | 1            
|================================================== 
</code></pre>

<p>Now this looks like a reasonable design matrix but I can't recreate it using a simple formula:</p>

<pre><code>model.matrix( ~ x1 + x1 : x2, data = test_data)
</code></pre>

<p>Gives: </p>

<pre><code>|================================================================================ 
| (Intercept) | x1b | x1a:x2B | x1b:x2B | x1a:x2C | x1b:x2C | x1a:x2D | x1b:x2D 
| 1           | 0   | 0       | 0       | 0       | 0       | 0       | 0       
| 1           | 0   | 0       | 0       | 0       | 0       | 0       | 0       
| 1           | 0   | 0       | 0       | 0       | 0       | 0       | 0       
| 1           | 0   | 1       | 0       | 0       | 0       | 0       | 0       
| 1           | 0   | 1       | 0       | 0       | 0       | 0       | 0       
| 1           | 0   | 1       | 0       | 0       | 0       | 0       | 0       
| 1           | 1   | 0       | 0       | 0       | 1       | 0       | 0       
| 1           | 1   | 0       | 0       | 0       | 1       | 0       | 0       
| 1           | 1   | 0       | 0       | 0       | 1       | 0       | 0       
| 1           | 1   | 0       | 0       | 0       | 0       | 0       | 1       
| 1           | 1   | 0       | 0       | 0       | 0       | 0       | 1       
| 1           | 1   | 0       | 0       | 0       | 0       | 0       | 1       
|================================================================================ 
</code></pre>

<p>Using some aggressive cleaning we get:</p>

<pre><code>model.matrix( ~ x1 + x1 : x2, data = test_data) %&gt;% 
  t %&gt;% 
  .[!duplicated(.),] %&gt;% 
  t

|============================================================ 
| (Intercept) | x1b | x1a:x2B | x1b:x2B | x1b:x2C | x1b:x2D 
| 1           | 0   | 0       | 0       | 0       | 0       
| 1           | 0   | 0       | 0       | 0       | 0       
| 1           | 0   | 0       | 0       | 0       | 0       
| 1           | 0   | 1       | 0       | 0       | 0       
| 1           | 0   | 1       | 0       | 0       | 0       
| 1           | 0   | 1       | 0       | 0       | 0       
| 1           | 1   | 0       | 0       | 1       | 0       
| 1           | 1   | 0       | 0       | 1       | 0       
| 1           | 1   | 0       | 0       | 1       | 0       
| 1           | 1   | 0       | 0       | 0       | 1       
| 1           | 1   | 0       | 0       | 0       | 1       
| 1           | 1   | 0       | 0       | 0       | 1       
|============================================================ 
</code></pre>

<p><strong>My question: Is there a more convenient method than the hand-made variables or should I use a different approach?</strong></p>
"
"0.21104259321029","0.203925096773833","188361","<p>I'm pretty new in using <code>lmer</code> and be confused about different p-values in Tukey post hoc tests associated with exactly the same estimates. I built a linear mixed model with monetary contributions of human subjects as response variable and their wealth and number of children as explanatory variables. The experiment was designed in a way to contribute for future generations. I don't have repeated measurements of the same individual but some individuals played within the same group. There are several subsets and additional random factors but here I only want to consider the following model where <code>totalcontSubject</code> means contribution of a subject over the entire game, <code>poverty</code> is a factor with 2 levels (rich and poor), and <code>children</code> is a factor with 2 levels (child or noChild). Particularly I'm interested in understanding the fixed effects part of the model.</p>

<pre><code> &gt; summary(TC1)
Linear mixed model fit by REML ['lmerMod']
Formula: totalcontSubject ~ poverty * children + (1 | group_2)
   Data: data

REML criterion at convergence: 414.6

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.42955 -0.45554 -0.09361  0.45228  2.33159 

Random effects:
 Groups   Name        Variance  Std.Dev. 
 group_2  (Intercept) 8.611e-15 9.280e-08
 Residual             1.042e+02 1.021e+01
Number of obs: 58, groups:  group_2, 10

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                    16.200      3.228   5.019
povertyrich                     8.600      3.953   2.175
childrennoChild                 2.800      4.565   0.613
povertyrich:childrennoChild    -4.489      5.642  -0.796

Correlation of Fixed Effects:
            (Intr) pvrty chldrC
povertyrch  -0.816              
chldrnnChld -0.707  0.577       
pvrtyrch:C   0.572 -0.701 -0.809
</code></pre>

<p>If I interpret fixed effects of the summary table in the right way, my intercept denotes poor people with children. The estimate also corresponds to the mean value of this combination in my data. According to my calculations the difference to rich people (shown as <code>povertyrich</code>) actually shows the difference of the intercept to rich people with children, even if not explicitly mentioned by <code>povertyrich</code>. This is the first issue I'm a bit confused. A reduced model only with fixed factor poverty is significant better by <code>anova()</code> but it seems data including children are used for this evaluation.</p>

<p>If I run a Tukey post hoc test by means of my TC1 model, I get a significant difference between rich and poor. But the estimates in the summary actually include children. Estimates of intercept and slope are the means of poor people with children and the difference to rich people with children. They don't correspond to the means of poor or rich data irrespective of parenthood. </p>

<pre><code>summary(glht(TC1, linfct=mcp(povertry=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ poverty * children + (1 | 
    group_2), data = data)

Linear Hypotheses:
                 Estimate Std. Error z value Pr(&gt;|z|)  
rich - poor == 0    8.600      3.953   2.175   0.0296 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>I get even more confused when I run a Tukey post hoc test for a subset where I coded interactions in a column such as poor people with children and rich people with children. In this output I have exactly the same estimates and parameters for these categories (like in the summary shown before for rich poor people exclusively) but the p-values are different. A visual check indicates that there is a significant difference between <code>richChild</code> and <code>poorChild</code> but outputs of <code>glht</code> <code>Interak</code> shows me it is not. Also, a comparison between models <code>anova()</code> with fixed factor poverty vs. fixed factors poverty and children indicates that I can get rid of the variable children in my model. Before I do so, I would like to understand the outputs better. I also worry about the high value for Residual and the correlations in the summary table. </p>

<pre><code>&gt; summary(glht(TC1_2, linfct=mcp(Interak=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ Interak + (1 | group_2), data = data)

Linear Hypotheses:
                               Estimate Std. Error z value Pr(&gt;|z|)
poorNoChild - poorChild == 0      2.800      4.565   0.613    0.927
richChild - poorChild == 0        8.600      3.953   2.175    0.129
richNoChild - poorChild == 0      6.911      4.026   1.717    0.312
richChild - poorNoChild == 0      5.800      3.953   1.467    0.454
richNoChild - poorNoChild == 0    4.111      4.026   1.021    0.735
richNoChild - richChild == 0     -1.689      3.316  -0.509    0.956
(Adjusted p values reported -- single-step method)
</code></pre>
"
"0.1576994291291","0.12280701754386","189396","<p><strong>Introduction</strong></p>

<p>I'm doing a small pilot research in bird aggression in a colonising frontier regarding their breeding ground.</p>

<p><strong>Background</strong></p>

<p>The study was conducted over multiple years, presenting the colonising (south) and settled (north) collared flycatcher males with conspecific and pied flycatcher males. Scoring their behaviour based upon a quantifiable set aggressive actions. They have found this Island 60 years ago and are steadily spreading from one point in their breeding ground and pushing their relative pied flycatcher away from the more insect bearing territories. In previous studies it was shown that more aggressive males are at the front of such colonising action. In the north sites there is a near 100% collared and the south still has a mixed population.</p>

<p><strong>Hypothesises</strong></p>

<p>In the south location male collared flycatcher will act with higher aggression towards both species. Males will react in the north relatively more to conspecifics than they would in the south.</p>

<p><strong>Problem</strong></p>

<p>After having scored all the interactions I'm now at a loss at what test to use to present the data. Many people give different advice for <code>lm</code> or simple <code>Anova</code> etc. I have been learning R and statistics at the same time but many terms still confuse me and questions and answers found on the internet I found difficult to interpret to my data. </p>

<p><strong>Question</strong></p>

<p>What test out of the following three could be best used to show that there is or is not a statistical significance?</p>

<ul>
<li><code>Anova(lm(score~dummy_species*location))</code></li>
<li><code>summary(aov(score~dummy_species*location))</code></li>
<li><code>summary(lm(score~dummy_species*location))</code></li>
</ul>

<p><strong>Data structure</strong></p>

<p>The data is unfortunately unbalanced.</p>

<p>The amount of conspecific trials was 104 of which 77 were in the northern test area and 27 in the south.  Similarly of the 50 pied flycatcher dummy tests 36 were in the north and 14 in the south.</p>

<pre><code>'data.frame':   154 obs. of  8 variables:
 $ location        : Factor w/ 2 levels ""N"",""S"": 1 1 1 1 1 1 1 1 2 1 ...
 $ score           : int  1 4 0 1 1 8 9 9 4 3 ...
 $ dummy_species   : Factor w/ 2 levels ""CF"",""PF"": 1 1 2 2 1 1 1 1 1 2 ...
</code></pre>

<p>Location is north and south</p>

<p>Score is 0 to 7 with 7 being the highest score for aggressive behaviour. Dummy species represents conspecific and heterospecific types. So dependent variable has two two level independent factors</p>

<pre><code>model.tables(aov(scoreCF$score~scoreCF$location),""means"")

Tables of means
Grand mean
2.993506

 dummy_species 
     CF    PF
  3.529  1.88

rep 104.000 50.00

 location 
      N      S
      2.742  3.686
rep 113.000 41.000

 dummy_species:location 
         location
dummy_species N     S    
      CF   3.19  4.48
      rep 77.00 27.00
      PF   1.81  2.07
      rep 36.00 14.00

TukeyHSD(aov(score~dummy_species*location))

Tukey multiple comparisons of means
95% family-wise confidence level

Fit: aov(formula = score ~ dummy_species * location)

$dummy_species
       diff       lwr        upr     p adj
PF-CF -1.648846 -2.613568 -0.6841239 0.0009332

$location
     diff         lwr    upr     p adj
S-N 0.9440487 -0.07800284 1.9661 0.0699746

$`dummy_species:location`
           diff        lwr        upr     p adj
PF:N-CF:N -1.389250 -2.8774793 0.09898005 0.0766924
CF:S-CF:N  1.286676 -0.3619293 2.93528192 0.1824646
PF:S-CF:N -1.123377 -3.2649782 1.01822492 0.5246337
CF:S-PF:N  2.675926  0.7993571 4.55249475 0.0016744
PF:S-PF:N  0.265873 -2.0557788 2.58752484 0.9908082
PF:S-CF:S -2.410053 -4.8376320 0.01752615 0.0524523
</code></pre>

<p><strong>Results</strong></p>

<pre><code>Anova(lm(score~dummy_species*location))
Anova Table (Type II tests)

Response: score
                    Sum Sq  Df F value    Pr(&gt;F)  
dummy_species            93.91   1 11.6673 0.0008186 ***
location                 26.82   1  3.3326 0.0699100 .  
dummy_species:location    6.98   1  0.8675 0.3531437    
Residuals              1207.39 150         

summary(aov(score~dummy_species*location))*

                    Df Sum Sq Mean Sq F value   Pr(&gt;F)    
dummy_species            1   91.8   91.80  11.405 0.000933 ***
location                 1   26.8   26.82   3.333 0.069910 .  
dummy_species:location   1    7.0    6.98   0.868 0.353144    
Residuals              150 1207.4    8.05          

summary(lm(score~dummy_species*location))
Call:
lm(formula = score ~ dummy_species * location)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.4815 -2.1948 -0.8056  2.1280  6.9286 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)                 3.1948     0.3233   9.881   &lt;2e-16 ***
dummy_speciesPF            -1.3892     0.5728  -2.425   0.0165 *  
locationS                   1.2867     0.6346   2.028   0.0444 *  
dummy_speciesPF:locationS  -1.0208     1.0960  -0.931   0.3531    

Residual standard error: 2.837 on 150 degrees of freedom
Multiple R-squared:  0.09423,   Adjusted R-squared:  0.07611 
F-statistic: 5.202 on 3 and 150 DF,  p-value: 0.001909
</code></pre>

<p>Ideally given the time investment (in the field and behind the screen) I would love to have it that male aggression is likely influenced by both location and species. But only if the lm approach would be relevant.</p>
"
"0.114407190489069","0.120912708351669","191987","<p>I would need some help how to write the model formula of an ANOVA in R. 
The experiment is a 4x4 Latin Square with subtreatments. Each plot has 4 rows and 4 columns, each row and each column contain exact 1 of 4 different plant species. Within each of the 16 cells there are 5 irrigation lines with different water quantities (the subtreatment), each irrigation line has 10 individual plants (replications?). </p>

<p>I would like to compare the rows and columns for random effects and the species for fixed effects between the different cells as well as the water quantity and the interaction between water quantity and species within the cells.</p>

<p>How can I put that into an R formula?</p>

<p>EDIT:</p>

<p>This is getting very close:</p>

<pre><code>aov(response ~ row+column+species*water_quant + Error(line/(water_quant)) + row+column+species, data=dat.x))
</code></pre>

<p>I just would like to include 3 Errors:<br>
a) For error between cells<br>
b) For error between water quantities<br>
c) For error between water quantities and species interaction</p>

<p>How would I add that to the formula?</p>

<p>EDIT 2:
Okay, so this might be basically the same formula, I don't know:</p>

<pre><code>aov(l1_fake ~ row+column+species+water_quant + species:water_quant + Error(cell), data=dat.z)
</code></pre>

<p>Now I optain the following ANOVA table:</p>

<pre><code>Error: cell
      Df Sum Sq Mean Sq F value   Pr(&gt;F)    
row        3   3354    1118   2.245 0.183575    
column     3   2186     729   1.463 0.315827    
species    3  59127   19709  39.570 0.000239 ***
Residuals  6   2988     498                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: cell:water_quant
                Df Sum Sq Mean Sq F value Pr(&gt;F)
water_quant          4    777   194.3   0.424  0.791
species:water_quant 12   4414   367.8   0.803  0.646
Residuals           48  21998   458.3  
</code></pre>

<p>Now this is almost what I want, except for one thing:
Is it possible to split the remaining error term (of the within error) in two error terms, one caused by water_quant and one caused by the interaction species:water_quant?</p>
"
"0.162468952598533","0.163902517026796","192173","<p>I'm working on the similarity of categorical regression with exclusively  dummy variables and ANOVA. There are lots of references, like Gujarati &amp; Porter (2009), which have mentioned that those two are equivalent. Everything is okay when distribution of residuals is normal, variances are homogeneous and regression model is significant. My questions are there. We have a category with 3 levels (red,blue,green), a numeric variable ""allscore""( -5 &lt;= allscore &lt;= +5). I played with R and made data and ran models (regression and variance).</p>

<pre><code># creating data 
bluescore  &lt;- rnorm(n=100, mean=-1, sd=1)
redscore   &lt;- rnorm(n=100, mean=2,  sd=1)
greenscore &lt;- rnorm(n=100, mean=.1, sd=2)
for (i in 1:100) {
  if (bluescore[i] &lt; -5)  bluescore[i]  &lt;- -5
  if (bluescore[i] &gt; 5)   bluescore[i]  &lt;-  5
  if (redscore[i] &lt; -5)   redscore[i]   &lt;- -5
  if (redscore[i] &gt; 5)    redscore[i]   &lt;-  5
  if (greenscore[i] &lt; -5) greenscore[i] &lt;- -5
  if (greenscore[i] &gt; 5)  greenscore[i] &lt;-  5
}
color &lt;- as.factor(c(rep(1,100), rep(2,100), rep(3,100)))
allscore &lt;- c(bluescore, redscore, greenscore)
table &lt;- data.frame(color, allscore)
randtable &lt;- table[sample(nrow(table)),]
finaltable &lt;- data.frame(randtable$color, randtable$allscore)
colnames(finaltable) &lt;- c(""color"", ""score"")
# plot
plot(randtable$allscore ~ randtable$color, data=finaltable)
# saving data for SPSS
library(rio)
export(finaltable, ""dummy.sav"")
write.csv(finaltable, ""finaltable.csv"")
# making dummy variables
dummyred   &lt;- NULL
dummygreen &lt;- NULL
dummyblue  &lt;- NULL
for(i in 1:NROW(finaltable)) {
  if (randtable$color[i]==2) dummyred[i]=1 else dummyred[i]=0
      if (randtable$color[i]==3) dummygreen[i]=1 else dummygreen[i]=0
  if (randtable$color[i]==1) dummyblue[i]=1 else dummyblue[i]=0
}
t1 = cbind(randtable, dummyred, dummygreen)
# run regression model 
mosel.1 &lt;- lm(formula = allscore~dummyred + dummygreen + dummyblue -1, data=t1)
ttt &lt;- summary(mosel.1)
ttt

# **test of homogenity**
# Bartlettâ€™s test
bartlett.test(randtable$allscore ~ randtable$color, data=finaltable)
# Leveneâ€™s test
library(car)

leveneTest(randtable$allscore ~ randtable$color, data=finaltable)
# Fligner-Killeen test
fligner.test(randtable$allscore ~ randtable$color, data=finaltable)
# ANOVA mode
hh &lt;- aov(randtable$allscore ~ randtable$color, data=finaltable)
hh
summary(hh)
# post hoc test
TukeyHSD(hh)
</code></pre>

<p>Output would be something like this:  </p>

<p><a href=""http://i.stack.imgur.com/v0o8x.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/v0o8x.png"" alt=""enter image description here""></a></p>

<pre><code>Call:
lm(formula = allscore ~ dummyred + dummygreen + dummyblue - 1, 
    data = t1)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.0152 -0.7880  0.0043  0.8088  3.3731 

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)    
dummyred    2.02102    0.13273  15.227  &lt; 2e-16 ***
dummygreen  0.01525    0.13273   0.115    0.909    
dummyblue  -1.04294    0.13273  -7.858 7.24e-14 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.327 on 297 degrees of freedom
Multiple R-squared:  0.4971,    Adjusted R-squared:  0.4921 
F-statistic: 97.87 on 3 and 297 DF,  p-value: &lt; 2.2e-16

&gt;  
&gt; # test of homogenity
&gt; # Bartlettâ€™s test
&gt; bartlett.test(randtable$allscore ~ randtable$color, data=finaltable)

    Bartlett test of homogeneity of variances

data:  randtable$allscore by randtable$color
Bartlett's K-squared = 94.825, df = 2, p-value &lt; 2.2e-16

&gt; # Leveneâ€™s test
&gt; library(car)
&gt; 
&gt; leveneTest(randtable$allscore ~ randtable$color, data=finaltable)
Levene's Test for Homogeneity of Variance (center = median)
       Df F value    Pr(&gt;F)    
group   2  43.995 &lt; 2.2e-16 ***
      297                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; # Fligner-Killeen test
&gt; fligner.test(randtable$allscore ~ randtable$color, data=finaltable)

    Fligner-Killeen test of homogeneity of variances

data:  randtable$allscore by randtable$color
Fligner-Killeen:med chi-squared = 66.204, df = 2, p-value = 4.207e-15

&gt; # ANOVA mode
&gt; hh &lt;- aov(randtable$allscore ~ randtable$color, data=finaltable)
&gt; hh
Call:
   aov(formula = randtable$allscore ~ randtable$color, data = finaltable)

Terms:
                randtable$color Residuals
Sum of Squares         484.3572  523.2176
Deg. of Freedom               2       297

Residual standard error: 1.327281
Estimated effects may be unbalanced
&gt; summary(hh)
                 Df Sum Sq Mean Sq F value Pr(&gt;F)    
randtable$color   2  484.4  242.18   137.5 &lt;2e-16 ***
Residuals       297  523.2    1.76                   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; # post hoc test
&gt; TukeyHSD(hh)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = randtable$allscore ~ randtable$color, data = finaltable)

$`randtable$color`
         diff        lwr       upr p adj
2-1  3.063958  2.6218117  3.506104 0e+00
3-1  1.058184  0.6160382  1.500330 1e-07
3-2 -2.005774 -2.4479195 -1.563628 0e+00
</code></pre>

<ul>
<li>Is variance homogeneity check essential for regression model as assumption (because it compares means and equivalent to ANOVA)?</li>
<li>What is assumption for this regression model?</li>
<li>How can I interpret ""greendummy"" variable insignificance? Can I omit it from model? What theory support this omission? Is it means green color has no effect on scores? Is it equivalent to heterogeneity of variances?</li>
<li>How about ANOVA model, what can I say about the results?  </li>
<li>Can I remove green level from ANOVA?</li>
</ul>

<blockquote>
  <p>Gujarati, Damodar N.; Porter, Dawn C. (2009): Basic econometrics. 5th
  ed. Boston: McGraw-Hill Irwin (The McGraw-Hill series, economics).</p>
</blockquote>
"
"0.0511644510096651","0.0540738070435875","194909","<p>Stats newb here, I have to determine if two time series are really different instead of being part of the same population with noise in the samples.</p>

<p>The data is a comparison between two algorithms ctr by day, a control one (a) and an experimental (b)</p>

<p><a href=""http://i.stack.imgur.com/H2SXn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/H2SXn.png"" alt=""Plot of algorithms comparison""></a></p>

<pre><code>a      b
1 3.6162 3.6808
2 3.8967 4.0155
3 4.0669 4.2945
4 4.3680 4.4321
5 4.0558 4.2071
6 3.9234 3.9131
7 3.7467 3.9533
</code></pre>

<p>We can see the mean is bigger in b (4.070914 vs 3.953386)
But are they statistically significant?
To see that im doing ANOVA to get the p-value and compare to the alpha of 0.05 as far as i know, if it is smaller then the null hypothesis H0 of being equals is false.</p>

<p>The problem is when i do the oneway.test i get a p-value really big, am i doing something wrong?</p>

<pre><code>data
        x name
1  3.6162    a
2  3.6808    b
3  3.7467    a
4  3.8967    a
5  3.9131    b
6  3.9234    a
7  3.9533    b
8  4.0155    b
9  4.0558    a
10 4.0669    a
11 4.2071    b
12 4.2945    b
13 4.3680    a
14 4.4321    b

 oneway.test(x~name, data = data)

    One-way analysis of means (not assuming equal variances)

data:  x and name
F = 0.77477, num df = 1.00, denom df = 11.97, p-value = 0.3961 
</code></pre>

<p>Thanks a lot!</p>
"
"NaN","NaN","198035","<p>I took few courses in stats so go easy on me. I used R on a dataset of the influence of pests on sugar cane production (<a href=""http://www.stat.ufl.edu/~winner/datasets.html"" rel=""nofollow"">http://www.stat.ufl.edu/~winner/datasets.html</a>).</p>

<p>What I found with the regular ANOVA test is a high F value (40.79) and low probability value (2.27*10^-10) - which are both good.</p>

<p>But TukeyHSD analysis shows that much higher p-values when comparing variables against each other(excluding the control).</p>

<p>I think I should reject the means.Should I? What conclusions can I draw from this? What are the next steps for the study? Thanks, Matthew Mano</p>
"
"0.125326797943307","0.121415466064296","199422","<p>Following <a href=""http://www.stat.columbia.edu/~martin/W2024/R8.pdf"" rel=""nofollow"">this</a> and other sources of information on how to perform ANOVA and ANCOVA in R, I got very confused on the difference between the two on how to compute this difference. Please consider the following two examples</p>

<p><strong>ANCOVA</strong></p>

<pre><code>require(ggplot2)


&gt; anova(lm(price~table+depth, data = diamonds))  
Response: price
             Df     Sum Sq    Mean Sq  F value  Pr(&gt;F)    
depth         1 9.7323e+07 9.7323e+07   6.2202 0.01263 *  
table         1 1.4462e+10 1.4462e+10 924.2957 &lt; 2e-16 ***
Residuals 53937 8.4391e+11 1.5646e+07                     
</code></pre>

<p>and </p>

<pre><code>&gt; anova(lm(price~depth+table, data = diamonds))  
Response: price
             Df     Sum Sq    Mean Sq F value    Pr(&gt;F)    
table         1 1.3876e+10 1.3876e+10 886.825 &lt; 2.2e-16 ***
depth         1 6.8360e+08 6.8360e+08  43.691 3.882e-11 ***
Residuals 53937 8.4391e+11 1.5646e+07                      
</code></pre>

<p>The sum of squares, pvalues and other values all changed depending on the order. This lead me to think that I just performed an ANCOVA. </p>

<p><strong>ANOVA</strong></p>

<p>The example comes form <a href=""http://www.r-bloggers.com/two-way-analysis-of-variance-anova/"" rel=""nofollow"">here</a></p>

<pre><code>delivery.df = data.frame(
  Service = c(rep(""Carrier 1"", 15), rep(""Carrier 2"", 15),
    rep(""Carrier 3"", 15)),
  Destination = c(rep(c(""Office 1"", ""Office 2"", ""Office 3"",
    ""Office 4"", ""Office 5""), 9)),
  Time = c(15.23, 14.32, 14.77, 15.12, 14.05,
  15.48, 14.13, 14.46, 15.62, 14.23, 15.19, 14.67, 14.48, 15.34, 14.22,
  16.66, 16.27, 16.35, 16.93, 15.05, 16.98, 16.43, 15.95, 16.73, 15.62,
  16.53, 16.26, 15.69, 16.97, 15.37, 17.12, 16.65, 15.73, 17.77, 15.52,
  16.15, 16.86, 15.18, 17.96, 15.26, 16.36, 16.44, 14.82, 17.62, 15.04)
)


&gt; anova(lm(Time ~ Service*Destination, data = delivery.df))
Response: Time
                    Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
Service              2 23.1706 11.5853 161.5599 &lt; 2.2e-16 ***
Destination          4 17.5415  4.3854  61.1553 5.408e-14 ***
Service:Destination  8  4.1888  0.5236   7.3018 2.360e-05 ***
Residuals           30  2.1513  0.0717                       
</code></pre>

<p>and </p>

<pre><code>&gt; anova(lm(Time ~Destination*Service, data = delivery.df))
Response: Time
                    Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
Destination          4 17.5415  4.3854  61.1553 5.408e-14 ***
Service              2 23.1706 11.5853 161.5599 &lt; 2.2e-16 ***
Destination:Service  8  4.1888  0.5236   7.3018 2.360e-05 ***
Residuals           30  2.1513  0.0717                       
</code></pre>

<p>Here the values do not depend on the order suggesting that I did an ANOVA</p>

<p><strong>Questions</strong></p>

<ul>
<li>Am I right to think that I first did an ANCOVA and then an ANOVA?</li>
<li>Where did the code differ to cause one analysis to be an ANOVA and the other an ANCOVA?</li>
<li>Because, the order matters in ANCOVA, I would have thought I would be able to compute the interaction before the main effect if I desire. I tried <code>anova(lm(Time ~ Destination:Service+Destination+Service, data = delivery.df))</code> but the interaction remains at the end.</li>
</ul>
"
"0.109082976072021","0.126814318375447","199462","<p>I am attempting to perform post-hoc tests on a mixed-effects ANOVA, and I am running into issues with the typical R-based solutions, which I think stem from the fact that the groups in my between-subjects manipulation have different <em>n</em>'s.</p>

<p>This is a snippet example of what my data looks like:</p>

<pre><code>DT &lt;- as.data.table(read.csv('~/data.csv))
print(DT)

       Exp.Condition    Subject    time          y
  1:             2            1      0    0.49777778
  2:             2            1      1    0.39377778
  3:             3            2      0    0.77155556
  4:             3            2      1    0.32311111
  5:             4            3      0    0.52733333
  6:             4            3      1    0.53756565
  7:             5            4      0    0.47688889
  8:             5            4      1    0.44333333
  9:             6            5      0    0.35555556
 10:             6            5      1    0.33866667
 11:             7            6      0    0.40000000
 12:             7            6      1    0.39333333
 13:       control            7      0    0.48355556
 14:       control            7      1    0.08355556
 .........
</code></pre>

<p>The size of each group is as follows:</p>

<pre><code>DT[time==0,.N, Exp.Condition]

   Exp.Condition     N
1:             2    17
2:             3    17
3:             4    15
4:             5    16
5:             6    13
6:             7    17
7:       control    60
</code></pre>

<p>After some searching I found that a common way to run post-hoc tests on the between-subjects factors of a mixed-effects ANOVA's in R is to to use lme(), so I created this model:</p>

<pre><code>model &lt;- lme(y ~ Exp.Condition * time, random = ~ 1 | Subject/Exp.Condition, data=DT)
print(anova(model))

&gt;                   numDF denDF   F-value p-value
&gt; (Intercept)            1   148 1899.4409  &lt;.0001
&gt; Exp.Condition          6   148    5.3273  0.0001
&gt; time                   1   148   31.0840  &lt;.0001
&gt; Exp.Condition:time     6   148    3.2913  0.0045
</code></pre>

<p>and attempted this:</p>

<pre><code>summary(glht(model, linfct=mcp(Exp.Condition=""Tukey"")))
</code></pre>

<p>but got this output:</p>

<pre><code>Error in glht.matrix(model = list(modelStruct = list(reStruct = list(Exp.Condition = -9.60134899880588,  : 
  â€˜ncol(linfct)â€™ is not equal to â€˜length(coef(model))â€™
</code></pre>

<p>I did some more searching and found another potential solution for making pairwise comparisons in mixed-effects models:</p>

<pre><code>lsmeans(model, pairwise ~ Exp.Condition, adjust='tukey')
</code></pre>

<p>But that gave this output:</p>

<pre><code>Error in adjustSigma &amp;&amp; object$method == ""ML"" : 
  invalid 'x' type in 'x &amp;&amp; y'
</code></pre>

<p>I also found a manual solution in <a href=""https://gribblelab.wordpress.com/2009/03/09/repeated-measures-anova-using-r/"" rel=""nofollow"">this blog post</a> (under '1. Univariate approach using aov()'), but it seems to imply that n should be the same in each group, and I couldn't figure out a workaround. Is it possible to perform pairwise comparisons on the different levels of a between-subjects variable in a mixed-effects ANOVA with unequal <em>n</em>'s? Is there a test that I'm missing, or am I headed off into a deep rabbit hole with no end in sight?</p>
"
"0.135670238492329","0.143384833669101","200558","<p>IÂ´m trying to sort things out on the evaluation of the following data, as I have contradictory results in R vs. Matlab. IÂ´m new to R and data evaluation ;-) and I hope I will provide enogh information to answer the question.</p>

<p>Background:
The design for the experiment was a 2x3 factorial (IV1 = low;high, IV2 = low;mid;high). Subjects were randomly assigned to IV2 and had to evaluate IV1 in both conditions. So I thought of it as a repeated measure, a within factor in an ANOVA design. IV1 was randomly assigned to the position within the experiment. I thought of IV2 as the between factor in the design. The DV was a measured on a 5-point lickert scale.</p>

<p>The data basically looks like as follows:</p>

<pre><code>     x &lt;- Dat       

    str(Dat)
    'data.frame':   3958 obs. of  4 variables:
    $ Id_new : Factor w/ 93 levels ""2"",""3"",""4"",""5"",..: 1 1 2 2 3 3 4 4 5 5 ...
        $ within : Factor w/ 2 levels ""1"",""2"": 2 1 2 1 1 2 1 2 1 2 ...
    $ between: Factor w/ 3 levels ""1"",""2"",""3"": 3 3 3 3 3 3 3 3 3 3 ...
        $ DV     : num  1 2 3 1 0 4 2 0 3 2 ...

     head(Dat)
     Id_new within between DV
        2      2       3    1
        2      1       3    2
        3      2       3    3
        3      1       3    1
        4      1       3    0
        4      2       3    4
</code></pre>

<p>At first I ran an ANOVA</p>

<pre><code>    aov(DV ~ between * within + Error((Id_new)/within), data = (x))
</code></pre>

<p>which returned the following results:</p>

<pre><code>    Error: Id_new
              Df Sum Sq Mean Sq F value  Pr(&gt;F)   
    between    2  20.64  10.318   5.494 0.00561 **
    Residuals 90 169.04   1.878                   

    Error: Id_new:within
                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    
    within          1  92.48   92.48   72.84 3.19e-13 ***
    between:within  2   1.19    0.60    0.47    0.626    
    Residuals      90 114.26    1.27                     

    Error: Within
                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    
    between           2     54   27.11   15.41 2.17e-07 ***
    between:within    2     76   38.21   21.72 4.18e-10 ***
    Residuals      3768   6629    1.76                     

    n in IV(1,1) / IV (2,1) = 662
    n in IV(1,1) / IV (2,2) = 658
    n in IV(1,1) / IV (2,3) = 699
    for IV2 ist the same. So data is balanced for ""within"" but not for ""between"".
    N = 3958
</code></pre>

<p>but because I was unsure whether I did the ANOVA the right way, i aksed for someone to do the ANOVA. But his results showed way bigger F-values: 66.x (p = 7.xe-16) for ""within"" and 28.x (p = 8.xe-13) for ""between"".
Unforntunately this person doesnÂ´t know R so we coulnÂ´t figure out the problem.</p>

<p>Furthermore, beacuse of imbalance in the between grops factor I tried:</p>

<pre><code>     e &lt;- lme(DV ~ between * within, data = (x), random = ~1|Id_new/within)
     anova(e)
                    numDF denDF   F-value p-value
     (Intercept)        1  3768 10175.364  &lt;.0001
     between            2  3768    17.927  &lt;.0001
     within             1    92    52.772  &lt;.0001
     between:within     2  3768    21.932  &lt;.0001
</code></pre>

<p>and again I get different F-values...especially for ""within""
So I am not shure if what Im doing here is anyhow correct. With the use of aov() in the first place and switching to lme()... </p>

<p>Do I handle the data the wrong way?</p>

<p>Is data in long format not appropriate for the use in aov(), lme() or </p>

<p>do I need some more ordering for the data?</p>

<p>I also do not understand yet why DFs of residuals are varying...</p>

<p>Thank you very much for your interest and time in advance!</p>
"
"0.135368413338721","0.143065845879825","201934","<p>I've developed a risk score that predicts patient survival. Now I want to see whether my risk score is independent of cancer stage. I've already determined that there's no interaction between the two, and I've used ANOVA to determine that stage is a significant covariate <em>after</em> adjustment for the effect of risk score.</p>

<p>So I've got two Cox models, one that models score alone:</p>

<pre><code>&gt; summary(coxph(Surv(days_survived, vital_status) ~ score, data = metadata))
Call:
coxph(formula = Surv(days_survived, vital_status) ~ score, data = metadata)

  n= 237, number of events= 65 

        coef exp(coef) se(coef)    z Pr(&gt;|z|)    
score 0.6229    1.8643   0.1469 4.24 2.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

      exp(coef) exp(-coef) lower .95 upper .95
score     1.864     0.5364     1.398     2.486

Concordance= 0.633  (se = 0.044 )
Rsquare= 0.055   (max possible= 0.894 )
Likelihood ratio test= 13.43  on 1 df,   p=0.000248
Wald test            = 17.97  on 1 df,   p=2.239e-05
Score (logrank) test = 19.35  on 1 df,   p=1.09e-05
</code></pre>

<p>And another model that looks at both risk score and cancer stage.</p>

<pre><code>&gt; summary(coxph(Surv(days_survived, vital_status) ~ score + stage, data = metadata))
Call:
coxph(formula = Surv(days_survived, vital_status) ~ score + stage, 
    data = metadata)

  n= 236, number of events= 65 

           coef exp(coef) se(coef)     z Pr(&gt;|z|)    
score    0.7220    2.0585   0.1598 4.517 6.28e-06 ***
stageII  0.8063    2.2395   0.3196 2.522   0.0117 *  
stageIII 1.4354    4.2013   0.3318 4.326 1.52e-05 ***
stageIV  0.8258    2.2836   0.5022 1.644   0.1001    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

         exp(coef) exp(-coef) lower .95 upper .95
score        2.058     0.4858    1.5048     2.816
stageII      2.240     0.4465    1.1969     4.190
stageIII     4.201     0.2380    2.1927     8.050
stageIV      2.284     0.4379    0.8534     6.111

Concordance= 0.728  (se = 0.044 )
Rsquare= 0.129   (max possible= 0.894 )
Likelihood ratio test= 32.57  on 4 df,   p=1.465e-06
Wald test            = 33.92  on 4 df,   p=7.724e-07
Score (logrank) test = 38.63  on 4 df,   p=8.303e-08
</code></pre>

<p>According to <a href=""https://www.youtube.com/watch?v=pTm4ED8A08E"" rel=""nofollow"">this guy</a> and <a href=""https://books.google.com/books?id=hNDkBwAAQBAJ"" rel=""nofollow"">this guy</a> I should look for coefficient fold change and a decrease in the confidence interval for my risk score variable. So the coefficient changes from 0.62 to 0.72, about a 16% increase, but the confidence interval increases slightly. <strong>What kind of coefficient fold change means a covariate is confounding factor? How much of a increase in precision of the confidence interval means a variable is confounding? What if those two are conflicting?</strong></p>

<p>Thanks for any and all help!</p>
"
"0.0723574605292422","0.0764719112901873","202032","<p>I have the following data that I wish to analyze using R:</p>

<pre><code>     Resilience     PartsA       PartsB
1   4.805032           1           1
2   4.657384           1           2
3   4.703198           1           3
4   3.993497           1           4
5   4.645764           1           5
6   4.603158           1           1
7   4.811521           1           2
8   4.682717           1           3
9   4.728485           1           4
10  4.734114           1           5
11  4.532497           1           1
12  4.885308           1           2
13  4.702712           1           3
14  4.692207           1           4
15  4.740994           1           5
16  4.572724           1           1
17  4.919445           1           2
18  4.650043           1           3
19  4.761368           1           4
20  4.790507           1           5
21  4.653509           2           1
22  4.720434           2           2
23  4.833647           2           3
24  4.997706           2           4
25  4.630829           2           5
26  4.690605           2           1
27  4.681007           2           2
28  4.784369           2           3
29  4.704247           2           4
30  4.575493           2           5
31  4.553369           2           1
32  4.758170           2           2
33  4.855304           2           3
34  4.903961           2           4
35  5.002031           2           5
36  4.769658           2           1
37  4.651714           2           2
38  4.929959           2           3
39  4.648468           2           4
40  4.788978           2           5
41  4.812591           3           1
42  4.877903           3           2
43  4.928751           3           3
44  4.925799           3           4
45  4.005860           3           5
46  4.662776           3           1
47  4.896822           3           2
48  4.904109           3           3
49  4.971777           3           4
50  4.832897           3           5
</code></pre>

<p>I want to perform some kind of analysis in order to understand which Parts from A and B (which combination, such as 1 from PartsA and 3 from PartsB) cause the most deviation from the mean in the final result (material resilience).</p>

<p>From PartsA, 3 same parts are used, but from different sources (in the construction of a material), and PartsB that's used in the construction is brought in from 5 different sources.</p>

<p>Basically I want to test to see whether or not using parts from different sources creates a significant difference on the results or if all parts render same results (null hypothesis). Essentially a test for the significance that PartsA and PartsB play in the final outcome.</p>

<p>I've thought about using ANOVA, in order to analyze the variance but I am rather unsure about how to interpret the results. 
Any help would be greatly appreciated. Many thanks </p>
"
"0.114407190489069","0.0967301666813349","202193","<p>I am new at r and I tried to do a Kruskal Wallis and then a Dunn test for with data without normality neither homocedasticity (I've tried the log, a sqrt transformations but it wasn't useful). Well, my problem is that I had results that I am not able to understand. I got a small p-value (significant) with the Krustal Wallis but the Dunn test don't show any combination of groups (in my case months) different from eachother. Something is wrong but what? I work with the number of counts of a species of rockfish and I have 15 months with diferent sample size between them. I want to know which months are different from each other. The issue might be related with  the distribution of the data (skewed or not in the same direction) but no idea of how to check that and how to fix it. I also do not understand why the output of the Dunn test is the same with an alpha of 0.05 (data not shown) or 0.01. If it helps, this is my script and what I got:</p>

<p>Importing data from tab delimited file (replace stars with an appropriate object name e.g.,rockfish) </p>

<pre><code>rockfish &lt;-read.table(file.choose(), header=T)
names(rockfish)
[1] ""months"" ""counts""
</code></pre>

<p>Conducting a Kruskal-Wallis Anova (replace stars with appropriate text e.g., months,counts):</p>

<pre><code>kruskal.test(rockfish$counts~rockfish$months)

    Kruskal-Wallis rank sum test

data:  rockfish$counts by rockfish$months
Kruskal-Wallis chi-squared = 29.232, df = 13, p-value = 0.006068

library(""dunn.test"", lib.loc=""~/R/win-library/3.2"")
dunn.test(rockfish$counts, g=rockfish$months, kw=TRUE, method = ""Bonferroni"", alpha = 0.01)
  Kruskal-Wallis rank sum test

data: x and group
Kruskal-Wallis chi-squared = 29.2316, df = 13, p-value = 0.01

                           Comparison of x by group                            
                                 (Bonferroni)                                  
Col Mean-|
Row Mean |      apr13      apr14      aug13      des13      feb13      feb14
---------+------------------------------------------------------------------
   apr14 |   0.879015
         |     1.0000
         |
   aug13 |   0.832672  -0.060825
         |     1.0000     1.0000
         |
   des13 |   1.187794   0.273902   0.341798
         |     1.0000     1.0000     1.0000
         |
   feb13 |   0.771037  -0.080607  -0.022883  -0.348631
         |     1.0000     1.0000     1.0000     1.0000
         |
   feb14 |  -1.034189  -1.895217  -1.866861  -2.238531  -1.757098
         |     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   jan14 |   0.850220  -0.050744   0.010750  -0.333754   0.033296   1.892852
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   jul13 |   2.160807   1.209424   1.296702   0.963946   1.255722   3.234036
         |     1.0000     1.0000     1.0000     1.0000     1.0000     0.0555
         |
   jun13 |   1.057552   0.167672   0.231966  -0.103183   0.244048   2.082939
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   mar13 |  -0.629163  -1.587098  -1.552930  -1.964295  -1.440244   0.518166
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   may13 |   0.467668  -0.412120  -0.357916  -0.702341  -0.318816   1.493055
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   nov13 |   2.297948   1.364141   1.451952   1.128504   1.405573   3.348685
         |     0.9812     1.0000     1.0000     1.0000     1.0000     0.0369
         |
   oct13 |   1.566284   0.660026   0.733611   0.403551   0.722355   2.600473
         |     1.0000     1.0000     1.0000     1.0000     1.0000     0.4236
         |
   sep13 |   2.415561   1.502453   1.589976   1.276179   1.539858   3.440949
         |     0.7148     1.0000     1.0000     1.0000     1.0000     0.0264
Col Mean-|
Row Mean |      jan14      jul13      jun13      mar13      may13      nov13
---------+------------------------------------------------------------------
   jul13 |   1.296949
         |     1.0000
         |
   jun13 |   0.223169  -1.044262
         |     1.0000     1.0000
         |
   mar13 |  -1.580526  -3.077551  -1.793553
         |     1.0000     0.0950     1.0000
         |
   may13 |  -0.371446  -1.656015  -0.584947   1.140412
         |     1.0000     1.0000     1.0000     1.0000
         |
   nov13 |   1.453266   0.189546   1.203587   3.200510   1.802746
         |     1.0000     1.0000     1.0000     0.0624     1.0000
         |
   oct13 |   0.728849  -0.535397   0.495401   2.366799   1.085285  -0.706602
         |     1.0000     1.0000     1.0000     0.8164     1.0000     1.0000
         |
   sep13 |   1.592075   0.364092   1.346645   3.297191   1.931592   0.175774
         |     1.0000     1.0000     1.0000     0.0444     1.0000     1.0000
Col Mean-|
Row Mean |      aug13      des13      feb13      feb14      jan14      jul13      jun13      mar13      may13      nov13      oct13
---------+-------------------------------------------------------------------------------------------------------------------------
   des13 |   0.341798   0.771037  -0.080607  -0.022883  -0.348631  -1.034189  -1.895217  -1.866861  -2.238531  -1.757098   0.850220
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   feb13 |  -0.022883  -0.348631  -1.034189  -1.895217  -1.866861  -2.238531  -1.757098   0.850220  -0.050744   0.010750  -0.333754
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   feb14 |  -1.866861  -2.238531  -1.757098   0.850220  -0.050744   0.010750  -0.333754   0.033296   1.892852   2.160807   1.209424
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   jan14 |   0.010750  -0.333754   0.033296   1.892852   2.160807   1.209424   1.296702   0.963946   1.255722   3.234036   1.296949
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     0.0555     1.0000
         |
   jul13 |   1.296702   0.963946   1.255722   3.234036   1.296949   1.057552   0.167672   0.231966  -0.103183   0.244048   2.082939
         |     1.0000     1.0000     1.0000     0.0555     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   jun13 |   0.231966  -0.103183   0.244048   2.082939   0.223169  -1.044262  -0.629163  -1.587098  -1.552930  -1.964295  -1.440244
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   mar13 |  -1.552930  -1.964295  -1.440244   0.518166  -1.580526  -3.077551  -1.793553   0.467668  -0.412120  -0.357916  -0.702341
         |     1.0000     1.0000     1.0000     1.0000     1.0000     0.0950     1.0000     1.0000     1.0000     1.0000     1.0000
         |
   may13 |  -0.357916  -0.702341  -0.318816   1.493055  -0.371446  -1.656015  -0.584947   1.140412   2.297948   1.364141   1.451952
         |     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     1.0000     0.9812     1.0000     1.0000
         |
   nov13 |   1.451952   1.128504   1.405573   3.348685   1.453266   0.189546   1.203587   3.200510   1.802746   1.566284   0.660026
         |     1.0000     1.0000     1.0000     0.0369     1.0000     1.0000     1.0000     0.0624     1.0000     1.0000     1.0000
         |
   oct13 |   0.733611   0.403551   0.722355   2.600473   0.728849  -0.535397   0.495401   2.366799   1.085285  -0.706602   2.415561
         |     1.0000     1.0000     1.0000     0.4236     1.0000     1.0000     1.0000     0.8164     1.0000     1.0000     0.7148
         |
   sep13 |   1.589976   1.276179   1.539858   3.440949   1.592075   0.364092   1.346645   3.297191   1.931592   0.175774   0.862608
         |     1.0000     1.0000     1.0000     0.0264     1.0000     1.0000     1.0000     0.0444     1.0000     1.0000     1.0000
</code></pre>
"
"0.0361787302646211","0.0382359556450936","203291","<p>I have a time series for 24 individuals over the course of one month measured on day 0,2,4,6,8,11,14 and 30. Each individual was infected by a viral pathogen (on day 0) and virus titer, acute phase proteins and cytokines were measured at on each day. </p>

<p>I would like to answer the following questions with a univariate analysis for each variable:</p>

<p>1) When do biomarkers increase after exposure (day 0)?
2) How long do they stay increased for (i.e. when do they start to decrease)?
3) What are the mean values for each biomarker at each time point?</p>

<p>I think I may be able to answer this question by looking at multiple change points in a panel model but am not sure if R code has been developed for this purpose. I am also open to suggestions for other analyses (I have also already looked at using ANOVA).  </p>
"
"0.0723574605292422","0.0637265927418227","204314","<p>Sorry, I am not very good in posting code and data frames on the internet, so probably I could write this post in a better way.</p>

<p>I am trying to perform a 2 way anova with repeated measurements in the R environment.</p>

<p>I have 16 subjects, 2 different conditions for every subject (HDBR and HOWI) and 6 time phases for every condition (phasepre, phase1, phase2, phase3, phase4, phase5)</p>

<p>I found two different scripts online :</p>

<p>1) <a href=""http://www.r-bloggers.com/two-way-anova-with-repeated-measures/"" rel=""nofollow"">http://www.r-bloggers.com/two-way-anova-with-repeated-measures/</a></p>

<p>2) <a href=""http://rtutorialseries.blogspot.de/2011/02/r-tutorial-series-two-way-repeated.html"" rel=""nofollow"">http://rtutorialseries.blogspot.de/2011/02/r-tutorial-series-two-way-repeated.html</a></p>

<p>The first needs a dataframe long, this is what I did:</p>

<pre><code>Alpha1 &lt;- read.csv(""Alpha1_trasponed.csv"")
Alpha1 &lt;- Alpha1[order(Alpha1$subject), ]
    head(Alpha1)
    Alpha1.mean &lt;- aggregate(Alpha1$value,
                         by = list(Alpha1$subject, Alpha1$condition,
                                   Alpha1$phase),
                         FUN = 'mean')

colnames(Alpha1.mean) &lt;- c(""subject"",""condition"",""phase"",""value"")

Alpha1.mean &lt;- Alpha1.mean[order(Alpha1.mean$subject), ]
head(Alpha1.mean)
value.aov &lt;- with(Alpha1.mean,
                   aov(value ~ condition * phase +
                         Error(subject / (condition * phase)))
)

summary(value.aov)
</code></pre>

<p>The second one needs a dataframe wide and this is what I did:</p>

<pre><code>Alpha1 &lt;- read.csv(""Alpha1.csv"")
idata &lt;- read.csv(""idata.csv"")
idata
interestBind &lt;- cbind(Alpha1$HOWIphasepre, Alpha1$HOWIphase1, Alpha1$HOWIphase2, Alpha1$HOWIphase3, Alpha1$HOWIphase4, Alpha1$HOWIphase5, Alpha1$HDBRphasepre, Alpha1$HDBRphase1, Alpha1$HDBRphase2, Alpha1$HDBRphase3, Alpha1$HDBRphase4, Alpha1$HDBRphase5)
interestModel &lt;- lm(interestBind ~ 1)
library(car)
analysis &lt;- Anova(interestModel, idata = idata, idesign = ~Condition * Phase)
#summary(analysis)
summary(analysis, multivariate=FALSE)
</code></pre>

<p>These are my .csv files (I used x instead of real values)</p>

<p>Idata:</p>

<pre><code>Condition   Phase
HOWI    phasepre
HOWI    phase1
HOWI    phase2
HOWI    phase3
HOWI    phase4
HOWI    phase5
HDBR    phasepre
HDBR    phase1
HDBR    phase2
HDBR    phase3
HDBR    phase4
HDBR    phase5
</code></pre>

<p>Alpha1:</p>

<pre><code>Subject HOWIphasepre    HOWIphase1  HOWIphase2  HOWIphase3  HOWIphase4  

HOWIphase5  HDBRphasepre
1   x   x   x   x   x   x   x
2   x   x   x   x   x   x   x
3   x   x   x   x   x   x   x
4   x   x   x   x   x   x   x
5   x   x   x   x   x   x   x
6   x   x   x   x   x   x   x
7   x   x   x   x   x   x   x
8   x   x   x   x   x   x   x
9   x   x   x   x   x   x   x
10  x   x   x   x   x   x   x
11  x   x   x   x   x   x   x
12  x   x   x   x   x   x   x
13  x   x   x   x   x   x   x
14  x   x   x   x   x   x   x
15  x   x   x   x   x   x   x
16  x   x   x   x   x   x   x
</code></pre>

<p>Alpha1_trasponed:</p>

<pre><code>condition   phase   value   subject
    HOWI    phasepre    x   1
    HOWI    phase1  x   1
    HOWI    phase2  x   1
    HOWI    phase3  x   1
    HOWI    phase4  x   1
    HOWI    phase5  x   1
    HDBR    phasepre    x   1
    HDBR    phase1  x   1
    HDBR    phase2  x   1
    HDBR    phase3  x   1
    HDBR    phase4  x   1
    HDBR    phase5  x   1
    HOWI    phasepre    x   2
    HOWI    phase1  x   2
    HOWI    phase2  x   2
    HOWI    phase3  x   2
    HOWI    phase4  x   2
    HOWI    phase5  x   2
    HDBR    phasepre    x   2
    HDBR    phase1  x   2
    HDBR    phase2  x   2
    HDBR    phase3  x   2
    HDBR    phase4  x   2
    HDBR    phase5  x   2
    HOWI    phasepre    x   3
    HOWI    phase1  x   3
    HOWI    phase2  x   3
    HOWI    phase3  x   3
    HOWI    phase4  x   3
    HOWI    phase5  x   3
    HDBR    phasepre    x   3
    HDBR    phase1  x   3
    HDBR    phase2  x   3
    HDBR    phase3  x   3
    HDBR    phase4  x   3
    HDBR    phase5  x   3
    HOWI    phasepre    x   4
    HOWI    phase1  x   4
    HOWI    phase2  x   4
    HOWI    phase3  x   4
    HOWI    phase4  x   4
    HOWI    phase5  x   4
    HDBR    phasepre    x   4
    HDBR    phase1  x   4
    HDBR    phase2  x   4
    HDBR    phase3  x   4
    HDBR    phase4  x   4
    HDBR    phase5  x   4
    HOWI    phasepre    x   5
    HOWI    phase1  x   5
    HOWI    phase2  x   5
    HOWI    phase3  x   5
    HOWI    phase4  x   5
    HOWI    phase5  x   5
    HDBR    phasepre    x   5
    HDBR    phase1  x   5
    HDBR    phase2  x   5
    HDBR    phase3  x   5
    HDBR    phase4  x   5
    HDBR    phase5  x   5
    HOWI    phasepre    x   6
    HOWI    phase1  x   6
    HOWI    phase2  x   6
    HOWI    phase3  x   6
    HOWI    phase4  x   6
    HOWI    phase5  x   6
    HDBR    phasepre    x   6
    HDBR    phase1  x   6
    HDBR    phase2  x   6
    HDBR    phase3  x   6
    HDBR    phase4  x   6
    HDBR    phase5  x   6
    HOWI    phasepre    x   7
    HOWI    phase1  x   7
    HOWI    phase2  x   7
    HOWI    phase3  x   7
    HOWI    phase4  x   7
    HOWI    phase5  x   7
    HDBR    phasepre    x   7
    HDBR    phase1  x   7
    HDBR    phase2  x   7
    HDBR    phase3  x   7
    HDBR    phase4  x   7
    HDBR    phase5  x   7
    HOWI    phasepre    x   8
    HOWI    phase1  x   8
    HOWI    phase2  x   8
    HOWI    phase3  x   8
    HOWI    phase4  x   8
    HOWI    phase5  x   8
    HDBR    phasepre    x   8
    HDBR    phase1  x   8
    HDBR    phase2  x   8
    HDBR    phase3  x   8
    HDBR    phase4  x   8
    HDBR    phase5  x   8
    HOWI    phasepre    x   9
    HOWI    phase1  x   9
    HOWI    phase2  x   9
    HOWI    phase3  x   9
    HOWI    phase4  x   9
    HOWI    phase5  x   9
    HDBR    phasepre    x   9
    HDBR    phase1  x   9
    HDBR    phase2  x   9
    HDBR    phase3  x   9
    HDBR    phase4  x   9
    HDBR    phase5  x   9
    HOWI    phasepre    x   10
    HOWI    phase1  x   10
    HOWI    phase2  x   10
    HOWI    phase3  x   10
    HOWI    phase4  x   10
    HOWI    phase5  x   10
    HDBR    phasepre    x   10
    HDBR    phase1  x   10
    HDBR    phase2  x   10
    HDBR    phase3  x   10
    HDBR    phase4  x   10
    HDBR    phase5  x   10
    HOWI    phasepre    x   11
    HOWI    phase1  x   11
    HOWI    phase2  x   11
    HOWI    phase3  x   11
    HOWI    phase4  x   11
    HOWI    phase5  x   11
    HDBR    phasepre    x   11
    HDBR    phase1  x   11
    HDBR    phase2  x   11
    HDBR    phase3  x   11
    HDBR    phase4  x   11
    HDBR    phase5  x   11
    HOWI    phasepre    x   12
    HOWI    phase1  x   12
    HOWI    phase2  x   12
    HOWI    phase3  x   12
    HOWI    phase4  x   12
    HOWI    phase5  x   12
    HDBR    phasepre    x   12
    HDBR    phase1  x   12
    HDBR    phase2  x   12
    HDBR    phase3  x   12
    HDBR    phase4  x   12
    HDBR    phase5  x   12
    HOWI    phasepre    x   13
    HOWI    phase1  x   13
    HOWI    phase2  x   13
    HOWI    phase3  x   13
    HOWI    phase4  x   13
    HOWI    phase5  x   13
    HDBR    phasepre    x   13
    HDBR    phase1  x   13
    HDBR    phase2  x   13
    HDBR    phase3  x   13
    HDBR    phase4  x   13
    HDBR    phase5  x   13
    HOWI    phasepre    x   14
    HOWI    phase1  x   14
    HOWI    phase2  x   14
    HOWI    phase3  x   14
    HOWI    phase4  x   14
    HOWI    phase5  x   14
    HDBR    phasepre    x   14
    HDBR    phase1  x   14
    HDBR    phase2  x   14
    HDBR    phase3  x   14
    HDBR    phase4  x   14
    HDBR    phase5  x   14
    HOWI    phasepre    x   15
    HOWI    phase1  x   15
    HOWI    phase2  x   15
    HOWI    phase3  x   15
    HOWI    phase4  x   15
    HOWI    phase5  x   15
    HDBR    phasepre    x   15
    HDBR    phase1  x   15
    HDBR    phase2  x   15
    HDBR    phase3  x   15
    HDBR    phase4  x   15
    HDBR    phase5  x   15
    HOWI    phasepre    x   16
    HOWI    phase1  x   16
    HOWI    phase2  x   16
    HOWI    phase3  x   16
    HOWI    phase4  x   16
    HOWI    phase5  x   16
    HDBR    phasepre    x   16
    HDBR    phase1  x   16
    HDBR    phase2  x   16
    HDBR    phase3  x   16
    HDBR    phase4  x   16
    HDBR    phase5  x   16
</code></pre>

<p>The problem is that I got different p values!</p>

<p>I suppose that is my fault... Which script is correct?</p>

<p>Where did I made mistakes?</p>

<p>Do anybody have a good ""two way anova with repeated measurements"" script?</p>

<p>Thank you in advance, I hope my doubt is clear!</p>

<p>Dorian</p>
"
"0.0886194286901087","0.0780488176318078","204839","<p>Further to <a href=""http://stats.stackexchange.com/questions/200460/multiple-imputation-for-predictive-analysis-using-mice-package-in-r"">my prior question</a> on multivariable adjustment in regression models, using covariates which are available only for some cases, I have researched in some detail the main methods for limited dependent variables, including Heckman correction or tobit models. However, I fear that they do not apply to my issue, which has more to do with <strong>limited independent variables</strong>.</p>

<p>In particular, I am giving below an example of the dataset and the possible analysis in R (disregard the overfitting, it's just to make an example, my actual dataset has at least 10,000 cases):</p>

<pre><code>dep &lt;- c(8, 9, 21, -3, 4, 6, 9, 10, 8, 9, 11, 39, 91, 51, 38, 28, 21)
cov1 &lt;- c(68, 58, 42, 19, 39, 49, 29, 38, 25, 22, 19, 36, 39,90, 105, 73, 25)
cov2 &lt;- c(0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
cov3 &lt;- c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1)
cov4 &lt;- c(NA, NA, NA, NA, NA, NA, 56, 33, 45, 44, 56, 49, 36, 39, 40, 41, 59)
cov5 &lt;- c(NA, NA, NA, NA, NA, NA, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0)
mydata &lt;- data.frame(cbind(dep, cov1, cov2, cov3, cov4, cov5)) 
mydata

reg1 &lt;- lm(dep ~ cov1 + cov2, data = mydata, na.action = na.omit)
anova(reg1)
summary(reg1)

reg2 &lt;- lm(dep ~ cov1 + cov2 + cov3 + cov4 + cov5, data = mydata, na.action = na.omit)
anova(reg2)
summary(reg2)
</code></pre>

<p>What should I do to best adjust for covariates cov1, cov2, cov3, cov4 and cov5, having dep as dependent variable, given that cov4 and cov5 are available only for patients with cov3 = 1? </p>

<p>Should I discard all cases with cov3 = 0, or should I conduct two separate analyses and then pool the regression coefficients according to their standard error? Or is there any other more reasonable approach?</p>

<p>Unfortunately I did not find anything meaningful searching Google, Google Scholar, or PubMed:</p>

<p><a href=""https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable"" rel=""nofollow"">https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable</a></p>

<p><a href=""https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable"" rel=""nofollow"">https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable</a></p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable</a>*</p>

<p>To further clarify what is at stake, this is my real problem: I want to create a clinical prediction score (to predict prognosis and future quality of life) for patients undergoing myocardial perfusion imaging (a non-invasive cardiac test used in subjects with or at risk for coronary artery disease). The imaging test follows immediately an exercise stress test in fit patients, and a pharmacologic stress test in those who are not fit. The latter test is worse than the former, and does not provide several important prognostic features (eg maximum heart rate, or workload), so I must include exercise test variables in the multivariable model. But if I do so, I lose more than 1000 patients who only underwent a pharmacologic stress test.</p>
"
"0.114407190489069","0.084638895846168","205151","<p>I am using a generalized Linear Mixed-Effects model to look at the effects of different treatments on a density of trichomes.</p>

<p>The model is :</p>

<pre><code>fitPoisson = glmer(Count_trichomes ~ Treatment1*Treatment2*Treatment3 + 
                         (1 | Block/Code) + offset(log(Length)), family=poisson(), data=dataset)
</code></pre>

<p>Treatment 1 and 2 has 2 levels (0 and 1) and Treatment 3 has 3 levels (0,1,2). Block accounts for the replicates and Code, for each individual. Length is in cm.</p>

<p>An anova(fitPoisson) told me that treatments 1 and 3 are significant and that there is no interactions. What I want now is to know what the density is for each level of treatments.</p>

<p>So I used a lsmeans to look at the differences : </p>

<pre><code>    &gt; lsmeans(fitPoisson, ~ Treatment1)

     Treatment1   lsmean         SE df asymp.LCL asymp.UCL
     0           5.309106 0.06113705 NA  5.189280  5.428933
     1           5.471452 0.06114033 NA  5.351619  5.591285

     Results are averaged over the levels of: Treatment2, Treatment3
     Results are given on the log (not the response) scale. 
     Confidence level used: 0.95
</code></pre>

<p>I can see that the density of level 0 is lower than the density of level 1, but I dont understand what are the units used. It doesn't seems like it is for trichomes/cm, since the mean for level 0 is 107 trichomes/cm and the mean for level 1 is 131 trichomes/cm (calculated in excel).</p>

<p>When I transform back from the log scale, it gives me : </p>

<pre><code>    &gt; summary(lsmeans(fitPoisson, ~ Treatment1), type = ""response"")

     Treatment1   rate       SE df asymp.LCL asymp.UCL
     0           202.1694 12.36004 NA  179.3393  227.9058
     1           237.8053 14.53949 NA  210.9496  268.0799

    Results are averaged over the levels of: Treatment2, Treatment3 
    Confidence level used: 0.95 
    Intervals are back-transformed from the log scale 
</code></pre>

<p>Which is still far from the means I found in excel.</p>

<p>Maybe I just don't understand the information lsmeans is giving me, or I am not using the right function.</p>
"
"NaN","NaN","205227","<p>I'm a little new to R and I haven't done stats in a while. I know a one way ANOVA is the same as a linear regression, but is there a difference between a two way ANOVA and a linear regression with two covariates? And if they are different I'm not sure which one I performed. Below is my sample code:</p>

<pre><code>data.frame[[""Acute""]] = factor(data.frame[[""Acute""]])
data.frame[[""Frequency""]] = factor(data.frame[[""Frequency""]])
DishMortalityVsTime.Total.Acute.Freq = aov(Dish.Mortality ~ Time * Acute * Frequency, data=data.frame)
summary(DishMortalityVsTime.Total.Acute.Freq)
</code></pre>

<p>and the output</p>

<pre><code>                      Df Sum Sq Mean Sq F value               Pr(&gt;F)    
Days                   1  1.352  1.3524  65.189  0.00000000000000429 ***
Acute                  2  5.885  2.9423 141.822 &lt; 0.0000000000000002 ***
Frequency              3  0.539  0.1795   8.653  0.00001279126504853 ***
Days:Acute             2  1.672  0.8361  40.302 &lt; 0.0000000000000002 ***
Days:Frequency         3  0.050  0.0165   0.796                0.496    
Acute:Frequency        6  0.787  0.1311   6.320  0.00000192315201011 ***
Days:Acute:Frequency   6  0.038  0.0064   0.309                0.932    
Residuals            552 11.452  0.0207 
</code></pre>

<p>Any help would be appreciated, Thanks!</p>
"
"0.0886194286901087","0.0936585811581694","206251","<p>I am a noob in R, and have been breaking my head for the past three hours over the pairwise.t.test in R. My data frame is structured as given below. My data frame has the age of flight of bees and the duration which they fly for. I want to check the pairwise difference in duration across days. </p>

<blockquote>
  <p>head(a3)</p>
</blockquote>

<pre><code>      age dur.
    1   3  343
    2   2  640
    3   1  333
    4   3  253 
    5   3   66
    6   3  686
</code></pre>

<p>When I do one-way ANOVA and follow it up with a TukeyHSD, the console prints the values for adjusted p-values.</p>

<pre><code>&gt; an3=with(a3, aov(dur.~factor(age)))
&gt; TukeyHSD(an3)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = dur. ~ factor(age))

$`factor(age)`
             diff           lwr        upr     p adj
2-1    -16.666667 -1215.7557943 1182.42246 1.0000000
3-1   -105.717444 -1158.4831001  947.04821 1.0000000
4-1   -144.603500 -1198.2056799  908.99868 1.0000000
5-1   -194.405800 -1244.3222876  855.51069 1.0000000
6-1   -223.139535 -1273.5866854  827.30762 1.0000000
7-1   -143.568966 -1190.9244371  903.78651 1.0000000
8-1   -208.927536 -1254.8670531  837.01198 1.0000000
9-1   -255.583333 -1304.7863200  793.61965 0.9999999
</code></pre>

<p>I want to carry out pairwise.t.test with the Bonferroni p-value adjustment on the data frame. When I go ahead and do that, I get the following output.</p>

<blockquote>
  <p>pairwise.t.test(a3<code>$dur.,a3</code>$age,p.adjust.method='bonf')</p>
</blockquote>

<pre><code>        Pairwise comparisons using t tests with pooled SD 

data:  a3$dur. and a3$age 

   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
2  - - - - - - - - - -  -  -  -  -  -  -  -  -  -  -  -  -  -  - 
3  - - - - - - - - - -  -  -  -  -  -  -  -  -  -  -  -  -  -  - 
4  - - - - - - - - - -  -  -  -  -  -  -  -  -  -  -  -  -  -  - 
5  - - - - - - - - - -  -  -  -  -  -  -  -  -  -  -  -  -  -  - 
6  - - - - - - - - - -  -  -  -  -  -  -  -  -  -  -  -  -  -  - 
7  - - - - - - - - - -  -  -  -  -  -  -  -  -  -  -  -  -  -  - 
8  - - - - - - - - - -  -  -  -  -  -  -  -  -  -  -  -  -  -  -
...
P value adjustment method: bonferroni 
</code></pre>

<p>I do not know why I get blanks for all fields, when in all examples I have seen online, the result is a lower triangular matrix. Has anyone faced similar problems? Are there any solutions for the same?</p>

<p>Cheers</p>

<p>P.S. I have tried the example given in R documentation and it seems to work fine. It's not working for my data set though. I have also ensured that the example data and my data are both data frames.</p>
"
"0.140119619801808","0.148087219439773","206894","<p>Background on what I am doing...</p>

<p>I have 31 years of Landsat satellite data, and have extracted spectral reluctance and calculated 13 unique spectral based vegetation metrics for a series of 16 field plots have, which were then split into two groups, deciduous dominant and spruce dominant.  My goal is to identify the change in spectral seperability between the two groups over time, following a fire in 1994.  </p>

<p>To identify seperability, I have used both ANOVA anova(lm()) and t.test() in R, with the goal of identifying statistically significant differences in the mean spectral reflectance of the two groups. </p>

<p>For one iteration of my analysis, I end up with 31 years years of t tests for 113 metrics, or 403 t tests.  I have run several iterations, using different classification criteria for vegetation type dominance.  So a TON of t tests.  I have all of the T statistics organized in a tidy .csv.   </p>

<p>My t test is set up as follows: </p>

<p>t.test(data1_1984, data2_1984, paired=FALSE, var.equal=FALSE)</p>

<p>where data1_1984 is a list of spectral reflectance values for the deciduous group, and data2 is spectral reflectance for the spruce group. </p>

<p>I fundamentally understand that I should look at each resulting t statistic, to determine whether t statistic is > critical t value, as determined by the df of the sample.  Given that the same set of samples is used for each test, I assumed that there would be one common df, with one common critical value used to interpret all tests.  But when I look at the df of the resulting tests, they are highly variable, non-integer numbers.   </p>

<p>An example of my t_test t statistic results : </p>

<p><a href=""http://i.stack.imgur.com/w55sM.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/w55sM.jpg"" alt=""enter image description here""></a></p>

<p>as well as an example of the df for each respective record in the t_test table:</p>

<p><a href=""http://i.stack.imgur.com/55z1c.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/55z1c.jpg"" alt=""enter image description here""></a></p>

<p>So my question is: </p>

<p>1) Is there some optional argument that I am not setting correctly which is causing the df to be calculated for each individual test</p>

<p>or</p>

<p>2) If the variability in df is to be expected, a) could someone explain how these values are calculated, and b) can anyone suggest an automated way to analyze some 1200 t test results?? My original method was to use conditional formatting in Excel, to simply highlight any cells that were >= the critical value... </p>
"
"0.0738495239084239","0.0936585811581694","207851","<p>I am trying to get an ANOVA table for a mixed model in R with nested design fit with package <code>lme4</code>, function <code>lmer()</code> in R. When I obtain the ANOVA table, R includes only the sums of squares for the fixed factor, and does not include anything for the random factor. Beyond R usage I'm questioning the underlying statistical process at work here - shouldn't random factors be included on an ANOVA table for a mixed model?</p>

<p>For example, I fit the model as:  </p>

<pre><code>lmer(response ~ A + (1 | A:B))  # where A is fixed, B is random, and B is nested within A. 
</code></pre>

<p>I use the code below and just obtain output for the A fixed factor:  </p>

<pre><code>anova(lmer(response ~ A + (1 | A:D)))

   Sum Sq  Mean Sq NumDF DenDF F.value    Pr(&gt;F)    
A 0.19812 0.099058     2     9  33.437 6.818e-05 ***
</code></pre>
"
"0.169853904989375","0.171707398789977","208273","<p>I want to analyse the effect of different treatment types (<code>control, treatment1, ..., treatment4</code>) on the surface of specimens made of certain materials (<code>plastic, metal</code>). The undamaged area of the surface is measured <code>before</code> and <code>after</code> the treatment.</p>

<p>According to this design I specified a mixed model using lme4 as follows:</p>

<pre><code>require(""lme4"")
data &lt;- read.csv(""http://pastebin.com/raw/G4D8dh1f"")
mm1  &lt;- lmer(undamaged_area ~ time*material*treatment + (1|specimen_id), data)
</code></pre>

<p><strong>Questions:</strong> </p>

<ol>
<li><p>Is the mixed model the optimal choice in this case? I found some hints that an ANCOVA (something like <code>lm(undamaged_area_after ~ material*treatment + undamaged_area_before, data)</code>) might be an alternative approach.</p>

<p>A closer look on the diagnostic plots of the mixed model makes me very suspicious:</p>

<pre><code>plot(mm1); require(""lattice""); qqmath(mm1)  
</code></pre>

<p><a href=""http://i.stack.imgur.com/LDxEY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LDxEY.png"" alt=""diagnostic plots""></a></p></li>
<li><p>Does the plots actually indicate a violation of model assumptions? Does the strange pattern come from a misspecification of the model? </p></li>
</ol>

<p><strong>Progress after <a href=""http://stats.stackexchange.com/a/208463/112794"">donlelek's answer</a> (=mixed model not required):</strong></p>

<p>Just to be clear: The treatments were measured in different pieces of metal/plastic. So every piece is exactly measured twice - before and after the treatment. Thus, we are aiming for the ANOVA on damage, I guess. I had the impression to lose informations by just substracting the pre-post values. I did further research in the literature (with my limited knowledge in statistics). But according to <a href=""https://pdfs.semanticscholar.org/b764/e331525ec9ba814b51ee890aea7f663e175d.pdf"" rel=""nofollow"">""Pretest-posttest designs and measurement of
change""</a> the use of such gain scores seems to be ok:</p>

<blockquote>
  <p>""First, contrary to the
  traditional misconception, the reliability of gain scores is high in many practical situations, particularly when the pre- and posttest scores do not have equal variance and equal reliability.""</p>
</blockquote>

<p>so we have the following model:</p>

<pre><code>library(tidyr)
library(dplyr)

data &lt;- read.csv(""http://pastebin.com/raw/G4D8dh1f"")    
data_wide  &lt;- data %&gt;% 
  spread(time, undamaged_area) %&gt;% 
  separate(specimen_id, c(""mat"", ""id"", ""tx"")) %&gt;% 
  mutate(damage = before - after, 
         unique_id = paste(mat, id, sep = ""_"")) %&gt;% 
  select(-mat, -tx, -id)

# model for full factorial with replications
mm2  &lt;- lm(damage ~ material * treatment , data = data_wide)
</code></pre>

<p>The variance problem still remains. Confirmed by Levene's test:</p>

<pre><code>library(car)
leveneTest(damage ~ material * treatment, data_wide)

# Levene's Test for Homogeneity of Variance (center = median)
#       Df F value    Pr(&gt;F)    
# group  9  4.8619 2.646e-05 ***
#       90                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1    
</code></pre>

<p>Following the link suggested by donlelek I found different approaches for <a href=""http://stats.stackexchange.com/questions/91872/alternatives-to-one-way-anova-for-heteroskedastic-data/91881#91881"">anovas with heteroskedastic data</a>. I tried to stabilize the variance by using log-transformation. Then Levene's test says that heterogeneity of the variance diappears:</p>

<pre><code>data_wide &lt;- within(data_wide, log_damage &lt;- log(damage+1))
leveneTest(log_damage ~ material * treatment, data_wide)

# Levene's Test for Homogeneity of Variance (center = median)
#       Df F value Pr(&gt;F)
# group  9  0.6916 0.7147
#       90
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1   
</code></pre>

<p>The diagnostic plots seems not to be as weird as the previous ones (see <a href=""http://stats.stackexchange.com/a/208463/112794"">donlelek's answer</a>):</p>

<pre><code>mm3 &lt;- lm(log_damage ~ material * treatment, data_wide)
plot(fitted(mm3), residuals(mm3, type = ""pearson""))
qqnorm(residuals(mm3, type = ""pearson""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/NDTAj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NDTAj.png"" alt=""enter image description here""></a></p>

<p>The anova table gives the following output:</p>

<pre><code>anova(mm3)

# Analysis of Variance Table
#
# Response: log_damage
#                    Df Sum Sq Mean Sq F value    Pr(&gt;F)    
# material            1  0.436  0.4362  0.7462      0.39    
# treatment           4 83.652 20.9129 35.7786 &lt; 2.2e-16 ***
# material:treatment  4 20.213  5.0532  8.6452 5.966e-06 ***
# Residuals          90 52.606  0.5845                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Just for double checking:</p>

<p>The <code>Anova</code> function from <code>car</code> package offers an option for heteroscedasticity correction. Interestingly, this function generates a roughly similar table for the ""non-transformed"" <code>mm2</code>:</p>

<pre><code>Anova(mm2, white.adjust=TRUE)

# Analysis of Deviance Table (Type II tests)
# 
# Response: damage
#                    Df       F    Pr(&gt;F)    
# material            1  1.4251    0.2357    
# treatment           4 28.2422 3.329e-15 ***
# material:treatment  4  9.5739 1.701e-06 ***
# Residuals          90                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p><strong>New questions:</strong> </p>

<p>This double check gives me more confidence in the results. But do you think that the log-transformation is a reasonable approach? Can I trust the model now?</p>
"
"0.0626633989716535","0.0662266178532522","209081","<p>I'm using R studio to analyse my data:
I have a dataset of zebrafish with independent variables: treatment and sex as in <a href=""http://i.stack.imgur.com/2AwS5.png"" rel=""nofollow"">this attached graph</a>. I'm testing the percentage of 600s that the fish spend in the half of the tank containing conspecifics. </p>

<p>I have performed a 2-way anova to test the difference between the variables and the interaction, but I cant work out how to test each variable against the null hypothesis that the fish spend 50% of their time with the conspecifics. </p>

<p>I performed my anova using this script:</p>

<pre><code>sp1 &lt;- lm(percent ~ treatment + sex + treatment*sex, social.prefss)
anova(sp1)
</code></pre>

<p>I'm sorry if I've explained this poorly, I'm only an undergrad and havent got a great grasp of statistics!</p>

<p>EDIT:
I've just tried to do a one sample t-test by using this script:</p>

<pre><code>per&lt;-social.prefss$percent
treat&lt;-social.prefss$treatment
sex&lt;-social.prefss$sex
x&lt;-social.prefss$percent
t.test(x,mu=50
</code></pre>

<p>and this is what came out:</p>

<blockquote>
  <p>One Sample t-test</p>
  
  <p>data:  x</p>
  
  <p>t = 9.2808, df = 39, p-value = 2.028e-11</p>
  
  <p>alternative hypothesis: true mean is not equal to 50</p>
  
  <p>95 percent confidence interval:</p>
  
  <p>65.36873 73.93460</p>
  
  <p>sample estimates:</p>
  
  <p>mean of x </p>
  
  <p>69.65167 </p>
</blockquote>
"
"0.0511644510096651","0.0540738070435875","211541","<p>I am having trouble figuring out the correct degrees of freedom of residuals for an experiment I performed:</p>

<ul>
<li>within-subject experiment</li>
<li>2x2 study design: let's call the conditions cond_A and cond_B for simplicity. </li>
<li>each condition only has two values (yes or no).</li>
<li>one dependent variable (time <em>t</em>)</li>
<li>24 participants </li>
<li>each experienced every combination of cond_A and cond_B several times (how often each combination was experienced differs).</li>
<li>487 measurements total.</li>
</ul>

<p>I was following <a href=""http://ron.dotsch.org/degrees-of-freedom/"" rel=""nofollow"">http://ron.dotsch.org/degrees-of-freedom/</a> who explained that </p>

<blockquote>
  <p><em>df2 = df_total â€“ df_subjects â€“ df_factor</em></p>
</blockquote>

<p>which in my case would be 487 observations - (24 participants - 1) - (4 levels - 1) = 461.</p>

<p>However, if I run my calculations in R I get:</p>

<pre><code>&gt; summary(aov(t ~ (cond_A * cond_B) + Error(participant/(cond_A * cond_B)), data=anova_data))
Error: Within
                 Df Sum Sq Mean Sq F value  Pr(&gt;F)   
cond_A            1    832     832   0.550 0.45863   
cond_B            1  11540   11540   7.629 0.00596 **
cond_A:cond_B     1    757     757   0.501 0.47954   
Residuals        479 724556    1513
</code></pre>

<p>Which tells me the DOF of residuals is 479.</p>

<p>The data set contains ALL measurements, not just averages per participant/condition.</p>

<p>I read here <a href=""http://sherifsoliman.com/2014/12/10/ANOVA_in_R/"" rel=""nofollow"">http://sherifsoliman.com/2014/12/10/ANOVA_in_R/</a> that <code>aov</code> may not always be trusted, so I also used ezANOVA to double-check my results:</p>

<pre><code>&gt; ezANOVA(anova_data, dv=t, wid=participant, within= .(cond_A, cond_B), detailed=TRUE)
          Effect DFn DFd          SSn       SSd            F            p p&lt;.05          ges
1    (Intercept)   1  23 580516.15076 72972.543 182.97116929 1.954366e-12     * 0.8395005806
2        cond_A    1  23   3574.12976  7452.345  11.03075343 2.973947e-03     * 0.0311988224
3        cond_B    1  23     45.81809 22679.055   0.04646649 8.312302e-01       0.0004126587
4 cond_A:cond_B    1  23    218.33558  7881.692   0.63713709 4.329143e-01       0.0019633793
</code></pre>

<p>Now, not only are the F and p values different (which is bad enough, and I'd be happy on opinions which test to trust), but it also suggests a dof of 23.</p>

<p>What is the correct DOF of residuals and why are they calculated differently in these cases?</p>

<p>Thank you very much!</p>
"
"0.214118878229906","0.213722381148002","212068","<p>I have a question about what the difference is in how Stata and R compute ANOVAs. I have run exactly the same ANOVA in both softwares, but curiously get a different F-statistics for one of the predictors. IÂ´m not too familiar with Stata, but as far as I understood it, I do a Type 2 SS ANOVA for both.  </p>

<p>To understand my output, this is my model:<br>
Outcome variable is a continuous variable called <code>vertrauen</code> (=trust)<br>
predictor 1 is a 2-level factor called <code>trustee</code> in R and <code>Goodguy</code> in Stata<br>
predictor 2 is also a 2 level factor called <code>Group</code> in R and <code>uw</code> in Stata.</p>

<p>This is the R output:  </p>

<pre> 
>m2-lm(vertrauen~trustee*Group,data=RTG.UWD.short.50)
> Anova(m2,type=""2"")
>Anova Table (Type II tests)

>Response: vertrauen
>              Sum Sq Df F value    Pr(>F)      
>trustee       2.4928  1 24.5497    1.367e-05 ***  
>Group         0.0030  1  0.0292    0.8651      
>trustee:Group 0.1137  1  1.1200    0.2963      
>Residuals     4.0617 40                        
>  
>Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1  
>
</pre>

<p>This is the Stata output:</p>

<pre>
. anova vertrauen uw Goodguy uw#Goodguy

                         Number of obs =         44    R-squared     =  0.3912
                         Root MSE      =    .318658    Adj R-squared =  0.3455

                  Source | Partial SS         df         MS        F    Prob>F
              -----------+----------------------------------------------------
                   Model |  2.6095358          3   .86984526      8.57  0.0002
                         |
                      uw |  .00296733          1   .00296733      0.03  0.8651
                 Goodguy |  1.2981586          1   1.2981586     12.78  0.0009
              uw#Goodguy |  .11373073          1   .11373073      1.12  0.2963
                         |
                Residual |  4.0617062         40   .10154266  
              -----------+----------------------------------------------------
                   Total |   6.671242         43   .15514516  
</pre>  

<p>As you can see, the F-statistics for the Group (<code>UW</code>) main effect and for the Group (<code>UW</code>) x trustee (<code>Goodguy</code>) interaction are the same, but for the <code>trustee</code> (<code>Goodguy</code>) main effect they differ. In R itÂ´s almost twice as high as in Stata. I tried to change the order of the predictor and the reference levels, but that didnÂ´t change my R output.  </p>

<p>Does anyone know what causes the difference in the F-statistic here? IÂ´m really puzzled about it. I expected it to be the same.  </p>

<p>Here is the Stata output without the interaction:  </p>

<pre>
. anova vertrauen uw Goodguy

                         Number of obs =         44    R-squared     =  0.3741
                         Root MSE      =    .319124    Adj R-squared =  0.3436

                  Source | Partial SS         df         MS        F    Prob>F
              -----------+----------------------------------------------------
                   Model |   2.495805          2   1.2479025     12.25  0.0001
                         |
                      uw |  .00296733          1   .00296733      0.03  0.8653
                 Goodguy |  2.4928377          1   2.4928377     24.48  0.0000
                         |
                Residual |   4.175437         41   .10183993  
              -----------+----------------------------------------------------
                   Total |   6.671242         43   .15514516  
</pre>  

<p>And here is the R output without the interaction:</p>

<pre>
> m2.4-lm(vertrauen~trustee+Group,data=RTG.UWD.short.50)
> Anova(m2.4)
Anova Table (Type II tests)

Response: vertrauen
          Sum Sq Df F value    Pr(>F)    
trustee   2.4928  1 24.4780 1.328e-05 ***
Group     0.0030  1  0.0291    0.8653    
Residuals 4.1754 41                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
> 
</pre>  

<p>ItÂ´s the same, thus it has to do something with how the two softwares incorporate the interaction term.  </p>

<p>I also tried to manually compute the interaction term and found something interesting:  </p>

<p>Here is the R output:</p>

<pre>
RTG.UWD.short.50$interaction-as.numeric(RTG.UWD.short.50$trustee)*as.numeric(RTG.UWD.short.50$Group)
> m2.7 Anova(m2.7)
Anova Table (Type II tests)

Response: vertrauen
            Sum Sq Df F value    Pr(>F)    
trustee     1.2982  1 12.7844 0.0009316 ***
Group       0.0030  1  0.0292 0.8651282    
interaction 0.1137  1  1.1200 0.2962617    
Residuals   4.0617 40                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
> 
</pre>  

<p>And here is the Stata output:</p>

<pre>
. gen interaction=uw*Goodguy

. anova vertrauen uw Goodguy interaction

                         Number of obs =         44    R-squared     =  0.3912
                         Root MSE      =    .318658    Adj R-squared =  0.3455

                  Source | Partial SS         df         MS        F    Prob>F
             ------------+----------------------------------------------------
                   Model |  2.6095358          3   .86984526      8.57  0.0002
                         |
                      uw |   .0399785          1    .0399785      0.39  0.5339
                 Goodguy |  2.3984067          1   2.3984067     23.62  0.0000
             interaction |  .11373073          1   .11373073      1.12  0.2963
                         |
                Residual |  4.0617062         40   .10154266  
             ------------+----------------------------------------------------
                   Total |   6.671242         43   .15514516
</pre>

<p>Thus it seems that there is a difference in how R/ Stata computes the interactions. The R output of the manually computed interaction matches the automatically computed interaction output in Stata.  </p>

<p>And finally the descriptives from R:</p>

<pre>
> describe(RTG.UWD.short.50$vertrauen)
RTG.UWD.short.50$vertrauen 
      n missing  unique    Info    Mean   
     44       0      43       1  0.5046
> describe(RTG.UWD.short.50$Group)
RTG.UWD.short.50$Group 
      n missing  unique 
     44       0       2 

1 (34, 77%), 2 (10, 23%) 
> describe(RTG.UWD.short.50$trustee)
RTG.UWD.short.50$trustee 
      n missing  unique 
     44       0       2 

bad (22, 50%), good (22, 50%) 
</pre>

<p>and from Stata:</p>

<pre>
. sum vertrauen uw Goodguy

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
   vertrauen |         44    .5045969    .3938847    .000998          1
          uw |         44    .2272727    .4239151          0          1
     Goodguy |         44          .5    .5057805          0          1
</pre>
"
"0.194936753283675","0.192286650133743","212533","<p>Third Update: Output from suggested code:</p>

<pre><code>&gt; fit1&lt;- lm(cbind(Risk_Pct, PCT_Stocks_MF_1) ~ US_Born, regdata)
&gt; summary(fit1)
Response Risk_Pct :
</code></pre>

<p>Call:
lm(formula = Risk_Pct ~ US_Born, data = regdata)</p>

<p>Residuals:
   Min     1Q Median     3Q    Max 
-6.527 -1.319  0.681  1.681 91.681 </p>

<p>Coefficients:
            Estimate Std. Error t value Pr(>|t|)<br>
(Intercept)  6.26699    0.05146 121.777   &lt;2e-16 ***</p>

<h2>US_Born      0.05210    0.03113   1.673   0.0943 .</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Residual standard error: 2.289 on 5041 degrees of freedom</p>

<p>(10957 observations deleted due to missingness)
Multiple R-squared:  0.0005551, Adjusted R-squared:  0.0003569 
F-statistic:   2.8 on 1 and 5041 DF,  p-value: 0.09432</p>

<p>Response PCT_Stocks_MF_1 :</p>

<p>Call:
lm(formula = PCT_Stocks_MF_1 ~ US_Born, data = regdata)</p>

<p>Residuals:
   Min     1Q Median     3Q    Max 
-229.2 -155.2 -130.2 -130.2  812.7 </p>

<p>Coefficients:
            Estimate Std. Error t value Pr(>|t|)<br>
(Intercept)  241.195      7.577  31.831   &lt;2e-16 ***</p>

<h2>US_Born      -10.976      4.584  -2.394   0.0167 *</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Residual standard error: 337 on 5041 degrees of freedom
  (10957 observations deleted due to missingness)
Multiple R-squared:  0.001136,  Adjusted R-squared:  0.0009379 
F-statistic: 5.733 on 1 and 5041 DF,  p-value: 0.01668</p>

<p><strong>I think a problem with this may be that in the raw data for ""PCT_Stocks_MF_1 there codes for ""don't know"" and ""refused to answer"" are given values of 998/999 which brings up the mean and messes with the results since no one actually has 998% of their assets invested in stocks</strong></p>

<hr>

<p>Second Update: Sample of 100 rows of the data</p>

<pre><code> regdata &lt;- data.frame(HHID, US_Born, Born_In_US, Risk_Pct, PCT_Stocks_MF_1, Stocks_Pct, age, gender, Own_Home, Marital_Status, current_job_status,Total_Wealth,stock_market_expectations )
</code></pre>

<blockquote>
<pre><code>head(regdata, n = 100)
</code></pre>
</blockquote>

<pre><code>      HHID US_Born Born_In_US Risk_Pct PCT_Stocks_MF_1 Stocks_Pct
1   010004       1         NA        7              50         NA
2   010013       1         NA       10              NA         50
3   010038       1         NA        8              40         65
4   010038       1         NA        8              40         NA
5   010038       1         NA        8              40         85
6   010050       1         NA        5             998        998
7   010050       1         NA        5             998         NA
8   010325       1         NA        2             998         NA
9   010397       1         NA        3              75         NA
10  010397       1         NA        3              75        100
11  010433       1         NA        5              NA         NA
12  010451       5         NA        6              NA         50
13  010451       5         NA        6              NA         50
14  010451       5         NA        2              NA         NA
15  010481       1         NA        5             998         NA
16  010481       1         NA        5             998         NA
17  010481       1         NA        5             998         NA
18  010565       1         NA        7              NA         NA
19  010565       1         NA        7              NA         NA
20  010565       1         NA        7              NA         NA
21  010577       1         NA        5              NA         NA
22  010592       5         NA        8              NA         NA
23  010592       5         NA        8              NA         NA
24  010611       1         NA        0              NA         NA
25  010645       1         NA        4              NA         NA
26  010645       1         NA        4              NA         NA
27  010648       1         NA        3              NA         NA
28  010648       1         NA        5              NA         NA
29  010696       5         NA       NA              NA         NA
30  010769       1         NA        4              60         NA
31  010769       1         NA        4              60         50
32  010773       5         NA        2              50         NA
33  010773       5         NA        7              50          0
34  010773       5         NA        7              50         NA
35  010893       1         NA        6              NA         30
36  010893       1         NA        6              NA        100
37  010893       1         NA        6              NA         NA
38  010893       1         NA        6              NA         NA
39  010893       1         NA        6              NA         NA
40  010962       1         NA        5              NA         NA
41  010989       1         NA        4              NA         NA
42  010989       1         NA        4              NA         NA
43  011067       1         NA        8             998         NA
44  011256       1         NA        5              NA         NA
45  011332       1         NA        2              NA         NA
46  011341       1         NA        5              80        998
47  011341       1         NA        5              80         NA
48  011377       1         NA        5              NA        998
49  011377       1         NA        5              NA         NA
50  011377       1         NA        5              NA         NA
51  011377       1         NA        5              NA         NA
52  011378       5         NA        5              NA         NA
53  011466       1         NA        6              NA         NA
54  011620       1         NA        8             100        100
55  011620       1         NA        8             100         NA
56  011620       1         NA        8             100         NA
57  011620       1         NA        8             100         60
58  011620       1         NA        8             100         60
59  011626       5         NA        3              NA         NA
60  011626       5         NA        3              NA        998
61  011802       1         NA       10              NA         NA
62  011802       1         NA       10              NA         NA
63  011802       1         NA        8              NA        100
64  011802       1         NA        8              NA         NA
65  011802       1         NA        8              NA         NA
66  011810       1         NA       10              NA        999
67  011810       1         NA       10              NA         NA
68  011841       1         NA       10              NA        998
69  011881       5         NA        5              NA        100
70  011881       5         NA        5              NA        998
71  011902       1         NA        0              NA        999
72  011902       1         NA        0              NA         NA
73  011902       1         NA        0              NA         NA
74  011911       1         NA        7              NA        998
75  011911       1         NA        7              NA         NA
76  011911       1         NA        7              NA        998
77  011911       1         NA        7              NA         NA
78  011936       1         NA        6              NA          0
79  011936       1         NA        6              NA          0
80  011936       1         NA        6              NA         NA
81  011983       1         NA        8              NA         25
82  011983       1         NA        8              NA         NA
83  011999       1         NA        7              NA         NA
84  012005       1         NA        5              NA         NA
85  012005       1         NA        5              NA        998
86  012009       1         NA        3             998         50
87  012009       5         NA        6             998        998
88  012009       5         NA        6             998         NA
89  012033       1         NA        5             998         NA
90  012033       1         NA        0              NA          0
91  012104       1         NA        5              NA        998
92  012104       1         NA        5              NA          0
93  012112       1         NA        6              NA          0
94  012112       1         NA        6              NA        998
95  012161       1         NA        2              NA         NA
96  012161       1         NA        2              NA         NA
97  012161       1         NA        2              NA         NA
98  012166       1         NA       NA              NA         NA
99  012166       1         NA       NA              NA         NA
100 012166       1         NA        6              NA         NA
age gender Own_Home Marital_Status current_job_status
1    68      2        1              5                  5
2    76      1        2              4                  5
3    71      2        1              1                  1
4    71      2        1              1                  1
5    71      2        1              1                  1
6    73      2        1              5                  1
7    73      2        1              5                  1
8    75      2        2              5                  5
9    73      1        1              5                  1
10   73      1        1              5                  1
11   80      2        3              5                  5
12   76      2        1              1                  5
13   76      2        1              1                  5
14   74      1        1              1                  5
15   74      2        2              1                  5
16   74      2        2              1                  5
17   74      2        2              1                  5
18   82      1        7              5                  7
19   82      1        7              5                  7
20   82      1        7              5                  7
21   75      2        1              5                  5
22   77      2        1              6                  4
23   77      2        1              6                  4
24   73      2        2              6                  5
25   67      2        2              5                  5
26   67      2        2              5                  5
27   73      1        1              1                  5
28   74      2        1              1                  5
29   73      2        1              4                  5
30   58      2        1              5                  6
31   58      2        1              5                  6
32   77      2        1              1                  5
33   86      1        1              1                  5
34   86      1        1              1                  5
35   74      2        1              4                  1
36   74      2        1              4                  1
37   74      2        1              4                  1
38   74      2        1              4                  1
39   74      2        1              4                  1
40   74      2        1              5                  1
41   73      2        1              1                  5
42   73      2        1              1                  5
43   72      1        1              1                  5
44   73      1        2              1                  1
45   74      2        1              4                  5
46   75      2        1              5                  2
47   75      2        1              5                  2
48   68      2        1              1                  3
49   68      2        1              1                  3
50   68      2        1              1                  3
51   68      2        1              1                  3
52   77      1       NA              3                  5
53   62      2        2              6                  4
54   73      1       NA              1                  5
55   73      1       NA              1                  5
56   55      2       NA              1                  1
57   55      2       NA              1                  1
58   55      2       NA              1                  1
59   65      2        1              1                  6
60   65      2        1              1                  6
61   80      1        1              1                  5
62   80      1        1              1                  5
63   58      2        1              1                  1
64   58      2        1              1                  1
65   58      2        1              1                  1
66   76      1        1              1                  5
67   76      1        1              1                  5
68   78      1        1              1                  5
69   69      2        1              4                  6
70   69      2        1              4                  6
71   66      2        2              6                  5
72   66      2        2              6                  5
73   66      2        2              6                  5
74   75      1        1              1                  1
75   75      1        1              1                  1
76   75      1        1              1                  1
77   75      1        1              1                  1
78   75      2       NA              4                  5
79   75      2       NA              4                  5
80   75      2       NA              4                  5
81   71      1        1              1                  5
82   71      1        1              1                  5
83   77      1        1              1                  5
84   81      2        2              5                  1
85   81      2        2              5                  1
86   74      2        1              1                  6
87   87      1        1              1                  5
88   87      1        1              1                  5
89   73      1       NA              3                  5
90   73      2       NA              3                  5
91   79      2        1              5                  6
92   79      2        1              5                  6
93   66      2        2              5                  5
94   66      2        2              5                  5
95   76      2        7              5                  5
96   76      2        7              5                  5
97   76      2        7              5                  5
98   76      1        1              1                  5
99   76      1        1              1                  5
100  69      2        1              1                  5
    Total_Wealth stock_market_expectations
1         901001                        NA
2           2000                        NA
3             NA                        NA
4             NA                        NA
5             NA                        NA
6        1224150                        75
7        1224150                        75
8          20000                        NA
9        1390000                        30
10       1390000                        30
11        196000                        50
12        194000                         5
13        194000                         5
14            NA                        80
15            NA                        10
16            NA                        10
17            NA                        10
18         51500                        NA
19         51500                        NA
20         51500                        NA
21         -2955                        NA
22        372000                        NA
23        372000                        NA
24          -925                        NA
25          4400                       100
26          4400                       100
27        303000                        20
28            NA                        50
29            NA                        NA
30            NA                        NA
31            NA                        NA
32            NA                        NA
33        304000                        NA
34        304000                        NA
35       1701000                        NA
36       1701000                        NA
37       1701000                        NA
38       1701000                        NA
39       1701000                        NA
40        -80500                         0
41            NA                        50
42            NA                        50
43       1072000                        60
44         22050                        NA
45            NA                        NA
46        135740                        NA
47        135740                        NA
48            NA                        NA
49            NA                        NA
50            NA                        NA
51            NA                        NA
52        112500                        50
53             0                        NA
54        400012                       100
55        400012                       100
56            NA                        NA
57            NA                        NA
58            NA                        NA
59        153700                        NA
60        153700                        NA
61        106000                        40
62        106000                        40
63            NA                       100
64            NA                       100
65            NA                       100
66            NA                        80
67            NA                        80
68         20000                        60
69            NA                        NA
70            NA                        NA
71            NA                        NA
72            NA                        NA
73            NA                        NA
74        353000                        50
75        353000                        50
76        353000                        50
77        353000                        50
78            NA                        30
79            NA                        30
80            NA                        30
81        771500                        NA
82        771500                        NA
83        100000                        75
84         42200                        40
85         42200                        40
86       1760500                        50
87            NA                        NA
88            NA                        NA
89         49000                        NA
90        -10500                        NA
91         17500                        NA
92         17500                        NA
93         54000                        NA
94         54000                        NA
95        -31000                        10
96        -31000                        10
97        -31000                        10
98            NA                        NA
99            NA                        NA
100           NA                        50    
</code></pre>

<hr>

<p>Update:I tried to run a ""MANOVA"" but am not entirely sure if I did this correct. 
Risk_Pct is the raw data risk measure 1 and PCT_Stocks_MF_1 is the raw data risk measure 2 </p>

<pre><code>&gt; y&lt;-cbind(Risk_Pct, PCT_Stocks_MF_1)

&gt; fit.manova&lt;-manova(y ~ US_Born)
 summary(fit.manova, test = ""Pillai"")
        Df    Pillai approx F num Df
US_Born      1 0.0016159   4.0788      2
Residuals 5041                          
      den Df  Pr(&gt;F)  
US_Born     5040 0.01698 *
Residuals                 
---
Signif. codes:  
  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1
  â€˜ â€™ 1
&gt; summary(fit.manova, test = ""Roy"")
            Df       Roy approx F num Df
US_Born      1 0.0016186   4.0788      2
Residuals 5041                          
          den Df  Pr(&gt;F)  
US_Born     5040 0.01698 *
Residuals                 
---
Signif. codes:  
  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1
</code></pre>

<h2>Also, if I have some variables I want to hold constant (such as ""wealth"") is it possible to do that in MANOVA? If so, how?</h2>

<p>Original post:
I'm trying to determine if immigration status is a significant determinant of risk preferences. In order to do this, I'm using two different measures of risk.</p>

<p>The first measure of risk has participants rate their level of risk on a scale from 0-10. I then divide that scale into 5 categories of risk: ""no"", ""low"", ""some"", ""high"", and ""substantial"" risk tolerance. 
  The second measure of risk is based on the percentage(0-100%) of assets participants invest in stocks. They are also divided into the same 5 categories of risk: ""no"", ""low"", ""some"", ""high"", and ""substantial"" risk tolerance.
In my preliminary analysis I found that using the first measure of risk I found that for both immigrants and natives the category with the most respondents was the ""high"" risk tolerance. Meanwhile, with the second measure of risk I found that for both immigrants and natives the category with the most respondents was ""substantial"" risk tolerance.
 How can I determine if there is a significant difference between these two measures of risk? I knew typically to find a significant difference you would use a t-test, but since they variables are categorical I can't find the mean. Even if I used the original data, not my groupings into risk tolerance data, the first variable is on  a scale from 0-10 and the second variable is on a scale of 0-100 so the means are totally different. I am using ""R"" to do my analysis.</p>

<p>If it helps, here is a table of the the breakdown of risk measure 1, by immigration status and risk tolerance group:</p>

<pre><code>             No     Low     Some    High    Substantial     Total
Native      4.5     3.54    31.74   52.11   8.11            100
Immigrant   9.34    3.67    23.19   47.71   16.08           99.99
</code></pre>

<p>And the same table, for risk measure 2:</p>

<pre><code>            No      Low     Some     High   Substantial DK/RF   Total
Native      0.08%   1.81%   6.51%   17.38%  57.64%      16.58%  100.00%
Immigrant   0.00%   4.32%   14.59%  20.27%  48.65%      12.16%  99.99%
</code></pre>

<ul>
<li>I don't think the tables show correctly when printed like this, so the attached image is of these two tables <a href=""http://i.stack.imgur.com/HsK74.png"" rel=""nofollow"">enter image description here</a>
Thanks so much for any help on this situation</li>
</ul>
"
"0.0808981002113217","0.0854981960070962","212592","<p>I am trying to find out whether it is true that variation in expenditure is greater, for more narrow subsets. e.g. is it more likely that an individual buys an orange instead of an apple, than it is for him to buy a potato instead of an apple.</p>

<p>So far i have used var() and f-tests(while i believe f-tests aren't really applicable to my problem either). I would however like to use more applicable models if that is possible. Every test or class of models i can find however, are meant to compare two samples' variances. While I want to test for each observation, whether they are more likely to go for alternatives, the more constrained the subset is.</p>

<p>I have a dataframe, with variables on multiple levels (drinks, soda, types of coke), and two time points of observation. As far as I am aware, two time points are not enough for a time-series analysis, and a repeated measures anova is not really applicable either.</p>

<p>Could anyone point me to a resource or name of a test / type of model that could help me solve the problem? or, are var() and var.test() the only tools that are applicable?</p>
"
"0.0808981002113217","0.0854981960070962","213301","<p>I have two groups, patients &amp; controls, and a dependent variable ""performance"".</p>

<p>I want to compare the means of the two groups, but after removing the effect of two confounding continuous variables: age &amp; scholarity.</p>

<p>I've thought of two possible ways:</p>

<p>1) manually removing the effects, and performing a t-test:</p>

<pre><code>lm1 = lm(performance ~ age + scholarity)
t.test(residuals(lm1) ~ group)
</code></pre>

<p>2) performing ANCOVA:</p>

<pre><code>lm1 = lm(performance ~ age + scholarity + group)
Anova(lm1)
</code></pre>

<p>Since the p-values are almost exactly the same, I assume the two methods are equal, but my question is which is the best way to report my results. Specifically:</p>

<ol>
<li>Should I plot data <strong>after</strong> having removed the covariates or before?</li>
<li>If ""after"" how do I get them on the original scale after having removed the covariates effect? (i.e. <code>residuals(lm1)</code> returns data centered on zero)</li>
<li>Which effect size should I report? Cohen's d or eta squared, or something else?</li>
<li>How do I compute confidence intervals? </li>
</ol>
"
"0.150002077605491","0.175219161012616","213592","<p>I prepared a mixed 2x2 ANOVA design analysis both in SPSS and in R. The SPSS script is correct, but in R script there is a mistake somewhere. To test that I generated artificial data from a normal distribution to simulate the interaction between two independent variables. There were no difference between the results in main effects, but results of simple effects analysis do not match when comparing between levels of variable which introduced repeated measures (GROUP A: PRE vs POST ; GROUP B: PRE vs POST).</p>

<p>I would be very thankful if you can help me.
The code below will do everything for you.</p>

<p><strong>Here is the code in R which:</strong>
- generates the data
- calculates mixed ANOVA
- prepares data to csv format to import to SPSS
- performs simple effect analysis (there is probably a mistake)</p>

<pre><code>N &lt;- 100
absMean &lt;- 1
sdCustom &lt;- 5

grA_pre &lt;- data.frame(ID = seq(N), lvl=rnorm(N, mean=absMean, sd=sdCustom), group=factor('A'), stage = factor('pre'))
grA_post &lt;- data.frame(ID = seq(N), lvl=rnorm(N, mean=-absMean, sd=sdCustom), group=factor('A'), stage = factor('post'))
grB_pre &lt;- data.frame(ID = seq(N+1,2*N), lvl=rnorm(N, mean=-absMean, sd=sdCustom), group=factor('B'), stage = factor('pre'))
grB_post &lt;- data.frame(ID = seq(N+1,2*N), lvl=rnorm(N, mean=absMean, sd=sdCustom), group=factor('B'), stage = factor('post'))

gr &lt;- rbind(grA_pre, grA_post, grB_pre, grB_post)
names(gr)
head(gr)

# save set to .csv to import to SPSS 
grSPSS &lt;- reshape(data = gr, timevar = ""stage"", idvar = c(""ID"", ""group""), direction = ""wide"")

write.csv2(grSPSS, file = ""sample2.csv"")

library(ggplot2)
library(plyr)
library(ez)

print(""Omnibus mixed ANOVA - main effects and interactions"")
ezPlot(data = gr, wid = ID, dv = lvl, between = group, within = stage, type = ""III"", x = group, split = stage, x_lab = ""Group"", y_lab = ""Level of experience"")
ezANOVA(data = gr, wid = ID, dv = lvl, between = group, within = stage, detailed = TRUE, type = ""III"")
#ezStats(data = gr, wid = ID, dv = lvl, between = group, within = stage, type = ""III"")


print(""Simple main effects analysis"")
dataA &lt;- subset(gr, group == ""A"" )
dataB &lt;- subset(gr, group == ""B"" )
dataPRE &lt;- subset(gr, stage == ""pre"" )
dataPOST &lt;- subset(gr, stage == ""post"" )

print(""GROUP = A: PRE vs POST"")
simpleEffControlANOVA &lt;- ezANOVA(data = dataA, dv = lvl, wid = ID, within = stage, detailed = TRUE, type = ""III"" )
print(simpleEffControlANOVA)

print(""GROUP = B: PRE vs POST"")
simpleEffControlANOVA &lt;- ezANOVA(data = dataB, dv = lvl, wid = ID, within = stage, detailed = TRUE, type = ""III"" )
print(simpleEffControlANOVA)

print(""STAGE = PRE: A vs B"")
simpleEffControlANOVA &lt;- ezANOVA(data = dataPRE, dv = lvl, wid = ID, between = group, detailed = TRUE, type = ""III"" )
print(simpleEffControlANOVA)

print(""STAGE = POST: A vs B"")
simpleEffControlANOVA &lt;- ezANOVA(data = dataPOST, dv = lvl, wid = ID, between = group, detailed = TRUE, type = ""III"" )
print(simpleEffControlANOVA)
</code></pre>

<p><strong>Here is the code for SPSS Syntax which:</strong>
- calculates everything on imported data, generated by R</p>

<pre><code>DATASET ACTIVATE DataSet1.
GLM lvl.pre lvl.post BY group
  /WSFACTOR=stage 2 Polynomial 
  /METHOD=SSTYPE(3)
  /POSTHOC=group(TUKEY T3) 
  /EMMEANS=TABLES(group) COMPARE ADJ(BONFERRONI)
  /EMMEANS=TABLES(stage) COMPARE ADJ(BONFERRONI)
  /EMMEANS=TABLES(group*stage) COMPARE(group)
  /EMMEANS=TABLES(group*stage) COMPARE(stage)
  /PLOT=PROFILE(group*stage)
  /PRINT=DESCRIPTIVE ETASQ OPOWER HOMOGENEITY 
  /CRITERIA=ALPHA(.05)
  /WSDESIGN=stage 
  /DESIGN=group.
</code></pre>
"
"0.0895377892669139","0.0946291623262781","214099","<p>I am a bit confused with a two-way ANOVA that I want to perform in R. I have a mixed linear model that I want to perform an ANOVA for:</p>

<pre><code>fit = lme(response ~ Factor1 * Factor2, data = my_data);
anova(fit);
</code></pre>

<p>Then, I could perform a post-hoc test with Tukey correction as follows:</p>

<pre><code>d = lsmeans(fit, pairwise~Factor1 *  Factor2,
            adjust = ""Tukey"");
</code></pre>

<p>In statistics class, you check that you have to check for normal distribution of <code>response</code> (in each <code>interaction(Factor1, Factor2)</code> and for variance homogeneity for a ""classic"" ANOVA.</p>

<p>Say, I have already checked for normality using q-q plots etc. and am now interested in the homogeneity of variance, e.g. as follows:</p>

<pre><code>bartlett.test(response ~ interaction(Factor1, Factor2),
              data = my_data);
</code></pre>

<p>This gives me something like this:</p>

<pre><code>    Bartlett test of homogeneity of variances

Bartlett's K-squared = 32.186, df = 8, p-value = 8.626e-05
</code></pre>

<p>I have few (10-14) points per <code>interaction(Factor1, Factor2)</code>.</p>

<p>If I understand correctly what I read around ANOVA, this I cannot perform an ANOVA for my data. Is this right? Is there a way around this?</p>

<p>Sorry if this is an easy duplicate of something else, but I could not deduce the answer from the responses I have found here so far.</p>
"
"0.10232890201933","0.0811107105653813","214572","<p>I have some data from a recent experiment. Two factors, each had 2 levels, resulting in 4 conditions. In each condition there were 12 participants, so 48 participants in total. Each participant finished only one condition and generated some short time series data. </p>

<p><strong>My questions</strong>:</p>

<ol>
<li>Is there a way to specify from which point the data in one condition becomes statistically different from the data in another condition?</li>
<li>A traditional way in my field is to calculate a mean of the whole time series data for each participants and then submit the means to ANOVA. However, just using means may shadow any difference in the data's pattern. For example, data in one condition start high and get lower, whereas data in another condition start low and get higher. Their patterns are different but the means may be statistically equal. Is there a way to compare the data in terms of the pattern? </li>
</ol>

<p>Any suggestion will be highly appreciated!</p>
"
"0.0626633989716535","0.0441510785688348","214613","<p>I am trying to make a simple linear regression to see if my variable ""totalssq"" has an influence on my variable ""hadsa"". (my data is ""dstatss"") Both are quantitative.
I made a model with lm() and tested it with an ANOVA.
Here are the outputs :</p>

<pre><code>    Analysis of Variance Table

    Response: dstatss$hadsa
             Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
      dstatss$totalssq  1  88.272  88.272  5.6848 0.03623 *
     Residuals        11 170.805  15.528                  
     ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The p-value is significant, but i don't know what it should mean to me ?
Does it means that there is a significant relationship between my variables ? I don't really know how to interpret this.</p>
"
"0.0957199230302734","0.101162829777814","214772","<p>I have data consisting of two measures per subject (variable score, measured at Day 0, and Day1 per each subject). 
There are two groups, and each subject belongs to one of them (variable Group).
I would like to know if there are differences in the score values between these two groups of subjects.
I tried aov and lme in R, and I got completely different results. </p>

<p>Why is the Group:Day effect significant using aov, and it is not significant using lme?
Here is the code I used:</p>

<pre><code>&gt; demo1.aov &lt;- aov(score ~ (Group*Day) + Error(Subject/Day), data = demo1)
&gt; summary(demo1.aov)


Error: Subject
       Df Sum Sq Mean Sq F value Pr(&gt;F)
       Group      1    0.2   0.198    0.15    0.7
       Residuals 50   65.9   1.318               

Error: Subject:Day
       Df Sum Sq Mean Sq F value Pr(&gt;F)  
Day        1    0.3    0.34    0.45  0.507  
Group:Day  1    4.0    4.02    5.29  0.026 *
Residuals 50   38.0    0.76                 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt; demo1.lme&lt;-lme(score ~ (Group*Day), random = ~ 1|Subject/Day, data = demo1)
&gt; anova(demo1.lme)

    numDF denDF F-value p-value
    (Intercept)     1    50   0.394   0.533
    Group           1    50   0.078   0.781
    Day             1    50   1.225   0.274
    Group:Day       1    50   2.064   0.157
</code></pre>
"
"0.0511644510096651","0.0270369035217938","215155","<p>I'm testing if/how the two factors <code>C</code> and <code>V</code> affect the dependent variables <code>fric</code> and <code>asp</code>. I have the following code, but I don't know how to interpret the output, for example, does <code>***</code> mean the factor significantly affects dependent variables? But what do <code>Pillai</code>  and <code>approx F</code> mean? What does  <code>1.954e-06 ***</code> mean? </p>

<pre><code>&gt; Y = cbind(data$fric, data$asp)
&gt; fit &lt;- manova(Y~data$C*data$V)
&gt; summary(fit, test=""Pillai"")

               Df        Pillai approx F num Df den Df    Pr(&gt;F)    
data$C          3       1.85017   699.75      6    340 &lt; 2.2e-16 ***
data$V          3       0.24953     8.08      6    340 3.601e-08 ***
data$C:data$V   9       0.31746     3.56     18    340 1.954e-06 ***
</code></pre>
"
"0.104438998286089","0.0993399267798783","217722","<p>As an exercise in understanding, I am wanting to partition the variance in what is effectively a repeated-measures ANOVA (I think) with two measurements - 'pre' and 'post' treatment. I want to equate this with the variances (between and within) that an intra-class correlation is based on.</p>

<p>I have written a simulation and the sums of squares (and mean squares) equal that given by an ANOVA. But that's not the same as the between and within variances for an ICC (obviously I am not correctly understanding how this is formulated). I know that:</p>

<p>rho(ICC) = sigma^2 (between)/sigma^2(total) </p>

<p>where sigma^2(total) = sigma^2(between) + sigma^2(within) </p>

<p>As I have specified the bivariate correlation above, we can therefore work out:</p>

<p>sigma^2 (between) = rho * sigma^2(total) </p>

<p>= sb2 in the simulation</p>

<p>How do I manually calculate SSB so that it equals sb2?</p>

<p>Thanks.</p>

<pre><code># Simulate n pre and post (mean = 20, sd = 5 for first and mean = 20, sd = 5 for second). Correlation between paired data = 0.7. 
rm(list=ls())
n &lt;- 1000 # subjects
k &lt;- 2 # groups
sd1 &lt;- 5 # sd of pre
sd2 &lt;- 5 # sd of post
rho &lt;- 0.7 # correlation between pre and post
data &lt;-rmvnorm(n,mean=c(20,20),sigma=matrix(c(sd1^2,rho*sd1*sd2,rho*sd1*sd2,sd2^2),2,2))     # Covariance matrix
data &lt;- as.data.frame(data)
names(data)[1] &lt;- ""pre""
names(data)[2] &lt;- ""post""
head(data)
ICC(data)
cor.test(data$pre,data$post)

(GM &lt;- mean(c(data$pre,data$post))) # Grand Mean
(mean_pre &lt;- mean(data$pre)) # Mean of pre- group
(mean_post &lt;- mean(data$post)) # Mean of post- group

# SST - Squared diff of each observation from the GM
(dft &lt;- ((n*k)-1))
(SST &lt;- sum((c(data$pre,data$post)-GM)^2))
(MST &lt;- SST/dft)

# SSB - Squared diff of group means from the GM (multiplied by n)
(dfb &lt;- (k-1))
(SSB &lt;- n*((mean_pre-GM)^2+(mean_post-GM)^2))
(MSB &lt;- SSB/dfb)

# SSW - Squared diff of each observation from its group mean
(dfw &lt;- (k*(n-1)))
(SSW &lt;- sum((data$pre-mean_pre)^2) + sum((data$post-mean_post)^2))
(MSW &lt;- SSW/dfw)

# Long format for ANOVA
iop &lt;- c(data$pre,data$post)
group &lt;- c(rep(0,1000),rep(1,1000))
data2 &lt;- as.data.frame(cbind(group,iop))
mod &lt;- lm(iop~group,data2)
anova(mod)

SST
SSB
SSW

MST
MSB
MSW

(st2 &lt;- sd(c(data$pre,data$post))^2) # This is the total variance (sb2 + sw2)
(sb2 &lt;- rho*st2) # This is the between variance
(sw2 &lt;- st2-sb2) # This is the within variance
</code></pre>
"
"0.0626633989716535","0.0662266178532522","218879","<p>I'm looking to model percent change in transaction year over year for sales people grouped in certain categories (7 categories total). The percent change in transactions would be Q1 of the current year divided by the number of transactions from Q1 of the previous year which gives me a percent change (positive or negative). </p>

<p>Since this is count data the correct model to use is the Poisson distribution and eventually I would like to do a Poisson regression to look at the effects of predictor variables on percent change in transactions.</p>

<p>But my question is, if I want to compare the means between the different groups what is the analogous version of the ANOVA for Poisson assumptions. (if that's the correct way to put it) </p>
"
"0.173657905270181","0.152943822580375","220551","<p>I am working on Two-Way ANOVA for an unbalanced design.</p>

<p>Will Tukey multiple comparison be the same for different types (I, II &amp; III) of  Sum Of Squares of ANOVA for an unbalanced design.</p>

<p>I am doing the ANOVA using the <code>car</code> package but the output of the Anova function <code>car</code> package does not work with the <code>TukeyHSD</code> function. The <code>TukeyHSD</code> expects the stats <code>aov</code> output.</p>

<p>I have seen posts suggesting the use of <code>HSD.test</code> from the <code>agricolae</code> package using the linear model <code>lm</code>. But the <code>lm</code> does not take a <code>type</code> argument for the Sum of Squares.</p>

<p>So, does the Tukey comparison be the same for different types of Sum of Squares used for ANOVA?</p>

<p>Below is the sample</p>

<p><strong>code</strong>:: (warpbreaks is a dataset available in R, I am removing 3 rows to make it unbalanced)</p>

<pre><code>library(car)
df &lt;- warpbreaks[-c(1:3), ,]
summary.aov(aov(breaks~wool*tension, df)) # wool: p=.058
car::Anova(aov(breaks~wool*tension, df), type=""II"") # wool: p=.058
car::Anova(aov(breaks~wool*tension, df), type=""III"") # wool: p&lt;.05

TukeyHSD(aov(breaks~wool*tension, df))
</code></pre>

<p><strong>output</strong>::</p>

<pre><code>&gt; summary.aov(aov(breaks~wool*tension, df)) # wool: p=.058
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
wool          1    327   327.1   2.940 0.093269 .  
tension       2   1954   976.8   8.780 0.000603 ***
wool:tension  2   1257   628.3   5.647 0.006484 ** 
Residuals    45   5006   111.3                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; car::Anova(aov(breaks~wool*tension, df), type=""II"") # wool: p=.058
Anova Table (Type II tests)

Response: breaks
             Sum Sq Df F value    Pr(&gt;F)    
wool          476.7  1  4.2847 0.0442255 *  
tension      1953.6  2  8.7800 0.0006034 ***
wool:tension 1256.5  2  5.6472 0.0064836 ** 
Residuals    5006.4 45                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; car::Anova(aov(breaks~wool*tension, df), type=""III"") # wool: p&lt;.05
Anova Table (Type III tests)

Response: breaks
              Sum Sq Df  F value    Pr(&gt;F)    
(Intercept)  14113.5  1 126.8594 1.100e-14 ***
wool          1480.3  1  13.3055 0.0006846 ***
tension       2641.6  2  11.8721 7.236e-05 ***
wool:tension  1256.5  2   5.6472 0.0064836 ** 
Residuals     5006.4 45                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; TukeyHSD(aov(breaks~wool*tension, df))
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = breaks ~ wool * tension, data = df)

$wool
         diff       lwr       upr     p adj
B-A -5.074074 -11.03392 0.8857766 0.0932694

$tension
          diff       lwr       upr     p adj
M-L -10.451852 -19.38891 -1.514795 0.0184375
H-L -15.174074 -24.11113 -6.237018 0.0004701
H-M  -4.722222 -13.24337  3.798927 0.3792099

$`wool:tension`
               diff       lwr        upr     p adj
B:L-A:L -20.2777778 -36.82155  -3.734005 0.0084280
A:M-A:L -24.5000000 -41.04377  -7.956227 0.0008625
B:M-A:L -19.7222222 -36.26600  -3.178449 0.0111663
A:H-A:L -23.9444444 -40.48822  -7.400671 0.0011784
B:H-A:L -29.7222222 -46.26600 -13.178449 0.0000407
A:M-B:L  -4.2222222 -19.01942  10.574978 0.9563376
B:M-B:L   0.5555556 -14.24164  15.352756 0.9999974
A:H-B:L  -3.6666667 -18.46387  11.130534 0.9761080
B:H-B:L  -9.4444444 -24.24164   5.352756 0.4157897
B:M-A:M   4.7777778 -10.01942  19.574978 0.9277630
A:H-A:M   0.5555556 -14.24164  15.352756 0.9999974
B:H-A:M  -5.2222222 -20.01942   9.574978 0.8980170
A:H-B:M  -4.2222222 -19.01942  10.574978 0.9563376
B:H-B:M -10.0000000 -24.79720   4.797200 0.3522966
B:H-A:H  -5.7777778 -20.57498   9.019423 0.8521900
</code></pre>

<p>Since for all three types of Sum of Squares (types I, II &amp; III) the Residuals are the same, will the Tukeys comparisons done by using type I ANOVA model with 
<code>TukeyHSD(aov(breaks~wool*tension, df))</code> 
still be valid for type II &amp; III ANOVA's.</p>

<p><strong>Further observation</strong>:
Wool has only two levels A and B so in effect the Tukeys comparison p-value for wool should agree with the ANOVA result.</p>

<p><a href=""http://i.stack.imgur.com/YGmEv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YGmEv.png"" alt=""p-value for wool from different tests""></a></p>

<p>As can be seen it matches type I and does not match the other two.</p>

<p>So if I have a P-threshold of 0.05, Type II &amp; Type III ANOVA suggest that wool is significant and it do not agree with the Type I ANOVA and the TukeyHSD.</p>

<p>Can it be concluded that Tukey test cannot be done using R for type II and III Sum of Squares?</p>
"
"0.228814380978138","0.235779781285754","220603","<p>I have some measurements (concentration) made in 4 groups (W, X, Y, Z) and time is my covariate. I make a linear model:</p>

<pre><code>fit &lt;- lm(concentration~group*year, data=data)
</code></pre>

<p>The results are as follows: ANOVA table:</p>

<pre><code>anova(fit)

Analysis of Variance Table

Response: concentration
           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
group       3 3600.7 1200.22 32.6132 4.081e-10 *** #!
year        1  559.7  559.71 15.2087 0.0004311 ***
group:year  3   97.3   32.42  0.8809 0.4607155    
Residuals  34 1251.3   36.80                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and pairwise comparison:</p>

<pre><code>summary(fit)
Call:
lm(formula = concentration ~ group * year, data = data)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -433.0108   828.4293  -0.523    0.605
groupX      -1574.0090  1170.3741  -1.345    0.188 #!
groupY      -1666.3673  1170.3741  -1.424    0.164 #!
groupZ      -1201.2766  1170.3891  -1.026    0.312 #!
year            0.2418     0.4128   0.586    0.562
groupX:year     0.7937     0.5831   1.361    0.182
groupY:year     0.8409     0.5831   1.442    0.158
groupZ:year     0.6104     0.5831   1.047    0.303

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09
</code></pre>

<p>Now I have a problem in the interpretation of this data. As far as I understand, since the interaction in the ANOVA table is nonsignificant, I can check the group effect, and it is significant. This means that the intercept in different groups should be [significantly] different. But when I look to the summary table, there is no significant difference, at least â€“ between group W and others (groupX, groupY and groupZ are nonsignificant). If I change the compared group from W to X or Y or Z the comparison results are still nonsignificant:</p>

<pre><code>data2 &lt;- data
data2$group[data2$group==""X""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -2.007e+03  8.267e+02  -2.428   0.0206 *
groupW       1.574e+03  1.170e+03   1.345   0.1876 #! 
groupY      -9.236e+01  1.169e+03  -0.079   0.9375 #! 
groupZ       3.727e+02  1.169e+03   0.319   0.7518 #!
year         1.035e+00  4.119e-01   2.514   0.0168 *
groupW:year -7.937e-01  5.831e-01  -1.361   0.1824  
groupY:year  4.717e-02  5.825e-01   0.081   0.9359  
groupZ:year -1.834e-01  5.825e-01  -0.315   0.7549  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09

data2 &lt;- data
data2$group[data2$group==""Y""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -2.099e+03  8.267e+02  -2.539   0.0158 *
groupW       1.666e+03  1.170e+03   1.424   0.1636 #! 
groupX       9.236e+01  1.169e+03   0.079   0.9375 #! 
groupZ       4.651e+02  1.169e+03   0.398   0.6933 #! 
year         1.083e+00  4.119e-01   2.628   0.0128 *
groupW:year -8.409e-01  5.831e-01  -1.442   0.1584  
groupX:year -4.717e-02  5.825e-01  -0.081   0.9359  
groupZ:year -2.305e-01  5.825e-01  -0.396   0.6948  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09

data2 &lt;- data
data2$group[data2$group==""Z""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -433.0108   828.4293  -0.523    0.605
groupX      -1574.0090  1170.3741  -1.345    0.188 #!
groupY      -1666.3673  1170.3741  -1.424    0.164 #!
groupZ      -1201.2766  1170.3891  -1.026    0.312 #!
year            0.2418     0.4128   0.586    0.562
groupX:year     0.7937     0.5831   1.361    0.182
groupY:year     0.8409     0.5831   1.442    0.158
groupZ:year     0.6104     0.5831   1.047    0.303

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09
</code></pre>

<p>How is it possible that there are no significant difference between any two groups when there is a significant group effect? Apparently my interpretation that significant group effect means that at least one group differ significantly from other in the intercept value is incorrect. So what is the correct interpretation of the significant group effect?  </p>
"
"0.0626633989716535","0.0220755392844174","220897","<p>The data that I am using consists of 74 people that answered three DV once (each of them once). I had a 2x2 design, with Context x Intensity as IVs. 
I have done a MANOVA on the data but my supervisor believes a repeated measures MANOVA with contrasts might show how the DVs compare (the 3 DVs are three distinct types of emotion regulation). </p>

<p>This said, I have some difficulties with the <code>R</code> code for the repeated measures with contrasts part.
Firstly, I do not get any F or p-values in the output.
Secondly, I have no clue how to do the contrasts.</p>

<p>This is my code:</p>

<pre><code>rep.data &lt;- rep.data[order(rep.data$data.PpNumber), ]
head(rep.data)
myData.mean &lt;- aggregate(rep.data$value,
                     by = list(rep.data$""data.PpNumber"", rep.data$""context"",
                               rep.data$intensity), 
                     FUN = 'mean')
colnames(myData.mean) &lt;- c(""Pp.Number"",""context"",""intensity"",""Regulation"")
myData.mean &lt;- myData.mean[order(myData.mean$Pp.Number), ]
head(myData.mean)
reg.aov &lt;- with(myData.mean,
               aov(Regulation ~ context * intensity +
                     Error(Pp.Number / (context * intensity)), contrasts =    contr.sum))
</code></pre>
"
"0.114882898114698","0.110377696422087","221023","<p>I have a dilema of the suitability of the analysis with my design.
I have 3 fixed factors:
- Photoperiod (2 levels: 16L8D; 10L14D)
- Temperature (2 levels: 6ÂºC; 25ÂºC)
- Time (4 levels: 50;70;90;150 days) </p>

<p>Photoperiod and Temperature are crossed, and Time is nested within the crossed factors. See this image:</p>

<p><img src=""http://i.stack.imgur.com/gEoAo.jpg"" alt=""enter image description here""></p>

<p>I have tried the following ANOVA nested model: (Y: dependent variable; df: dataframe)</p>

<pre><code>aov(Y ~ (Photoperiod * Temperature) + Error((Photoperiod * Temperature)/Time), data=df)
</code></pre>

<p>And I get that results:</p>

<pre><code>Call:
aov(formula = Y ~ (Photoperiod * Temperature) + 
    Error((Photoperiod * Temperature)/Time), data=df)

Grand Mean: 4.492955

Stratum 1: Photoperiod

Terms:
                 Photoperiod
Sum of Squares  197.7843
Deg. of Freedom        1

1 out of 2 effects not estimable
Estimated effects are balanced

Stratum 2: Temperature

Terms:
                Temperature
Sum of Squares   3795.089
Deg. of Freedom         1

1 out of 2 effects not estimable
Estimated effects are balanced

Stratum 3: Photoperiod:Temperature

Terms:
                Photoperiod:Temperature
Sum of Squares           197.7843
Deg. of Freedom                 1

Estimated effects are balanced

Stratum 4: Photoperiod:Temperature:Time

Terms:
                Residuals
Sum of Squares   626.4977
Deg. of Freedom         2

Residual standard error: 17.69884

Stratum 5: Within

Terms:
                Residuals
Sum of Squares   30658.85
Deg. of Freedom       182

Residual standard error: 12.97903
</code></pre>

<p>I don't know if this approach is right, and how can I get p-values from those results. </p>
"
"0.0808981002113217","0.0854981960070962","221321","<p>I am new to statistics and I am trying to conduct an analysis in <code>R</code> on data containing read count information for 45 samples. I carried out an <code>ANOVA</code> (using the <code>aov()</code> function), comparing the means of each sample, and have also run a <code>post-hoc</code> <code>Tukey</code> test (using the <code>TukeyHSD(</code>) function). </p>

<p>I have used the <code>multcompLetters()</code> function from the '<code>multcompView</code>' package to cluster the samples into groups that differ significantly based on the result of the <code>Tukey</code> test. It has produced a list of groups named 'a', 'ab' and 'b'. </p>

<p>I understand that the samples assigned to group 'a' are significantly different from those assigned to group 'b', but I was hoping someone might be able to help me understand what it means when a sample is assigned the 'ab' group?</p>

<p>Thank you!</p>
"
"0.0808981002113217","0.0683985568056769","221406","<p>I am using R to perform an anova analysis on model with a single factor (7 levels).</p>

<p>I am interested in finding the table of means and standard errors for an balanced design. </p>

<pre><code>print(model.tables(anovaname,""means"",se=TRUE))
</code></pre>

<p>When I do this, I obtain a standard error for the estimate of difference in means. For a balanced design, R returns me a single value for my factor. However, this is the standard error of the estimate of difference in means. Is there a way to find the standard error of estimate of a single mean? </p>
"
"0.119991273679223","0.103757169579911","222043","<p>I have been trying to process data I collected in R, but not getting anywhere, so I hope someone can help me out. My question could be interesting for anyone who wants to compare more than 2 groups that are not normally distributed.</p>

<p>Goal:
I have 4 datasets of 4 different tree species. They contain</p>

<ul>
<li>different tree parameters (height, crown volume...)</li>
<li>in 3 types of locations (street, park, square)</li>
</ul>

<p>Now want to know if the location has an influence on the parameters.  For example, if park trees have    significantly different crown volumes than street trees or trees in squares.</p>

<p>The data looks approximately like this, where ""Loc"" gives the kind of location:</p>

<pre><code>head(Tili)
Loc    Age   DBH   Height  Crown_Pos Crown_Rad Crown_Len Crown_Vol
Str    30.0  22.0    9.9       2.8      3.3       7.1      240.8
Squ    27.6  20.0    9.3       3.1      3.1       6.2      187.6
Pa     36.4  27.5   11.0       2.9      3.5       8.1      315.6
Str    79.5  61.6   18.0       3.7      4.8      14.3     1041.4
Squ    18.6  13.6    5.6       2.2      2.2       3.4       52.7
Pa     51.6  38.5   15.6       4.2      4.8      11.4      825.1
</code></pre>

<p>Problems:</p>

<p>1) I wanted to work with ANOVAs, however, I found...</p>

<p>...1.1) No normal distribution
I used the shapiro.test() and found that many of my parameters are not normally distributed. Or sometimes, just one category (like street) is not normally distributed.</p>

<p>...1.2) No homogenity of variance
I used the leveneTest() to see if my 3 categories have homogenous variances. If I interpret Levene's test in R right, that means that my categories are homogenous when Pr(>F)>0.01. Depending on the tree parameter, my results were between 0.001783 (**) and 0.05888 (.). That also leads to the problem that not all parameters are homogenous across categories.</p>

<p>I read that this means that my data is non-parametric and therefore I should use a wilcox.test(). It seems that this test only compares 2 categories though, when I need 3!</p>

<p>2) Different category sizes</p>

<p>Two categories (square, park) always contain ~20 measurements each, while street is ~60.
I am not sure if this means I have to take any additional steps.</p>

<p>Questions:</p>

<p>Q1) Are my assumptions about not being able to use ANOVA correct?</p>

<p>Q2) If so, how can I find out if my three types of locations influence the tree parameters anyway? I'm looking for a solution as elegant as possible as I have to perform it many times.</p>

<p>Thanks a lot in advance!</p>
"
"0.0957199230302734","0.086710996952412","222788","<p>I am analyzing some data that has violated assumptions of ANOVA and am
 using the WRS2 package in R. I am concerned about the post hoc tests. </p>

<p>I am comparing three groups, Dx, (schizophrenia, schizoaffective and control) on various variables, one of which is premorbid IQ (WRAT4_Std_Score).</p>

<pre><code>Call:
t1way(formula = WRAT4_Std_Score ~ Dx, data = clinicaldemographic, 
    tr = 0.2)

Test statistic: 5.7839 
Degrees of Freedom 1: 2 
Degrees of Freedom 2: 49.6 
p-value: 0.00552 

tapply(clinicaldemographic$WRAT4_Std_Score, clinicaldemographic$Dx, mean, tr=.2, na.rm=TRUE)
  Schizophrenia Schizoaffective         Control 
       89.29412        95.83333        95.54545 

Call:
lincon(formula = WRAT4_Std_Score ~ Dx, data = clinicaldemographic, 
    tr = 0.2)

                                    psihat  ci.lower ci.upper p.value
Control vs. Schizoaffective       -6.53922 -12.59597 -0.48246 0.01041
Control vs. Schizophrenia         -6.25134 -10.87531 -1.62737 0.00162
Schizoaffective vs. Schizophrenia  0.28788  -4.67998  5.25573 0.88534
</code></pre>

<p>The post hoc test shows that the schizophrenia and controls group differ
 significantly which makes sense when you look at their trimmed means;
 however, what does not make sense is that the control and schizoaffective
 disorder groups differ significantly from each other (their means are
 almost identical). Additionally, if the schizophrenia and control group
 differ significantly, shouldn't the schizophrenia group also differ
 significantly from the control group too, as the control group and
 schizoaffective disorder groups have very similar means. It seems like the levels of the variables are not matching the contrast statements. Any help is greatly appreciated.</p>
"
"0.153493353028995","0.162221421130763","223439","<p>I am running an analysis on a national sample of 20,000, representative at the province level (34 provinces)</p>

<p>After checking for linearity and normality of my dependent variable I have run a preliminary OLS in order to see how the covariates perform in explaining the variation of the variable of interest. 
I have selected the relevant independent variables following accreditee literature in my field of analysis, explored the covariance matrix in order to avoid problems of multicollinearity etc..</p>

<p>The result from the OLS is good in term of significance level of the coefficients, the sign and the magnitute of the latter fullfil my expectations and match the results find by other analysis.
However, the value of <code>R^2</code> is quite low: only <code>0.09</code> . Thus, knowing that some variation could be explained by the differences between provinces I have first estimate the OLS adding first provincial dummies and after distric dummies (398 districts).</p>

<p>The <code>R^2</code> improved much, reaching respectively the <code>36%</code> and <code>41%</code>.
However, what I would like to see are the underling cause of the regional differences: why do they perform differently? </p>

<p>Among the variables I have some take a unique value according to each observation's province. I cannot use them in the OLS while using the province dummies because there would be perfect collinearity.</p>

<p>In my view using a mixed linear model would help.</p>

<p>I have run a random intercept null.model in which only the dependent variable is regressed against an intercept. For the estimation I have used the command <code>lmer</code> from the <code>{lme4}</code>package in <code>R</code>.</p>

<p>The InterCorrelation Coeffient equals <code>0.30</code>, suggesting that the 30% of the variation happens between groups, the values of the group-mean reliance are quite high too (not less than <code>0.9</code>)- I repeat myself: the sample is province representative.</p>

<p>I finally run a set of random mixed intercept model with 2 levels:</p>

<p>where: <code>i</code> indicates the household and <code>j</code> indicates the province.</p>

<ol>
<li><code>Y_i = beta0j + beta1 X_i + e_ij</code> </li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 Z_j + e_ij</code></li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 W_j + e_ij</code></li>
</ol>

<p>The <strong>first question</strong> is: am I allowed to include a variable which varies only at the provincial level (as Z and W do) given that their coefficient is not computed by taking in cosideration the groups?
As far as I have understood it would be a mistake to use those variables in the random part of the model in order to get random coefficients.</p>

<p>The <strong>second question</strong>: given that by running the command <code>anova()</code>, also in <code>lme4</code>, model 1 is statistically different from model 2, and model 2 is  statistically not different from model 3, can I say that Z and W have the same power in explaing the variation in Y despite the fact that Z's coefficient is significant and W's is not significant?</p>

<p>Think as if Z and W were proxies for the same dimension. They are in fact statistically and conceptually high correlated.</p>

<p>Sorry but I cannot give more details on the actual problem I am woking on.</p>

<p>Thanks in advance. </p>
"
"0.114407190489069","0.120912708351669","223612","<p>I have a data set containing root, shoot, and seedling growth measurements from 5 treatment groups (at different concentrations, 3 replicates) including control. There were no separate control groups for each treatment group. Each treatment had different concentration ranges. Thus the dataset  looks like the following:</p>

<p>Treatment:  5 treatment (each with different concentrations which varied for each treatment)</p>

<p>Variable:  three variables (root, shoot and seedling -considered as factor) </p>

<p>length: measurement of variable response</p>

<p>Actually, I was trying to see how certain chemical treatments affect the growth of root, shoot, and seedling and whether there is any interaction in the outcome. Assuming that the â€œconcentration X treatmentâ€  interaction term in did not make sense (as the levels- concentrations- were different for different treatments), I modified the analysis by splitting the data by each treatment. Thus each dataset  included the respose variables (root, shoot, and seedling- responses used as factor called variable) at different concentrations for ONE treatment. Thus the ANOVA reduced to two-way and required log transformation .</p>

<p>I have the following doubts (The code for analysis is given below, the data set is attached):</p>

<p>Questions:  </p>

<ol>
<li>would it be meaningful to use the concentration X variable term (since the concentrations differ in each treatment group)? </li>
<li>Should I use the same transformed data (used for two-way analysis) for the one â€“way model required  during multiple comparisons even though the untransformed one-way model meets the assumptions? </li>
<li>What type of transformation should I use for the data (continuous) which could not be fitted even after log transformation?</li>
</ol>
"
"0.114407190489069","0.120912708351669","224509","<p>I'm conducting a meta-analysis on standardised mean difference scores. Some studies provide multiple effect sizes, thereby violating the assumption of independence. An example is given below (all effect sizes were calculated with regard to a pre-test). In study A, all participants received the same treatment (watching a video), and were tested repeatedly. In study B, there were two different treatment groups (one group watched a video, the other group listened to an audio book), and everyone was tested once. Study C provided only one effect size.</p>

<pre><code>study        treatment          testing_moment         effect_size

A            video              immediately            0.6
A            video              delayed                0.5
B            video              immediately            0.9
B            audio_book         immediately            0.7
C            audio_book         delayed                0.4
</code></pre>

<p>I'm using the <em>metafor</em> package in <em>R</em>, in which you can fit a multilevel model to account for non-independent sampling errors. </p>

<p>What I've done:</p>

<pre><code>rma.mv(effect_size_vector, variance_vector, mods = ~ testing_moment, 
  random = ~ 1 | treatment/study, data = rev)
</code></pre>

<p>Could anyone please have a look whether this approach is correct? I'm especially unsure about whether I've correctly indicated the clustering using slash (/) (this decision was based on <a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">this page</a>), and whether the model as a result indeed takes into account the non-independence of effect sizes. </p>

<p>I'm also wondering whether somehow it should be corrected that the samples in study A are dependent and in study B they are independent. Or is that already accounted for by virtue of the treatment variable being the same for both samples in study A?</p>
"
"0.162468952598533","0.156097635263616","225241","<p>Consider a mixed model as follows.</p>

<pre><code>library(lme4)
# Load data
data &lt;- structure(list(blk = c(1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3L),
                       gent = c(1, 2, 3, 4, 7, 11, 12, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 8, 6, 10L),
                       yld = c(83, 77, 78, 78, 70, 75, 74, 79, 81, 81, 91, 79, 78, 92, 79, 87, 81, 96, 89, 82L),
                       syld = c(250, 240, 268, 287, 226, 395, 450, 260, 220, 237, 227, 281, 311, 258, 224, 238, 278, 347, 300, 289L)),
                  .Names = c(""blk"", ""gent"", ""yld"", ""syld""), class = ""data.frame"", row.names = c(NA, -20L))
data$blk &lt;- as.factor(data$blk)
data$gent &lt;- as.factor(data$gent)
</code></pre>

<p>The data is unbalanced.</p>

<pre><code># Mixed effect model
frmla &lt;- ""syld ~ 1 + gent + (1|blk)""
library(lme4)
model &lt;- lmer(formula(frmla), data = data)

model
Linear mixed model fit by REML ['merModLmerTest']
Formula: syld ~ 1 + gent + (1 | blk)
   Data: data
REML criterion at convergence: 73.9572
Random effects:
 Groups   Name        Std.Dev.
 blk      (Intercept)  9.385  
 Residual             16.919  
Number of obs: 20, groups:  blk, 3
Fixed Effects:
(Intercept)        gent2        gent3        gent4        gent5        gent6        gent7        gent8        gent9  
    256.000      -28.000       -8.333        8.000       32.127       43.678      -36.805       90.678       62.127  
     gent10       gent11       gent12  
     32.678      132.195      187.195  
</code></pre>

<p>Primarily I want to compare the <code>gent</code> levels by LS means.</p>

<pre><code>library(""lmerTest"")
lsmeans(model)
Least Squares Means table:
         gent Estimate Standard Error   DF t-value Lower CI Upper CI p-value    
gent  1   1.0    256.0           11.2  6.9    22.9      229      283  &lt;2e-16 ***
gent  2   5.0    228.0           11.2  6.9    20.4      201      255  &lt;2e-16 ***
gent  3   6.0    247.7           11.2  6.9    22.2      221      274  &lt;2e-16 ***
gent  4   7.0    264.0           11.2  6.9    23.6      237      291  &lt;2e-16 ***
gent  5   8.0    288.1           18.5  8.0    15.6      245      331  &lt;2e-16 ***
gent  6   9.0    299.7           18.5  8.0    16.2      257      342  &lt;2e-16 ***
gent  7  10.0    219.2           18.5  8.0    11.8      177      262  &lt;2e-16 ***
gent  8  11.0    346.7           18.5  8.0    18.8      304      389  &lt;2e-16 ***
gent  9  12.0    318.1           18.5  8.0    17.2      275      361  &lt;2e-16 ***
gent  10  2.0    288.7           18.5  8.0    15.6      246      331  &lt;2e-16 ***
gent  11  3.0    388.2           18.5  8.0    21.0      346      431  &lt;2e-16 ***
gent  12  4.0    443.2           18.5  8.0    24.0      401      486  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In addition I am interested in variance partitioning.</p>

<p>The variance component due to random effect and residual can be estimated as follows.</p>

<pre><code>VCrandom &lt;- VarCorr(model)
print(VCrandom, comp = ""Variance"")
 Groups   Name        Variance
 blk      (Intercept)  88.083 
 Residual             286.250
</code></pre>

<p>How to partition the total variance into components due to each of the factors <code>gent</code> and <code>blk</code> along with the residual ? Something similar to the output given by <code>PROC MIXED</code> of <code>SAS</code>, where MSE is computed even when estimation is by ML or REML instead of least squares.</p>

<p>Should I treat the fixed effect as random just for the purpouse of getting variance component ?</p>

<pre><code>frmla2 &lt;- ""syld ~ 1 + (1|gent) + (1|blk)""
model2 &lt;- lmer(formula(frmla2), data = data)
model2

VCrandom2 &lt;- VarCorr(model2)
print(VCrandom2, comp = ""Variance"")
 Groups   Name        Variance
 gent     (Intercept) 4152.08 
 blk      (Intercept)  116.11 
 Residual              274.92 
</code></pre>

<p>If there is no random effect, variance components can be estimated using the least squares approach (ANOVA, Sum of squares, MSE).</p>

<p>The package <code>mixlm</code> has provision for variance partitioning using SS in case of mixed models.</p>

<pre><code>library(mixlm)

mixlm &lt;- lm(syld ~ 1 + r(gent) + r(blk), data)

Anova(mixlm, type=""III"")

Analysis of variance (unrestricted model)
Response: syld
          Mean Sq   Sum Sq Df F value Pr(&gt;F)
gent      5360.49 58965.36 11   18.73 0.0009
blk        638.58  1277.17  2    2.23 0.1886
Residuals  286.25  1717.50  6       -      -

            Err.term(s) Err.df VC(SS)
1 gent              (3)      6 3044.5
2 blk               (3)      6   52.8
3 Residuals           -      -  286.3
(VC = variance component)

               Expected mean squares
gent      (3) + 1.66666666666667 (1)
blk       (3) + 6.66666666666667 (2)
Residuals (3)                       

WARNING: Unbalanced data may lead to poor estimates
</code></pre>

<p>The estimates are different</p>

<pre><code># Total variance
var(data$syld)

|source   |  model1|  model2|  mixlm|
|:--------|-------:|-------:|------:|
|gent     |      NA| 4152.08| 3044.5|
|blk      |  88.083|  116.11|   52.8|
|Residual | 286.250|  274.92|  286.3|
</code></pre>

<p>Can fixed effect variance be extracted using <code>predict</code> function as suggested here <a href=""https://sites.google.com/site/alexandrecourtiol/what-did-i-learn-today/inrhowtoextractthedifferentcomponentsofvarianceinalinearmixedmodel"" rel=""nofollow"">In R: How to extract the different components of variance in a linear mixed model!</a> ?</p>

<pre><code>var(predict(model))
</code></pre>

<p>Which is the most appropriate method compatible with <code>(RE)ML</code> estimates in lme4 ?</p>
"
"0.10232890201933","0.0946291623262781","226266","<p>I am trying to fit some mixed models for unbalanced data as follows.</p>

<pre><code>library(easyanova)
data(data13)
</code></pre>

<h3>1) <code>genotypes</code> as random</h3>

<pre><code>frmla &lt;- ""yield ~ 1 + (1|blocks) + (1|genotypes)""

model1 &lt;- lmer(formula(frmla), data = data13)

# adjusted means - BLUPs for genotypes
newdata13 &lt;- expand.grid(genotypes = levels(data13$genotypes), blocks = levels(data13$blocks))
newdata13$pred &lt;- predict(model1, newdata=newdata13)
tapply(newdata13$pred, newdata13$genotypes, mean)
</code></pre>

<h3>1) <code>genotypes</code> as fixed</h3>

<pre><code>frmla &lt;- ""yield ~ 1 + (1|blocks) + genotypes""

model2 &lt;- lmer(formula(frmla), data = data13)

# adjusted means - BLUEs for genotypes
newdata13 &lt;- expand.grid(genotypes = levels(data13$genotypes), blocks = levels(data13$blocks))
newdata13$pred &lt;- predict(model2, newdata=newdata13)
tapply(newdata13$pred, newdata13$genotypes, mean)
</code></pre>

<p>For further calculations I need to compute <strong>mean variance of difference of adjusted means</strong> (BLUPs or BLUEs). A method is given (<a href=""https://static-content.springer.com/esm/art%3A10.1186%2F1471-2164-14-860/MediaObjects/12864_2013_5591_MOESM1_ESM.doc"" rel=""nofollow"">https://static-content.springer.com/esm/art%3A10.1186%2F1471-2164-14-860/MediaObjects/12864_2013_5591_MOESM1_ESM.doc</a>)
to compute it from <strong>variance-covariance matrix of adjusted means</strong>.</p>

<p>How to get the variance-covariance matrix of adjusted means for <code>model1</code> and <code>model2</code> ?</p>

<p>When <code>genotypes</code> are fixed in <code>model2</code> does <code>vcov(model2)</code> give the variance-covariance matrix of adjusted means ?</p>
"
"0.0511644510096651","0.0540738070435875","226884","<p>In many textbooks and R tutorials the repeated measures (within subjects) ANOVA seems to be very straightforward, following the formula</p>

<pre><code>aov.ww &lt;- aov(y ~ w1*w2 + Error(subject/(w1*w2)), data=data.long)
</code></pre>

<p>where <code>w1</code> and <code>w2</code> are two within-subjects variables and hence <code>subject</code> is included in the error term. However, this formula does not seem to treat the observations as dependent, evinced by the very large df. </p>

<p>I have used workarounds like first computing means across participants and then running the <code>aov</code> function on the reduced dataset. Also, many sources point to <code>lmer</code> as an alternative to <code>aov</code> for repeated measures design.</p>

<p>What would be the most robust and straightforward (in terms on interpretation or the results) method for analyzing data from repeated measures experiments?</p>
"
"0.0511644510096651","0.0540738070435875","227073","<p>I have a small perplexity some of you might be able to help me with. 
I have fitted a linear model in R of the form</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
</code></pre>

<p>and I want to obtain Likelihood Ratio Tests on the regression coefficients for <code>X1</code> and <code>X2</code>. 
One way to get them is using:</p>

<pre><code>anova(fullmodel, test=""LRT"")
</code></pre>

<p>But, in my understanding, if I use <code>anova</code> on the full model it removes covariates and performs LRT sequentially, indeed results differed depending on ordering of predictors.
<code>drop1</code>, on the other hand, drops one covariate at a time and leaves the rest untouched; thus I could use:</p>

<pre><code>drop1(fullmodel, test=""Chisq"")
</code></pre>

<p>This should work. Yet, out of curiosity, I also tried the following:</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
reducedmodel1 = lm(Y ~ X1)
reducedmodel2 = lm(Y ~ X2)

anova(fullmodel, reducedmodel1, test=""LRT"")
anova(fullmodel, reducedmodel2, test=""LRT"")
</code></pre>

<p>In my understanding, the two procedures (<code>drop1</code> and the two separate <code>anova</code>) have identical meaning and should give exactly the same p-values. That's not the case, though; they differ already at the 3rd decimal number. 
Can anyone explain to me why this happens? Am I doing something wrong?</p>
"
"0.0511644510096651","0.0540738070435875","229343","<p>I've got a dataset with patients (n=50) with 10 readings each (so overall, n=500) who suffered syncope (1 or 0), and 2 continuous predictors (rate and doppler).</p>

<p>I'm trying to see if 1 predictor is more effective than another. I'm currently synthesising data, then creating a model for each using glm(syncope~predictor,family=""binomial""), and then using an ANOVA on these. The code is as follows.</p>

<pre><code>n_pts &lt;- 50
n_reads_per_pt &lt;- 10
intercept = log(0.2)
gradient = 2.5
x &lt;- rnorm(n_pts*n_reads_per_pt,mean=0,sd=1)

x_doppler &lt;- x
x_rate &lt;- x + (rnorm(n_pts*n_reads_per_pt)) #Add a second random factor to make rate a less good predictor

y &lt;- intercept + gradient*x
p &lt;- exp(y)/(1+exp(y))
tmp &lt;- runif(n_pts*n_reads_per_pt)
syncope &lt;- (tmp &lt; p)

glm_rate &lt;- glm(syncope~x_rate,family=""binomial"")
glm_doppler &lt;- glm(syncope~x_doppler,family=""binomial"")

anova(glm_rate, glm_doppler,test=""Chisq"")
</code></pre>

<p>The problem is the output of the ANOVA is:</p>

<pre><code>&gt; anova(glm_rate, glm_doppler,test=""Chisq"")
Analysis of Deviance Table

Model 1: syncope ~ x_rate
Model 2: syncope ~ x_doppler
  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
1       498     585.72                     
2       498     386.71  0   199.01  
</code></pre>

<p>Note no p value. I assume this is because I have 0 Df left?</p>

<p>How would you recommend I then compare such a dataset, where 2 continuous predictors are being compared to assess one binary outcome?</p>
"
"0.125699240957384","0.132846856888409","229722","<p>Thank anyone who look my question. I'm doing a linguistic experiment. I let people in two second language proficiency levels (inter and advanced) and living in two places (city A and B) do a same rating test. The rating test have 6 types of questions, each type have 6 tokens, in total 36 test items for each subject. I also have a native speaker group as control (L1). The picture shows how I code data.
<a href=""http://i.stack.imgur.com/1a2sM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1a2sM.png"" alt=""enter image description here""></a></p>

<p>The followings are my code.</p>

<pre><code>library(plyr)

# Read data
data = read.csv(""Ba_rang_bei"", header = TRUE)
# Summarise data for table
sum = ddply(.data=data, c(""type"", ""level""), summarise, mean =mean(rating,na.rm=TRUE), sd = sd(rating, na.rm=TRUE))
sum
# Summarise data for analysis
agg = ddply(.data=data, c(""ID"", ""type"", ""level""), summarise, mean = mean(rating, na.rm=TRUE))
# Run anova with 'rating' as the dependent factor, 'type'as a with-subject factor and 'level'as a between-subject factor.
# Include interaction
anova1= aov(mean ~ type*level+Error(ID/type), data = agg)
summary(anova1)
</code></pre>

<p>The result shows:
<a href=""http://i.stack.imgur.com/6kGKI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6kGKI.png"" alt=""enter image description here""></a><br>
I want to do multiple ANOVA comparisons on subjects' mean rating scores between types and between levels. Like I want to know ""whether A-inter group behave significantly different with the native group on Type A"", ""whether inter group (both in city A and city B) behave significantly different from advaned group (both in city A and city B), ""whether A-inter group's ratings on Type A significantly different on Type B, C, D, E and F.The Tukey HSD test doesn't work in my case, so I have to find an appropriate linear model to do multiple ANOVA comparisons. Can anybody give me any suggestions on building linear models and do multiple ANOVA comparisons? Please help me.</p>
"
"0.16969328660358","0.179342528949657","230734","<p>I've been running GLMMs in the R package ""glmmadmb"" looking at the effects of different sizes of pan trap on the abundance of their catch, using the following code: <code>glmm5 &lt;- glmmadmb(ab$Totalpolls ~ ab$Pan_size+ab$Treatment+log(ab$Nectar+1)+log(ab$Mean.nectar+1)+ab$Max_temp+ab$Season+(1|Year)+(1|Transect), data = ab, zeroInflation = FALSE, family = ""nbinom"")</code></p>

<p>The basic output look like this:</p>

<pre><code>Anova(glmm5)

Analysis of Deviance Table (Type II tests)
Response: ab$Total_polls  Df Chisq Pr(&gt;Chisq)    
ab$Pan_size               3 41.6487  4.763e-09 ***
ab$Treatment              2 14.8347  0.0006007 ***
log(ab$Nectar + 1)        1  8.0988  0.0044295 ** 
log(ab$Mean.nectar + 1)   1  5.0591  0.0244971 *  
ab$Max_temp               1  8.5233  0.0035062 ** 
ab$Season                 4 46.4576  1.978e-09 ***
Residuals               212                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and...</p>

<blockquote>
  <p>summary(glmm5)</p>
</blockquote>

<pre><code>Call:
glmmadmb(formula = ab$Total_polls ~ ab$Pan_size + ab$Treatment + 
log(ab$Nectar + 1) + log(ab$Mean.nectar + 1) + ab$Max_temp + 
ab$Season + (1 | ab$Year) + (1 | ab$Transect), data = ab, 
family = ""nbinom"", zeroInflation = FALSE)

AIC: 855.6 

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)              -1.1797     0.5312   -2.22  0.02638 *  
ab$Pan_size2              0.5272     0.1587    3.32  0.00089 ***
ab$Pan_size5.5            0.0926     0.1668    0.56  0.57882    
ab$Pan_size12             0.9026     0.1538    5.87  4.4e-09 ***
ab$Treatment24            0.0540     0.1368    0.39  0.69286    
ab$Treatment48            0.4973     0.1291    3.85  0.00012 ***
log(ab$Nectar + 1)        0.0739     0.0260    2.85  0.00443 ** 
log(ab$Mean.nectar + 1)   0.0934     0.0415    2.25  0.02450 *  
ab$Max_temp              -0.0513     0.0176   -2.92  0.00351 ** 
ab$Season5                0.6176     0.2051    3.01  0.00260 ** 
ab$Season6                1.2434     0.2229    5.58  2.4e-08 ***
ab$Season7                0.7909     0.2338    3.38  0.00072 ***
ab$Season8                0.6476     0.3554    1.82  0.06840 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of observations: total=228, ab$Year=2, ab$Transect=19 
Random effect variance(s):
Group=ab$Year
             Variance   StdDev
(Intercept) 1.142e-07 0.000338
Group=ab$Transect
            Variance StdDev
(Intercept)  0.01727 0.1314

Negative binomial dispersion parameter: 5.6367 (std. err.: 2.0044)

Log-likelihood: -411.816
</code></pre>

<p>Which shows that pan sizes 12 and 2 are significantly different to size 1, and size 5.5 isn't significantly different at all. However, when I put this model through post hoc tests using the following code: <code>summary(glht(glmm5, lsm(pairwise ~ ab$Pan_size)))</code> (using the glht interface in R package ""Lsmeans"") it gives me this:</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses

Fit: glmmadmb(formula = ab$Total_polls ~ ab$Treatment + ab$Pan_size + 
log(ab$Nectar + 1) + log(ab$Mean.nectar + 1) + ab$Max_temp + 
ab$Season + (1 | ab$Year) + (1 | ab$Transect), data = ab, 
family = ""nbinom"", zeroInflation = FALSE)

Linear Hypotheses:
              Estimate Std. Error z value Pr(&gt;|z|)    
1 - 2 == 0     -0.1465     0.1239  -1.182  0.62507    
1 - 5.5 == 0   -0.3086     0.1252  -2.464  0.06256 .  
1 - 12 == 0    -0.6975     0.1421  -4.907  &lt; 0.001 ***
2 - 5.5 == 0   -0.1621     0.1063  -1.525  0.40911    
2 - 12 == 0    -0.5510     0.1666  -3.308  0.00498 ** 
5.5 - 12 == 0  -0.3889     0.1327  -2.931  0.01685 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>Which suggests that size 12 is not better than size 1 and that size two is also not better than size 1; it also indicates that size 5.5 is better than size 1. This all seems to contradict what's in the model summary, which has got me slightly puzzled. All of the other post hoc tests I've run for the other categorical variables in the model have run fine and present results as expected. I've tried changing the position of <code>ab$pan_size</code> in the model, but that doesn't improve things.</p>

<p>Here's a list of the basic code that I'm using:</p>

<pre><code>#pollinator abundance vs. pan trap size and time left active

ab&lt;-read.csv(""Total_polls.csv"")

ab
names(ab)
str(ab)
summary(ab)

# create factors from numerical variables

ab$Year&lt;-as.factor(ab$Year)
ab$Transect&lt;-as.factor(ab$Transect)
ab$Treatment&lt;-as.factor(ab$Treatment)
ab$Pan_size&lt;-as.factor(ab$Pan_size)
ab$Season&lt;-as.factor(ab$Season)

library(glmmADMB)
library(RVAideMemoire)
library(car)

glmm5 &lt;- glmmadmb(ab$Total_polls ~ ab$Treatment+ab$Pan_size+log(ab$Nectar+1)+log(ab$Mean_nectar+1)+ab$Max_temp+ab$Season+(1|ab$Year)+(1|ab$Transect), data = ab, zeroInflation = FALSE, family = ""nbinom"")

Anova(glmm5) #(Package: car)
summary(glmm5) 

# pairwise multiple comparisons tests between multi-level fixed effects (pan_size, treatment (time), and season)

library(multcomp)
library(lsmeans)

glht1 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Pan_size)))
glht2 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Treatment)))
glht3 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Season)))

summary(glht1)
</code></pre>

<p>I'm using R version 3.2.3 (2015-12-10) -- ""Wooden Christmas-Tree"". Could anyone help me to sort this out? I'm not exactly stats savvy, so you may have to be kind with the mathematical language.</p>

<p>Many thanks,
Tom</p>
"
"0.140119619801808","0.148087219439773","231059","<p>So first of all I did some research on this forum, and I know <a href=""http://stats.stackexchange.com/questions/140991/comparing-difference-between-two-polynomial-regression-models-in-r"">extremely similar</a>  questions have been asked but they usually haven't been answered properly or sometimes the answer are simply not detailed enough for me to understand. So this time my question is : I have two sets of data, on each, I do a polynomial regression like so :</p>

<pre><code>Ratio&lt;-(mydata2[,c(2)])
Time_in_days&lt;-(mydata2[,c(1)])
fit3IRC &lt;- lm( Ratio~(poly(Time_in_days,2)) )
</code></pre>

<p>The polynomial regressions plots are:</p>

<p><a href=""http://i.stack.imgur.com/T7r3i.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T7r3i.png"" alt=""enter image description here""></a></p>

<p>The coefficients are :</p>

<pre><code>&gt; as.vector(coef(fit3CN))
[1] -0.9751726 -4.0876782  0.6860041
&gt; as.vector(coef(fit3IRC))
[1] -1.1446297 -5.4449486  0.5883757 
</code></pre>

<p>And now I want to know, if there is a way to use an R function to do a test that would tell me whether or not there is a statistical significance in the difference between the two polynomials regression knowing that the relevant interval of days is [1,100].</p>

<p>From what I understood I can not apply directly the anova test because the values come from two different sets of data nor the AIC, which is used to compare model/true data.</p>

<p>I tried to follow the instructions given by @Roland in the related question but I probably misunderstood something when looking at my results :</p>

<p>Here is what I did : </p>

<p>I combined both my datasets into one.</p>

<p><code>f</code> is the variable factor that @Roland talked about. I put 1s for the first set and 0s for the other one.</p>

<pre><code>y&lt;-(mydata2[,c(2)])
x&lt;-(mydata2[,c(1)])
f&lt;-(mydata2[,c(3)])

plot(x,y, xlim=c(1,nrow(mydata2)),type='p')

fit3ANOVA &lt;- lm( y~(poly(x,2)) )

fit3ANOVACN &lt;- lm( y~f*(poly(x,2)) )
</code></pre>

<p>My data looks like this now :</p>

<p><a href=""http://i.stack.imgur.com/dNpMQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dNpMQ.png"" alt=""enter image description here""></a></p>

<p>The red one is <code>fit3ANOVA</code> which is still working but I have a problem with the blue one <code>fit3ANOVACN</code> the model has weird results. I don't know if the fit model is correct, I do not understand what @Roland meant exactly.</p>

<p>Considering @DeltaIV solution I suppose that in that case :
<a href=""http://i.stack.imgur.com/HLLp9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HLLp9.png"" alt=""enter image description here""></a>
The models are significantly different even though they overlap. Am I right to assume so ?</p>
"
"0.0886194286901087","0.0936585811581694","231377","<p>I am trying to carry out an lmerTest on two separate datasets, and for some reason I am getting the following error for one of the datasets.</p>

<blockquote>
  <p>In pf(F.stat, qr(Lc)$rank, nu.F) : NaNs produced</p>
</blockquote>

<p><a href=""https://drive.google.com/file/d/0B9jz9CiotnoER1dzUzRrVllVcm8/view?usp=sharing"" rel=""nofollow"">This dataset</a> gives me the p-value of the interaction term between <code>habitat</code> and <code>soil</code> without issue.</p>

<blockquote>
  <p>anova(lmer(sqrt(abs) ~ habitat*soil + (1|species), data=frl_light,
  REML=T))</p>
</blockquote>

<pre><code>Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq  Mean Sq NumDF  DenDF F.value  Pr(&gt;F)  
habitat      0.057617 0.028809     2 8.8434  1.0880 0.37805  
soil         0.232708 0.232708     1 2.6732  8.7888 0.06848 .
habitat:soil 0.308003 0.154001     2 2.7134  5.8163 0.10443  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p><a href=""https://drive.google.com/file/d/0B9jz9CiotnoEdWkzbGhHM0RSVnM/view?usp=sharing"" rel=""nofollow"">This dataset</a> which has a similar structure however throws the error, and fails to give the p-value for the interaction between <code>habitat</code> and <code>light</code>. The density degree of freedom measurement is also 0, which is probably the problem.</p>

<blockquote>
  <p>anova(lmer(sqrt(abs) ~ habitat*light + (1|species), data=frl_soil,
  REML=T))</p>
</blockquote>

<pre><code>Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq  Mean Sq NumDF  DenDF F.value Pr(&gt;F)
habitat       0.00845 0.004223     2 7.9751  0.3494 0.7154
light         0.01634 0.016336     1 1.9241  1.3517 0.3689
habitat:light 0.42813 0.214067     2 0.0000 17.7124       
Warning message:
In pf(F.stat, qr(Lc)$rank, nu.F) : NaNs produced
</code></pre>

<p>I have no idea why lmerTest works for one dataset but not the other as both datasets appear to me at least, to be virtually indistinguishable from one another. If there is anyone who can shed light on the matter, please help.</p>
"
"0.0808981002113217","0.0683985568056769","231854","<p>I am trying to perform Mauchly's test of sphericity on a set of Data for which I have done a repeated-measures ANOVA using aov. I wanted to test the ""Participant"" as an independant variable, as in my experiment, participants who are presented with the same system will not perceive it the same way. My anova therefore looks like</p>

<pre><code>res.aov &lt;- aov(Attribute~System+Participant+Repetition
+System:Participant+System:Repetition+Participant:Repetition,data=sample1Norm)
</code></pre>

<p>which gives me the following summary:</p>

<pre><code>                        Df Sum Sq Mean Sq F value Pr(&gt;F)  
System                   6   6721  1120.2   2.723 0.0168 *
Participant              9      0     0.0   0.000 1.0000  
Repetition               2      0     0.0   0.000 1.0000  
System:Participant      54  32531   602.4   1.464 0.0475 *
System:Repetition       12   4877   406.4   0.988 0.4653  
Participant:Repetition  18      0     0.0   0.000 1.0000  
Residuals              108  44432   411.4                 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, if I try to do the same with ezANOVA, I am not sure how to affect ""Participant"" as an independant variable:</p>

<pre><code>ezANOVA(data=sample1Norm, dv=.(Attribute), wid=.(Participant), 
within=.(Repetition,System), detailed=TRUE, type=1)
</code></pre>

<p>gives me the following result:</p>

<pre><code>$ANOVA
             Effect DFn DFd          SSn          SSd         F         p p&lt;.05          ges
1        Repetition   2  18 5.446667e-07 2.187057e-05 0.2241368 0.8014030       7.076965e-12
2            System   6  54 6.720992e+03 3.253121e+04 1.8594123 0.1048659       8.031364e-02
3 Repetition:System  12 108 4.876884e+03 4.443211e+04 0.9878433 0.4652740       5.959033e-02
</code></pre>

<p>Can anybody tell me what I am doing wrong here?</p>

<p>(and here's the data)</p>

<pre><code>Participant,Repetition,System,Attribute

1,1,1,44

1,1,2,67

1,1,3,67

1,1,4,16

1,1,5,54

1,1,6,84

1,1,7,18

1,2,1,50.286

1,2,2,52.286

1,2,3,74.286

1,2,4,48.286

1,2,5,72.286

1,2,6,31.286

1,2,7,21.286

1,3,1,43.143

1,3,2,49.143

1,3,3,60.143

1,3,4,24.143

1,3,5,71.143

1,3,6,64.143

1,3,7,38.143

2,1,1,50

2,1,2,50

2,1,3,50

2,1,4,50

2,1,5,50

2,1,6,50

2,1,7,50

2,2,1,47.143

2,2,2,52.143

2,2,3,47.143

2,2,4,47.143

2,2,5,47.143

2,2,6,60.143

2,2,7,49.143

2,3,1,50

2,3,2,50

2,3,3,50

2,3,4,50

2,3,5,50

2,3,6,50

2,3,7,50

3,1,1,70.286

3,1,2,47.286

3,1,3,28.286

3,1,4,69.286

3,1,5,38.286

3,1,6,52.286

3,1,7,44.286

3,2,1,27.857

3,2,2,36.857

3,2,3,72.857

3,2,4,55.857

3,2,5,48.857

3,2,6,46.857

3,2,7,60.857

3,3,1,84.714

3,3,2,49.714

3,3,3,85.714

3,3,4,44.714

3,3,5,18.714

3,3,6,17.714

3,3,7,48.714

4,1,1,65.286

4,1,2,51.286

4,1,3,40.286

4,1,4,16.286

4,1,5,78.286

4,1,6,50.286

4,1,7,48.286

4,2,1,34.286

4,2,2,74.286

4,2,3,75.286

4,2,4,48.286

4,2,5,61.286

4,2,6,22.286

4,2,7,34.286

4,3,1,85.143

4,3,2,55.143

4,3,3,47.143

4,3,4,39.143

4,3,5,66.143

4,3,6,49.143

4,3,7,8.1429

5,1,1,39.286

5,1,2,49.286

5,1,3,44.286

5,1,4,47.286

5,1,5,47.286

5,1,6,61.286

5,1,7,61.286

5,2,1,45.429

5,2,2,55.429

5,2,3,47.429

5,2,4,47.429

5,2,5,53.429

5,2,6,54.429

5,2,7,46.429

5,3,1,48.857

5,3,2,47.857

5,3,3,52.857

5,3,4,48.857

5,3,5,47.857

5,3,6,51.857

5,3,7,51.857

6,1,1,83.571

6,1,2,19.571

6,1,3,60.571

6,1,4,53.571

6,1,5,14.571

6,1,6,68.571

6,1,7,49.571

6,2,1,130.86

6,2,2,24.857

6,2,3,4.8571

6,2,4,13.857

6,2,5,130.86

6,2,6,32.857

6,2,7,11.857

6,3,1,24.286

6,3,2,61.286

6,3,3,100.29

6,3,4,5.2857

6,3,5,100.29

6,3,6,41.286

6,3,7,17.286

7,1,1,65.429

7,1,2,14.429

7,1,3,84.429

7,1,4,25.429

7,1,5,77.429

7,1,6,12.429

7,1,7,70.429

7,2,1,42.429

7,2,2,74.429

7,2,3,80.429

7,2,4,68.429

7,2,5,35.429

7,2,6,-8.5714

7,2,7,57.429

7,3,1,91.286

7,3,2,23.286

7,3,3,77.286

7,3,4,-12.714

7,3,5,59.286

7,3,6,67.286

7,3,7,44.286

8,1,1,51.143

8,1,2,58.143

8,1,3,58.143

8,1,4,18.143

8,1,5,58.143

8,1,6,50.143

8,1,7,56.143

8,2,1,44.429

8,2,2,48.429

8,2,3,52.429

8,2,4,50.429

8,2,5,52.429

8,2,6,52.429

8,2,7,49.429

8,3,1,60.429

8,3,2,53.429

8,3,3,60.429

8,3,4,-3.5714

8,3,5,60.429

8,3,6,57.429

8,3,7,61.429

9,1,1,55.429

9,1,2,30.429

9,1,3,62.429

9,1,4,52.429

9,1,5,40.429

9,1,6,52.429

9,1,7,56.429

9,2,1,50.286

9,2,2,52.286

9,2,3,48.286

9,2,4,45.286

9,2,5,51.286

9,2,6,47.286

9,2,7,55.286

9,3,1,42.857

9,3,2,59.857

9,3,3,60.857

9,3,4,32.857

9,3,5,46.857

9,3,6,45.857

9,3,7,60.857

10,1,1,45

10,1,2,44

10,1,3,45

10,1,4,83

10,1,5,50

10,1,6,53

10,1,7,30

10,2,1,48.571

10,2,2,41.571

10,2,3,36.571

10,2,4,83.571

10,2,5,45.571

10,2,6,40.571

10,2,7,53.571

10,3,1,55.286

10,3,2,28.286

10,3,3,52.286

10,3,4,105.29

10,3,5,30.286

10,3,6,53.286

10,3,7,25.286
</code></pre>

<p>Bonus question: why am I not getting the details of the results (including Mauchly's test of sphericity)?</p>

<p>Thanks!</p>

<p>Laurent</p>
"
"0.135670238492329","0.152943822580375","232109","<p>I have been looking through <a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">this overview of lm/lmer R formulas by @conjugateprior</a> and got confused by the following entry:</p>

<blockquote>
  <p>Now assume A is random, but B is fixed and B is nested within A.</p>

<pre><code>aov(Y ~ B + Error(A/B), data=d)
</code></pre>
</blockquote>

<p>Below analogous mixed model formula <code>lmer(Y ~ B + (1 | A:B), data=d)</code>  is provided for the same case.</p>

<p><strong>I do not quite understand what it means.</strong> In an experiment where subjects are divided into several groups, we would have a random factor (subjects) nested within a fixed factor (groups). But how can a fixed factor be nested within a random factor? Something fixed nested within random subjects? Is it even possible? If it is not possible, do these R formulas make sense?</p>

<hr>

<p>This overview is mentioned to be partially based on the <a href=""http://www.personality-project.org/r/r.guide.html#anova"" rel=""nofollow"">personality-project's pages on doing ANOVA in R</a> based itself on this <a href=""http://www.jason-french.com/tutorials/repeatedmeasures.html"" rel=""nofollow"">tutorial on repeated measures in R</a>. There the following example for the repeated measures ANOVA is given:</p>

<pre><code>aov(Recall ~ Valence + Error(Subject/Valence), data.ex3)
</code></pre>

<p>Here subjects are presented with words of varying valence (factor with three levels) and their recall time is measured. Each subject is presented with words of all three valence levels. I do not see anything nested in this design (it appears crossed, as per <a href=""http://stats.stackexchange.com/questions/228800"">the great answer here</a>), and so I would naively think that <code>Error(Subject)</code> or <code>(1 | Subject)</code> should be appropriate random term in this case. The <code>Subject/Valence</code> ""nesting"" (?) is confusing.</p>

<p>Note that I do understand that <code>Valence</code> is a <em>within-subject</em> factor. But I think it is <em>not</em> a ""nested"" factor within subjects (because all subjects experience all three levels of <code>Valence</code>).</p>

<hr>

<p><strong>Update.</strong> I am exploring questions on CV about coding repeated measures ANOVA in R.</p>

<ul>
<li><p><a href=""http://stats.stackexchange.com/questions/14088"">Here</a> the following is used for fixed within-subject/repeated-measures A and random <code>subject</code>: </p>

<pre><code>summary(aov(Y ~ A + Error(subject/A), data = d))
anova(lme(Y ~ A, random = ~1|subject, data = d))
</code></pre></li>
<li><p><a href=""http://stats.stackexchange.com/questions/13784"">Here</a> for two fixed within-subject/repeated-measures effects A and B:</p>

<pre><code>summary(aov(Y ~ A*B + Error(subject/(A*B)), data=d))
lmer(Y ~ A*B + (1|subject) + (1|A:subject) + (1|B:subject), data=d) 
</code></pre></li>
<li><p><a href=""http://stats.stackexchange.com/questions/117660"">Here</a> for three within-subject effects A, B, and C:</p>

<pre><code>summary(aov(Y ~ A*B*C + Error(subject/(A*B*C)), data=d))
lmer(Y ~ A*B*C + (1|subject) + (0+A|subject) + (0+B|subject) + (0+C|subject) + (0+A:B|subject) + (0+A:C|subject) + (0+B:C|subject), data = d)
</code></pre></li>
</ul>

<p>My questions: </p>

<ol>
<li>Why <code>Error(subject/A)</code> and not <code>Error(subject)</code>? </li>
<li>Is it <code>(1|subject)</code> or <code>(1|subject)+(1|A:subject)</code> or simply <code>(1|A:subject)</code>? </li>
<li>Is it <code>(1|subject) + (1|A:subject)</code> or <code>(1|subject) + (0+A|subject)</code>, and why not simply <code>(A|subject)</code>?</li>
</ol>

<p>By now I have seen some threads that claim that some of these things are equivalent (e.g., the first: <a href=""http://stats.stackexchange.com/questions/60108"">a claim that they are the same</a> but <a href=""http://stackoverflow.com/questions/37497948/"">an opposite claim on SO</a>; the third: kind of <a href=""http://stats.stackexchange.com/a/122662/2866"">a claim that they are the same</a>). Are they?</p>
"
"0.0361787302646211","0.0382359556450936","232271","<p>I want to perform One-way ANOVA in R. I do not have the raw data, but only the mean, standard error and size of each group (there are three groups). If I understand correctly, the <code>ind.oneway.second</code> command in the <code>rpsychi</code> package is relevant only when the groups are the same size. Is there a way to perform One-way ANOVA in R when the groups are not the same size?</p>
"
"0.0957199230302734","0.101162829777814","233619","<p>I have data from an EEG experiment. Originally, the design was balanced, but because in EEG you loose a lot of data to noise and malfunctioning equipment, the end-result is unbalanced data. </p>

<p>I am working with R. In R, my data looks like this:</p>

<pre><code>          Name                 Condition Channel EpochCountStd EpochCountDeviant ErpMinTime ErpMinVoltage ErpMinAv Frequency
380   AY11042016B1-Deci.EEG      HFMM     AF3           423               103        203          -1.2     -1.1      High
388 AvdP23052016B1-Deci.EEG      HFMM     AF3           410               101        144          -3.7     -3.2      High
397   EW20042016B1-Deci.EEG      HFMM     AF3           457               118        123          -2.6     -2.3      High
413  IG160312016B1-Deci.EEG      HFMM     AF3           435               105        214          -2.2     -1.6      High
422  IJB18042016B1-Deci.EEG      HFMM     AF3           408               110        121          -1.3     -1.1      High
439   MC31032016B1-Deci.EEG      HFMM     AF3           438               101        116          -4.0     -3.3      High
        Hemisphere  Region      Lexicality Subject
380       Left AnterioFrontal       Word Subject08
388       Left AnterioFrontal       Word Subject14
397       Left AnterioFrontal       Word Subject12
413       Left AnterioFrontal       Word Subject02
422       Left AnterioFrontal       Word Subject10
439       Left AnterioFrontal       Word Subject05
</code></pre>

<p>For present purposes, my dependent variable is <code>ErpMinAv</code>, a continuus numerical variable and my independent variables are <code>Lexicality</code> (2 levels), <code>Frequency</code>(2 levels), <code>Hemisphere</code>(3 levels) and <code>Region</code> (5 levels).</p>

<p>Moreover, I have tested for sphericity and the result is highly significant, meaning my data violate the sphericity assumptions. </p>

<p>I have already run anovas and regressions on my data, but I am always onsure of some things:</p>

<ol>
<li>Should I be using anova if I am violating the sphericity assumption? I understand most statistical programs include corrections in the tests themselves, but from what I have read on the internet, this is not necessarily the case in R. I have seen people recommending the use of <code>lm()</code> or <code>lme4()</code>. </li>
<li><p>How should I order my independent variables in the R syntax? So far, I tried:</p>

<pre><code>MM_Model &lt;- aov(ErpMinAv ~ Lexicality*Frequency*Hemisphere*Region 
                + Error(Subject),data = MM_Table)
</code></pre></li>
</ol>

<p>But I have seen different ways of ordering the factors (e.g. with sums instead of multiplication signs) and many different ways of writing the <code>Error()</code> term (e.g. like this: <code>Error(Subject/(v1*v2*v3*v4</code>). I have not, however, been able to find explanations for a case where there is no sphericity and the data is unbalanced. </p>
"
"0.0626633989716535","0.0662266178532522","234652","<p>I have a dataframe which has <code>SUBJECT_ID</code>, <code>VISIT_NAME</code> and <code>BIOMARKER</code> measurement levels and <code>ARM_GROUP</code> as the four columns. The visit names has three levels - visit 1, visit 2 and visit 3. Subjects fall into two groups - cases and controls. </p>

<p>The study is unbalanced. </p>

<p>The objective is to find if there are any differences in the biomarker measurements between the 3 visits and the 2 groups. This calls for a repeated measures ANOVA. </p>

<p>The R code I use for this:</p>

<pre><code>anova = aov(BIOMARKER~VISIT_NAME*ARM_GROUP + Error(SUBJECT_ID) , data = studyData)
</code></pre>

<p>Now, the output from R looks like this:</p>

<pre><code>Error: SUBJECT_ID
                                      Df Sum Sq Mean Sq F value Pr(&gt;F)
ARM_GROUP                             1     29    29.1   0.074  0.786
VISIT_NAME                            2   1220   610.0   1.551  0.214
ARM_GROUP:VISIT_NAME                  2    945   472.7   1.202  0.302
Residuals                             269 105790   393.3               

Error: Within
                                      Df Sum Sq Mean Sq F value   Pr(&gt;F)    
VISIT_NAME                            2   2182  1091.2  13.759 2.54e-06 ***
PLANNED_ARM_DESC:VISIT_NAME           2    175    87.4   1.102    0.334    
Residuals                             198  15703    79.3                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>This is confusing because <code>Error (SUBJECT_ID)</code> contains variables from within subjects analysis also. Is this because the study is unbalanced? What will be the correct way to approach this problem?</p>
"
"0.130444267050272","0.127256952595156","235018","<p>I am trying to use lmer function from lme4 package to estimate differences between two response curves from a control and treatment responses over time, leaving Subjects as random effect. Here the data:</p>

<pre><code>&gt; df
   Day Subject    Levels   Response
1   10    A001   Control 0.19672131
2   10    A002 Treatment 0.16830515
3   10    A003   Control 0.21355398
4   10    A004   Control 0.18644068
5   10    A005 Treatment 0.17231538
6   10    A007 Treatment 0.18448729
7   11    A001   Control 0.23774081
8   11    A002 Treatment 0.25000000
9   11    A003   Control 0.17288616
10  11    A004   Control 0.25843209
11  11    A005 Treatment 0.29505507
12  11    A007 Treatment 0.27315358
13  12    A001   Control 0.37851189
14  12    A002 Treatment 0.39753941
15  12    A003   Control 0.30925738
16  12    A004   Control 0.45247148
17  12    A005 Treatment 0.37485050
18  12    A007 Treatment 0.41668477
19  13    A001   Control 0.47589286
20  13    A002 Treatment 0.48965316
21  13    A003   Control 0.46696617
22  13    A004   Control 0.50611299
23  13    A005 Treatment 0.41968785
24  13    A007 Treatment 0.51708049
25  14    A001   Control 0.58793970
26  14    A002 Treatment 0.45247189
27  14    A003   Control 0.43121189
28  14    A004   Control 0.56663276
29  14    A005 Treatment 0.37929057
30  14    A007 Treatment 0.46441606
31  15    A001   Control 0.44310684
32  15    A002 Treatment 0.38066676
33  15    A003   Control 0.32576304
34  15    A004   Control 0.39422772
35  15    A005 Treatment 0.28628568
36  15    A007 Treatment 0.34023209
37  16    A001   Control 0.25967359
38  16    A002 Treatment 0.20789686
39  16    A003   Control 0.23629368
40  16    A004   Control 0.22833444
41  16    A005 Treatment 0.24163539
42  16    A007 Treatment 0.21100646
43  17    A001   Control 0.17009653
44  17    A002 Treatment 0.13781610
45  17    A003   Control 0.19149637
46  17    A004   Control 0.21317316
47  17    A005 Treatment 0.17746651
48  17    A007 Treatment 0.15096285
49  18    A001   Control 0.15408115
50  18    A002 Treatment 0.16038546
51  18    A003   Control 0.18361628
52  18    A004   Control 0.18867523
53  18    A005 Treatment 0.20131984
54  18    A007 Treatment 0.19504027
55  19    A001   Control 0.21285064
56  19    A002 Treatment 0.19435679
57  19    A003   Control 0.23979739
58  19    A004   Control 0.24010952
59  19    A005 Treatment 0.20209201
60  19    A007 Treatment 0.25806452
61  20    A001   Control 0.23613019
62  20    A002 Treatment 0.20014232
63  20    A003   Control 0.26122983
64  20    A004   Control 0.26375544
65  20    A005 Treatment 0.17656201
66  20    A007 Treatment 0.22391777
67  21    A001   Control 0.20523904
68  21    A002 Treatment 0.18967355
69  21    A003   Control 0.22878808
70  21    A004   Control 0.26186233
71  21    A005 Treatment 0.18644467
72  21    A007 Treatment 0.18347698
73  22    A001   Control 0.19849361
74  22    A002 Treatment 0.16430202
75  22    A003   Control 0.23331322
76  22    A004   Control 0.25791045
77  22    A005 Treatment 0.18159936
78  22    A007 Treatment 0.17076203
79  23    A001   Control 0.17558492
80  23    A002 Treatment 0.12551814
81  23    A003   Control 0.21406131
82  23    A004   Control 0.22028128
83  23    A005 Treatment 0.17529323
84  23    A007 Treatment 0.14576150
85  24    A001   Control 0.15733775
86  24    A002 Treatment 0.12099877
87  24    A003   Control 0.22833499
88  24    A004   Control 0.15324628
89  24    A005 Treatment 0.15217124
90  24    A007 Treatment 0.09604689
</code></pre>

<p>Now I try to fit a 6th order polynomial with a base model with no categorical variables, one to assess the intercept and one to assess the interaction between terms</p>

<pre><code>library(lme4)

model.base=lmer(Response ~ poly(Day, 6, raw=FALSE)+(Day | Subject), df)
model.1=lmer(Response ~ poly(Day, 6, raw=FALSE)+Levels+(Day | Subject), df)
model.2=lmer(Response ~ poly(Day, 6, raw=FALSE)*Levels+(Day | Subject), df)
</code></pre>

<p>Then I use <code>anova</code> function to assess the model improvement</p>

<pre><code>&gt; anova(model.base,model.1,model.2)
refitting model(s) with ML (instead of REML)
Data: df
Models:
model.base: Response ~ poly(Day, 6, raw = FALSE) + (Day | Subject)
model.1: Response ~ poly(Day, 6, raw = FALSE) + Levels + (Day | Subject)
model.2: Response ~ poly(Day, 6, raw = FALSE) * Levels + (Day | Subject)
           Df     AIC     BIC logLik deviance   Chisq Chi Df Pr(&gt;Chisq)   
model.base 11 -302.85 -275.35 162.42  -324.85                             
model.1    12 -309.60 -279.61 166.80  -333.60  8.7579      1   0.003083 **
model.2    18 -313.00 -268.00 174.50  -349.00 15.3978      6   0.017378 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and now my question is how can I plot the fitted data from the model and their confidence interval around the fitted lines similar to this example in <code>ggplot</code></p>

<pre><code>ggplot(df, aes(Day, Response, color = Levels)) +
  geom_point()+
  scale_x_continuous(breaks = c(seq(10,26,2)), limits = c(9.5,26.5))+
  stat_smooth(method=""lm"", se=TRUE, 
              formula=y ~ poly(x, 6, raw=FALSE))
</code></pre>

<p><a href=""http://i.stack.imgur.com/9pEhF.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9pEhF.jpg"" alt=""Model fit""></a></p>

<p>So far I have tried <code>confint</code>, <code>effects</code> and <code>lsmeans</code> packages to extract the confidence intervals, being unsuccessful.</p>

<p>Do you have any idea how this could be done?</p>
"
