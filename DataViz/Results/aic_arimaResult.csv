"V1","V2","V3","V4"
"0.144588807813564","0.140719508946058","  6329","<p>I've been using the ets() and auto.arima() functions from the <a href=""http://robjhyndman.com/software/forecast/"">forecast package</a> to forecast a large number of univariate time series.  I've been using the following function to choose between the 2 methods, but I was wondering if CrossValidated had any better (or less naive) ideas for automatic forecasting.</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"") {
    XP=ets(x, ic=ic) 
    AR=auto.arima(x, ic=ic)

    if (get(ic,AR)&lt;get(ic,XP)) {
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
        model
}
</code></pre>

<p>/edit: What about this function?</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"",holdout=0) {
    S&lt;-start(x)[1]+(start(x)[2]-1)/frequency(x) #Convert YM vector to decimal year
    E&lt;-end(x)[1]+(end(x)[2]-1)/frequency(x)
    holdout&lt;-holdout/frequency(x) #Convert holdout in months to decimal year
    fitperiod&lt;-window(x,S,E-holdout) #Determine fit window

    if (holdout==0) {
        testperiod&lt;-fitperiod
    }
    else {
        testperiod&lt;-window(x,E-holdout+1/frequency(x),E) #Determine test window
    }

    XP=ets(fitperiod, ic=ic)
    AR=auto.arima(fitperiod, ic=ic)

    if (holdout==0) {
        AR_acc&lt;-accuracy(AR)
        XP_acc&lt;-accuracy(XP)
    }
    else {
        AR_acc&lt;-accuracy(forecast(AR,holdout*frequency(x)),testperiod)
        XP_acc&lt;-accuracy(forecast(XP,holdout*frequency(x)),testperiod)
    }

    if (AR_acc[3]&lt;XP_acc[3]) { #Use MAE
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
    model
}
</code></pre>

<p>The ""holdout"" is the number of periods you wish to use as an out of sample test.  The function then calculates a fit window and a test window based on this parameter.  Then it runs the auto.arima and ets functions on the fit window, and chooses the one with the lowest MAE in the test window.  If the holdout is equal to 0, it tests the in-sample fit.</p>

<p>Is there a way to automatically update the chosen model with the complete dataset, once it has been selected?</p>
"
"0.096392538542376","0.0938130059640389","  6513","<p>I want to predict inter-day electricity load. My data are electricity loads for 11 months, sampled in 30 minute intervals. I also got the weather-specific data from a meteorological station (temperature, relative humidity, wind direction, wind speed, sunlight). From this, I want to predict the electricity load until the end of the day. </p>

<p>I can run my algorithm until 10:00 of the present day and after that it should give the prediction of loads in 30 minute intervals. So, it should tell the load at 10:30, 11:00, 11:30 and so on until 24:00.</p>

<p>My first attempt was to create a <strong>linear model</strong> in R.</p>

<pre><code>BP.TS &lt;- ts(Buying.power, frequency = 48)
a &lt;- data.frame(
Time, BP.TS, Weekday, Pressure, Temperature, RelHumidity, AvgWindSpeed, AvgWindDirection, MaxWindSpeed, MaxWindDirection, SunLightTime,
m, Buying.2dayago, AfterHolidayAndBPYesterday8, MovingAvgLast7DaysMidnightTemp
)
a &lt;- a[(6*48+1):nrow(a),]

start = 9716
steps.ahead = 21
par(mfrow=c(5,2))
for (i in 1:10) {
    train &lt;- a[1:(start+(i-1)*48),]
    test &lt;- a[((i-1)*48+start+1):((i-1)*48+start+steps.ahead),]
    summary(reg &lt;- lm(log(BP.TS)~., data=train, na.action=NULL))
    pred &lt;- exp(predict(reg, test))

    plot(test$BP.TS, type=""o"")
    lines(pred, col=2)
    cat(""MAE"", mean(abs(test$BP.TS - pred)), ""\n"")
}
</code></pre>

<p>This is not very succesful. Now I try to model the data with ARIMA. I used auto.arima() from the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"">forecast package</a>. These are the results I got:</p>

<pre><code>&gt; auto.arima(BP.TS)
Series: BP.TS 
ARIMA(2,0,1)(1,1,2)[48]                    

Call: auto.arima(x = BP.TS) 

Coefficients:
         ar1      ar2     ma1    sar1     sma1    sma2
      1.1816  -0.2627  -0.554  0.4381  -1.2415  0.3051
s.e.  0.0356   0.0286   0.033  0.0952   0.0982  0.0863

sigma^2 estimated as 256118:  log likelihood = -118939.7
AIC = 237893.5   AICc = 237893.5   BIC = 237947
</code></pre>

<p>Now if I try something like:</p>

<pre><code>reg = arima(train$BP.TS, order=c(2,0,1), xreg=cbind(
train$Time, 
train$Weekday, 
train$Pressure, 
train$Temperature, 
train$RelHumidity, 
train$AvgWindSpeed, 
train$AvgWindDirection, 
train$MaxWindSpeed, 
train$MaxWindDirection, 
train$SunLightTime,
train$Buying.2dayago,
train$MovingAvgLastNDaysLoad,
train$X1, train$X2, train$X3, train$X4, train$X5, train$X6, train$X7, train$X8, train$X9, 
train$X11, train$X12, train$X13, train$X14, train$X15, train$X16, train$X17, train$X18, 
train$MovingAvgLast7DaysMidnightTemp
))

p &lt;- predict(reg, n.ahead=21, newxreg=cbind(
test$Time, 
test$Weekday, 
test$Pressure, 
test$Temperature, 
test$RelHumidity, 
test$AvgWindSpeed, 
test$AvgWindDirection, 
test$MaxWindSpeed, 
test$MaxWindDirection, 
test$SunLightTime,
test$Buying.2dayago,
test$MovingAvgLastNDaysLoad,
test$X1, test$X2, test$X3, test$X4, test$X5, test$X6, test$X7, test$X8, test$X9, 
test$X11, test$X12, test$X13, test$X14, test$X15, test$X16, test$X17, test$X18, 
test$MovingAvgLast7DaysMidnightTemp
))

plot(test$BP.TS, type=""o"", ylim=c(6300,8300))
par(new=T)
plot(p$pred, col=2, ylim=c(6300,8300))
cat(""MAE"", mean(p$se), ""\n"")
</code></pre>

<p>I get even worse results. Why? I ran out of ideas, so please help. If there is additional information I need to give, please ask.</p>
"
"0.0590281336100955","0.0574484989621426","  8750","<p>If I have an arima object like <code>a</code>:</p>

<pre><code>set.seed(100)
x1 &lt;- cumsum(runif(100))
x2 &lt;- c(rnorm(25, 20), rep(0, 75))
x3 &lt;- x1 + x2

dummy = c(rep(1, 25), rep(0, 75))

a &lt;- arima(x3, order=c(0, 1, 0), xreg=dummy)
print(a)
</code></pre>

<p>.</p>

<pre><code>Series: x3 
ARIMA(0,1,0)                    

Call: arima(x = x3, order = c(0, 1, 0), xreg = dummy) 

Coefficients:
        dummy
      17.7665
s.e.   1.1434

sigma^2 estimated as 1.307:  log likelihood = -153.74
AIC = 311.48   AICc = 311.6   BIC = 316.67
</code></pre>

<p>How do calculate the R squared of this regression?</p>
"
"0.166956774225936","0.162488892740478"," 13950","<p>As with my previous question, I'm looking at ways to impute missing data in a hierarchical time series data.</p>

<p>With al my other procedures, including the experimentation of imputation packages (<code>Amelia</code>, <code>HoltWinters</code> from <code>Forecast</code> and <code>MICE</code> imputation) I've only been able to use the time series data prior to the missing gap.</p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2001 220 194 238 190 217 244 242 225 242 259 267 244
2002 212 246 250 236 261 286 265 269 226 267 234 246
2003 202 199 297 272 236 266 235 226 260 183 226 265
2004 211 215 219 213 240 236 273 266 262 244 241 235
2005 212 198 233 251 259 282 305 267 241 264 222 269
2006 182 220 250 287 279 281 286 332 300 272 221 233
2007  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
2008 193 215 235 242 246 315 326 280 279 239 236 258
2009 246 189 257 241 268 223 260 288 234 260 216 195
</code></pre>

<p>I'm trying to do simple imputation procedure that uses forecasting and backcasting estimates from the time series model. Forecasting using prior data to predict the future and backcasting  using the later data to â€œpredictâ€ the past.</p>

<p>I would then like to combine the forecast and backcast value to use as imputation. After which I will look at the fit etc.</p>

<p>How do I go about this in coding? </p>

<p>For example, I'm able to determine what SARIMA model exist for the first period 2001-end2006. But not the full period (because my basic functions I know from R does not support the NA values.)</p>

<p>This is only for the period 2001-end2006:</p>

<pre><code>ARIMA(2,0,2)(1,0,1)[12] with non-zero mean 

Call: auto.arima(x = ts.datt) 

Coefficients:
         ar1      ar2      ma1     ma2    sar1     sma1  intercept
      1.3610  -0.8258  -1.2407  0.9191  0.8982  -0.7560   244.8374
s.e.  0.0884   0.0960   0.0878  0.1127  0.2190   0.3335     6.1894

sigma^2 estimated as 605.9:  log likelihood = -335.01
AIC = 686.02   AICc = 688.3   BIC = 704.23
</code></pre>

<p>Should I just model the first period, predict by <code>forecast</code>; model then the last period separately and then backcast? How will I do this backcasting (ie. 'predicting' the past)?</p>

<p><strong>EDIT:</strong>
What I'm asking:
1) How do I use the data from years 2008 &amp; 2009 to BACKCAST? I already know how to use 2001-2006 to forecast. </p>

<p>2) How do I determine the SARIMA model for the whole period? (2001-2009) ie. </p>
"
"0.216671721458714","0.250412201434081"," 14742","<p>I am fitting an ARIMA model on a daily time series.
Data are collected daily from 02-01-2010 to 30-07-2011 and are about newspaper sales.
Since a weekly pattern in sales can be found (the daily average amount of copies sold is usually the same from Monday to Friday, then increases on Saturday and Sunday), I am trying to capture this ""seasonality"".
Given the sales data ""data"", I create the time series as follows:</p>

<pre><code>salests&lt;-ts(data,start=c(2010,1),frequency=365)
</code></pre>

<p>and then I use the auto.arima(.) function to select the best ARIMA model via AIC criterion. The result is always a non-seasonal ARIMA model, but if I try some SARIMAs model with the following syntax as example:</p>

<pre><code>sarima1&lt;-arima(salests, order = c(2,1,2), seasonal = list(order = c(1, 0, 1), period = 7))
</code></pre>

<p>I can obtain better results.
Is there anything wrongs in the ts command / arima specification? The weekly pattern is very strong so I would not expect so many difficulties in capturing it. 
Any help would be very useful.
Thank you,
Giulia Deppieri</p>

<p>Update:</p>

<p>I have already changed some arguments. More precisely, the procedure selects ARIMA(4,1,3) as the best model when I set <code>D=7</code>, but AIC and the others good of fit indexes and forecasts as well) do not improve at all. I guess there's some mistakes due to confusion between seasonality and periodicity..?! </p>

<p>Auto.arima call used and output obtained:</p>

<pre><code>modArima&lt;-auto.arima(salests,D=7,max.P = 5, max.Q = 5)



 ARIMA(2,1,2) with drift         : 1e+20
 ARIMA(0,1,0) with drift         : 5265.543
 ARIMA(1,1,0) with drift         : 5182.772
 ARIMA(0,1,1) with drift         : 1e+20
 ARIMA(2,1,0) with drift         : 5137.279
 ARIMA(2,1,1) with drift         : 1e+20
 ARIMA(3,1,1) with drift         : 1e+20
 ARIMA(2,1,0)                    : 5135.382
 ARIMA(1,1,0)                    : 5180.817
 ARIMA(3,1,0)                    : 5117.714
 ARIMA(3,1,1)                    : 1e+20
 ARIMA(4,1,1)                    : 5045.236
 ARIMA(4,1,1) with drift         : 5040.53
 ARIMA(5,1,1) with drift         : 1e+20
 ARIMA(4,1,0) with drift         : 5112.614
 ARIMA(4,1,2) with drift         : 4953.417
 ARIMA(5,1,3) with drift         : 1e+20
 ARIMA(4,1,2)                    : 4960.516
 ARIMA(3,1,2) with drift         : 1e+20
 ARIMA(5,1,2) with drift         : 1e+20
 ARIMA(4,1,3) with drift         : 4868.669
 ARIMA(5,1,4) with drift         : 1e+20
 ARIMA(4,1,3)                    : 4870.92
 ARIMA(3,1,3) with drift         : 1e+20
 ARIMA(4,1,4) with drift         : 4874.095

 Best model: ARIMA(4,1,3) with drift        
</code></pre>

<p>So I assume the arima function should be used as:</p>

<pre><code>bestOrder &lt;- cbind(modArima$arma[1],modArima$arma[5],modArima$arma[2])
sarima1&lt;-arima(salests, order = c(4,1,3))
</code></pre>

<p>with no seasonal component parameters and period specifications.
Data and exploratory analysis show that the same weekly pattern can be approximatively considered for each week, with the only exception of August 2010 (when a consistent increase in sales is registered). Unfortunately I have no expertise in timeseries modeling at all, in fact I am trying this approach in order to find an alternative solution to other parametric e non-parametric models I have tried to fit for these problematic data.
I have also many dependent numeric variables but they have shown low power in explaining the response variable: undoubtedly, the most difficult part to model is the time component. Moreover, the construction of dummy variables to represent months and weekdays turned out not to be a robust solution.  </p>
"
"0.0885422004151433","0.114896997924285"," 19549","<p>I have univariate time series data (windspeed at a particular place) measured at 1 hour interval for 5 years. </p>

<p>I used <code>auto.arima()</code> to get the following parameters:</p>

<pre><code>              ar1      ar2     ma1     ma2    intercept
             1.5314  -0.55   -0.1261  0.032    10.1223
     s.e.    0.0105  0.0103   0.011   0.006     0.1211

     sigma^2 estimated as 0.4865 : log likelihood = -83546.65
     AIC = 167105.3   AICc = 167105.3    BIC = 167161    
</code></pre>

<p>I am forecasting using the following equation:</p>

<pre><code>e[t] &lt;- rnorm(1, 0, sqrt(sigma^2))
x[t] &lt;- ar1*x[t-1] + ar2*x[t-2] + e[t] + ma1*e[t-1] + ma2*e[t-2]
</code></pre>

<p>When the result is compared with <code>forecast()</code> function, I get completely different answers. The freq spectrum of <code>forecast()</code> function's output resembles original time-series freq spectrum. While the manual forecast signal looks like noise in freq spectrum.</p>

<p>I can't use <code>forecast()</code> function because the application is in C++. Are the equations correct? What's the right way of forecasting from coefficients?    </p>
"
"0.146087177447694","0.162488892740478"," 20254","<p>With the arima function I found some nice results, however now I have trouble interpreting them for use outside R.
I am currently struggling with the MA terms, here is a short example:</p>

<pre><code>ser=c(1, 14, 3, 9)        #Example series
mod=arima(ser,c(0,0,1))   #From {stats} library
mod

#Series: ser
#ARIMA(0,0,1) with non-zero mean
#
#Coefficients:
#          ma1  intercept
#      -0.9999     7.1000
#s.e.   0.5982     0.8762
#
#sigma^2 estimated as 7.676:  log likelihood = -10.56
#AIC = 27.11   AICc = Inf   BIC = 25.27

mod$resid

#Time Series:
#Start = 1
#End = 4
#Frequency = 1
#[1] -4.3136670  3.1436951 -1.3280435  0.6708065

predict(mod,n.ahead=5)

#$pred
#Time Series:
#Start = 5
#End = 9
#Frequency = 1
#[1] 6.500081 7.100027 7.100027 7.100027 7.100027
#
#$se
#Time Series:
#Start = 5
#End = 9
#Frequency = 1
#[1] 3.034798 3.917908 3.917908 3.917908 3.917908
?arima
</code></pre>

<p>When looking at the specification this formula is presented:
<code>X[t] = a[1]X[t-1] + â€¦ + a[p]X[t-p] + e[t] + b[1]e[t-1] + â€¦ + b[q]e[t-q]</code></p>

<p>Given my choice of AR and MA terms, and considering that I have included a constant this should reduce to:
<code>X[t] =  e[t] + b[1]e[t-1] + constant</code></p>

<p>However this does not hold up when i compare the results from R with manual calculations:
<code>6.500081 != 6.429261 == -0.9999 * 0.6708065 + 7.1000</code></p>

<p>Furthermore I can also not succeed in reproducing the insample errors, assuming i know the first one this should be possible:
<code>-4.3136670 * -0.9999 +7.1000 !=  14 - 3.1436951</code>
<code>3.1436951 * -0.9999 +7.1000 !=   3 + 1.3280435</code>
<code>-1.3280435 * -0.9999 +7.1000 !=  9 - 0.6708065</code></p>

<p>I hope someone can shed some light on this matter so I will actually be able to use the nice results that I have obtained.</p>
"
"NaN","NaN"," 20586","<p>I was wondering: is there are a package in R for automated <a href=""http://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity#GARCH"" rel=""nofollow"">GARCH</a> model selection?  I'm thinking of something like what the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast package</a> does for <a href=""http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=forecast%3aauto.arima"" rel=""nofollow"">ARIMA models</a>.</p>

<p>If I implement this myself, would it be appropriate to just do a grid search over the possible parameters for the GARCH and ARIMA parts of the model (using the <a href=""http://cran.r-project.org/web/packages/rugarch/index.html"" rel=""nofollow"">rugarch package</a>), and select the one with the lowest AIC (or BIC)?</p>
"
"0.10223972648865","0.0995037190209989"," 23881","<p>I've got an ARIMA(1,1,4) model using external regressor with acceptable output but I'm not able to reproduce it outside the R. </p>

<p>this is the result for the model:</p>

<pre><code>Coefficients:
         ar1      ma1     ma2      ma3     ma4  XRegressor[1:39, ]_coeff
      0.9500  -1.0202  0.3977  -0.8283  0.6030                0.0084
s.e.  0.1106   0.1999  0.1953   0.2003  0.1526                0.0059

sigma^2 estimated as 9619542:  log likelihood=-360.56
AIC=735.11   AICc=738.84   BIC=746.57
</code></pre>

<p>The formula I'm using is as follows:</p>

<pre><code>x(t) = x(t-1)(1+ar1) - ar1*x(t-2) + XRegressor[1:39, ]_coeff*
  [xreg(t) - (1+ar1)*xreg(t-1) + ar1*xreg(t-2)] + 
  ma1*e(t-1) + ma2*e(t-2) + ma3*e(t-3) + ma4*e(t-4)
</code></pre>

<p>I'm using residuals as error term in above formula. I could get right result in one step ahead forecast and for further steps, I won't have residuals to substitute in formula. Even by deleting MA part from model, it's not working. Do I miss something here? Can I say by deleting MA part, I'm erasing residual effects?</p>

<p>Thanks a lot for your help in advance.</p>
"
"0.118056267220191","0.0861727484432139"," 26999","<p>My data is a time series of employed population, L, and the time span, year.</p>

<pre><code>n.auto=auto.arima(log(L),xreg=year)
summary(n.auto)
Series: log(L) 
ARIMA(2,0,2) with non-zero mean 

Coefficients:
         ar1      ar2      ma1     ma2  intercept    year
      1.9122  -0.9567  -0.3082  0.0254    -3.5904  0.0074
s.e.     NaN      NaN      NaN     NaN     1.6058  0.0008

sigma^2 estimated as 1.503e-06:  log likelihood=107.55
AIC=-201.1   AICc=-192.49   BIC=-193.79

In-sample error measures:
           ME          RMSE           MAE           MPE          MAPE 
-7.285102e-06  1.225907e-03  9.234378e-04 -6.836173e-05  8.277295e-03 
         MASE 
 1.142899e-01 
Warning message:
In sqrt(diag(x$var.coef)) : NaNs produced
</code></pre>

<p>why does this happen? Why would auto.arima selects the best model with std error of these ar* ma* coefficients Not a Number? Is this selected model valid after all?</p>

<p>My goal is to estimate the parameter n in the model L=L_0*exp(n*year). Any suggestion of a better approach?</p>

<p>TIA.</p>

<p>data:</p>

<pre><code>L &lt;- structure(c(64749, 65491, 66152, 66808, 67455, 68065, 68950, 
69820, 70637, 71394, 72085, 72797, 73280, 73736, 74264, 74647, 
74978, 75321, 75564, 75828, 76105), .Tsp = c(1990, 2010, 1), class = ""ts"")
year &lt;- structure(1990:2010, .Tsp = c(1990, 2010, 1), class = ""ts"")
L
Time Series:
Start = 1990 
End = 2010 
Frequency = 1 
 [1] 64749 65491 66152 66808 67455 68065 68950 69820 70637 71394 72085 72797
[13] 73280 73736 74264 74647 74978 75321 75564 75828 76105
</code></pre>
"
"0.0417391935564841","0.0812444463702388"," 28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.156173761888606","0.108567458176987"," 28737","<p>I have time series as </p>

<pre><code>0.4385487 0.7024281 0.9381081 0.8235792 0.7779642 1.1670665 1.1958634 1.1958634 0.8235792 0.8530141 0.8802216 1.1958634 1.1235897 1.3542734 1.3245534 0.9381081 1.1670665 1.1958634 0.8802216 1.3542734 1.1670665 4.9167998 0.9651803 0.8221709 1.1070461 1.2006974 1.3542734 0.9651803 0.9381081 0.9651803 0.8854192 1.3245534 1.1235897 1.2006974 1.1958634 0.4385487 1.3245534 4.9167998 1.2277843 0.8530141 1.0018480 0.3588158 0.8530141 0.8867365 1.3542734 1.1958634 1.1958634 0.9651803 0.8802216 0.8235792 4.9167998 1.1958634 0.9651803 0.8854192 0.8854192 1.2006974 0.8867365 0.9381081 0.8235792 0.9651803 0.4385487 0.9936722 0.8821301 1.3542734 1.1235897 1.6132899 1.3245534 1.3542734 0.8132233 0.8530141 1.1958634 1.2279813 0.8354292 1.3578511 1.1070461 0.8530141 0.9670581 1.1958634 0.7779642 1.2006974 1.1958634 0.8235792 1.3245534 0.5119648 2.3386331 0.8890464 0.8867365 4.9167998 1.2006974 1.2006974 0.6715839 4.9167998 0.7747481 4.9167998 0.8867365 1.2277843 0.8890464 1.2277843 0.8890464 1.0541099 0.8821301 
</code></pre>

<p>I am using package ""itsmr""-autofit(),""forecast""-auto.arima(),""package""--functions</p>

<ol>
<li><p>Autoregressive model</p>

<pre><code>&gt; ar(t)

Call:
    ar(x = t)

    Order selected 0  sigma^2 estimated as  0.9222 
</code></pre></li>
<li><p>ARMA model</p>

<pre><code>&gt; autofit(t)
    $phi
    [1] 0

    $theta
    [1] 0

    $sigma2
    [1] 0.9130698

    $aicc
    [1] 279.4807

    $se.phi
    [1] 0

    $se.theta
    [1] 0
</code></pre></li>
<li><p>ARIMA model</p>

<pre><code>    &gt; auto.arima(t)
    Series: t 
    ARIMA(0,0,0) with non-zero mean 

    Coefficients:
          intercept
             1.2623
    s.e.     0.0951

    sigma^2 estimated as 0.9131:  log likelihood=-138.72
    AIC=281.44   AICc=281.56   BIC=286.67
</code></pre>

<p>The auto.arima function automatically differences time series: we don't have to worry about transformation.</p>

<pre><code>&gt; auto.arima(AirPassengers)
Series: AirPassengers 
ARIMA(0,1,1)(0,1,0)[12]                    

Coefficients:
          ma1
      -0.3184
s.e.   0.0877

sigma^2 estimated as 137.3:  log likelihood=-508.32
AIC=1020.64   AICc=1020.73   BIC=1026.39`
</code></pre></li>
</ol>

<p>Which model should I select to get p,q values &amp; for forecasting purpose?</p>
"
"0.0834783871129682","0.0812444463702388"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.146087177447694","0.162488892740478"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.032742917296329","0.0318666936471454"," 37908","<p>I am new to R. I am trying to apply forecasting model Time Series (TS) Model
as follows:  </p>

<ol>
<li>Plotting original data, </li>
<li>Simple Moving Average,  </li>
<li>Auto correction(AC), Partial AC, Differencing of TS etc to get stationary time series,  </li>
<li>Fitting optimal model which gives minimum AIC, residuals from ARIMA/ARMA  </li>
<li>Normality test for residuals  </li>
<li>forecasting for future values  </li>
</ol>

<p>The forecast figures are not coming out with the accuracy that I expected. Please find following weekly incidents. </p>

<p><strong>Can anyone please help me with the right approach and sample code?</strong></p>

<p>There are some outliers in the data (# of incidents per week) due to new release of application, seasonality effect and holiday period.  </p>

<pre><code>March 11, 2011/ March 25, 2011/ June 24, 2011/December 02, 2011/ December 30, 2011/ 
March 30, 2012/ April 20, 2012/


            Time_Stamp Wkly_Cnt
1    November 19, 2010        9
2    November 26, 2010       22
3    December 03, 2010       11
4    December 10, 2010       12
5    December 17, 2010       18
6    December 31, 2010       17
7     January 07, 2011       14
8     January 14, 2011       21
9     January 21, 2011       16
10    January 28, 2011       22
11   February 04, 2011       20
12   February 11, 2011       31
13   February 18, 2011       38
14   February 25, 2011       37
15      March 04, 2011       32
16      March 18, 2011       34
17      April 01, 2011       28
18      April 08, 2011       32
19      April 15, 2011       30
20      April 29, 2011       30
21        May 06, 2011       25
22        May 13, 2011       19
23        May 20, 2011       17
24        May 27, 2011       28
25       June 03, 2011       13
26       June 10, 2011       17
27       June 17, 2011       17
28       July 01, 2011       14
29       July 08, 2011       22
30       July 15, 2011       19
31       July 22, 2011       11
32       July 29, 2011       14
33     August 05, 2011       14
34     August 12, 2011       21
35     August 19, 2011       20
36     August 26, 2011       16
37  September 02, 2011       16
38  September 09, 2011       10
39  September 16, 2011       24
40  September 23, 2011       12
41  September 30, 2011       17
42    October 07, 2011       32
43    October 14, 2011       29
44    October 21, 2011       19
45    October 28, 2011       13
46   November 04, 2011       12
47   November 11, 2011       18
48   November 18, 2011       14
49   November 25, 2011       17
50   December 09, 2011       36
51   December 16, 2011       20
52   December 23, 2011       22
53    January 06, 2012       31
54    January 13, 2012       29
55    January 20, 2012       20
56    January 27, 2012       27
57   February 03, 2012       14
58   February 10, 2012       23
59   February 17, 2012       20
60   February 24, 2012       15
61      March 02, 2012       26
62      March 09, 2012       19
63      March 16, 2012       25
64      March 23, 2012       26
65      April 06, 2012       12
66      April 13, 2012       20
67      April 27, 2012       20
68        May 04, 2012       16
69        May 11, 2012       17
70        May 18, 2012       17
71        May 25, 2012       20
72       June 01, 2012       14
73       June 08, 2012       23
74       June 15, 2012       21
75       June 22, 2012       22
76       June 29, 2012       19
</code></pre>
"
"0.166956774225936","0.162488892740478"," 43588","<p>The main <strong>problem</strong> is:  I cannot obtain similar parameter estimates with EViews and R.</p>

<p>For reasons I do not know myself, I need to estimate parameters for certain data using EViews. This is done by picking the NLS (nonlinear least squares) option and using the following formula: <code>indep_var c dep_var ar(1)</code></p>

<p>EViews <a href=""http://forums.eviews.com/viewtopic.php?f=7&amp;t=465"" rel=""nofollow"">claims</a> that they estimate linear AR(1) processes such as:
$$
Y_t = \alpha + \beta X_t + u_t
$$
where $u_t$ errors are defined as: 
$$
u_t = \rho \cdot u_{t-1} + \varepsilon
$$
by using an equivalent equation (with some algebraic substitutions):
$$
Y_t = (1 - \rho) \alpha + \rho Y_{t - 1} + \beta X_t - \rho \beta X_{t - 1} + \varepsilon_t
$$
Furthermore, <a href=""http://forums.eviews.com/viewtopic.php?f=4&amp;t=3768"" rel=""nofollow"">this thread over at the EViews forums</a> suggests that their NLS estimations are generated by the Marquardt algorithm.</p>

<p>Now, the go-to R function to estimate AR(1) processes is <code>arima</code>. However, there are two problems:  </p>

<ol>
<li>the estimates are maximum likelihood estimates; </li>
<li>the intercept estimate is not actually the intercept estimate (according to R.H. Shumway &amp; D.S. Stoffer).</li>
</ol>

<p>Therefore, I turned to the <code>nlsLM</code> function from the minpack.lm package. This function uses the Marquardt algorithm to achieve nonlinear least squares estimates, which should yield the same results as the EViews implementation (or very similar ones, at least).</p>

<p>Now the code. I have a data frame (<code>data</code>) with an independent variable and a dependent variable such as the one generated by the following code:</p>

<pre><code>data &lt;- data.frame(independent = abs(rnorm(48)), dependent = abs(rnorm(48)))
</code></pre>

<p>To estimate parameters in the equation EViews claims to estimate (3<sup>rd</sup> one on this post), I use the following commands:</p>

<pre><code>library(minpack.lm)
result &lt;-
nlsLM(dependentB ~ ((1 - theta1) * theta2) + (theta1 * dependentA) +
                    (theta3 * independentB) - (theta1 * theta3 * independentA),
data = list(dependentB = data$dependent[2:48], dependentA = data$dependent[1:47],
   independentB = data$independent[2:48], independentA = data$independent[1:47]),
start = list(theta1 = -10, theta2 = -10, theta3 = -10)
)
</code></pre>

<p>Unfortunately, the estimates output by <code>nlsLM</code> are not close to the ones output by EViews. Do you have any idea of what might be causing this? Or maybe my code is wrong?</p>

<p>Finally, I would like to say that I personally am an R user - that is exactly why I'm trying to do this in R instead of EViews. I would also love to provide you the data I'm working with but it's impossible since it's confidential data.</p>
"
"0.133863224475948","0.130280949812384"," 47419","<p>I am trying to fit a time series using the function auto.arima and I face some strange results.</p>

<p>As a first try, I use the command</p>

<pre><code>auto.arima(data,d=0,D=1,max.p=2,max.q=2,max.P=2,max.Q=2,max.order=8, xreg=xreg_past,trace=TRUE,ic=""aic"")
</code></pre>

<p>The model I get is an ARIMA(2,0,2)(0,1,1)[12] with an AIC equal to -300.14.</p>

<p>But since I know that this command will make use of the stepwise selection algorithm, I want to make a try with the tests of all possible models using the option stepwise=FALSE. </p>

<p>I thus try the command</p>

<pre><code>auto.arima(data,d=0,D=1,max.p=2,max.q=2,max.P=2,max.Q=2,max.order=8, xreg=xreg_past,stepwise=FALSE,trace=TRUE,ic=""aic"")
</code></pre>

<p>And now, the model I get is an ARIMA(0,0,2)(2,1,0)[12] with an AIC equal to -293.14. Since my second attempt takes all the models into account, this result is strange as the previous model had a lower AIC. Furthermore, If I take a look in the trace of the last function call, I see that the ARIMA(2,0,2)(0,1,1)[12] model has now an AIC of -245.13 which explains why it has been rejected. Why did the AIC value change ?</p>

<p>At least, if I use the simple command </p>

<pre><code>arima(data, order=c(2,0,2), seasonal= list(order=c(2,1,2), period=12), xreg=xreg_past)
</code></pre>

<p>I get an AIC value of -319.15, which is better that the two models provided before.</p>

<p>I think I am missing something important but I am not able to see what. Can somebody help me ?</p>

<p>Thanks in advance,</p>

<p>Regards,</p>

<p>Ludo</p>
"
"0","0.0574484989621426"," 53051","<p>I was working on ARIMA in R and I am trying not to use library <code>forecast</code> as much as possible. I have a code for finding the best ARIMA model, but it is showing some warning messages.</p>

<p>Here is my function:</p>

<pre><code>best.aic&lt;-1e8
n&lt;-length(x.ts)
for(p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
{   
        fit&lt;-arima(x.ts,order=c(p,d,q))
    fit.aic&lt;--2*fit$loglik+(log(n)+1)*length(fit$coef)
    if(fit.aic&lt;best.aic)
    {
        best.aic&lt;-fit.aic
        best.fit&lt;-fit
        best.model&lt;-c(p,d,q)
    }
}
list(best.aic,best.fit,best.model)
</code></pre>

<p>It gives the same warning 4 times:</p>

<pre><code>In arima(x.ts, order = c(p, d, q)) :
possible convergence problem: optim gave code=1
</code></pre>

<p>Also, I will appreciate any help concerning simulation of ARIMA by hand, not by <code>arima.sim</code> function. Thank you.</p>
"
"0.096392538542376","0.140719508946058"," 55961","<p>I am analyzing some tree physiology data (transpiration) in relation to a number of environmental variables (many of which are predictors such as temperature, PAR and vapour pressure deficit). </p>

<p>I have fine-scale (30 min intervals) data of these various measurements, and there are two objectives I am trying to achieve:</p>

<ol>
<li>Use the various predictors (glm?) to see which among these explain the most amount of variation in transpiration. However, since there is clear autocorrelation at this scale (i.e., trans at time $t$ is highly correlated with trans at $t+1$ etc.), I am looking to use ARIMA models with regressors. </li>
<li>I would like to construct a final predictive ARIMA model that explains the highest variation in trans, from all the different candidate models.</li>
</ol>

<p>So far, I have noticed that ccf plots show -ve lags between trans and a number of variables (rightly so, e.g., as you expect temp at time $t$ to influence transpiration at $t+1$).</p>

<p>My questions are:</p>

<ol>
<li>How do you perform an ARIMA with transpiration as the response variable and several regressors? </li>
<li>How do you know which one of the regressors to leave out? Does this have to be done manually in R (as in, add each regressor to the model, and inspect the resulting AIC)? </li>
<li>Is <code>auto.arima</code> the best way to determine the differencing term (etc.)?
(E.g., <code>auto.arima(trans, xreg=temp+vpd+......)</code>.)</li>
<li>How do you account for the lag between response variable at time $t$ and predictors at $t-1$?</li>
</ol>
"
"0.160178867394262","0.190535115826549"," 56374","<p>as I am stepping into forecasting with ARIMA models, I am trying to understand how I can improve a forecast based on ARIMA fit with seasonality and drift. </p>

<p>My data is the following time series ( over 3 years, with clear trend upwards and visible seasonality, which seems to be not supported by autocorrelation at lags 12, 24, 36??). </p>

<pre><code>    &gt; bal2sum3years.ts
             Jan     Feb     Mar     Apr     May     Jun     Jul     Aug          
    2010 2540346 2139440 2218652 2176167 2287778 1861061 2000102 2560729 
    2011 3119573 2704986 2594432 2362869 2509506 2434504 2680088 2689888 
    2012 3619060 3204588 2800260 2973428 2737696 2744716 3043868 2867416 
             Sep     Oct     Nov     Dec
    2010 2232261 2394644 2468479 2816287
    2011 2480940 2699780 2760268 3206372
    2012 2951516 3119176 3032960 3738256
</code></pre>

<p>The model that was suggested by <code>auto.arima(bal2sum3years.ts)</code> gave me the following model:</p>

<pre><code>    Series: bal2sum3years.ts 
    ARIMA(0,0,0)(0,1,0)[12] with drift         

    Coefficients:
              drift
          31725.567
    s.e.   2651.693

    sigma^2 estimated as 2.43e+10:  log likelihood=-321.02
    AIC=646.04   AICc=646.61   BIC=648.39
</code></pre>

<p>However, the <code>acf(bal2sum3years.ts,max.lag=35)</code> does not show acf coefficients higher than 0.3. The seasonality of the data is, however, pretty obvious - spike at the beginning of every year. This is what the series looks like on the graph:
<img src=""http://i.stack.imgur.com/kQi5N.png"" alt=""Original Time Series""></p>

<p>The forecast using <code>fit=Arima(bal2sum3years.ts,seasonal=list(order=c(0,1,0),period=12),include.drift=TRUE)</code> , called by function <code>forecast(fit)</code>, results in the next 12months's means being equal to the last 12 months of the data plus constant. This can be seen by calling <code>plot(forecast(fit))</code>, </p>

<p><img src=""http://i.stack.imgur.com/GJqcG.png"" alt=""Actual and Forecasted Data""></p>

<p>I have also checked the residuals, which are not autocorrelated but have positive mean ( non zero). </p>

<p>The fit does not model the original time series precisely, in my opinion ( blue the original time series, red is the <code>fitted(fit)</code>:</p>

<p><img src=""http://i.stack.imgur.com/ux3i7.png"" alt=""Original vs fit""></p>

<p>The guestion is, is the model incorrect? Am I missing something? How can I improve the model? It seems that the model literally takes the last 12 months and adds a constant to achieve the next 12 months. </p>

<p>I am a relative beginner in time series forecasting models and statistics. </p>

<p>Thank you very much for your answers!</p>
"
"NaN","NaN"," 56429","<p>I am using the example from Shumway's book. How do I get the p-values for all the coefficients? Do I divide the coeffients by the standard errors?</p>

<pre><code> Call:
    arima(x = cmort, order = c(2, 0, 0), xreg = cbind(trend, temp, temp2, part))

    Coefficients:
             ar1     ar2  intercept    trend     temp   temp2    part
          0.3848  0.4326  3075.1482  -1.5165  -0.0190  0.0154  0.1545
    s.e.  0.0436  0.0400   834.7157   0.4226   0.0495  0.0020  0.0272

    sigma^2 estimated as 26.01:  log likelihood = -1549.04,  aic = 3114.07
</code></pre>
"
"0.187439498562524","0.199007438041998"," 58097","<p>Suppose I have the following ACF and PACF (<a href=""http://uploadeasy.net/upload/cygrd.rar"" rel=""nofollow"">data</a>:
<img src=""http://i.stack.imgur.com/A7B44.png"" alt=""ap"">
I want to fit an ARMA-GARCH process. Currently I want to do the first step, specify the mean equation. The first model just uses a constant $\mu$, so no ARMA. In the second model I was thinking about a modified ARMA(1,1) or ARMA(4,4), I don't know what this is called. I want to only use the 4th lag order in the AR and MA part. So this is basically an ARMA(4,4) where the coefficients of the first three lags are set to zero.
$r_t=\delta + \epsilon_t + \alpha_4 r_{t-4}+ b_4 \epsilon_{t-4}$</p>

<p>How can I fit this model in R?</p>

<p>I tried</p>

<pre><code>   arima(logloss, order=c(4,0,4),fixed=c(0,0,0,NA,0,0,0,NA,NA))
</code></pre>

<p>First of all: Is this correct?</p>

<p>Second: Does this make sense?</p>

<p>My output is the following:</p>

<p><img src=""http://i.stack.imgur.com/nfi5A.png"" alt=""ts""></p>

<p>If I calculate the p-values via</p>

<pre><code># p-values
(1-pnorm(abs(aa$coef)/sqrt(diag(aa$var.coef))))/2
</code></pre>

<p>I get</p>

<pre><code>&gt; (1-pnorm(abs(aa$coef)/sqrt(diag(aa$var.coef))))/2
         ar1          ar2          ar3          ar4          ma1          ma2 
2.500000e-01 2.500000e-01 2.500000e-01 4.431378e-08 2.500000e-01 2.500000e-01 
         ma3          ma4    intercept 
2.500000e-01 2.523225e-06 1.886732e-01 
&gt; 
</code></pre>

<p>So can I say, that the both coefficients of the 4th lag order are highly significant, but the intercept is not significant, correct? So should I also fix it to zero?</p>

<p>If I just fit a model with a mean, so no AR or MA, I get:
<img src=""http://i.stack.imgur.com/Mr8Gp.png"" alt=""ts2""></p>

<p>So the mean is also not significant. What should I do? Fit a GARCH without a mean equation? So no mean, no AR or MA part?</p>

<p>EDIT: I played around with it and I found, that an ARIMA(5,0,5) with the first 3 lags fixed to zero and the mean fixed to zero seems to be approrpriate. The output is:
<img src=""http://i.stack.imgur.com/VxuTJ.png"" alt=""ts4"">
The AIC is smaller than in case of the ARIMA(4,0,4) with mean fixed to zero and the residuals look ok. Are my model building steps correct?</p>
"
"0.0834783871129682","0.0406222231851194"," 58265","<p>I'm currently sifting through my copy of Analysis of Financial Time Series 2nd Edition by Ruey Tsay, and one of the sections involves fitting a MA model to certain data (data set is <a href=""http://faculty.chicagobooth.edu/ruey.tsay/teaching/fts/m-ew.dat"" rel=""nofollow"">here</a>). Here's the fit with exact maximum likelihood according to the text, with certain insignificant parameters removed:</p>

<p>rt = 0.013 + a(t) + 0.181a(tâˆ’1) âˆ’ 0.121a(tâˆ’3) + 0.122a(tâˆ’9)</p>

<p>Ïƒ(a) = 0.0724</p>

<p>However, when I try to fit it with R...</p>

<pre><code>&gt; mew = read.table(""m-ew.dat"")
&gt; arima(mew,order = c(0,0,9),fixed = c(NA,0,NA,rep(0,5),NA,NA),method = ""ML"")
Call:
arima(x = mew, order = c(0, 0, 9), fixed = c(NA, 0, NA, rep(0, 5), NA, NA), 
method = ""ML"")

Coefficients:
        ma1  ma2      ma3  ma4  ma5  ma6  ma7  ma8     ma9  intercept
      0.180    0  -0.1318    0    0    0    0    0  0.1373     0.0132
s.e.  0.031    0   0.0362    0    0    0    0    0  0.0327     0.0029

sigma^2 estimated as 0.005282:  log likelihood = 1039.1,  aic = -2068.21
</code></pre>

<p>As you can see, the ma1 coefficients are the same, but ma3 and ma9 are different, even with method = ""ML"", i.e. maximum likelihood. Why is this?</p>

<p>Also, from a practical standpoint, while ma2 and ma4-ma8 may be 0 (their 95% confidence intervals overlap with 0), removing them from the model raises the AIC, lowers the p-value with regards to the Ljung-Box test on the residuals, and also lowers the log-likelihood value. Is it even worth removing these parameters if such things happen?</p>
"
"0.0834783871129682","0.0406222231851194"," 59467","<p>I am trying to learn the difference between the three approaches and their applications.</p>

<p>a) As I understand,</p>

<pre><code>AIC = -LL+K 

BIC = -LL+(K*logN)/2
</code></pre>

<p>Unless I am missing something, shouldn't the K that minimizes the AIC minimize BIC as well since N is constant. </p>

<p>I looked at this <a href=""http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other/767#767"">thread</a> but couldn't find a satisfactory answer. </p>

<p>b) According to Witten's book on Data Mining (pg 267) the definition of MDL for evaluating the quality of network is the same as BIC. Is there a difference between BIC and MDL?</p>

<p>c) What are the different approaches to compute MDL? I am looking for its application in Clustering, Time Series Analysis (ARIMA and Regime Switching) and Attribute Selection. While almost all commonly used packages in R report AIC and BIC, I couldn't find any that implements MDL and I wanted to see if I can write it myself.</p>

<p>Thank you.</p>
"
"0.0590281336100955","0.0574484989621426"," 59712","<p>I am trying to understand the way MA(q) models work.
For this purpose I have created a simple data set with only
three values. I then adapted a MA(1) model to it. The results
are shown below:</p>

<pre><code>x&lt;-c(2,5,3)
m&lt;-arima(x,order=c(0,0,1))

Series: x 
ARIMA(0,0,1) with non-zero mean 

Coefficients:
          ma1  intercept
      -1.0000     3.5000
s.e.   0.8165     0.3163

sigma^2 estimated as 0.5:  log likelihood=-3.91
AIC=13.82   AICc=-10.18   BIC=11.12
</code></pre>

<p>While the MA(1) model looks like this: 
$$X_t = c +a_t - \theta*a_{t-1}$$</p>

<p>and $a_t$ is White Noise.</p>

<p>What I cant figure out is how to get the fitted values:</p>

<pre><code>library(forecast)
fitted(m)
Time Series:
Start = 1 
End = 3 
Frequency = 1 
[1] 3.060660 4.387627 3.000000
</code></pre>

<p>I tried different ways, but I cant find out how the fitted values (<code>3.060660</code>, <code>4.387627</code> and <code>3.000000</code>) are calculated.</p>

<p>I would be very thankful for an answer!</p>
"
"0.131990919337114","0.128458748884677"," 65585","<p>I have a daily weather data set, which has, unsurprisingly, very strong seasonal effect.</p>

<p><img src=""http://i.stack.imgur.com/B5Zpo.jpg"" alt=""enter image description here""></p>

<p>I adapted an ARIMA model to this data set using the function auto.arima from forecast package.
To my surprise the function does not apply any seasonal operations- seasonal differencing, seasonal ar or ma components. Here is the model it estimated:</p>

<pre><code>library(forecast)
data&lt;-ts(data,frequency=365)
auto.arima(Berlin)

Series: data
ARIMA(3,0,1) with non-zero mean 

Coefficients:
         ar1      ar2     ar3      ma1  intercept
      1.7722  -0.9166  0.1412  -0.8487   283.0378
s.e.  0.0260   0.0326  0.0177   0.0214     1.7990

sigma^2 estimated as 5.56:  log likelihood=-8313.74
AIC=16639.49   AICc=16639.51   BIC=16676.7
</code></pre>

<p>And also the forecasts using this model are not really satisfying. Here is the plot of the forecast:
<img src=""http://i.stack.imgur.com/IkpIq.jpg"" alt=""enter image description here""></p>

<p>Can anyone give me a hint what is wrong here?</p>
"
"0.105592735469691","0.128458748884677"," 68379","<p>I'm working on the forecasting of life expectancy actually. I have written code following the  usual procedure. The results are not trustworthy because the life expectancy should have a positive slope (logically) but in my case it comes flat for the 50 years ahead. 
The ARIMA(1,1,2) is the best model with the lowest AICc. I'm in trouble with the point forecast where the straight line is flat. How do you think I can modify my code in order to get more powerful results 
<img src=""http://i.stack.imgur.com/yLPE0.jpg"" alt=""enter image description here""> </p>

<p>Here you find my code and the plot. The life expectancy is I(1). I have differenced it and the first difference is I(0). The first difference of life expectancy is stationary. Then I got the best model with the lowest AICc and forecast the the life expectancy level.</p>

<pre><code>library(forecast)
AA&lt;-Alberta$Male
AA1&lt;-ts(AA,start=1921,end=2009,frequency=1)
A2&lt;-diff(AA1,1)
fit1&lt;-arima(A2, order=c(1,1,0))
fit2 &lt;-arima(A2, order=c(1,1,1))
fit3&lt;-arima(A2, order=c(1,1,2))
fit4&lt;-arima(A2,order=c(2,1,0))
fit5&lt;-arima(A2,order=c(2,1,1))
fit6&lt;-arima(A2,order=c(2,1,2))
fit1
fit2
fit3
fit4
fit5
fit6
fit15&lt;-arima(AA1,order=c(1,1,2))
ARIMA50ALBERTA&lt;-forecast(fit15,50)
plot(ARIMA50ALBERTA)
</code></pre>

<p>These are the results:</p>

<pre><code> Point     Forecast    Lo 80    Hi 80    Lo 95    Hi 95
2010       78.63251 77.74617 79.51885 77.27697 79.98806
2011       78.59585 77.37187 79.81984 76.72393 80.46778
2012       78.61099 77.16457 80.05740 76.39888 80.82309
2013       78.60474 76.95134 80.25814 76.07608 81.13340
2014       78.60732 76.77553 80.43911 75.80584 81.40880
2015       78.60625 76.60992 80.60259 75.55312 81.65938
2016       78.60669 76.45917 80.75422 75.32234 81.89105
2017       78.60651 76.31746 80.89557 75.10570 82.10732
2018       78.60659 76.18437 81.02880 74.90213 82.31104
2019       78.60656 76.05809 81.15502 74.70901 82.50410
2020       78.60657 75.93783 81.27531 74.52509 82.68805
2021       78.60656 75.82274 81.39039 74.34907 82.86405
2022       78.60657 75.71223 81.50090 74.18006 83.03307
2023       78.60656 75.60578 81.60734 74.01727 83.19586
2024       78.60656 75.50299 81.71014 73.86006 83.35307
2025       78.60656 75.40349 81.80964 73.70789 83.50524
2026       78.60656 75.30699 81.90614 73.56030 83.65283
2027       78.60656 75.21323 81.99990 73.41691 83.79622
2028       78.60656 75.12200 82.09113 73.27738 83.93575
2029       78.60656 75.03309 82.18004 73.14141 84.07172
2030       78.60656 74.94635 82.26678 73.00874 84.20439
2031       78.60656 74.86161 82.35152 72.87915 84.33398
2032       78.60656 74.77874 82.43439 72.75242 84.46071
2033       78.60656 74.69764 82.51549 72.62838 84.58475
2034       78.60656 74.61818 82.59495 72.50686 84.70627
2035       78.60656 74.54027 82.67286 72.38771 84.82542
2036       78.60656 74.46383 82.74930 72.27080 84.94233
2037       78.60656 74.38878 82.82435 72.15602 85.05711
2038       78.60656 74.31504 82.89809 72.04324 85.16989
2039       78.60656 74.24254 82.97059 71.93236 85.28077
2040       78.60656 74.17123 83.04190 71.82330 85.38983
2041       78.60656 74.10104 83.11209 71.71596 85.49717
2042       78.60656 74.03193 83.18119 71.61027 85.60286
2043       78.60656 73.96386 83.24927 71.50615 85.70698
2044       78.60656 73.89676 83.31637 71.40354 85.80959
2045       78.60656 73.83061 83.38252 71.30237 85.91076
2046       78.60656 73.76536 83.44777 71.20258 86.01055
2047       78.60656 73.70098 83.51215 71.10412 86.10901
2048       78.60656 73.63743 83.57570 71.00694 86.20619
2049       78.60656 73.57469 83.63844 70.91098 86.30215
2050       78.60656 73.51272 83.70041 70.81620 86.39693
2051       78.60656 73.45149 83.76164 70.72256 86.49057
2052       78.60656 73.39098 83.82214 70.63002 86.58311
2053       78.60656 73.33117 83.88196 70.53855 86.67458
2054       78.60656 73.27203 83.94110 70.44810 86.76503
2055       78.60656 73.21353 83.99960 70.35864 86.85449
2056       78.60656 73.15567 84.05746 70.27014 86.94299
2057       78.60656 73.09841 84.11472 70.18257 87.03056
2058       78.60656 73.04174 84.17139 70.09590 87.11723
2059       78.60656 72.98564 84.22749 70.01010 87.20303
</code></pre>
"
"0.0590281336100955","0.0574484989621426"," 68966","<p>I am trying to manually estimate the non-seasonal components of an SARIMA (p,d,q)x(P,D,Q)[s]. I thought the estimation is going the same way like in ARIMA, but the output says somehow something different. </p>

<p>I have an autocorrelation in the acf correlogram and one significance bound at lag 1 in the pacf. That means I have an autocorrelation first order.</p>

<p>I'm confused now, why <code>auto.arima</code> is giving me the result (0,1,1)x(0,0,1)[12] instead of (1,1,0)x(0,0,1)[12]</p>

<p>Here is my code example:</p>

<pre><code>timeseries &lt;- ts(daten, start=c(1955,1), freq=12)

&gt; timeseries
      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
1955  1.8  1.7  1.5  1.2  1.5  1.5  1.6  1.8  1.5  1.5  1.6  1.3
1956  0.7  0.6  0.4  0.9  0.9  0.8  0.8  0.6  0.6  0.4  0.4  0.2
1957  0.2  0.1  0.6  0.8  0.3  0.4  0.5  0.7  0.8  0.9  1.0  1.3
1958  1.7  1.7  1.4  1.0  0.9  1.3  1.3  1.0  1.5  1.4  1.4  2.2
1959  1.3  1.7  1.7  2.2  2.8  2.5  2.2  2.3  1.8  1.6  1.3  1.4
1960  2.2  1.8  1.9  1.6  1.1  0.8  1.1  1.1  1.1  1.4  1.2  1.2
1961  0.9  1.2  1.3  0.9  0.7  0.8  0.8  1.2  1.0  1.0  1.4  1.0
1962  1.1  0.8  1.1  1.7  2.1  2.0  2.1  2.1  2.0  2.3  2.0  2.3
1963  1.6  1.9  1.6  1.4  1.6  1.8  1.8  1.9  2.5  2.3  2.2  2.1
1964  2.1  2.1  1.9  2.3  2.1  2.0  2.1  1.8  1.0  1.1  1.5  1.4
1965  1.8  1.9  2.0  2.0  2.0  2.0  2.0  2.0  2.7  2.7  3.3  3.1
1966  2.9  3.0  3.3  2.6  3.1  3.4  3.5  3.3  3.0  2.5  1.4  1.1
1967  0.9  1.0  0.4  0.8  0.0  0.0 -0.7 -0.1 -0.5 -0.1  0.3  0.8
1968  0.8  0.5  1.2  1.0  1.2  0.8  1.2  1.0  1.3  1.3  1.6  1.9
1969  2.0  2.2  2.3  2.7  2.4  2.4  2.6  2.5  2.9  2.9  2.8  2.3
1970  2.3  2.5  2.3  2.2  2.2  2.0  1.9  2.2  2.1  2.1  1.9  2.0
1971  1.9  1.8  1.8  1.1  1.6  1.9  1.9   NA 

diffts &lt;- diff(timeseries,12)
tsdisplay(diffts, lag.max=36)
</code></pre>

<p><img src=""http://i.stack.imgur.com/2MgzU.jpg"" alt=""enter image description here""></p>

<p>But <code>auto.arima</code> is giving me the following output:</p>

<pre><code>auto.arima(timeseries)

Series: timeseries 
ARIMA(0,1,1)(0,0,1)[12]                    

Coefficients:
          ma1     sma1
      -0.1280  -0.7260
s.e.   0.0684   0.0584

sigma^2 estimated as 0.07113:  log likelihood=-23.77
AIC=53.54   AICc=53.66   BIC=63.42
</code></pre>
"
"0.10223972648865","0.0995037190209989"," 69405","<p>I am fitting a model using the <code>auto.arima</code> function in package <code>forecast</code>. I get a model that is AR(1), for example. I then extract residuals from this model. How does this generate the same number of residuals as the original vector? If this is an AR(1) model then the number of residuals should be 1 less than the dimensionality of the original time series. What am I missing?</p>

<p>Example:</p>

<pre><code>require(forecast)
arprocess = as.numeric(arima.sim(model = list(ar=.5), n=100))
#auto.arima(arprocess, d=0, D=0, ic=""bic"", stationary=T)
#  Series: arprocess 
#  ARIMA(1,0,0) with zero mean     

#  Coefficients:
#          ar1
#       0.5198
# s.e.  0.0867

# sigma^2 estimated as 1.403:  log likelihood=-158.99
# AIC=321.97   AICc=322.1   BIC=327.18
r = resid(auto.arima(arprocess, d=0, D=0, ic=""bic"", stationary=T))
&gt; length(r)
  [1] 100
</code></pre>

<p>Update: Digging into the code of auto.arima, I see that it uses Arima which in turn uses <code>stats:::arima</code>. Therefore the question is really how does <code>stats:::arima</code> compute residuals for the very first observation?</p>
"
"0.196457503777974","0.159333468235727"," 76761","<p><strong>Problem:</strong> I would like to extract the BIC and AICc from an arima() object in R.</p>

<p><strong>Background:</strong> The arima() function produces an output of results, which includes the estimated coefficients, standard errors, AIC, BIC, and AICc. Let's run some sample code to see what this looks like:</p>

<pre><code># Load the sunspots dataset
data(sunspots)
# Build an ARIMA(2,0,2) model and store as an object
model &lt;- arima(x=sunspots, order=c(2,0,2), method=""ML"")
# Show a summary of the model
model 
</code></pre>

<p>The output of results for the model appears like this:</p>

<pre><code>Series: sunspots 
ARIMA(2,0,2) with non-zero mean 

Coefficients:
         ar1     ar2      ma1      ma2  intercept
      0.9822  0.0004  -0.3997  -0.1135    51.2652
s.e.  0.1221  0.1196   0.1206   0.0574     8.1441

sigma^2 estimated as 247.9:  log likelihood=-11775.69
AIC=23563.39   AICc=23563.42   BIC=23599.05
</code></pre>

<p>On the bottom line, we can see values for AIC, BIC, and AICc. (Note: this is the output shown by arima() when the forecast package has been loaded, i.e. library(forecast))</p>

<p>Accessing the AIC value is quite easy. One can simply type:</p>

<pre><code>&gt; model$aic
[1] 23563.39
</code></pre>

<p>Access to the AIC value in this manner is made possible due to the fact that it's stored as one of the model's attributes. The following code and output will make this clear:</p>

<pre><code>&gt; attributes(model)
$names
 [1] ""coef""      ""sigma2""    ""var.coef""  ""mask""      ""loglik""   
 [6] ""aic""       ""arma""      ""residuals"" ""call""      ""series""   
[11] ""code""      ""n.cond""    ""model""    

$class
[1] ""Arima""
</code></pre>

<p>Notice, however, that bic and aicc are not model attributes, so the following code is no use to us:</p>

<pre><code>&gt; model$bic
NULL
&gt; model$aicc
NULL
</code></pre>

<p>The BIC and AICc values are, indeed, calculated by the arima() function, but the object that it returns does not give us direct access to their values. This is inconvenient and I've come across others who've raised the issue. Unfortunately, I've not found a solution to the problem.</p>

<p>Can anyone out there help? Which method can I use to access the BIC and AICc from the Arima class of object.</p>

<p><strong>Note:</strong> I've suggested an answer below, but would like to hear improvements and suggestions.</p>

<p>Edit (Version details as requested):</p>

<pre><code>&gt; R.Version()
$platform
[1] ""i686-pc-linux-gnu""

$arch
[1] ""i686""

$os
[1] ""linux-gnu""

$system
[1] ""i686, linux-gnu""

$status
[1] """"

$major
[1] ""3""

$minor
[1] ""0.2""

$year
[1] ""2013""

$month
[1] ""09""

$day
[1] ""25""

$`svn rev`
[1] ""63987""

$language
[1] ""R""

$version.string
[1] ""R version 3.0.2 (2013-09-25)""

$nickname
[1] ""Frisbee Sailing""
</code></pre>
"
"0.195774171259654","0.155892367494449"," 77285","<p>I have two groups of time-series, each group represents one type of data. However within each group, each time series may be fitted with a different ARIMA(p,d,q) from the other time series in the same group. </p>

<p>I need to create a single model for each group (<code>Model_group1</code>, <code>Model_group2</code>). I tried the approach mentioned by Rob Hyndman in: 
<a href=""http://stats.stackexchange.com/questions/23036/estimating-same-model-over-multiple-time-series"">Estimating same model over multiple time series</a>.</p>

<p>I need to use these two models to classify any time series to one of these two groups. For each time series, I calculated the AIC of <code>Model_group1</code> and <code>Model_group2</code>, and the model with smaller AIC will mean that the time series belongs to its corresponding group. </p>

<p>I have three problems: </p>

<ol>
<li><p>I received a warning message </p>

<pre><code>Series: ts 
ARIMA(3,0,2) with non-zero mean 

Coefficients:
         ar1     ar2     ar3     ma1      ma2  intercept
      0.0714  0.1417  0.0000  0.0893  -0.0871     0.1169
s.e.     NaN  0.1381  0.0127     NaN   0.1436     0.0026

sigma^2 estimated as 0.2202:  log likelihood=-33822.63
AIC=67659.26   AICc=67659.26   BIC=67725.99
Warning message:
In sqrt(diag(x$var.coef)) : NaNs produced
</code></pre>

<p>This message was returned by only one of the group models. Does that mean that the fitted model is not correct? </p></li>
<li><p>I got two different results using </p>

<pre><code>auto.arima(ts, allowdrift=FALSE, stepwise=FALSE)
auto.arima(ts, allowdrift=FALSE, stepwise=TRUE)
</code></pre></li>
<li><p>When I tested the resulting models, the majority of the time-series were classified as <code>group_1</code>, even when I test one of the time series used to build the long time series of <code>group_2</code>. I need to mention here that the composed time series of <code>group_1</code> is quite shorter than the time series of <code>group_2</code>. Are there any expected reasons for that? </p></li>
</ol>
"
"0.0834783871129682","0.0812444463702388"," 77531","<p>I have obtained the following estimations and forecasts in R for a seasonal ARIMA(1, 0, 1)(1, 0, 1)[7]</p>

<blockquote>
  <p>model1</p>
</blockquote>

<p>Series: PO </p>

<p>ARIMA(1,0,1)(1,0,1)[7] with zero mean     </p>

<p>Coefficients:</p>

<pre><code>      ar1      ma1    sar1     sma1
      0.9895  -0.8241  0.9974  -0.9551
s.e.  0.0053   0.0223  0.0018   0.0136
  sigma^2 estimated as 0.07273:  log likelihood=-109.35
   AIC=228.7   AICc=228.76   BIC=252.96
</code></pre>

<blockquote>
  <p>forecast(model1, h=2)</p>
</blockquote>

<p>Point   Forecast     Lo 80    Hi 80     Lo 95    Hi 95</p>

<p>t+1      <strong>1.404053</strong> 1.0584363 1.749670 0.8754777 1.932629</p>

<p>t+2      1.266133 0.9158214 1.616444 0.7303778 1.801888</p>

<p>However, considering the back errors and observed values from below, I cannot seem to replicate the forecast for t+1 which is 1.404053, using the formula:</p>

<pre><code>x[t+1]=0.9895*x[t]+0.9974*x[t-6]-0.9895*0.9974*x[t-7]-(-0.8241)*e[t]-(-0.9551)*e[t-6]+(-0.8241)*(-0.9551)*e[t-7]
</code></pre>

<p>Instead of 1.404053 I get 1.34755</p>

<p>Point   Errors      Fitted          Observed</p>

<p>... ...</p>

<p>t-8 0.091543793 1.439935124     1.5314789</p>

<p>t-7 -0.146540485    1.40181299      1.2552725</p>

<p>t-6 -0.031449518    1.235569501     1.20412</p>

<p>t-5 -0.008707829    1.263980334     1.2552725</p>

<p>t-4 -0.009736316    1.472134314     1.462398</p>

<p>t-3 0.079273123 1.477029378     1.5563025</p>

<p>t-1 0.064970255 1.440179723     1.50515</p>

<p>t   0.135555386 1.444228211     1.5797836</p>

<p>Can you please help me understand what I am doing wrong?
Thank you!</p>
"
"0.250435161338905","0.23019259804901"," 83868","<p>There are two simple questions at the end, but I think it is also useful to share the background that motivated them. It comes from <a href=""http://stats.stackexchange.com/questions/31073/flat-ets-forecast-of-clearly-increasing-time-series"">this question</a> on an unexpected forecast from the fully automatic methodology behind the forecast::ets function in R. The code, plot and forecast are given below for convenience:</p>

<pre><code>library(forecast);options(scipen=999)
usage &lt;- ts(scan('http://cl.ly/102L0j3o1p2m0m3p0t2o/usage'), frequency = 24)

plot(f1&lt;-forecast(m1&lt;-HoltWinters(usage), h = 168))
plot(f2&lt;-forecast(m2&lt;-ets(usage), h = 168));AIC(m2) #replication OK
</code></pre>

<p><img src=""http://i.stack.imgur.com/CJ4N3.png"" alt=""enter image description here""></p>

<pre><code>plot(f3&lt;-forecast(m3&lt;-ets(usage,additive.only=TRUE), h = 168))
plot(f4&lt;-forecast(m4&lt;-ets(usage,additive.only=TRUE,damped=FALSE), h = 168))
</code></pre>

<p>Just by looking at the plot of the data it appears to me immediately that ARIMA(0,2,2) or 
ETS(AAN) will be among the best non-seasonal models (and their point forecasts will not differ much). Following the usual advice that the AIC of only a small set of potentially useful models should be compared, I can see no reason to consider multiplicative models here, nor can I see how a damped forecast will be useful for me. With this information in, the ""best"" ets model m4 and its associated forecast f4 is what was expected, but the 
process is not a fully automatic one.</p>

<p>With many time series, which I would not have time or desire to look at, I would hardly have a better option than to blindly use ets(data). The ets documentation page assures me that <em>""The methodology is fully automatic. The only required argument for ets is the time series. The model is chosen automatically if not specified""</em>. The above example is not the first one to show that the methodology that works is at best semi-automatic and the fully automatic one may fail to work as expected even in simpler cases. </p>

<p>One way to rectify the problem is to consider n $\ge$ 1 models with suitably low values of AIC as equally supported by the data. I agree with <a href=""http://stats.stackexchange.com/questions/31073/flat-ets-forecast-of-clearly-increasing-time-series"">RJH</a> (<em>""Although the model may not give the best AIC, it may give forecasts that are more useful to you.""</em>) that the most useful forecasts need not be those from the model with the smallest value of the AIC. But then we have the question of when the AIC is useful for selecting a model that forecasts best. It can be deduced from, for example, <a href=""http://stats.stackexchange.com/questions/78949/when-is-it-appropriate-to-select-models-by-minimising-the-aic"">quotes in this question</a> that if the AICs of all models considered differ by no more than some small number c, then the AIC loses its power to distinguish between these models and <a href=""http://stats.stackexchange.com/questions/81552/what-do-i-do-when-values-of-aic-are-low-but-approximately-equal"">other factors/ideas</a> must be considered.  But how small is c?</p>

<p>Those working with Akaike (Y Sakamoto and M Ishiguro and G. Kitagawa) in the book entitled ""Akaike Information Criterion statistics"" (in the section entitled ""Some remarks on the use of the AIC"") mentioned c=1. The number c=2 is often mentioned (e.g. a quote from Brian Ripley can be added to those two linked to above). The number c=4 was mentioned by Chris Chatfield in one of his books. I have not seen anything explicit on the values of c greater than four, but this probably depends on the variability of data, sampling error of the deviance and related factors. </p>

<p>In the above example, AIC(m3,m4) suggests that the AIC of the model with a more useful forecast is greater than the AIC of the other model by ""only"" 78348.75-78337.22=11.52. Are there any formulae, guidelines or rules of thumb for useful values of c given data? Have values of c greater than four been mentioned in the literature? </p>
"
"0.0681598176590997","0.0995037190209989"," 84255","<p>I am new to R and the ARIMA model and I am attemping to forecast 1440 values into the future using a base of roughly 5000 numbers. It is data extracted roughly every minute from a machine log(performance values). The intend to forecast 1 day into the future, which explains the 1440 values(as they are minutes).</p>

<p>Here is my result using the following commands:
    datats&lt;-c(data);
    arima&lt;-auto.arima(datats);
    fcast&lt;-forecast(arima, h=1440);</p>

<p><img src=""http://i.imgur.com/N7aCqvx.png"" alt=""forecast""> </p>

<p>The prediction begins at the flat line on the right hand side.</p>

<p>Forecast method: ARIMA(0,1,1)                   </p>

<p>Model Information:
Series: datats 
ARIMA(0,1,1)                    </p>

<p>Coefficients:
          ma1
      -0.9373
s.e.   0.0071</p>

<p>sigma^2 estimated as 86737:  log likelihood=-21221.46
AIC=42446.93   AICc=42446.93   BIC=42458.93</p>

<p>Error measures:
                    ME     RMSE      MAE       MPE     MAPE      MASE
Training set 0.6506441 294.4619 196.7211 -59.85254 85.45473 0.7637028
                   ACF1
Training set 0.01519673</p>

<p>Dataset here: <a href=""http://pastebin.com/92ssDExn"" rel=""nofollow"">http://pastebin.com/92ssDExn</a></p>

<p>Is the issue too little past values?
To many values to be predicted?</p>

<p>Any information or advice would be extremely welcomed, any other information required will be provided. </p>
"
"0.0590281336100955","0.0574484989621426"," 88722","<p>I am building a regression model of time series data in R, where my primary interest is the coefficients of the independent variables. The data exhibit strong seasonality with a trend.</p>

<p><img src=""http://i.stack.imgur.com/GYxaU.png"" alt=""Original data""></p>

<p>The model looks good, with four of the six regressors significant:
<img src=""http://i.stack.imgur.com/ZmoSd.png"" alt=""Model""></p>

<p>Here are the OLS residuals:
<img src=""http://i.stack.imgur.com/EIybo.png"" alt=""Residuals""></p>

<p>I used auto.arima to select the sARIMA structure, and it returns the model (0,1,1)(1,1,0)[12].</p>

<pre><code>fit.ar &lt;- auto.arima(at.ts, xreg = xreg1, stepwise=FALSE, approximation=FALSE)
summary(fit.ar)

Series: at.ts 
ARIMA(0,1,1)(1,1,0)[12]                    

Coefficients:
          ma1    sar1      v1       v2      v3       v4         v5
      -0.7058  0.3974  0.0342  -0.0160  0.0349  -0.0042  -113.4196
s.e.   0.1298  0.2043  0.0239   0.0567  0.0555   0.0333   117.1205

sigma^2 estimated as 3.86e+10:  log likelihood=-458.13
AIC=932.26   AICc=936.05   BIC=947.06

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 7906.896 147920.3 103060.4 0.1590107 3.048322 0.1150526
</code></pre>

<p>My question is this: based on the parameter estimates and s.e. of the regressors, I believe that none of them are significant - is this correct, and if so, what does it imply if my goal is to interpret the relative importance of these predictors as opposed to forecasting?</p>

<p>Any other advice relative to the process of building this model is welcome and appreciated.</p>

<p>Here are the ACF and PACF for the residuals:</p>

<p><img src=""http://i.stack.imgur.com/a3Gvy.png"" alt=""ACF-PACF""></p>

<pre><code>&gt; durbinWatsonTest(mod.ols, max.lag=12)
 lag Autocorrelation D-W Statistic p-value
   1     0.120522674     1.6705144   0.106
   2     0.212723044     1.4816530   0.024
   3     0.159828108     1.5814771   0.114
   4     0.031083831     1.8352377   0.744
   5     0.081081308     1.6787808   0.418
   6    -0.024202465     1.8587561   0.954
   7    -0.008399949     1.7720761   0.944
   8     0.040751905     1.6022835   0.512
   9     0.129788310     1.4214391   0.178
  10    -0.015442379     1.6611922   0.822
  11     0.004506292     1.6133994   0.770
  12     0.376037337     0.7191359   0.000
 Alternative hypothesis: rho[lag] != 0
</code></pre>
"
"NaN","NaN"," 89316","<p>I am using <code>auto.arima()</code> for prediction, and getting the following warning message. I want to know if I can ignore this warning message or if I should be worried.</p>

<p><code>Warning message:</code><br>
<code>In auto.arima(forecast_data_ts) :</code><br>
<code>Unable to fit final model using maximum likelihood. AIC value approximated</code></p>
"
"0.118056267220191","0.114896997924285"," 89422","<p>I have many time series(retail data). Some with trends, some seasonal, 
and some with neither. With period day, week or month. I need to make forecast, for each time serie. </p>

<p>I'm looking for the most efficient methods for forecasting in R ?
Which significant things should I know for it? 
Maybe someone has experience with random forest forecasting and would share with me?</p>

<p>Any help would be truly appreciated.</p>

<p>UPDATE 1:
For example, one of mine time series is x:</p>

<pre><code>   &gt; dput(x)
 c(1.07328072153326, 1.07385697538101, 1.10947204968944, 1.10501567398119, 
1.08808510638298, 1.07468423942889, 1.06658878504673, 1.10157194679565, 
1.10297619047619, 1.09510682288077, 1.07372549019608, 1.08457943925234, 
1.09101316542645, 1.10577472841624, 1.08926553672316, 1.0929326655537, 
1.08484848484848, 1.09699769053118, 1.10987124463519, 1.08726673984632, 
1.09157959434542, 1.10070384407147, 1.08625486922649, 1.11432506887052, 
1.0828313253012, 1.08040626322471, 1.07157157157157, 1.08369098712446, 
1.08045977011494, 1.10748560460653, 1.11616161616162, 1.08371040723982, 
1.10213414634146, 1.06835306781485, 1.07926829268293, 1.08721886999451, 
1.10216718266254, 1.1241610738255, 1.08231707317073, 1.07698961937716, 
1.08569953536396, 1.09771181199753, 1.07181984175289, 1.07288828337875, 
1.07820419985518, 1.07210031347962, 1.07450628366248, 1.06662870159453, 
1.07235494880546, 1.0979020979021, 1.08494690818239, 1.06716417910448, 
1.08305369127517, 1.08023307933662, 1.07635746606335, 1.07701786814541, 
1.08310249307479, 1.0768253968254, 1.096, 1.06787687450671, 1.07353535353535, 
1.11226993865031, 1.07641196013289, 1.08066298342541, 1.09431605246721, 
1.06678539626002, 1.06646525679758, 1.09977728285078, 1.07646420824295, 
1.0973341599504, 1.0906432748538, 1.09831824062096, 1.09302325581395, 
1.08199121522694, 1.073753605274, 1.0616937745373, 1.07997481108312, 
1.08239202657807, 1.08798283261803, 1.07748776508972, 1.0552611657835, 
1.0817746846455, 1.08978032473734, 1.08414985590778, 1.08205756276791, 
1.11405835543767, 1.11866969009826, 1.07441154138193, 1.09642703400775, 
1.07393209200438, 1.08049535603715, 1.09371428571429, 1.09732824427481, 
1.10526315789474, 1.11575091575092, 1.08680994521702, 1.10028929604629, 
1.09176340519624, 1.07464266807835, 1.10190664036818, 1.08295281582953, 
1.08928571428571, 1.09341998375305, 1.0958605664488, 1.07885714285714, 
1.07466814159292, 1.09463722397476, 1.07281903388609, 1.0812324929972, 
1.08226102941176, 1.07101616628176, 1.08390410958904, 1.08528528528529, 
1.09333333333333, 1.08073929961089, 1.09380234505863, 1.08012968967114, 
1.07717391304348, 1.07066508313539, 1.06838106370544, 1.07199032062916, 
1.08235294117647, 1.08157524613221, 1.11082474226804, 1.08620689655172, 
1.08299477655252, 1.10016420361248, 1.10140093395597, 1.08766485647789, 
1.10094850948509, 1.13925191527715, 1.11293859649123, 1.12204234122042, 
1.10141364474493, 1.11103495544894, 1.09365558912387, 1.10044313146233, 
1.11116279069767, 1.11053240740741, 1.09810671256454, 1.09899823217443, 
1.10986101919259, 1.09649805447471, 1.08765778401122, 1.09922928709056, 
1.07868303571429, 1.07439104674128, 1.08457374830852, 1.09739714525609, 
1.0873440285205, 1.07574536663981, 1.10498812351544, 1.11056105610561, 
1.09443402126329, 1.09200240529164, 1.1076573161486, 1.10090237899918, 
1.09986225895317, 1.10569105691057, 1.09090909090909, 1.10409356725146, 
1.107, 1.15349143610013, 1.08992562542258, 1.09016393442623, 
1.08549783549784, 1.07950780880265, 1.08859223300971, 1.06225680933852, 
1.08606557377049, 1.07929176289453, 1.09641873278237, 1.07554585152838, 
1.05761316872428, 1.08054085831864, 1.09245172615565, 1.09028727770178, 
1.06859756097561, 1.08278388278388, 1.06620808254514, 1.07001522070015, 
1.06319485078994, 1.06764705882353, 1.08654416123296, 1.09310113864702, 
1.06369008535785, 1.13811922753988, 1.12487100103199, 1.14294330518697, 
1.15353181552831, 1.14426229508197, 1.1380042462845, 1.16727806309611, 
1.09280544912729, 1.10660426417057, 1.13093858632677, 1.12244897959184, 
1.09134045077106, 1.10821382007823, 1.09921875, 1.12583967756382, 
1.0998268897865, 1.10657894736842, 1.12752114508783, 1.08413001912046, 
1.14484272128749, 1.0859167404783, 1.09041501976285, 1.0887537993921, 
1.05695364238411, 1.04765146358067, 1.04174820613177, 1.05854800936768, 
1.04042904290429, 1.07479752262982, 1.07179197286603, 1.05997624703088, 
1.06460369163952, 1.07920193470375, 1.081811541271, 1.08351810790835, 
1.0703933747412, 1.07135523613963, 1.0532319391635, 1.05964730290456, 
1.07206703910615, 1.07498383968972, 1.05938566552901, 1.08185840707965, 
1.06121372031662, 1.05117647058824, 1.0734494015234, 1.05576208178439, 
1.08180628272251, 1.06072555205047, 1.09534671532847, 1.08269794721408, 
1.0863453815261, 1.07660577489688, 1.11460957178841, 1.09818731117825, 
1.06873428331936, 1.08247925817472, 1.06818181818182, 1.09494725152693, 
1.11903160726295, 1.10917361637604, 1.09464701318852, 1.10445468509985, 
1.08333333333333, 1.06683804627249, 1.06380575945793, 1.07498766650222, 
1.07160253287871, 1.07565588773642, 1.05174927113703, 1.07279344858963, 
1.06560283687943, 1.06727037516171, 1.05085682697623, 1.06547285954113, 
1.08014705882353, 1.0575296108291, 1.05748725081131, 1.04852071005917, 
1.05421686746988, 1.05314846909301, 1.0538885486834, 1.04618937644342, 
1.04105344694036, 1.06053604436229, 1.06058788242352, 1.04755700325733, 
1.04994511525796, 1.05405405405405, 1.06622516556291, 1.07163323782235, 
1.07538994800693, 1.06018957345972, 1.07800751879699, 1.07815198618307, 
1.07247665629169, 1.07490217998882, 1.06998939554613, 1.05968331303289, 
1.05139565795304, 1.07414104882459, 1.09087423312883, 1.06742556917688, 
1.06096361848574, 1.07464929859719, 1.09754281459419, 1.10085400569337, 
1.08974358974359, 1.09106168694922, 1.09333865177503, 1.08897569444444, 
1.07627737226277, 1.14392723381487, 1.06422018348624, 1.07022471910112, 
1.07848837209302, 1.06617647058824, 1.0828331332533, 1.08257858284497, 
1.07761904761905, 1.06547619047619, 1.07017543859649, 1.06287069988138, 
1.09431751611013, 1.09341500765697, 1.06916019760056, 1.06135831381733, 
1.06491326245104, 1.06208955223881, 1.06825232678387, 1.06939409632315, 
1.05837912087912)
</code></pre>

<hr>

<pre><code>  x&lt;-ts(x, frequency=7)
</code></pre>

<p>When I try to:</p>

<pre><code>  plot(forecast(ets(x),h=60))
  plot(forecast(x,h=60))
</code></pre>

<p><img src=""http://i.stack.imgur.com/qubPZ.png"" alt=""plot""></p>

<p>I get the same results. Maybe someone could explain, why exponential smoothing in this case makes no difference?</p>

<p>Also I have tryed to use </p>

<pre><code> &gt; plot(forecast(auto.arima(x),h=60))

 Warning message:
In auto.arima(x) :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>
"
"0.221355501037858","0.22979399584857"," 89808","<p>I am trying to model daily sales for a take out restaurant. They are only open on business days - no holidays or weekends - as their primary clients are office workers on their lunch breaks.</p>

<p>Below is what two years of the daily sales time series looks like.</p>

<p><img src=""http://i.stack.imgur.com/UhHDR.png"" alt=""enter image description here""></p>

<p>The days with zero sales, as you can see above, are the days that the restaurant was closed due to a public holiday (Easter Monday etc.). There is definitely a weekly pattern: sales tend to peak on Thursdays. Furthermore, the presence of a holiday changes the sales pattern in the surrounding weeks (the week before and the week after).</p>

<p>You might notice that there are sales spikes before or after certain holidays. An example of this: if Monday is a holiday, sales tend to be much lower on the Friday before that long weekend - presumably office workers leaving work early.</p>

<p>There are also yearly seasonal patterns. Sales are lower in the summer, for example, presumably, in part, because many office workers are taking their vacations.</p>

<p>My approach has been to use a ARIMAX model to fit the data (using R). I've followed the approach suggested by Rob Hyndman <a href=""http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r"">here</a>. The difference is that I'm using only Mon-Fri, so my frequency is 5, and I've added dummy variables for all of the days that the restaurant is closed (holidays).</p>

<p>The model fit is not very good so far, of course. I haven't done anything to take into account the effect of a holiday on the surrounding days. Further, I'm including the holidays as days with sales equal to zero, so this must throw off the model.</p>

<p>Here is what R returns:</p>

<pre><code>ARIMA(0,1,1)(1,0,1)[5]                    

Coefficients:
     ma1     sar1    sma1      Mon       Tue       Wed      Thu     Day    Newyears   FamilyDay  GoodFriday      Easter  VictoriaDay   CanadaDay    CivicDay
  -0.804  -0.2608  0.3255  54.4530  113.8052  152.0052  -6.3025  0.0388  -1545.1973  -1604.5038  -1581.6740  -1586.8710    -1628.253  -1437.6075  -1181.0054
s.e.   0.028   0.4641  0.4529  23.2788   23.3748   23.4900  23.4367  1.5546    117.6128    113.6446    113.8825    114.5609      114.786    112.7561    114.0031
   LabourDay  Thanksgiving
  -1310.3416    -1332.8028
s.e.    113.5081      113.5179

sigma^2 estimated as 28269:  log likelihood=-3305.08
AIC=6646.16   AICc=6647.56   BIC=6722.2
</code></pre>

<p>I think I should include the month of the year as a dummy variable to capture yearly seasonal effects as well.</p>

<p>My questions:</p>

<ol>
<li>What can I do to capture ""long weekend effects""? Should I included a dummy variable for every Friday that proceeds a long weekend etc?</li>
<li>How should I deal with the holidays that the restaurant was closed for? If I remove them, then the week lengths will not be the same. If I include them, then they are outliers that throw everything off.</li>
<li>What else can I do to improve my model?</li>
</ol>

<p>Thanks very much for any input.</p>
"
"0.220863052149693","0.184245086143522"," 89851","<p>I've heard a bit about using neural networks to forecast time series. </p>

<p>How can I compare, which method for forecasting my time-series (daily retail data) is better: auto.arima(x), ets(x) or nnetar(x).</p>

<p>I can compare auto.arima with ets by AIC or BIC. But how I can compare them with neural networks?</p>

<p>For example:</p>

<pre><code>   &gt; dput(x)
 c(1774, 1706, 1288, 1276, 2350, 1821, 1712, 1654, 1680, 1451, 
 1275, 2140, 1747, 1749, 1770, 1797, 1485, 1299, 2330, 1822, 1627, 
 1847, 1797, 1452, 1328, 2363, 1998, 1864, 2088, 2084, 594, 884, 
 1968, 1858, 1640, 1823, 1938, 1490, 1312, 2312, 1937, 1617, 1643, 
 1468, 1381, 1276, 2228, 1756, 1465, 1716, 1601, 1340, 1192, 2231, 
 1768, 1623, 1444, 1575, 1375, 1267, 2475, 1630, 1505, 1810, 1601, 
 1123, 1324, 2245, 1844, 1613, 1710, 1546, 1290, 1366, 2427, 1783, 
 1588, 1505, 1398, 1226, 1321, 2299, 1047, 1735, 1633, 1508, 1323, 
 1317, 2323, 1826, 1615, 1750, 1572, 1273, 1365, 2373, 2074, 1809, 
 1889, 1521, 1314, 1512, 2462, 1836, 1750, 1808, 1585, 1387, 1428, 
 2176, 1732, 1752, 1665, 1425, 1028, 1194, 2159, 1840, 1684, 1711, 
 1653, 1360, 1422, 2328, 1798, 1723, 1827, 1499, 1289, 1476, 2219, 
 1824, 1606, 1627, 1459, 1324, 1354, 2150, 1728, 1743, 1697, 1511, 
 1285, 1426, 2076, 1792, 1519, 1478, 1191, 1122, 1241, 2105, 1818, 
 1599, 1663, 1319, 1219, 1452, 2091, 1771, 1710, 2000, 1518, 1479, 
 1586, 1848, 2113, 1648, 1542, 1220, 1299, 1452, 2290, 1944, 1701, 
 1709, 1462, 1312, 1365, 2326, 1971, 1709, 1700, 1687, 1493, 1523, 
 2382, 1938, 1658, 1713, 1525, 1413, 1363, 2349, 1923, 1726, 1862, 
 1686, 1534, 1280, 2233, 1733, 1520, 1537, 1569, 1367, 1129, 2024, 
 1645, 1510, 1469, 1533, 1281, 1212, 2099, 1769, 1684, 1842, 1654, 
 1369, 1353, 2415, 1948, 1841, 1928, 1790, 1547, 1465, 2260, 1895, 
 1700, 1838, 1614, 1528, 1268, 2192, 1705, 1494, 1697, 1588, 1324, 
 1193, 2049, 1672, 1801, 1487, 1319, 1289, 1302, 2316, 1945, 1771, 
 2027, 2053, 1639, 1372, 2198, 1692, 1546, 1809, 1787, 1360, 1182, 
 2157, 1690, 1494, 1731, 1633, 1299, 1291, 2164, 1667, 1535, 1822, 
 1813, 1510, 1396, 2308, 2110, 2128, 2316, 2249, 1789, 1886, 2463, 
 2257, 2212, 2608, 2284, 2034, 1996, 2686, 2459, 2340, 2383, 2507, 
 2304, 2740, 1869, 654, 1068, 1720, 1904, 1666, 1877, 2100, 504, 
 1482, 1686, 1707, 1306, 1417, 2135, 1787, 1675, 1934, 1931, 1456)
</code></pre>

<p>Using auto.arima:</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
points(1:length(x),fitted(y),type=""l"",col=""green"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/uwSqY.png"" alt=""enter image description here""></p>

<pre><code>&gt; summary(y)
Series: x 
ARIMA(5,1,5)                    

Coefficients:
         ar1      ar2     ar3      ar4      ar5      ma1     ma2      ma3     ma4      ma5
      0.2560  -1.0056  0.0716  -0.5516  -0.4822  -0.9584  1.2627  -1.0745  0.8545  -0.2819
s.e.  0.1014   0.0778  0.1296   0.0859   0.0844   0.1184  0.1322   0.1289  0.1388   0.0903

sigma^2 estimated as 58026:  log likelihood=-2191.97
AIC=4405.95   AICc=4406.81   BIC=4447.3

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 1.457729 240.5059 173.9242 -2.312207 11.62531 0.6157512
</code></pre>

<p>Using ets:</p>

<pre><code>fit &lt;- ets(x)
plot(forecast(fit,h=30))
points(1:length(x),fitted(fit),type=""l"",col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/9UngX.png"" alt=""enter image description here""></p>

<pre><code> &gt; summary(fit)
 ETS(M,N,N) 

 Call:
  ets(y = x) 

   Smoothing parameters:
     alpha = 0.0449 

   Initial states:
     l = 1689.128 

   sigma:  0.2094

      AIC     AICc      BIC 
 5570.373 5570.411 5577.897 

 Training set error measures:
                    ME     RMSE      MAE      MPE     MAPE      MASE
 Training set 7.842061 359.3611 276.4327 -4.81967 17.98136 0.9786665
</code></pre>

<p>In this case auto.arima fits better then ets.</p>

<p>Let's try sing neural network:</p>

<pre><code> library(caret)
 fit &lt;- nnetar(x)
 plot(forecast(fit,h=60))
 points(1:length(x),fitted(fit),type=""l"",col=""green"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/M8HIT.png"" alt=""enter image description here""></p>

<p>From the graph, I can see, that neural network model fits quite well, but how can I compare it with auto.arima/ets? How can I compute AIC?</p>

<p>Another question is, how to add confidence interval for neural network,if it is possible, like it is added automatically for auto.arima/ets.?</p>

<p>Any help and advises would be really appreciated.</p>
"
"NaN","NaN"," 91706","<p>I am trying fit an ARIMA model to stock returns.</p>

<p>I have reached a decent model using the AIC criterion. </p>

<p>However, the ljung-box p value under a diagnostic plots are pretty weird. 
The null hypothesis get rejected at higher lags.
I tried modifying the parameters, but L-B p value betters only marginally, with a loss in AIC.</p>

<p>Any help how I can balance the two ?
Also any reasons why the p-value is so low for higher lags.</p>

<p>Ihave attached the diagnostic's image:</p>

<p><img src=""http://i.stack.imgur.com/AL5p5.png"" alt=""tsdiag""></p>
"
"0.144588807813564","0.0938130059640389","100702","<p>A particular series (std), seems to exhibit a trend-like behavior. According to the ADF test for this series:</p>

<pre><code>Dickey-Fuller = -2.8618, Lag order = 6, p-value = 0.2131
</code></pre>

<p>Therefore, I am taking the first difference of std with this code</p>

<pre><code>stddif1&lt;- diff(std)
</code></pre>

<p>Here is the tricky part, the acf and pacf suggest that this would be an ARMA process (2,1), with a d=1. But the code shows both different estimates and different AIC values, when (I think) this <em>shouldn't</em> be the case:</p>

<p>For std with no difference:</p>

<pre><code>&gt; arima(std, order=c(2,1,1))

Call:
arima(x = std, order = c(2, 1, 1))

Coefficients:
     ar1     ar2      ma1
  0.5206  0.2697  -0.7638
s.e.  0.1218  0.0552   0.1153

sigma^2 estimated as 0.06355:  log likelihood = -13.3,  aic = 34.6
</code></pre>

<p>And, for the differenced std (stddif):</p>

<pre><code>&gt; arima(stddif, order=c(2,0,1))

Call:
arima(x = stddif, order = c(2, 0, 1))

Coefficients:
     ar1     ar2      ma1  intercept
  0.5188  0.2695  -0.7620    -0.0003
s.e.  0.1223  0.0554   0.1159     0.0157

sigma^2 estimated as 0.06355:  log likelihood = -13.3,  aic = 36.6
</code></pre>

<p>The values for the AR1, AR2, MA1 as well as the AIC are different. Why is this?</p>

<p>This was all done in R, the relevant package is 'tseries'.</p>
"
"0.173535255260473","0.199598843322149","104977","<p>I understand we should use ARIMA for modelling a non-stationary time series. Also, everything I read says ARMA should only be used for stationary time series.</p>

<p>What I'm trying to understand is, what happens in practice when misclassifying a model, and assuming <code>d = 0</code> for a time series that's non-stationary? For example: </p>

<pre><code>controlData &lt;- arima.sim(list(order = c(1,1,1), ar = .5, ma = .5), n = 44)
</code></pre>

<p>control data looks like this:</p>

<pre><code> [1]   0.0000000   0.1240838  -1.4544087  -3.1943094  -5.6205257
 [6]  -8.5636126 -10.1573548  -9.2822666 -10.0174493 -11.0105225
[11] -11.4726127 -13.8827001 -16.6040541 -19.1966633 -22.0543414
[16] -24.8542959 -25.2883155 -23.6519271 -21.8270981 -21.4351267
[21] -22.6155812 -21.9189036 -20.2064343 -18.2516852 -15.5822178
[26] -13.2248230 -13.4220158 -13.8823855 -14.6122867 -16.4143756
[31] -16.8726071 -15.8499558 -14.0805114 -11.4016515  -9.3330560
[36]  -7.5676563  -6.3691600  -6.8471371  -7.5982880  -8.9692152
[41] -10.6733419 -11.6865440 -12.2503202 -13.5314306 -13.4654890
</code></pre>

<p>Assuming I didn't know the data was <code>ARIMA(1,1,1)</code>, I might have a look at <code>pacf(controlData)</code>.</p>

<p><img src=""http://i.stack.imgur.com/IOXJf.jpg"" alt=""pacf(controlData)""></p>

<p>Then I use Dickey-Fuller to see if the data is non-stationary:</p>

<pre><code>require('tseries')
adf.test(controlData)

# Augmented Dickey-Fuller Test
#
# data:  controlData
# Dickey-Fuller = -2.4133, Lag order = 3, p-value = 0.4099
# alternative hypothesis: stationary

adf.test(controlData, k = 1)

# Augmented Dickey-Fuller Test
#
#data:  controlData
# Dickey-Fuller = -3.1469, Lag order = 1, p-value = 0.1188
# alternative hypothesis: stationary
</code></pre>

<p>So, I might assume the data is ARIMA(2,0,*) Then use <code>auto.arima(controlData)</code> to try to get a best fit?</p>

<pre><code>require('forecast')
naiveFit &lt;- auto.arima(controlData)
navifeFit
# Series: controlData 
# ARIMA(2,0,1) with non-zero mean 
# 
# Coefficients:
#          ar1      ar2     ma1  intercept
#      1.4985  -0.5637  0.6427   -11.8690
# s.e.  0.1508   0.1546  0.1912     3.2647
#
# sigma^2 estimated as 0.8936:  log likelihood=-64.01
# AIC=138.02   AICc=139.56   BIC=147.05
</code></pre>

<p>So, even though the past and future data is ARIMA(1,1,1), I might be tempted to classify it as ARIMA(2,0,1). <code>tsdata(auto.arima(controlData))</code> looks good too.</p>

<p>Here is what an informed modeler would find:</p>

<pre><code>informedFit &lt;- arima(controlData, order = c(1,1,1))
# informedFit
# Series: controlData 
# ARIMA(1,1,1)                    
#
# Coefficients:
#          ar1     ma1
#       0.4936  0.6859
# s.e.  0.1564  0.1764
#
# sigma^2 estimated as 0.9571:  log likelihood=-62.22
# AIC=130.44   AICc=131.04   BIC=135.79
</code></pre>

<p>1) Why are these information criterion better than the model selected by <code>auto.arima(controlData)</code>?</p>

<p>Now, I just graphically compare the real data, and the 2 models:</p>

<pre><code>plot(controlData)
lines(fitted(naiveFit), col = ""red"")
lines(fitted(informedFit), col = ""blue"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sy3YR.jpg"" alt=""tsPlots""></p>

<p>2) Playing devil's advocate, what kind of consequences would I pay by using an ARIMA(2, 0, 1) as a model? What are the risks of this error? </p>

<p>3) I'm mostly concerned about any implications for multi-period forward predictions. I assume they would be less accurate? I'm just looking for some proof.</p>

<p>4) Would you suggest an alternative method for model selection? Are there any problems with my reasoning as an ""uninformed"" modeler?</p>

<p>I'm really curious what are the other consequences of this kind of misclassification. I've been looking for some sources and just couldn't find anything. All the literature I could find only touches on this subject, instead just stating the data should be stationary before performing ARMA, and if it's non-stationary, then it needs to be differenced d times.</p>

<p>Thanks!</p>
"
"0.12049067317797","0.117266257455049","105367","<p>I have the weekly revenue data for an electronics company the decomposed plot of which is as follows:  </p>

<p><img src=""http://i.stack.imgur.com/9HWE6.png"" alt=""enter image description here""></p>

<p>I have decided to keep the seasonality and apply a suitable forecasting technique. I tried auto.arima:</p>

<pre><code>&gt; Elec &lt;- read.xlsx(""C:/Users/Himanshu.raunak/Revenue/Electronics.xlsx"", 1)
&gt; Elec$Date &lt;- as.Date(Elec$Date, format=""%Y-%m-%d"")
&gt; ElecTimeSeries &lt;- ts(Elec$Revenue, frequency=52)
&gt; ElecArima &lt;- auto.arima(ElecTimeSeries)
&gt; plot(forecast(ElecArima))
</code></pre>

<p>I get the following plot:</p>

<p><img src=""http://i.stack.imgur.com/fYKNN.png"" alt=""enter image description here""> </p>

<p>And the following warning messages:</p>

<p>1: In myarima(x, order = c(p, d, q), seasonal = c(P, D, Q),  ... :
  Unable to check for unit roots</p>

<p>2: In myarima(x, order = c(p, d, q), seasonal = c(P, D, Q),  ... :
  Unable to check for unit roots</p>

<p>3: In myarima(x, order = c(max.p > 0, d, 0), seasonal = c((m >  ... :
  Unable to check for unit roots</p>

<p>and so on.</p>

<p>The ARIMA parameters come out to be as follows:</p>

<p>ARIMA(2,1,2)(0,0,1)[52] with drift</p>

<p>Coefficients:</p>

<pre><code>       ar1      ar2      ma1     ma2    sma1     drift

      0.5282  -0.0316  -1.3125  0.3225  0.2283  2497.993

s.e.    NaN      NaN      NaN     NaN    0.0728    NaN

sigma^2 estimated as 3.563e+11:  log likelihood=-2931.26

AIC=5876.51   AICc=5877.1   BIC=5899.6
</code></pre>

<p>I realize that the AIC values are quite large.</p>

<p>Could you please point out at what I am doing incorrectly (warning messages and large ARIMA parameters) and provide a better solution. Also I need help understanding the ARIMA plot.</p>
"
"0.204479452977299","0.199007438041998","108374","<p>I have a monthly time series with an intervention and I would like to quantify the effect of this intervention on the outcome. I realize the series is rather short and the effect is not yet concluded.</p>

<p><strong>The Data</strong></p>

<pre><code>  cds&lt;- structure(c(2580L, 2263L, 3679L, 3461L, 3645L, 3716L, 3955L, 
    3362L, 2637L, 2524L, 2084L, 2031L, 2256L, 2401L, 3253L, 2881L, 
    2555L, 2585L, 3015L, 2608L, 3676L, 5763L, 4626L, 3848L, 4523L, 
    4186L, 4070L, 4000L, 3498L), .Dim = c(29L, 1L), .Dimnames = list(
        NULL, ""CD""), .Tsp = c(2012, 2014.33333333333, 12), class = ""ts"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/lNOEk.jpg"" alt=""enter image description here""></p>

<p><strong>The methodology</strong></p>

<p>1) The pre-intervention series (up until October 2013) was used with the <code>auto.arima</code> function. The model suggested was ARIMA(1,0,0) with non-zero mean. The ACF plot looked good.</p>

<pre><code>pre&lt;-window(cds,start = c(2012,01), end=c(2013,09))

mod.pre&lt;-auto.arima(log(pre))

Coefficients:
         ar1  intercept
      0.5821     7.9652
s.e.  0.1763     0.0810

sigma^2 estimated as 0.02709:  log likelihood=7.89
AIC=-9.77   AICc=-8.36   BIC=-6.64
</code></pre>

<p>2) Given the plot of the full series, the pulse response was chosen below, with T = Oct 2013,</p>

<p><img src=""http://i.stack.imgur.com/YU3nB.jpg"" alt=""enter image description here""></p>

<p>which according to cryer and chan can be fit as follows with the arimax function:</p>

<pre><code>   mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
            xtransf=data.frame(Oct13=1*(seq(cds)==22)),
            transfer=list(c(1,1))
          )

    mod.arimax


Series: log(cds) 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept  Oct13-AR1  Oct13-MA0  Oct13-MA1
      0.7619     8.0345    -0.4429     0.4261     0.3567
s.e.  0.1206     0.1090     0.3993     0.1340     0.1557

sigma^2 estimated as 0.02289:  log likelihood=12.71
AIC=-15.42   AICc=-11.61   BIC=-7.22
</code></pre>

<p>The residuals from this appeared OK:</p>

<p><img src=""http://i.stack.imgur.com/wvdXD.jpg"" alt=""enter image description here""></p>

<p>The plot of fitted and actuals:</p>

<pre><code>plot(fitted(mod.arimax),col=""red"", type=""b"")
lines(window(log(cds),start=c(2012,02)),type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/kJ1pj.jpg"" alt=""enter image description here""></p>

<p><strong>The Questions</strong></p>

<p>1) Is this methodology correct for intervention analysis?</p>

<p>2) Can I look at estimate/SE for the components of the transfer function and say that the effect of the intervention was significant?</p>

<p>3) How can one visualize the transfer function effect (plot it?)</p>

<p>4) Is there a way to estimate how much the intervention increased the output after 'x' months? I guess for this (and maybe #3) I am asking how to work with an equation of the model - if this were simple linear regression with dummy variables (for example) I could run scenarios with and without the intervention and measure the impact - but I am just unsure how to work this this type of model.</p>

<p><strong>ADD</strong></p>

<p>Per request, here are the residuals from the two parametrizations.</p>

<p>First from the fit:</p>

<pre><code>fit &lt;- arimax(log(cds), order = c(1,0,0), 
              xtransf = data.frame(Oct13a = 1*(seq_along(cds)==22), Oct13b = 1*(seq_along(cds)==22)),
              transfer = list(c(0,0), c(1,0)))

plot(resid(fit), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sqMZN.jpg"" alt=""enter image description here""></p>

<p>Then, from this fit</p>

<pre><code>mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
                   xtransf=data.frame(Oct13=1*(seq(cds)==22)),
                   transfer=list(c(1,1))
)

mod.arimax
plot(resid(mod.arimax), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/DjAyu.jpg"" alt=""enter image description here""></p>
"
"0.216671721458714","0.250412201434081","109835","<p>While working on a big data set made of 10-minutes-points of information - i.e. <code>144</code> points per day, <code>1008</code> per week and <code>52560</code> per year - I encountered a few problem in R. The information concerns electricity load on a source substation during the year.</p>

<h3>Multiple seasonality :</h3>

<p>The data set clearly shows multiple seasonalities, which are daily, weekly and yearly. From <a href=""http://stats.stackexchange.com/questions/47729/two-seasonal-periods-in-arima-using-r"">there</a> I understood that R doesn't handle multiple seasonality within the ARIMA modeling functions.  I would really like to work with ARIMA models though, because my previous work is based on ARIMA models and I know approximatively how to translate a model into an equation.  </p>

<h3>Long seasonality :</h3>

<p>Each of the seasonalities is of high value, with the shortest one being the daily seasonality at 144. Unfortunately from the SARIMA general equation which is<br>
$\phi(B)\Phi(B^s)W_t = \theta(B)\Theta(B^s)Z_t$<br>
I guessed that the maximum lag for a given model <code>SARIMA(p,d,q)(P,D,Q)144</code> is<br>
$max((p+P*144), (q+Q*144))$</p>

<p>I would really like to try and fit models with values of P and/or Q greater than 1, but R doesn't allow me since the <code>maximum supported lag = 350</code>. To do so I found <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">this link</a> which is really interesting and led to new functions in the forecast package by M. Hyndman, called <code>fourier</code> and <code>fourierf</code> which you can find <a href=""http://www.inside-r.org/packages/cran/forecast/docs/fourier"" rel=""nofollow"">here</a>. But since I am not a specialist in forecasting nor in statistics, I have some difficulties understanding how I can make this work.  </p>

<hr>

<p>The thing is I thing this whole fourier regressors package could help me a lot. From what I understood I could use it to simulate the long-seasonality of my data set, maybe use it to simulate multiple seasonality, and even more it could allow me to introduce exogenous variables - which are the <code>temperature</code> and (<code>public holiday + sundays</code>).<br>
I also tried doing some regression following <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">this example</a> but I couldn't make it work because :</p>

<pre><code>Error in forecast.Arima(bestfit, xreg = fourierf(gas, K = 12, h = 1008)) : 
Number of regressors does not match fitted model
</code></pre>

<p>I really hope somebody can help me get a better understanding of these functions. Thanks.</p>

<p><strong>Edit :</strong> So I tried my best with the fourier example given <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a> but couldn't figure out how it handles the fitting. Here is the code (I copy-pasted M. Hyndman one and adapted to my data set - unsuccessfully) :</p>

<pre><code>n &lt;- 50000
m &lt;- 144
y &lt;- read.table(""auch.txt"", skip=1)
fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}

library(forecast)
fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008)))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m), fourier(n+1:(14*m),4,1008))))
</code></pre>

<p>So I wanted to ""force"" the model to be a <code>SARIMA(2,1,5)(1,2,8)[144]</code> but when I type <code>arimod</code>this is the result of the Arima fitting :</p>

<pre><code>&gt; fit  
Series: y[1:n, 1] , 
ARIMA(2,1,5)                  

sigma^2 estimated as 696895:  log likelihood=-407290.2  
AIC=814628.3   AICc=814628.3   BIC=814840
</code></pre>

<p>It doesn't even take into consideration the seasonal part of the model, and I don't know much about the range the AIC values can take, but it seems way too high to be a good fitting model right there. I think it all comes down to my misunderstanding of the use of Fourier terms as regressors, but I can't figure out why.</p>

<p><strong>Edit 2 :</strong> Also I can't seem to be able to add another exogenous variable to the Arima function. I need to use <code>temperature</code> - probably as a lead - to fit the <code>SARIMAX</code> model, but as soon as I write this :</p>

<pre><code>fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008), tmp[1:n]))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m),fourier(n+1:(14*m),4,1008), tmp[n+1:(14*m)])))
</code></pre>

<p>Nothing is plotted besides the initial data set. There is no forecast while without <code>tmp</code> as an <code>xreg</code> I still get some results.</p>
"
"0.146087177447694","0.142177781147918","110589","<p>I'm using R to do some time series estimation.  I'm trying to rebuild the fitted values from an Arima model by hand to use in an Excel spreadsheet using the estimated coefficients and the input data. I can use the fitted command, but I'm trying to understand more how it works. Ex:  </p>

<pre><code>library(MASS)
library(tseries)
library(forecast)

set.seed(1)
N = ts(mvrnorm(50, mu=c(0,0), Sigma=matrix(c(1,0.56,0.56,1), ncol=2), 
       empirical=TRUE), frequency=12)
head(N)

&gt;            [,1]       [,2]
&gt;[1,] -0.05270976  0.7239571
&gt;[2,] -0.67232349 -0.6631604
&gt;[3,] -0.20193415  0.8176053
&gt;[4,] -0.54278281 -2.0458285
&gt;[5,]  1.38279994  0.9405811
&gt;[6,]  1.39979731  2.1717733

# Model: x(t) = a * x(t-1) + e(t)
fit = Arima(N[,1], order=c(1,0,0), include.constant=FALSE)

&gt; fit  
&gt;Series: N[, 1]  
&gt;ARIMA(1,0,0) with zero mean          
&gt;
&gt;Coefficients:  
&gt;         ar1  
&gt;       0.0293
&gt;s.e.   0.1400  
&gt;
&gt;sigma^2 estimated as 0.9791:  log likelihood=-70.42
&gt;AIC=144.84   AICc=145.1   BIC=148.66

# Build the fitted values: x(t)=a * x(t-1) 
pred  = fit$coef[1] * lag(fit$x, -1) 
pred1 = fitted(fit)
head(cbind(pred, pred1))   

&gt;             pred         pred1
&gt;[1,]           NA -2.255567e-05
&gt;[2,] -0.001541849 -1.541849e-03
&gt;[3,] -0.019666597 -1.966660e-02
&gt;[4,] -0.005906915 -5.906915e-03
&gt;[5,] -0.015877313 -1.587731e-02
&gt;[6,]  0.040449232  4.044923e-02 
</code></pre>

<p>In this case, <code>pred</code> and <code>pred1</code> match.  </p>

<p>However when I add in an <code>xreg</code>:  </p>

<pre><code># Model: x(t) = a*x(t-1) + b*xreg + e(t)
fit1 = Arima(N[,1], order=c(1,0,0), xreg=N[,2], include.constant=FALSE)

&gt;fit  
&gt;Series: N[, 1]  
&gt;ARIMA(1,0,0) with zero mean         
&gt;
&gt;Coefficients:  
&gt;         ar1  N[, 5]  
&gt;       0.0860  0.5606  
&gt;s.e.   0.1401  0.1155  
&gt;
&gt;sigma^2 estimated as 0.6675:  log likelihood=-60.85
&gt;AIC=127.69   AICc=128.22   BIC=133.4

# Build the fitted values: x(t) = a*x(t-1) + b*xreg 
pred2  = fit1$coef[1]*lag(fit1$x, -1) + fit1$coef[2]*fit1$xreg 
pred21 = fitted(fit1) 
head(cbind(pred2, pred21))

&gt;              pred2     pred21
&gt;[1,]         NA  0.4041670
&gt;[2,]  0.4013329 -0.4112205
&gt;[3,] -0.4296032  0.4325201
&gt;[4,]  0.4410005 -1.2037229
&gt;[5,] -1.1936161  0.5792684
&gt;[6,]  0.6462336  1.2911169
</code></pre>

<p>In this case, <code>pred2</code> and <code>pred21</code> do not match, and the only thing changed was adding an <code>xreg</code>. The only time I cannot build out the fitted values by hand is when the AR part is included. I was able to do it when only MA parts were included with the <code>xreg</code>.  I would really appreciate knowing how <code>Arima</code> treats <code>xreg</code> when generating the fitted values. </p>
"
"0.0834783871129682","0.0812444463702388","111103","<p>I'm trying to apply R output to generate a scenario using external data, I'm not sure how exactly to use the coefficients in each from the R output.</p>

<p>I have an ARMAX(1, 1) model</p>

<ul>
<li>Coefficient of AR1: $A$</li>
<li>Coefficient of MA1: $M$</li>
<li>Intercept = $k$</li>
<li>Coefficient of external regressors: $B_1$, $B_2$</li>
<li>External regressors: $X_1$, $X_2$</li>
<li>Y actual = $Y_a$</li>
<li>Y predicted = $Y_p$</li>
</ul>

<p>The formula I'm using is $Y_p = k + A*(Y_a(t-1)-k) - M*(Y_a(t-1) - Y_p(t-1)) + B_1*(X_1(t) - X_1(t-1)) + B_2*(X_2(t) - X_2(t-1))$</p>

<p>Is there anything wrong with the formula I'm using?
I'm asking because of two things, one is that while AIC is much better for this model, the sum of squared errors is actually higher than if I just used AR(1) model; another thing is that when I adjust the AR term, the sum of squared residuals gets reduced, but I thought the model is supposed to minimize the sum of squared residuals for each of the terms?</p>

<p>I'm using the same data set for generating the model and to fit the model, I need to test  to see how much improvement this model offers compared to simpler models. I'm doing the fitting in Excel.</p>

<p>Any help would be greatly appreciated, and if additional information is needed to answer this please ask.</p>

<p>This is my R output</p>

<pre><code>ARMA11R2R30


Call:
arima(x = YieldReOLD[, 5], order = c(1, 0, 1), xreg = cbind(YieldReOLD[, 2], 
    YieldReOLD[, 4]))

Coefficients:
         ar1      ma1  intercept  cbind(YieldReOLD[, 2], YieldReOLD[, 4])1
      0.9872  -0.1970    -6.2862                                   -0.1867
s.e.  0.0072   0.0489     0.3743                                    0.0566

      cbind(YieldReOLD[, 2], YieldReOLD[, 4])2
                                       -0.3999
s.e.                                    0.1140
</code></pre>
"
"0.167997013412975","0.163501294390371","115154","<p>Relatively new to stats. I use linear regression  and get R^2, which is quite low.</p>

<p><strong>MODEL 1</strong></p>

<pre><code>    lmoutar=lm(formula = ts_y ~ ts_y_lag + ts_x)
</code></pre>

<p>So switched to arima with external regressor. Using ""auto.arima"", I formulate arimax model</p>

<p><strong>MODEL 2</strong></p>

<pre><code>    fitarima &lt;- auto.arima(ts_y, xreg=ts_x)
    arimaout&lt;-arima(ts_y,order=c(2,0,5),xreg=ts_x)
</code></pre>

<p>How can I compare the explanability of AR model with arima model. From the thread <a href=""http://stats.stackexchange.com/questions/8750/how-can-i-calculate-the-r-squared-of-a-regression-with-arima-errors-using-r"">How can I calculate the R-squared of a regression with arima errors using R?</a>, I understand R^2 is not an option for ARIMA.</p>

<p>From the thread <a href=""http://stats.stackexchange.com/questions/11850/model-comparison-between-an-arima-model-and-a-regression-model"">Model comparison between an ARIMA model and a regression model</a>, AIC/BIC is not the right criteria and MSE from forcast/predict can be possible criteria for comparison across AR and ARIMA model. Is MSE the best option for model comparison, if so how would I generate MSE for AR and ARIMA?</p>

<p>I tried to compare the above ar and arima model using anova, but I get following error message</p>

<pre><code>anova.lm(lmoutar,arimaout)
   Warning message:
    In anova.lmlist(object, ...) :
            models with response â€˜""NULL""â€™ removed because response differs from model 1
</code></pre>

<p>What does this error message mean? </p>

<p><strong><em>EDIT</em></strong></p>

<p>Thanks for the response so far and insight that AR is nested within ARIMA. How would one answer this question, if I rephrase  as ""How to compare AR, ARIMA and General Linear Models?"". The first model I listed has AR(1) and independent variable; it is a general linear model. So how would I compare a GLM versus ARIMAX model? Any thing else besides MSE that I could use to judge between GLM and ARIMAX</p>
"
"0.0590281336100955","0.0574484989621426","115642","<p>I wanted to fit an ARIMA model to a daily database for three years but <code>auto.arima</code> couldn't find a model and showed the following error:</p>

<pre><code>Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p>Is it because ARIMA is not good for data with long seasonality?</p>
"
"0.0885422004151433","0.0861727484432139","115712","<p>I am running ARIMA model in R and I used auto.arima(X) function to decide appropriate model.After using this function I found that the order of my model is ARIMA(2,1,0).
The problem is I run the same ARIMA(2,1,0) model using arima(X,order=c(2,1,0)) and I got AIC as AIC=832.16. but for same model by using auto.arima(X) as AIC=805.29. I dont know why for the same model AIC is different. Please hep me to over come this problem.
Thank you in advance.</p>
"
"0.0834783871129682","0.0812444463702388","117474","<p>A dataset I am working with (from the OECD), for harmonised unemployment seems to be seasonally adjusted:</p>

<blockquote>
  <p>The unemployment rates  shown here are calculated as the number of unemployed persons as a percentage of the labour force (i.e., the unemployed plus those in employment) and are seasonally adjusted.</p>
</blockquote>

<p>This is taken from their <a href=""http://www.oecd.org/std/labour-stats/44743407.pdf"" rel=""nofollow"">methodology notes</a>. Yet, while working with it, R (the software that I am using) shows a seasonal component in the decomposition of the series.</p>

<p>Working backwards (with <code>auto.arima</code>) for the series, this is the result I get for the series:</p>

<pre><code> Series: std 
 ARIMA(2,1,1)(0,0,2)[12]                    

 Coefficients:
     ar1     ar2      ma1     sma1    sma2
  0.5194  0.3131  -0.7888  -0.1570  0.0918
 s.e.  0.1031  0.0552   0.0957   0.0569  0.0569

 sigma^2 estimated as 0.06156:  log likelihood=-8.39
 AIC=28.77   AICc=29.04   BIC=51.44

 Training set error measures:
                   ME      RMSE       MAE       MPE     MAPE      MASE
 Training set 0.0006069076 0.2477273 0.1866529 -0.210822 5.128619 0.2163681
</code></pre>

<p>Whose criterions and error measurements seem to be unequivocally lower than any other model I have come up with <em>without</em> any seasonal component.</p>

<p>R suggests <em>two</em> Seasonal Moving Average components for this series, but I am unsure of the validity of this, given the fact that the series was already adjusted according to the OECD. A minor problem is that the MPE is negative.</p>

<p>I am worried about over estimation of the relevant parameters.</p>

<p>Here is the structure of the data:</p>

<pre><code> &gt; dput(std)
 structure(c(4.5, 4.7, 4.2, 4.4, 3.9, 3.9, 3.7, 3.7, 3.4, 3.6, 
 3.5, 3.1, 3.5, 3.3, 3.7, 3.7, 3.7, 3.8, 3.6, 3.5, 3.5, 3.3, 3.5, 
 3.5, 3.4, 3.1, 3, 2.9, 3.1, 2.9, 2.8, 3.1, 2.8, 2.5, 2.7, 2.7, 
 2.5, 2.5, 2.5, 2.7, 2.8, 3, 3.2, 3.1, 2.4, 3, 2.6, 2.6, 2.7, 
 2.6, 2.9, 2.6, 2.3, 2.2, 2.4, 3.3, 2.8, 2.9, 2.9, 2.8, 2.8, 3.1, 
 2.7, 2.7, 2.9, 2.8, 2.8, 2.6, 2.4, 2.6, 3.3, 2.8, 3, 3.5, 3.5, 
 3, 3.3, 3.2, 3.3, 4, 3.7, 3.4, 3.5, 3.6, 3.7, 3.6, 3.5, 3.6, 
 3.3, 3.4, 3.5, 3.6, 3.6, 3.9, 4.1, 4, 4.4, 5.1, 5.6, 6.1, 6.5, 
 6.7, 6.8, 7.6, 6.7, 6.6, 6.4, 6.6, 6.2, 6.1, 5.8, 5.8, 5.4, 5.6, 
 5.4, 5.3, 5.4, 5, 5.1, 4.4, 4.3, 3.8, 4.1, 4, 4, 3.6, 4, 3.6, 
 3.4, 3.3, 3.4, 3.1, 3.5, 3.4, 3.3, 3.1, 3.3, 3.3, 3.3, 3.1, 3.2, 
 3.1, 3, 3.1, 2.6, 3.1, 2.7, 2.8, 2.6, 2.7, 2.3, 2.5, 2.3, 2.4, 
 2.3, 2.4, 2.3, 2.1, 2.2, 2.8, 2.7, 2.7, 2.5, 2.7, 2.6, 2.5, 2.4, 
 2.5, 2.5, 3.2, 2.7, 2.7, 2.8, 2.7, 2.8, 2.3, 2.5, 2.9, 3, 3.1, 
 3.4, 3, 3, 3.1, 3.1, 3, 2.9, 2.8, 2.9, 2.8, 3, 2.7, 2.9, 3, 3.1, 
 3.1, 3.2, 3.3, 3.6, 3.7, 3.8, 3.8, 4, 3.4, 3.8, 4, 3.9, 4, 3.9, 
 4, 3.8, 4, 3.8, 3.9, 3.9, 4, 3.9, 3.6, 3.7, 3.8, 3.7, 3.9, 3.7, 
 3.5, 3.6, 3.4, 3.1, 3.2, 3.3, 3.6, 3.5, 3.4, 3.3, 3.6, 3.6, 3.8, 
 3.8, 3.7, 3.7, 3.8, 3.7, 3.9, 4.1, 3.7, 3.6, 3.5, 3.6, 3.7, 3.7, 
 3.8, 3.6, 3.8, 3.8, 3.8, 4, 3.7, 3.6, 3.7, 3.8, 4, 4, 4, 4.6, 
 4.8, 4.7, 5.2, 5.1, 5.4, 5.7, 5.4, 5.7, 5.9, 6, 5.8, 5.4, 5.3, 
 5.6, 5.4, 5.2, 5.5, 5.4, 5.2, 5.3, 5.1, 5.3, 5.6, 5.5, 5.5, 5.2, 
 5.4, 5, 5.2, 5.4, 5.5, 5.3, 5.4, 5.3, 4.9, 5.1, 5, 4.7, 5.3, 
 5, 4.9, 5, 4.9, 4.7, 5.1, 4.7, 4.9, 5.3, 5, 5.2, 4.9, 4.9, 5.1, 
 5.1, 5, 4.8, 4.9, 5, 4.9, 4.6, 4.8), .Tsp = c(1987, 2013.91666666667, 
 12), class = ""ts"")
</code></pre>
"
"0.133863224475948","0.151994441447782","118297","<p>I am learning arima by this site:</p>

<p><a href=""http://people.duke.edu/~rnau/411home.htm"" rel=""nofollow"">http://people.duke.edu/~rnau/411home.htm</a></p>

<p>and I want to get the same result as following notes:</p>

<p><a href=""http://people.duke.edu/~rnau/Review_of_basic_statistics_and_the_mean_model_for_forecasting--Robert_Nau.pdf"" rel=""nofollow"">http://people.duke.edu/~rnau/Review_of_basic_statistics_and_the_mean_model_for_forecasting--Robert_Nau.pdf</a></p>

<p>I was thinking that an arima with order [0, 0, 0] is the mean model, but the results is different from the notes, Here is the code:</p>

<pre><code>require(forecast);
x &lt;- c(114, 126, 123, 112, 68, 116, 50, 108, 163, 79,
      67, 98, 131, 83, 56, 109, 81, 61, 90, 92);
m &lt;- arima(x, order=c(0, 0, 0));
print(m);
print(forecast(m, 1));
print(predict(m)$se);
</code></pre>

<p>the output:</p>

<pre><code>&gt; print(m);
Series: x 
ARIMA(0,0,0) with non-zero mean 

Coefficients:
      intercept
        96.3500
s.e.     6.3124

sigma^2 estimated as 796.9:  log likelihood=-95.19
AIC=194.37   AICc=195.08   BIC=196.36

&gt; print(forecast(m, 1));
   Point Forecast    Lo 80    Hi 80   Lo 95    Hi 95
21          96.35 60.17192 132.5281 41.0204 151.6796

&gt; print(predict(m)$se);
Time Series:
Start = 21 
End = 21 
Frequency = 1 
[1] 28.2299
</code></pre>

<p>but the results in the notes are:</p>

<pre><code>SE_fcst = 29.68  (R result: 28.2299)
95% confidence intervals = 34, 158  (R result: 41, 152)
</code></pre>

<p>Where am I wrong?</p>

<p><strong>edit</strong></p>

<p>I do the simulation with random numbers, and the result is the same as the notes.</p>

<ol>
<li>make 21 normal random numbers with mu=100, sigma=30</li>
<li>calculate the error between the mean of first 20 numbers and the last number.</li>
<li>repeat 1 &amp; 2 for 100000 times</li>
</ol>

<p>Here is the python code that to do the simulation:</p>

<pre><code>import numpy as np
N = 1000000
n = 20
x = np.random.normal(100, 30, (N, n))
p = np.mean(x, axis=1)
nx = np.random.normal(100, 30, N)

err = p - nx
print (err**2).mean()**0.5

s = np.std(x, axis=1, ddof=1)
SE_mean = s / n**0.5
print (s**2 + SE_mean**2).mean()**0.5
</code></pre>

<p>the output is:</p>

<pre><code>30.7480552149 (the real standard error of forecast)
30.7375157915 (the estimated standard error of forecast by sqrt(s**2 + SE_mean**2))
</code></pre>
"
"0.0834783871129682","0.0812444463702388","121882","<p>I am facing a strange issue with auto.arima. On a dataset named data, I run the following code</p>

<pre><code>auto.arima(data,d=0,D=1,xreg=1:length(data),max.p=3,max.q=3,max.order=10,
       seasonal=TRUE,stepwise=FALSE,approximation=FALSE,ic=(""aic""),parallel=TRUE)
</code></pre>

<p>The outcome is </p>

<pre><code>   Series: data 
   ARIMA(1,0,3)(2,1,2)[12]                    

 Coefficients:
        ar1      ma1      ma2     ma3    sar1     sar2     sma1    sma2  1:length(data)
        0.6939  -0.6417  -0.2391  0.4350  0.6197  -0.5055  -1.3211  0.6027   5e-04
  s.e.  0.1349   0.1654   0.1169  0.1502  0.3040   0.1762   0.3907  0.4269   3e-04

 sigma^2 estimated as 0.001792:  log likelihood=143.69
 AIC=-267.38   AICc=-264.79   BIC=-241.73
</code></pre>

<p>Then I try to fit this model using the Arima function:</p>

<pre><code> Arima(data,order=c(1,0,3),seasonal=list(order=c(2,1,2),period=12),
       xreg=1:length(data),method=""ML"")
</code></pre>

<p>but I get the following error message:</p>

<pre><code> Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
   non-finite finite-difference value [1]
</code></pre>

<p>Does someone understand why this appears ? Thanks.</p>
"
"0.296849441423305","0.299223595205334","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.096392538542376","0.140719508946058","122704","<p>I have used auto.arima to fit a time series model (a linear regression with ARIMA errors, as described <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">on Rob Hyndman's site</a> )  When finished - the output reports that the best model has a (5,1,0) with drift structure - and reports back values of information criteria as </p>

<p>AIC:  2989.2
AICC:  2989.3
BIC: 3261.2</p>

<p>When I use Arima to fit a model with a (1,1,1) with drift structure - the output reports back noticeably lower IC's of</p>

<p>AIC:  2510.3
AICC:  2510.4
BIC:  2759</p>

<p>I can force auto.arima to consider the (1,1,1) with drift model (using the start.p and start.q parameters), and when I do that, and set ""trace=TRUE"" - I do see that the (1,1,1) with drift model is considered, but rejected, by auto.arima.  It still reports back the (5,1,0) with drift model as the best result.</p>

<p>Are there circumstances when auto.arima uses other criteria to choose between models?</p>

<p>Edited to add (in response to request)</p>

<p>Data for this example can be found at <a href=""https://drive.google.com/file/d/0B6afOuS0y79aenBMeFYyWVNwUUU/view?usp=sharing"" rel=""nofollow"">this Google spreadsheet</a></p>

<p>and R code to reproduce the example is</p>

<pre><code>repro = read.csv(""mindata.csv"")
reprots = ts(repro, start=1, frequency=24)
fitauto = auto.arima(reprots[,""lnwocone""],
xreg=cbind(fourier(reprots[,""lnwocone""], K=11),
reprots[,c(""temp"",""sqt"",""humidity"",""windspeed"",""mist"",""rain"")]),
start.p=1, start.q=1, trace=TRUE, seasonal=FALSE)
fitdirect &lt;- Arima(reprots[,""lnwocone""], order=c(1,1,1), seasonal=c(0,0,0),
xreg=cbind(fourier(reprots[,""lnwocone""], K=11),
reprots[,c(""temp"",""sqt"",""humidity"",""windspeed"",""mist"",""rain"")]), include.drift=TRUE)
summary(fitauto)
summary(fitdirect)
</code></pre>

<p>Apologies if the Google docs data - inline code is not the best way to provide the example.  I think I have seen in the past guidelines on the best way to do this - but could not locate those guidelines in searching this morning.</p>
"
"0.241147483119112","0.245362124178399","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"0.0590281336100955","0.0574484989621426","126773","<p>Lets say I have an ARMA with an AR(1,4) and an MA(1).</p>

<p><strong>Update</strong>
To be more specific the model to be estimated is:</p>

<p>$$y_t = \rho_1 y_{t-1} + \rho_4 y_{t-4} + \epsilon_t + \theta_1 \epsilon_{t-1}$$</p>

<pre><code>set.seed(1234)
test &lt;- arima.sim(n=1000, list(ar=c(0.3,0,0,0.5), ma=c(0.3)))
</code></pre>

<p><strong>How can this data be estimated?</strong>
For now I have tried:</p>

<pre><code>arima(test, c(4,0,1)
</code></pre>

<p>But the results are obviously not correct:</p>

<pre><code>## ============================
##                 Model 1     
## ----------------------------
## ar1                 0.26 ***
##                    (0.06)   
## ar2                 0.18 ** 
##                    (0.06)   
## ar3                -0.27 ***
##                    (0.05)   
## ar4                 0.63 ***
##                    (0.04)   
## ma1                 0.93 ***
##                    (0.06)   
## intercept          -0.21    
##                    (0.32)   
## ----------------------------
## AIC              2878.68    
## BIC              2913.03    
## Log Likelihood  -1432.34    
## Q(1mon)             0.00    
## Q(6mon)             0.00    
## Q(12mon)            0.00    
## Q(24mon)            0.00    
## ============================
## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05
</code></pre>
"
"0.0590281336100955","0","128709","<p>I wonder how the Arima() function in R computes the AIC. Applying the standard formula AIC= 2*k - 2 LN(L) (with k number of parameters and L maximized value of likelihood) doesn't reproduce the displayed result. Does the estimated variance counts as additional parameter?</p>

<p>Thank you for helping,</p>

<p>laterstat </p>

<p>PS: Here is an example:</p>

<pre><code>arima(x = example, order = c(2, 0, 2), method = ""ML"")

Coefficients:
          ar1      ar2     ma1     ma2  intercept
      -0.7508  -0.0367  1.3156  0.5433    -0.0183
s.e.   0.2591   0.1772  0.2424  0.1958     0.0244

sigma^2 estimated as 0.03748:  log likelihood = 35.67,  aic = -59.33

&gt; 5*2-2*35.67
</code></pre>

<p>[1] -61.34</p>
"
"0.0834783871129682","0.0812444463702388","128730","<p>If i understand correctly, the ARIMA function produces an estimate for the mean of the process instead of the intercept. It is possible to transform the mean into the intercept: mean= 1-Sum(AR-Coefficients). Is it also possible to transform the standard error of the mean into standard error of the intercept? </p>

<p>As an example:</p>

<pre><code>arima(x = example, order = c(2, 0, 2), method = ""ML"")

Coefficients:
          ar1      ar2     ma1     ma2  intercept
      -0.7508  -0.0367  1.3156  0.5433    -0.0183
s.e.   0.2591   0.1772  0.2424  0.1958     0.0244

sigma^2 estimated as 0.03748:  log likelihood = 35.67,  aic = -59.33

# mean:
&gt; 1+0.7508+0.0367 
[1] 1.7875
</code></pre>

<p>Thanks a lot</p>

<p>laterstat     </p>
"
"0.118056267220191","0.114896997924285","130152","<p>This is out of my curiosity trying to compare time series input to an ARMA model and reconstructed series after an ARMA estimate is obtained. These are the steps I am thinking:</p>

<ol>
<li><p>Construct simulation time series</p>

<pre><code>arma.sim &lt;- arima.sim(model=list(ar=c(0.9),ma=c(0.2)),n = 100)
</code></pre>

<p>estimate the model from arma.sim, assuming we know it is a (1,0,1) model</p>

<pre><code>arma.est1 &lt;- arima(arma.sim, order=c(1,0,1))
</code></pre></li>
<li><p>also say we get arma.est1 in this form, which is close to the original (0.9,0,0.2):</p>

<pre><code>Coefficients:
 ar1     ma1  intercept
 0.9115  0.0104    -0.4486
s.e.  0.0456  0.1270     1.1396

sigma^2 estimated as 1.15:  log likelihood = -149.79,  aic = 307.57
</code></pre></li>
<li><p>If I try to reconstruct another time series from <code>arma.est1</code>, how do I incorporate intercept or s.e. in <code>arima.sim</code>? Something like this doesn't seem to work well because <code>arma.sim</code> and <code>arma.rec</code> are far off:</p>

<pre><code>arma.rec &lt;- arima.sim(n=100, list(ar=c(0.9115),ma=c(0.0104)))
</code></pre></li>
</ol>

<p>Normally we use <code>predict()</code> to check the estimate. But is this a legit way to look at the estimate?</p>
"
"0.0681598176590997","0.0995037190209989","131393","<p>I would like to have the best ARIMA model prediction that has the lowest MAPE or lowest AIC/BIC. For example, I would want to change the Arima order automatically with loop or some other way and want to test with all possible combinations like below</p>

<pre><code>c(1,0,0)
c(1,1,0)
.
.
c(x,y,z)
</code></pre>

<p>Below is the reproducible example code but I do not know how to go with multiple order execution and comparison of MAPE/AIC/BIC.</p>

<pre><code>set.seed(1)
tsdata &lt;- ts(rnorm(50), start = c(1980,1), frequency = 12)
myts &lt;- tsdata

fit &lt;- Arima(myts,order=c(2,1,0))
forecast(fit, 3)
plot(forecast(fit, 3))
fit
accuracy(fit)
</code></pre>

<p>Is it possible to save all the Accuracy measures (<code>MAPE, AIC, BIC</code>) in a data frame or in a list then select the best order to execute a Arima model? I tested with <code>auto.arima</code> in my real data but it did not give me the best order. Thanks in advance for your help !</p>
"
"0.183209228398843","0.220260871434643","135565","<p>I have a few questions about turing a univariate time series into a multivariate time series and optimizing the predictors. Here is the univariate data:</p>

<pre><code>index
22
26
34
33
40
39
39
45
50
58
64
78
51
60
80
80
93
100
96
108
111
119
140
164
103
112
154
135
156
170
146
156
166
176
193
204
</code></pre>

<p>My first step here was to of course create a ts object in R and visualize the data:</p>

<pre><code>tsData &lt;- ts(data = dummyData, start = c(2012,1), end = c(2014,12), frequency = 12)

     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012  22  26  34  33  40  39  39  45  50  58  64  78
2013  51  60  80  80  93 100  96 108 111 119 140 164
2014 103 112 154 135 156 170 146 156 166 176 193 204

plot(tsData)
</code></pre>

<p>I interpreted this plot as a deterministic time series with a trend and perhaps a bit of seasonality</p>

<p><img src=""http://i.stack.imgur.com/YD0vW.png"" alt=""enter image description here"">
Examining the acf and pacf plot confirms the trend component of the time series</p>

<p><img src=""http://i.stack.imgur.com/c8eu3.png"" alt=""enter image description here""></p>

<p>My first question has to do with creating trend &amp; seasonal variables for the time series using the decompose() function in R which yields the following plots:</p>

<p><img src=""http://i.stack.imgur.com/9xXYx.png"" alt=""enter image description here""></p>

<p>I understand that the decompose() function in R has created a list of vectors for the trend, seasonal and random components of the original time series but what am I suppose to do with them? Should I cbind() them to my univariate data and model:</p>

<pre><code>lm(index ~ trend + seasonal + random)

         index     trend    seasonal       random
Jan 2012    22        NA -23.8940972           NA
Feb 2012    26        NA -19.4357639           NA
Mar 2012    34        NA   6.8350694           NA
Apr 2012    33        NA  -7.5399306           NA
May 2012    40        NA   4.3142361           NA
Jun 2012    39        NA   9.5017361           NA
Jul 2012    39  45.20833  -6.3524306   0.14409722
Aug 2012    45  47.83333  -0.8315972  -2.00173611
Sep 2012    50  51.16667  -1.1232639  -0.04340278
Oct 2012    58  55.04167   2.2517361   0.70659722
Nov 2012    64  59.20833  11.2100694  -6.41840278
Dec 2012    78  63.95833  25.0642361 -11.02256944
Jan 2013    51  68.87500 -23.8940972   6.01909722
Feb 2013    60  73.87500 -19.4357639   5.56076389
Mar 2013    80  79.04167   6.8350694  -5.87673611
Apr 2013    80  84.12500  -7.5399306   3.41493056
May 2013    93  89.83333   4.3142361  -1.14756944
Jun 2013   100  96.58333   9.5017361  -6.08506944
Jul 2013    96 102.33333  -6.3524306   0.01909722
Aug 2013   108 106.66667  -0.8315972   2.16493056
Sep 2013   111 111.91667  -1.1232639   0.20659722
Oct 2013   119 117.29167   2.2517361  -0.54340278
Nov 2013   140 122.20833  11.2100694   6.58159722
Dec 2013   164 127.75000  25.0642361  11.18576389
Jan 2014   103 132.75000 -23.8940972  -5.85590278
Feb 2014   112 136.83333 -19.4357639  -5.39756944
Mar 2014   154 141.12500   6.8350694   6.03993056
Apr 2014   135 145.79167  -7.5399306  -3.25173611
May 2014   156 150.37500   4.3142361   1.31076389
Jun 2014   170 154.25000   9.5017361   6.24826389
Jul 2014   146        NA  -6.3524306           NA
Aug 2014   156        NA  -0.8315972           NA
Sep 2014   166        NA  -1.1232639           NA
Oct 2014   176        NA   2.2517361           NA
Nov 2014   193        NA  11.2100694           NA
Dec 2014   204        NA  25.0642361           NA
</code></pre>

<p>When I use the auto.arima function in the forecast package is this all happening under the hood? It seems to me that the auto.arima() selected a MA(1) term and a differencing term to deal with the trend? Is my interpretation correct? What is drift?</p>

<pre><code>plot(forecast(auto.arima(tsData, stepwise=FALSE)))

Forecast method: ARIMA(0,0,1)(0,1,0)[12] with drift        

Model Information:
Series: tsData 
ARIMA(0,0,1)(0,1,0)[12] with drift         

Coefficients:
         ma1   drift
      0.9622  4.5780
s.e.  0.4698  0.4352

sigma^2 estimated as 176.6:  log likelihood=-44.52
AIC=95.05   AICc=96.25   BIC=98.58

Error measures:
                    ME     RMSE      MAE        MPE     MAPE       MASE
Training set 0.2459764 7.673429 4.967187 -0.7272714 4.661455 0.08876581
                   ACF1
Training set -0.0791942
</code></pre>

<p><img src=""http://i.stack.imgur.com/3TWL6.png"" alt=""enter image description here""></p>

<p>What happens if I'm interested in expanding the model to include other time series variables such as spend_1 and spend_2? do I need to create trend and seasonal and random variables for each of these spend variables or do I just plug them into the auto.arima as external variables:</p>

<pre><code>auto.ariam(tsData, xreg=spendData, stepwise= FALSE)

spend_1 spend_2
0   0
0   0
0   0
0   0
0   0
0   209
0   0
0   0
0   239
0   0
0   553
0   216
0   0
0   161
0   449
107 0
53  0
120 81
242 0
100 80
482 0
708 81
54  240
688 0
80  0
254 108
183 84
104 191
183 84
243 167
0   108
0   0
0   191
0   191
0   167
0   0
</code></pre>

<p>Once I build this multivariate time series model how do I interpret the coefficients for spend_1 and spend_2? How do to optimize them in order to maximize the index variable where the model was something like:</p>

<pre><code>lm(index ~ spend_1 + spend_2 + trend + seasonal + random)
</code></pre>

<p>Thanks all for the advice please let me know if I can clarify anything further.</p>
"
"0","0.0574484989621426","135651","<p>I've created an Arima model based on past forex closing prices using auto arima, which has generated a (0,1,0) ARIMA model.</p>

<pre><code>&gt; auto.arima(ma5)
Series: ma5 
ARIMA(0,1,0)                    

sigma^2 estimated as 5.506e-07:  log likelihood=11111.42
AIC=-22220.83   AICc=-22220.83   BIC=-22215.27
</code></pre>

<p>I next tried to plot the forecasted values, but as you can see all predictions are constant. Anyone know what I'm doing wrong?</p>

<p><img src=""http://i.stack.imgur.com/Er1k5.png"" alt=""enter image description here""></p>
"
"0.187439498562524","0.199007438041998","138108","<p>Using the attached data that has been recently updated I am not able to obtain a statistically significant forecast. The data is extremely seasonal. The data is stored here for easy replication: </p>

<p><a href=""http://ge.tt/1uihVfA2/v/0?c"" rel=""nofollow"">http://ge.tt/1uihVfA2/v/0?c</a></p>

<pre><code># 1. Make a R timeseries out of the rawdata: specify frequency &amp; startdate
gIIP &lt;- ts(Data, frequency=12, start=c(2003,11))
print(gIIP)
plot.ts(gIIP, type=""l"", col=""blue"", ylab=""MTD Ships"", lwd=2,
        main=""Full data"")
grid()
</code></pre>

<p>Using the auto.arima function I don't need to factor a Box-Cox because the auto.arima factors that into selecting the best model. </p>

<p>Upon ""selecting the best model"" I  The best model suggested was Arima(order = c(0, 0, 1),
      seasonal = list(order = c(1, 0, 1), period = 12) with non-zero mean </p>

<pre><code># 5. Perform estimation
library(forecast)
library(zoo)
library(stats)
auto.arima(gIIP, d=NA, D=NA, max.p=12, max.q=12,
           max.P=2, max.Q=2, max.order=12, max.d=2, max.D=2,
           start.p=2, start.q=2, start.P=1, start.Q=1,
           stationary=FALSE, seasonal=TRUE,
           ic=c(""aicc"",""aic"", ""bic""), stepwise=FALSE, trace=TRUE,
           approximation=FALSE | frequency(gIIP)&gt;12), xreg=NULL,
           test=c(""kpss"",""adf"",""pp""), seasonal.test=c(""ocsb"",""ch""),
           allowdrift=TRUE, lambda=TRUE, parallel=FALSE, num.cores=4
</code></pre>

<p>)</p>

<p>then proceed to conduct accuracy diagnostics but unable to obtain any output.</p>

<pre><code>#Check standard error etc of ""fitted"" ARIMA
pos.arima &lt;- function(gIIP, order = c(0, 0, 1),
      seasonal = list(order = c(1, 0, 1), period = 12),
      xreg = NULL, include.drift=TRUE, 
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c(""CSS-ML"", ""ML"", ""CSS""), 
      optim.method = ""BFGS"",
      optim.control = list(), kappa = 1e6)

acf(pos.arima) 
pacf(pos.arima)
</code></pre>

<p>The following step to conduct an ex ante (out of sample forecast) but also unable to obtain a statistically significant forecast---forecast with lowest standard error rate. I tested this by removing the last 5 observations to test the model. </p>

<pre><code># 7. Forecast Out-Of-Sample ---this used to work
fit &lt;- Arima(gIIP, order = c(0, 0, 1), seasonal = list(order = c(1, 0, 1), period = 12),
             xreg = TRUE, include.mean = TRUE, transform.pars = TRUE, 
             fixed = NULL, init = NULL, method = c(""CSS-ML"", ""ML"", ""CSS""), 
             optim.method = ""BFGS"", optim.control = list(), kappa = 1e6)
plot(forecast(fit,h=9))
print(forecast(fit,h=9))
</code></pre>

<p>Used to obtain output here. Can you please help me diagnose why there ARIMA model is not working like it once did for me? Thank you for your time.  </p>
"
"0.105592735469691","0.128458748884677","143358","<p>I'm working on a forecast for the following data:</p>

<pre><code>data &lt;-
c(1932, 4807, 6907, 8650, 10259, 11374, 8809, 6745, 7429, 
8041, 9740, 10971, 11953, 9227, 7401, 8355, 9681, 10438, 
11092, 11543, 9181, 7428, 8358, 10049, 10938, 12280, 
13063, 10022, 8125, 8763, 9330, 9919, 11309, 12169, 11063, 
10112, 10621, 11506, 12425, 12929, 13025, 10938, 9437, 
9910, 11104, 11985, 13024, 13962, 11900, 9576, 9590, 
10740, 11689, 13084, 13829, 11975, 10224, 10493, 11899, 
12697, 13959, 14415, 11650, 9477, 11166, 12327, 13238, 
13801, 13493, 11118, 9073, 9954, 11077, 12509, 12985, 
13380, 11454, 9265, 10053, 11443, 12132, 13733, 13850, 
11560, 9401, 9921, 11401, 12622, 14224, 14289, 12097, 
9623, 10630, 11572, 12816, 14180, 14125, 11667, 9328, 
9936, 11159, 12536, 13953, 13840, 11430, 9313, 9926, 
11557, 12428, 13802, 13041, 9927, 7448, 9143, 10872, 
12331, 14370, 14496, 13237, 11176, 11936, 12661, 14442, 
15005, 15359, 12871, 10505, 11231, 12078, 13307, 14027, 
14368, 12057, 9965, 10121, 11414, 13375, 14525, 14686, 
12243, 9833, 10722, 11778, 13143, 14844, 14856, 12745, 
9134, 7856, 9429, 11539, 13241, 14324, 12102, 10136, 
11107, 12028, 13999, 15130, 15488, 13379, 11028, 11708, 
13280, 14665, 15362, 15600, 12950, 10716, 10988, 12350, 
14163, 15264, 15724, 13374, 11764, 12711, 13239, 14849, 
15455, 15914, 13541, 10570, 9376, 10132, 11725, 12328, 
13105, 11022, 9710, 10659, 12068, 12890, 14242, 14294, 
11847, 9776, 10681, 12413, 13571, 14344, 14500, 12234, 
9961, 10699, 11626, 13135, 14387, 15282, 13028, 11211, 
11992, 13524, 15131, 15741, 15357, 12489, 9985, 10786, 
11492, 13851, 14509, 14751, 12327, 10023, 11315, 12363, 
13487, 14944, 15006, 12290, 9867, 11540, 12179, 14094, 
14941, 15006, 13585, 10769, 11408, 12634, 14073, 15361, 
15236, 13151, 9580, 8934, 10128, 12475, 13890, 14740, 
12617, 10358, 11648, 12418, 14094, 15127, 15775, 13647, 
11281, 11773, 13407, 15441, 15601, 15951, 13865, 11447, 
12422, 13725, 15766, 16389, 16868, 15221, 12503, 12780, 
14525, 16479, 17032, 17403, 14553, 12484, 13204, 13792, 
14896, 15673, 16332, 14196, 11749, 12977, 13886, 14931, 
15955, 16037, 14082, 11271, 12512, 13942, 16362, 17456, 
17446, 15509, 13069, 13524, 14918, 16161, 17524, 18138, 
14604, 12993, 13763, 14945, 16686, 17717, 17947, 15744, 
13388, 13177, 14588, 16075, 16705, 17074, 14415, 12766, 
13372, 14033, 14300, 12508, 11502, 9391, 7689, 9613, 
12291, 14448, 15075, 15670, 13929, 10989, 11875, 13409, 
15203, 15654, 16150, 13387, 10931, 11492, 12479, 13674, 
14519, 14241, 11685, 9486, 9990, 11440, 12415, 13505, 
12103, 10311, 8267, 7510, 8595, 10620, 11664, 3182, 6241, 
9365, 10965, 12372, 9958, 8088, 9290, 10665, 12132, 12827, 
13040, 10692, 8882, 9538, 10027, 12086, 13276, 13107, 
10680, 9136, 10744, 11733, 13334, 14654, 14830, 12189, 
9613, 11399, 12837, 13661, 15007, 15579, 12268, 9703, 
10627, 12077, 13287, 14459, 14825, 11958, 10049, 11512, 
12770, 13869, 14873, 15233, 12056, 9654, 10386, 11465, 
13354, 14601, 15161, 12324, 9782, 10791, 12502, 14111, 
14914, 15250, 12366, 10333, 11638, 12449, 13518, 14637, 
14756, 12011, 9878, 10976, 12464, 13674, 14979, 15312, 
12106, 10127, 11666, 12843, 13910, 15024, 15333, 12308, 
9992, 11278, 13364, 14966, 15231, 15507, 13744, 11417, 
12232, 14414, 15245, 15988, 15168, 11905, 9165, 10536, 
12570, 14106, 15204, 15509, 12821, 10321, 11282, 13133, 
14174, 15099, 14750, 12817, 10384, 11368, 12994, 14591, 
16154, 15904, 12784, 10737, 11865, 13809, 14721, 15202, 
15322, 12722, 10741, 11991, 13546, 14716, 15817, 15879, 
12679, 10390, 11524, 13140, 14426, 15613, 16212, 13088, 
10720, 11730, 13776, 14477, 15758, 15922, 13119, 9220, 
8372, 10239, 12397, 14740, 15550, 13306, 10833, 11892, 
13630, 15186, 16154, 16678, 12898, 10485, 11313, 13705, 
15572, 16086, 16305, 14129, 11066, 12251, 13830, 15345, 
16550, 16518, 13700, 10890, 12301, 14163, 15890, 16985, 
17544, 15337, 12633, 13383, 12813, 12051, 13149, 13636, 
10914, 9617, 10619, 12224, 13954, 15325, 15473, 12418, 
9730, 11214, 12572, 14565, 15287, 15721, 12519, 10689, 
11662, 13139, 14902, 16374, 16392, 13895, 11777, 12948, 
14326, 15625, 16745, 16980, 13946, 11181, 12665, 13678, 
15269, 16279, 16634, 14399, 11142, 11900, 13800, 14783, 
16626, 16861, 13917, 11228, 12531, 14206, 15773, 16344, 
16930, 13945, 11110, 12427, 14085, 15627, 16854, 17106, 
14677, 10410, 8550, 10626, 13366, 15337, 16460, 13619, 
11630, 12582, 13926, 15297, 16715, 17036, 14063, 11368, 
12246, 14111, 15525, 16900, 17272, 14254, 11961, 13155, 
14579, 16260, 17187, 17919, 15493, 13162, 13771, 15231, 
15836, 16880, 16976, 14728, 12106, 13030, 13848, 15344, 
16475, 17122, 13601, 10921, 12043, 14114, 15846, 16190, 
17125, 13769, 10768, 12336, 13849, 16138, 17507, 18050, 
15492, 12905, 12847, 14181, 15967, 16704, 17762, 14882, 
12591, 13807, 14959, 16933, 17369, 17453, 14351, 11582, 
13102, 14328, 16185, 16321, 16843, 13773, 11053, 12199, 
14147, 14470, 12598, 11916, 9185, 7903, 9742, 12691, 
15153, 15945, 16254, 13630, 11437, 12235, 14040, 15161, 
15995, 16291, 12944, 10947, 12055, 13444, 14852, 16029, 
16361, 13658, 10885, 11604, 13030, 13959, 14291, 14786, 
12002, 9014, 7610, 7426, 9602, 11077, 12544, 11334, 5710, 
9874, 11949, 10321, 8945, 10152, 11821, 13434, 15187, 
15269, 12661, 10699, 12040, 13154, 14149, 15472, 16569, 
13008, 10521, 11674, 13272, 14025, 15803, 16791, 13615, 
11043, 12448, 13929, 15158, 16610, 17520, 13900, 11095, 
11735, 13652, 14939, 16001, 16265, 13371, 11198, 11583, 
13377, 15361, 16420, 16765, 13800, 10866, 12026, 13908, 
14902, 16044, 16807, 13694, 11475, 13009, 14453, 16231, 
17093, 17411, 14433, 12242, 13035, 14304, 16309, 17026, 
16811, 13986, 11812, 13216, 14397, 16026, 17780, 17463, 
14717, 12029, 13046, 14820, 16626, 17564, 17802, 14134, 
13158, 15356, 16573, 16887, 17494, 17326, 13525, 11517, 
12410, 13817, 14933, 16399, 17019, 14008, 11808, 12599, 
14639, 16339, 17521, 17820, 14444, 11530, 13352, 14997, 
16038, 17631, 17614, 15601, 15176, 16930, 17979, 18772, 
19728, 19452, 16272, 14006, 15510, 17299, 17774, 18345, 
19080, 16486, 14242, 15465, 16973, 17971, 19068, 19075, 
15606, 13315, 14784, 16505, 17910, 18586, 18315, 15659, 
13621, 14673, 16037, 17467, 17972, 17676, 15452, 11850, 
10959, 13641, 15217, 16813, 17641, 15404, 13102, 14391, 
15764, 17326, 17715, 17947, 15272, 13078, 13962, 15372, 
18292, 18569, 16427, 13374, 14725, 15957, 17425, 18530, 
19251, 17094, 13711, 15275, 16663, 18254, 19023, 19787, 
16636, 14398, 15392, 16302, 15844, 14301, 14559, 11739, 
10080, 11690, 14352, 16702, 17810, 17898, 15159, 12527, 
14250, 15788, 17012, 18219, 17743, 15183, 12633, 14033, 
15528, 16984, 18041, 18388, 15248, 12831, 14289, 16143, 
17340, 18863, 18597, 15984, 13697, 14653, 16143, 17262, 
17805, 18565, 16147, 14734, 16548, 17410, 18044, 18705, 
18462, 15706, 13242, 14977, 16168, 17683, 18224, 18454, 
15784, 14003, 16605, 18013, 19361, 19204, 18970, 16655, 
12928, 11502, 13233, 15211, 16883, 17454, 15043, 12953, 
14515, 15846, 17501, 18922, 18903, 16175, 13492, 14150, 
15710, 18297, 18872, 19490, 15921, 13935, 14943, 16457, 
18425, 19975, 20440, 17716, 15059, 16086, 17290, 18477, 
19896, 20115, 17580, 15001, 15640, 17915, 18951, 20029, 
20221, 16653, 15063, 15726, 16849, 18121, 18843, 19112, 
16516, 13960, 15255, 16910, 18895, 20091, 20663, 17698, 
15441, 16775, 18158, 19897, 20424, 20111, 17784, 15044, 
16869, 17773, 19783, 21255, 20632, 18081, 15891, 17180, 
18143, 20197, 20926, 20639, 18407, 16313, 16998, 17860, 
19177, 19618, 19919, 17662, 16033, 17439, 18741, 18108, 
16641, 16319, 13221, 11160, 12783, 14876, 16831, 18379, 
18858, 16191, 14632, 16089, 16828, 18169, 19512, 18828, 
17364, 15516, 17065, 18245, 18684, 19472, 19235, 16885, 
14854, 14526, 12921, 12675, 14884, 15284, 13492, 11457, 
5938, 9694, 9429, 9142, 10648, 13235, 15610, 16868, 17364, 
16043, 14497, 15329, 16839, 17548, 18818, 19320, 15884, 
13834, 14748, 15784, 16729, 18274, 19138, 17413, 15394, 
16596, 17853, 18934, 20310, 20165, 18870, 16562, 16823, 
18051, 18816, 20410, 21211, 18551, 16274, 17289, 18317, 
20259, 19993, 19831, 18166, 16517, 17114, 17763, 19011, 
20541, 19974, 18105, 16130, 17422, 18472, 20213, 20721, 
20803, 19250, 16246, 16582, 18410, 19559, 20821, 20412, 
18576, 16272, 16917, 19027, 19917, 20418, 21188, 18382, 
16842, 17911, 19126, 20471, 21120, 20756, 18190, 15873, 
16924, 18468, 19579, 20877, 20726, 18525, 16110, 17480, 
19313, 20323, 20661, 20541, 18284, 16124, 17312, 18361, 
19170, 19945, 20548, 17605, 15973, 17488, 17444, 19086, 
19775, 19827, 17269, 14616, 15690, 16469, 18626, 19288, 
20111, 17769, 15738, 17060, 18885, 20010, 21371, 21541, 
18682, 15971, 16714, 18659, 19934, 21499, 22118, 18952, 
16025, 18120, 18897, 20630, 20286, 21077, 17710, 14857, 
16050, 17877, 19928, 21299, 21202, 18858, 14339, 13172, 
15521, 17434, 19823, 20679, 18288, 16798, 18673, 20628, 
21462, 22720, 22241, 20064, 17327, 18720, 19896, 19710, 
21185, 21916, 19661, 17134, 18027, 19449, 20912, 21234, 
21950, 19495, 17023, 18473, 19080, 20875, 21031, 21492, 
20091, 17511, 18834, 19126, 19922, 21215, 19017, 15506, 
12854, 14605, 16279, 18129, 20043, 21248, 18518, 15467, 
16586, 18277, 18915, 20597, 21244, 19024, 16294, 17234, 
18786, 20960, 21345, 22068, 19774, 17491, 18279, 19809, 
20757, 21618, 22131, 20214, 17581, 18321, 19590, 21486, 
22492, 23194, 20020, 16819, 17892, 18948, 20921, 21696, 
22549, 19559, 16404, 17301, 18659, 20430, 22300, 22569, 
19630, 16800, 17898, 19584, 21190, 21926, 22359, 20157, 
15823, 14136, 15930, 18341, 21044, 21204, 18994, 16973, 
18171, 19378, 20794, 22442, 22144, 19874, 17859, 18703, 
19082, 20781, 21860, 21536, 20172, 18429, 19221, 19824, 
21326, 22504, 23381, 21733, 19231, 20312, 21994, 22609, 
23317, 23074, 22005, 19209, 20734, 22513, 23017, 23698, 
24385, 22512, 19471, 20061, 21235, 22351, 22532, 22869, 
20409, 17908, 18722, 19894, 20960, 21999, 22125, 20797, 
19091, 19910, 20463, 22106, 22737, 22827, 21695, 19498, 
20180, 21204, 22272, 22803, 22808, 20979, 18952, 20365, 
20875, 22944, 23022, 22786, 21284, 19302, 20394, 21144, 
22633, 23511, 23355, 21979, 19988, 20143, 21966, 22574, 
19974, 19410, 15641, 13265, 14880, 16838, 19262, 19941, 
20479, 18929, 17760, 18078, 19055, 20553, 21732, 21671, 
19218, 18485, 18864, 20278, 21120, 21747, 21087, 17982, 
15115, 16518, 16282, 15032, 15658, 14966, 12172, 10336, 
12669, 14238, 14031, 12441, 13313, 11047, 10158, 12438, 
14255, 16434, 17873, 18481, 16360, 14479, 15595, 17392, 
18878, 19999, 19958, 16748, 13852, 14931, 16410, 18097, 
19654, 19480, 16387, 14515, 15205, 16854, 18544, 19510, 
20382, 17838, 14878, 15041, 16661, 19008, 20265, 20947, 
18048, 16472, 16434, 18250, 19571, 21148, 20117, 17788, 
14321, 14996, 15779, 17789, 18804, 18934, 17488, 15095, 
15859, 16691, 18369, 20012, 21073, 18029, 15582, 17247, 
18608, 19783, 20322, 20908, 18221, 15919, 17107, 18404, 
19262, 21741, 21514, 19798, 17410, 17973, 18469, 17910, 
14901)
</code></pre>

<p>The <code>ts.plot(data)</code> gives:<img src=""http://i.stack.imgur.com/E6WU0.jpg"" alt=""enter image description here""></p>

<p>With this data, I'm looking to forecast the values for the next year. This data is victim to both weekly and yearly seasonality. Due to this, I first attempted to use <code>tbats</code> from the <code>forecast</code> package but received an improper forecast that mirrors that found at <a href=""http://www.github.com/robjhyndman/forecast/issues/87"" rel=""nofollow"">http://www.github.com/robjhyndman/forecast/issues/87</a></p>

<p>Instead, I used the following code:</p>

<pre><code>n&lt;-length(data)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0
for(i in 1:20)
{
fit &lt;- auto.arima(data, xreg = fourier(1:n,i,m1) + fourier(1:n,i,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
if(fit$aicc &lt; bestfit$aicc)
{
    bestfit &lt;- fit
    bestk &lt;- i
}
}

k &lt;- bestk

bestfit &lt;- auto.arima(data, xreg = fourier(1:n,k,m1) + fourier(1:n,k,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
accuracy(bestfit)
fc &lt;- forecast(bestfit, xreg = fourier((n+1):(n+365),k,m1) + fourier((n+1):(n+365),k,m2), level = c(50,80,90), bootstrap = TRUE)
plot(fc)
</code></pre>

<p>This code is searching for the best ARIMA model through the use of Fourier terms in <code>xreg</code> to capture both seasonality components. This Fourier function is defined (per <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}
</code></pre>

<p>This forecasting gives me the following plot:<img src=""http://i.stack.imgur.com/2IsSD.jpg"" alt=""enter image description here""></p>

<p>In looking at this forecast, it seems by my naked eye to be off. Just by observation it appears that my forecast is not properly catching the small, but visible, increasing trend. Instead of being ""centered"" around the extended trendline, it appears that the forecast is ""centered"" around the mean of the entire dataset.</p>

<p>First off, am I doing something that is just blatantly wrong? (my mind is a little fuzzy this morning)</p>

<p>If my forecast is correct, how is it that it falls so much below the extended trendline?</p>

<p>Lastly, are there any other suggestions which might be beneficial to my forecasting?</p>
"
"0.0834783871129682","0.0812444463702388","143636","<p>I have daily visitors data for the last 10 years. I want to do some basic tests like which is the busiest day, which is the busiest month, busiest week etc. I used <code>auto.arima</code> function with argument <code>xreg</code> to find out the coefficients of all the days of the week, week of the month. This is the output I got:</p>

<pre><code>&gt; summary(arima1)
Series: dailysea 
ARIMA(1,1,2)                    

Coefficients:
          ar1      ma1      ma2         Sun        Mon         Tue        Wed         Thu
      -0.1250  -0.4506  -0.3712  -1466.6853  -3623.175  -3895.0555  -3722.146  -3327.4288
s.e.   0.1207   0.1117   0.0891    325.7253    386.738    379.8793    379.883    386.7512
            Fri
      -2146.910
s.e.    325.736

sigma^2 estimated as 7776468:  log likelihood=-6808.5
AIC=13637   AICc=13637.31   BIC=13682.92

Training set error measures:
                   ME     RMSE      MAE  MPE MAPE      MASE         ACF1
Training set 59.63838 2784.809 1952.625 -Inf  Inf 0.8353728 -0.001839015
</code></pre>

<p>Can I use these coefficients to conclude that Saturday is the busiest followed by Sunday, Friday etc.? Also I have infinite MAPE which is not making sense to me.</p>
"
"0.157408356293588","0.15319599723238","144158","<p>I am trying to do time series analysis and am new to this field. I have daily count of an event from 2006-2009 and I want to fit a time series model to it. Here is the progress that I have made:</p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=365.25)
plot.ts(timeSeriesObj)
</code></pre>

<p>The resulting plot I get is:</p>

<p><img src=""http://i.stack.imgur.com/q2Gf5.jpg"" alt=""Time Series Plot""></p>

<p>In order to verify whether there is seasonality and trend in the data or not, I follow the steps mentioned in this <a href=""http://stats.stackexchange.com/questions/57705/identify-seasonality-in-time-series-data"">post</a> :</p>

<pre><code>ets(x)
fit &lt;- tbats(x)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>and in Rob J Hyndman's <a href=""http://robjhyndman.com/hyndsight/detecting-seasonality/"" rel=""nofollow"">blog</a>:</p>

<pre><code>library(fma)
fit1 &lt;- ets(x)
fit2 &lt;- ets(x,model=""ANN"")

deviance &lt;- 2*c(logLik(fit1) - logLik(fit2))
df &lt;- attributes(logLik(fit1))$df - attributes(logLik(fit2))$df 
#P value
1-pchisq(deviance,df)
</code></pre>

<p>Both cases indicate that there is no seasonality.</p>

<p>When I plot the ACF &amp; PACF of the series, here is what I get:</p>

<p><img src=""http://i.stack.imgur.com/mgBav.jpg"" alt=""ACF"">
<img src=""http://i.stack.imgur.com/p4DYo.jpg"" alt=""PACF""></p>

<p>My questions are:</p>

<ol>
<li><p>Is this the way to handle daily time series data? This <a href=""http://www.r-bloggers.com/forecasting-with-daily-data/"" rel=""nofollow"">page</a> suggests that I should be looking at both weekly and annual patterns but the approach is not clear to me.</p></li>
<li><p>I do not know how to proceed once I have the ACF and PACF plots.</p></li>
<li><p>Can I simply use the auto.arima function?</p>

<p>fit &lt;- arima(myts, order=c(p, d, q)</p></li>
</ol>

<p>*****Updated Auto.Arima results******</p>

<p>When i change the frequency of the data to 7 according to Rob Hyndman's comments <a href=""http://stats.stackexchange.com/questions/14742/auto-arima-with-daily-data-how-to-capture-seasonality-periodicity"">here</a>, auto.arima selects a seasonal ARIMA model and outputs:</p>

<pre><code>Series: timeSeriesObj 
ARIMA(1,1,2)(1,0,1)[7]                    

Coefficients:
       ar1      ma1     ma2    sar1     sma1
      0.89  -1.7877  0.7892  0.9870  -0.9278
s.e.   NaN      NaN     NaN  0.0061   0.0162

sigma^2 estimated as 21.72:  log likelihood=-4319.23
AIC=8650.46   AICc=8650.52   BIC=8682.18 
</code></pre>

<p>******Updated Seasonality Check******</p>

<p>When I test seasonality with frequency 7, it outputs True but with seasonality 365.25, it outputs false. <strong>Is this enough to conclude a lack of yearly seasonality?</strong></p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=7)
fit &lt;- tbats(timeSeriesObj)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>returns:</p>

<pre><code>True
</code></pre>

<p>while </p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=365.25)
fit &lt;- tbats(timeSeriesObj)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>returns:</p>

<pre><code>False
</code></pre>
"
"0.454320359925712","0.428125523035833","144745","<p>I have 17 years (1995 to 2011) of death certificate data related to suicide deaths for a state in the U.S. There is a lot of mythology out there about suicides and the months/seasons, much of it contradictory, and of the literature I've reviewed, I do not get a clear sense of methods used or confidence in results.</p>

<p>So I've set out to see if I can determine whether suicides are more or less likely to occur in any given month within my data set. All of my analyses are done in R.</p>

<p>The total number of suicides in the data is 13,909.</p>

<p>If you look at the year with the fewest suicides, they occur on 309/365 days (85%). If you look at the year with the most suicides, they occur on 339/365 days (93%).</p>

<p>So there are a fair number of days each year without suicides. However, when aggregated across all 17 years, there are suicides on every day of the year, including February 29 (although only 5 when the average is 38).</p>

<p><img src=""http://i.stack.imgur.com/VMQYa.jpg"" alt=""enter image description here""></p>

<p>Simply adding up the number of suicides on each day of the year doesn't indicate a clear seasonality (to my eye).</p>

<p>Aggregated at the monthly level, average suicides per month range from:</p>

<p>(m=65, sd=7.4, to m=72, sd=11.1)</p>

<p>My first approach was to aggregate the data set by month for all years and do a chi-square test after computing the expected probabilities for the null hypothesis, that there was no systematic variance in suicide counts by month. I computed the probabilities for each month taking into account the number of days (and adjusting February for leap years).</p>

<p>The chi-square results indicated no significant variation by month:</p>

<pre><code># So does the sample match  expected values?
chisq.test(monthDat$suicideCounts, p=monthlyProb)
# Yes, X-squared = 12.7048, df = 11, p-value = 0.3131
</code></pre>

<p>The image below indicates total counts per month. The horizontal red lines are positioned at the expected values for February, 30 day months, and 31 day months respectively. Consistent with the chi-square test, no month is outside the 95% confidence interval for expected counts.
<img src=""http://i.stack.imgur.com/XRCzM.jpg"" alt=""enter image description here""></p>

<p>I thought I was done until I started to investigate time series data. As I imagine many people do, I started with the non-parametric seasonal decomposition method using the <code>stl</code> function in the stats package. </p>

<p>To create the time series data, I started with the aggregated monthly data:</p>

<pre><code>suicideByMonthTs &lt;- ts(suicideByMonth$monthlySuicideCount, start=c(1995, 1), end=c(2011, 12), frequency=12) 

# Plot the monthly suicide count, note the trend, but seasonality?
plot(suicideByMonthTs, xlab=""Year"",
  ylab=""Annual  monthly  suicides"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xSWJm.jpg"" alt=""enter image description here""></p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
1995  62  47  55  74  71  70  67  69  61  76  68  68
1996  64  69  68  53  72  73  62  63  64  72  55  61
1997  71  61  64  63  60  64  67  50  48  49  59  72
1998  67  54  72  69  78  45  59  53  48  65  64  44
1999  69  64  65  58  73  83  70  73  58  75  71  58
2000  60  54  67  59  54  69  62  60  58  61  68  56
2001  67  60  54  57  51  61  67  63  55  70  54  55
2002  65  68  65  72  79  72  64  70  59  66  63  66
2003  69  50  59  67  73  77  64  66  71  68  59  69
2004  68  61  66  62  69  84  73  62  71  64  59  70
2005  67  53  76  65  77  68  65  60  68  71  60  79
2006  65  54  65  68  69  68  81  64  69  71  67  67
2007  77  63  61  78  73  69  92  68  72  61  65  77
2008  67  73  81  73  66  63  96  71  75  74  81  63
2009  80  68  76  65  82  69  74  88  80  86  78  76
2010  80  77  82  80  77  70  81  89  91  82  71  73
2011  93  64  87  75 101  89  87  78 106  84  64  71
</code></pre>

<p>And then performed the <code>stl()</code> decomposition</p>

<pre><code># Seasonal decomposition
suicideByMonthFit &lt;- stl(suicideByMonthTs, s.window=""periodic"")
plot(suicideByMonthFit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/cS5pE.jpg"" alt=""enter image description here""></p>

<p>At this point I became concerned because it appears to me that there is both a seasonal component and a trend. After much internet research I decided to follow the instructions of Rob Hyndman and George AthanaÂ­sopouÂ­los as laid out in their on-line text ""Forecasting: principles and practice"", specifically to apply a seasonal ARIMA model.</p>

<p>I used <code>adf.test()</code> and <code>kpss.test()</code> to assess for <em>stationarity</em> and got conflicting results. They both rejected the null hypothesis (noting that they test the opposite hypothesis).</p>

<pre><code>adfResults &lt;- adf.test(suicideByMonthTs, alternative = ""stationary"") # The p &lt; .05 value 
adfResults

    Augmented Dickey-Fuller Test

data:  suicideByMonthTs
Dickey-Fuller = -4.5033, Lag order = 5, p-value = 0.01
alternative hypothesis: stationary

kpssResults &lt;- kpss.test(suicideByMonthTs)
kpssResults

    KPSS Test for Level Stationarity

data:  suicideByMonthTs
KPSS Level = 2.9954, Truncation lag parameter = 3, p-value = 0.01
</code></pre>

<p>I then used the algorithm in the book to see if I could determine the amount of differencing that needed to be done for both the trend and season. I ended  with 
nd = 1, ns = 0.</p>

<p>I then ran <code>auto.arima</code>, which chose a model that had both a trend and a seasonal component along with a ""drift"" type constant.</p>

<pre><code># Extract the best model, it takes time as I've turned off the shortcuts (results differ with it on)
bestFit &lt;- auto.arima(suicideByMonthTs, stepwise=FALSE, approximation=FALSE)
plot(theForecast &lt;- forecast(bestFit, h=12))
theForecast
</code></pre>

<p><img src=""http://i.stack.imgur.com/qTUi9.jpg"" alt=""enter image description here""></p>

<pre><code>&gt; summary(bestFit)
Series: suicideByMonthFromMonthTs 
ARIMA(0,1,1)(1,0,1)[12] with drift         

Coefficients:
          ma1    sar1     sma1   drift
      -0.9299  0.8930  -0.7728  0.0921
s.e.   0.0278  0.1123   0.1621  0.0700

sigma^2 estimated as 64.95:  log likelihood=-709.55
AIC=1429.1   AICc=1429.4   BIC=1445.67

Training set error measures:
                    ME    RMSE     MAE       MPE     MAPE     MASE       ACF1
Training set 0.2753657 8.01942 6.32144 -1.045278 9.512259 0.707026 0.03813434
</code></pre>

<p>Finally, I looked at the residuals from the fit and if I understand this correctly, since all values are within the threshold limits, they are behaving like white noise and thus the model is fairly reasonable. I ran a <em>portmanteau test</em> as described in the text, which had a p value well above 0.05, but I'm not sure that I have the parameters correct.</p>

<pre><code>Acf(residuals(bestFit))
</code></pre>

<p><img src=""http://i.stack.imgur.com/gso3q.jpg"" alt=""enter image description here""></p>

<pre><code>Box.test(residuals(bestFit), lag=12, fitdf=4, type=""Ljung"")

    Box-Ljung test

data:  residuals(bestFit)
X-squared = 7.5201, df = 8, p-value = 0.4817
</code></pre>

<p>Having gone back and read the chapter on arima modeling again, I realize now that <code>auto.arima</code> did choose to model trend and season. And I'm also realizing that forecasting is not specifically the analysis I should probably be doing. I want to know if a specific month (or more generally time of year) should be flagged as a high risk month. It seems that the tools in the forecasting literature are highly pertinent, but perhaps not the best for my question. Any and all input is much appreciated.</p>

<p>I'm posting a link to a csv file that contains the daily counts. The file looks like this:</p>

<pre><code>head(suicideByDay)

        date year month day_of_month t count
1 1995-01-01 1995    01           01 1     2
2 1995-01-03 1995    01           03 2     1
3 1995-01-04 1995    01           04 3     3
4 1995-01-05 1995    01           05 4     2
5 1995-01-06 1995    01           06 5     3
6 1995-01-07 1995    01           07 6     2
</code></pre>

<p><a href=""https://dl.dropboxusercontent.com/u/1252082/daily_suicide_counts.csv"" rel=""nofollow"">daily_suicide_data.csv</a></p>

<p>Count is the number of suicides that happened on that day. ""t"" is a numeric sequence from 1 to the total number of days in the table (5533).</p>

<p>I've taken note of comments below and thought about two things related to modeling suicide and seasons. First, with respect to my question, months are simply proxies for marking change of season, I am not interested in wether or not a particular month is different from others (that of course is an interesting question, but it's not what I set out to investigate). Hence, I think it makes sense to <strong>equalize</strong> the months by simply using the first 28 days of all months. When you do this, you get a slightly worse fit, which I am interpreting as more evidence towards a lack of seasonality. In the output below, the first fit is a reproduction from an answer below using months with their true number of days, followed by a data set <strong>suicideByShortMonth</strong> in which suicide counts were computed from the first 28 days of all months. I'm interested in what people think about wether or not this adjustment is a good idea, not necessary, or harmful?</p>

<pre><code>&gt; summary(seasonFit)

Call:
glm(formula = count ~ t + days_in_month + cos(2 * pi * t/12) + 
    sin(2 * pi * t/12), family = ""poisson"", data = suicideByMonth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4782  -0.7095  -0.0544   0.6471   3.2236  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         2.8662459  0.3382020   8.475  &lt; 2e-16 ***
t                   0.0013711  0.0001444   9.493  &lt; 2e-16 ***
days_in_month       0.0397990  0.0110877   3.589 0.000331 ***
cos(2 * pi * t/12) -0.0299170  0.0120295  -2.487 0.012884 *  
sin(2 * pi * t/12)  0.0026999  0.0123930   0.218 0.827541    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 302.67  on 203  degrees of freedom
Residual deviance: 190.37  on 199  degrees of freedom
AIC: 1434.9

Number of Fisher Scoring iterations: 4

&gt; summary(shortSeasonFit)

Call:
glm(formula = shortMonthCount ~ t + cos(2 * pi * t/12) + sin(2 * 
    pi * t/12), family = ""poisson"", data = suicideByShortMonth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.2414  -0.7588  -0.0710   0.7170   3.3074  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         4.0022084  0.0182211 219.647   &lt;2e-16 ***
t                   0.0013738  0.0001501   9.153   &lt;2e-16 ***
cos(2 * pi * t/12) -0.0281767  0.0124693  -2.260   0.0238 *  
sin(2 * pi * t/12)  0.0143912  0.0124712   1.154   0.2485    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 295.41  on 203  degrees of freedom
Residual deviance: 205.30  on 200  degrees of freedom
AIC: 1432

Number of Fisher Scoring iterations: 4
</code></pre>

<p>The second thing I've looked into more is the issue of using month as a proxy for season. Perhaps a better indicator of season is the number of daylight hours an area receives. This data comes from a northern state that has substantial variation in daylight. Below is a graph of the daylight from the year 2002. </p>

<p><img src=""http://i.stack.imgur.com/yvVXl.jpg"" alt=""enter image description here""></p>

<p>When I use this data rather than month of the year, the effect is still significant, but the effect is very, very small. The residual deviance is much larger than the models above. If daylight hours is a better model for seasons, and the fit is not as good, is this more evidence of very small seasonal effect? </p>

<pre><code>&gt; summary(daylightFit)

Call:
glm(formula = aggregatedDailyCount ~ t + daylightMinutes, family = ""poisson"", 
    data = aggregatedDailyNoLeap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.0003  -0.6684  -0.0407   0.5930   3.8269  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      3.545e+00  4.759e-02  74.493   &lt;2e-16 ***
t               -5.230e-05  8.216e-05  -0.637   0.5244    
daylightMinutes  1.418e-04  5.720e-05   2.479   0.0132 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 380.22  on 364  degrees of freedom
Residual deviance: 373.01  on 362  degrees of freedom
AIC: 2375

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I'm posting the daylight hours in case anyone wants to play around with this. Note, this is not a leap year, so if you want to put in the minutes for the leap years, either extrapolate or retrieve the data.</p>

<p><a href=""https://dl.dropboxusercontent.com/u/1252082/state.daylight.2002.csv"" rel=""nofollow"">state.daylight.2002.csv</a></p>

<p>[<strong>Edit</strong> to add plot from deleted answer (hopefully rnso doesn't mind me moving the plot in the deleted answer up here to the question. svannoy, if you don't want this added after all, you can revert it)]</p>

<p><img src=""http://i.stack.imgur.com/WiuvE.png"" alt=""enter image description here""></p>
"
"0.196457503777974","0.191200161882872","145251","<p>Sorry in advance if this is too basic of a question - I've been struggling with this data set for almost a month and feel like I'm going in circles, and the more I Google the more confused I get.</p>

<p>I have a time series of hourly activity levels (mean of 7 persons) for a period of about 2 months (1704 observations). There is obviously a strong ""seasonal"" component (freq=24) to this time series, with activity showing regular fluctuations between night and day. I am ultimately hoping to compare my activity time series to three other time series of environmental variables, to see how weather, temperature, etc affect people's activity on an hourly scale, following the methods in <a href=""http://cid.oxfordjournals.org/content/early/2012/05/21/cid.cis509.full"" rel=""nofollow"">this paper</a>. I'm not planning on doing forecasting, just wanting to know if these explanatory variables are affecting activity, and if so, how.</p>

<p>The paper linked above did their analysis in a few steps, if I understand correctly:</p>

<ol>
<li>Use stl to assess trend and seasonality.</li>
<li>Fit time series to ARIMA model.</li>
<li>Transform data into series of independent, identically distributed random variables</li>
<li>Choose best-fitting model by AIC</li>
<li>Use residuals for cross-correlating variables.</li>
</ol>

<p>Okay. Here are my questions:</p>

<ol>
<li><p>I can do step 1, but don't know how to relate that to step 2. Am I using the remainder from stl analysis for ARIMA modeling? If not, what's the point of step 1?</p></li>
<li><p>I understand how to choose some candidate models for ARIMA based on ACF, PACF, and auto.arima. But I can't get past the diagnostics. My Ljung-Box values are ALWAYS significant for ALL lags. Okay, so that means my residuals are correlated (I think). And since I want to use the residuals for cross-correlation, I assume that's bad. But no matter which models I try (I've tried maybe 6-10, is that enough?) I can't get good Ljung-Box p-values. The best fitting ARIMA so far (by AIC) is (1,0,2)x(1,1,2)24.</p></li>
</ol>

<p>Does this mean my time series doesn't fit an ARIMA model? How can I get iid residuals if I can't even get it to fit a model? Arrrghh.</p>

<p>So to be more succinct, my main question is: why do I always have these significant Ljung-Box values, and what can I do to fit a better model to get iid residuals?</p>

<p>Subsample of data (full set <a href=""https://www.dropbox.com/s/lhd9zu0x8r4o8pe/fitbit%20data.txt?dl=0"" rel=""nofollow"">here</a>):</p>

<pre><code>[1] 24 16 40 48 50 38 24  4  4  5  3  6  4  4  4  3 12 63 55 42 56 20 10 26 45 47 66 64 59
[30] 54 24  5  6  2  4  3  6 10  6  2 13 39 26 17 24 13 19 26 17 32 54 68 58 39 20  0  3  2
[59]  8  2  4  1  5 11  5 60 57 54 40 40 53 74 40 42 57 46 46 26  9  8  4  6 14  8  5  3  2
[88]  7 19 47 53 43 53 51 55 64 48 64 57 56 52 34 22  8  5  6  4  6  3  4  7  6 27 40 48 41
[117] 43 51 50 44 56 64 68 46 49 35 16  2 14  3  7  3 13  3  3  2 14 49 62 42 41 57 52 63 32
[146] 54 59 60 68 24 12  2  2  2  2  7  6  5  9 10 26 53 50 59 28 45 47 44 48 55 59 77 86 33
[175] 18 16 10  6  9  9 14  7  9  7  9 46 57 41 33 32 34 29 39 39 27 26  4 10  9  6  6  2  4
[204]  1  2  2  4  4 17 50 47 24 27 34 26 38 20  6 20 15 25  8  2  2  3  6  4  3  3  4  4  2
[233] 18 41 63 52 37 32 32 28 48 20  6 10  9  7  5 10  4  3  4  7  4  3  4 10  8 56 47 50 27
[262] 30 22 38 38 28 33 24 18 12 14  2 10  4 21  4  5  6  4  4 20 41 46 16  8 20 24 21 16 27
[291] 10  6 14  5  6  6 12  2 10  7  6  2  2  3 16 47 56 43 30 35 32 41 20 20 11 34 16  6 10
[320]  2  5 10  3 11  6  5  7  5 14 50 30 26 19 16 10  5 12 12 22 16 16 10  4  5  4  4  8 14
[349]  4  6  4  5 21 47 28 15  8 12 18 18 16 10  5  8 12  3  6  4  5 12 11  8  2  4  6 10 25
[378] 42 20 15  8 18 10 10  6 18 12  4  7  6  6  4  8 14  3 10 11  5 10  9 26 54 41 36 44  9
[407]  4  5  3  8 12 16 11 12 13 26  5 13 13  1  1  5 18  7 39 64 64 65 44 34 42 63 62 54 26
[436] 30 34 25 15  7  1  0  2  1  0  9 13 10 33 65 59 48 44 60 65 44 55 65 67 76 85 63 48  8
[465]  2  0  3  1  1  1  8 12 19 72 67 42 46 70 54 37 41 66 62 54 80 52 22  3  2  2  1  1  5
[494]  2  2  5 37 48 32 29 27 25 21  2 17  3 24  2  7  1  1  4  7  8  7  4  3  6  2  4 26 28
[523] 15  6  2  4  1 12  4  2  4 14 11  2  5  1 13 16 10  5 14  1  2  3 13 24 29 20 12  8  4
[552]  8  1 11  8 10  6  4  6  1  6  8  4  7 18 17 12  3 18 50 25 27 20 14 14  9 14 14 15  5
[581]  8  3  4  3  3 11 12 12  4 19 25  8 33 53 61 49 50 34 38 45 76 65 72 53 84 65 51 19  4
[610]  2 11  7  5  3  6  3 38 85 83 72 58 77 78 63 73 64 56 22  3 10 13 10  2  1  1  0  8  6
[639]  5  2 34 54 56 54 14  5 17 18 21  3 14 14  6  4  1  2  4 10  7  3  3  4 12 17 54 68 49
[668] 51 38 11 29 17  1  2  4  8  9  6  4  3 14  0  1 10  8  4  3  3 25 31  9  9 10  6  8  9
[697]  4 11  4  6  3  9  0  2  4  1 10 20 11  2  8  4 28 35 40 34 36 19 19 15 23 14  6  4  2
[726]  6  5  4  2  4  4  2  8 13 17  4 44 30 23 22 11  5 10 12  6  8 11  1 12 10  1  2  0  6
[755]  6  3  4  9  1  9 13 41  8  6  9 13 28  7  2  8  7  2  3  6  1  2  5  4  4  4  2  5  9
[784]  9 28 53 40 28  6  8  1  7  2 13 20  7  3  8  4  2  2  6  3  5 16  8  2 14 16 41 20 22
[813]  7  8 10 24 23 24 19 14  5  1  1  2  9  0  6  2 15  8  4  5 26 28  9  9 16 30 11 12  7
</code></pre>

<p>ACF/PACF after taking 24th difference: </p>

<p><img src=""http://i.stack.imgur.com/1SWHy.png"" alt=""ACF/PACF of time series after taking 24th difference""></p>

<p>Diagnostics of SARIMA(1,0,2)x(1,1,2)24 model (best model by AIC and as suggested by auto.arima):</p>

<p><img src=""http://i.stack.imgur.com/Tp70f.png"" alt=""enter image description here""></p>
"
"0.0834783871129682","0.0812444463702388","148371","<p>I'm trying to fit a sarima model on the univariate data with 180 points (periodicity=12). I use the auto.arima function in R. After fitting a model to the data, the only problem is the violation of the normality assumption. Then, I refit models after transforming the data but the residuals are still non-normal. For transformation of the data, I use both BoxCox.lambda (in forecast package) and boxcoxnc (in AID package) functions. Can anybody help me to fix this problem?</p>

<pre><code>ser=c(1.887090e+04, -6.023007e+00,  1.193635e-02, -1.455856e-05,  1.064251e-08, -4.953592e-12,  1.517229e-15, -3.090332e-19,
4.137144e-23, -3.491891e-27,  1.682794e-31, -3.527046e-36,  1.904962e+04, -7.394189e+00,  1.600849e-02, -2.077511e-05,
1.585519e-08,-7.587987e-12,    2.363570e-15, -4.859251e-19,  6.534816e-23, -5.525202e-27,  2.663420e-31, -5.580438e-36,
2.009098e+04, -1.061082e+01,  2.319182e-02, -2.917768e-05,  2.171827e-08, -1.019917e-11,  3.133564e-15, -6.379905e-19,
8.520995e-23, -7.168462e-27,  3.442102e-31, -7.188143e-36,  2.067028e+04, -8.034999e+00,  1.761326e-02, -2.240562e-05,
1.680919e-08, -7.961614e-12,  2.469832e-15, -5.081494e-19,  6.861040e-23, -5.835236e-27,  2.831898e-31, -5.974519e-36,
2.233604e+04, -1.033148e+01,  2.287039e-02, -2.952031e-05,  2.255568e-08, -1.086351e-11,  3.419260e-15, -7.123005e-19,
9.720229e-23, -8.341734e-27,  4.079166e-31, -8.660882e-36,  2.392045e+04, -8.246481e+00,  1.585412e-02, -2.056180e-05,
1.636424e-08, -8.253437e-12,  2.710813e-15, -5.858824e-19,  8.245204e-23, -7.258003e-27,  3.624039e-31, -7.827743e-36,
2.636514e+04, -9.886355e+00,  1.951992e-02, -2.504930e-05,  1.963158e-08, -9.789139e-12,  3.190186e-15, -6.856046e-19,
9.606813e-23, -8.427664e-27,  4.196799e-31, -9.046539e-36,  2.866210e+04, -8.866902e+00,  1.734494e-02, -2.387617e-05,
1.957175e-08, -9.993900e-12,  3.300201e-15, -7.152619e-19,  1.008517e-22, -8.892694e-27,  4.448060e-31, -9.626143e-36,
3.002254e+04, -1.007403e+01,  2.151203e-02, -2.984675e-05,  2.427803e-08, -1.226036e-11,  3.997630e-15, -8.550747e-19,
1.190499e-22, -1.037815e-26,  5.140218e-31, -1.103334e-35,  2.929311e+04, -1.123255e+01,  2.282206e-02, -2.968240e-05,
2.323868e-08, -1.146069e-11,  3.677709e-15, -7.777557e-19,  1.073806e-22, -9.301478e-27,  4.584147e-31, -9.800725e-36,
3.306894e+04, -1.396117e+01,  2.326777e-02, -2.724425e-05,  2.023428e-08, -9.690231e-12,  3.055811e-15, -6.392630e-19,
8.763020e-23, -7.552202e-27,  3.707622e-31, -7.901994e-36,  3.491666e+04, -1.315883e+01,  2.554492e-02, -3.194439e-05,
2.437661e-08, -1.184053e-11,  3.762542e-15, -7.896499e-19,  1.082565e-22, -9.310722e-27,  4.554895e-31, -9.664092e-36,
3.775600e+04, -2.101521e+01,  4.695457e-02, -6.000206e-05,  4.510264e-08, -2.134088e-11,  6.600784e-15, -1.352465e-18,
1.817468e-22, -1.538166e-26,  7.429410e-31, -1.560507e-35,  3.699341e+04, -1.019327e+01,  1.761360e-02, -2.428662e-05,
2.084200e-08, -1.112473e-11,  3.796505e-15, -8.415154e-19,  1.204392e-22, -1.072641e-26,  5.402195e-31, -1.174885e-35,
4.009280e+04, -1.887174e+01,  3.441926e-02, -4.161190e-05,  3.152055e-08, -1.535050e-11,  4.911316e-15, -1.040003e-18,
1.440215e-22, -1.251900e-26,  6.190925e-31, -1.327693e-35)

require(""forecast"")
fit=auto.arima(ser,d = 0,D = 1,max.p = 6, max.q = 6,max.P = 6, max.Q = 6, max.order = 25,start.p=1, start.q=1, start.P=1, start.Q=1,stationary = FALSE,
seasonal=TRUE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=FALSE,ic=""aicc"")
</code></pre>
"
"0.323478750062752","0.274200006499556","151657","<p>I am running X-13 SEATS on r for monthly data in six years of observations and I think I got a (sufficiently) reasonable fit for the ARIMA model, but the output also shows me that my original series does not have significant seasonality, as it follows:</p>

<pre><code> Call:
seas(x = data_r[, 1], transform.function = ""log"", regression.aictest = NULL, 
    outlier = NULL, arima.model = ""(0 1 1)(1 1 0)"")

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
AR-Seasonal-12     -0.6194     0.1110  -5.581 2.39e-08 ***
MA-Nonseasonal-01   0.6220     0.1093   5.690 1.27e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 773.4, BIC: 778.4  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 20.04   Shapiro (normality): 0.9754
    &gt; 
                qs p-val
    qsori        0     1
    qsorievadj   0     1
    qsrsd        0     1
    qssadj       0     1
    qssadjevadj  0     1
    qsirr        0     1
    qsirrevadj   0     1
</code></pre>

<p>(Still, there is also the fact that the irregular component seems to dominate the SI ratio for some specific months in some years. So maybe there is some dummy variable in the pre-adjustment that I am missing (right?)) </p>

<p>But when I run a regression on Stata for yearly and monthly dummies on the original series -- assuming the seasonality is deterministic --, I cannot reject with an F test that they are all equal to zero. What does this show me? That my ARIMA fit is not correct?</p>

<p>Also, if someone could point me out the difference in interpretation that you should have when running a regression on seasonal dummies and deseasonalizing data with a X-13 SEATS, it would be also very helpful. Maybe that is what I am missing here.</p>

<p>Edit: is it by any chance a common practice, in some particular situations (when you are deseasonalizing a set of series), still deseasonalize a given series even if that series does not show significant seasonality?</p>

<p>Edit2: Adding the results of the automatic adjustment:</p>

<pre><code>Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
Constant            59.1761    38.0551   1.555  0.11994    
Easter[15]        -903.6151   341.1891  -2.648  0.00809 ** 
MA-Nonseasonal-01    0.4974     0.1138   4.370 1.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)  Obs.: 60  Transform: none
AICc: 925.6, BIC: 933.2  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.):  21.9   Shapiro (normality): 0.9498 *

            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1 
</code></pre>

<p>I also, I get the following error for the monthplot function with the automatic adjustment: </p>

<pre><code>Error in `[.default`(x$data, , ""seasonal"") : subscript out of bounds
</code></pre>

<p>Following this result from the automatic adjustment, the use of the dummy for easter, with the original specification, does not change that much the first output:</p>

<pre><code>Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
Easter[15]        -0.08307    0.02690  -3.088  0.00202 ** 
AR-Seasonal-12    -0.63353    0.10816  -5.858  4.7e-09 ***
MA-Nonseasonal-01  0.50391    0.12075   4.173  3.0e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 767.9, BIC: 774.3  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 29.37   Shapiro (normality): 0.9721  
            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1
</code></pre>

<p>Most recent observation: Now I Think I am fairly sure that there is no significant seasonality in this series, but I would be thankful if someone could show me other problems that I might not be considering. Still, I would like a possible canonical/scholarly answer on why I can reject the null hypothesis for the whole set of seasonal dummies being zero (though I had a small result for the F test with my data, ~4, but I still reject the null) and still get a reasonable ARIMA fit with which I cannot reject no seasonality in my original data. Does that have something to do with the difference of the adjustment with ARIMA models and deterministic seasonality? An intuitive answer on this difference would be of some help.</p>
"
"0.118056267220191","0.114896997924285","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.149330678589311","0.145334483902552","153204","<p>I have a time series for sales data on a weekly and monthly basis.  I tried using <code>holt.winter</code> and <code>auto.arima</code>. <code>holt.winter</code> can work only on monthly data (freq = 12 &lt; 24), and gives good results, but <code>auto.arima</code> gives very bad results on both monthly and weekly data, just a straight line in the following figures:</p>

<p><img src=""http://i.stack.imgur.com/zNZ9i.png"" alt=""arima monthly"">  </p>

<p><img src=""http://i.stack.imgur.com/b6W12.png"" alt=""enter image description here""></p>

<p>I have the following questions:  </p>

<ol>
<li>Can anyone provide some theoretical basis on why ARIMA performs poorly and why HW performs better?  </li>
<li>Also what model should I use for relatively high frequency data (weekly or daily)?  </li>
</ol>

<p>Also if someone can guide me to advanced books in that area (I have done some reading in Brockwell, 2002, <em>Introduction to Time Series</em>).</p>

<p>[Update]</p>

<p>I tried holt-winter . auto.arima . arima and got the following results</p>

<pre><code>ARIMA (1,0,1)(1,1,1) : sigma^2 estimated as 94587:  log likelihood = -266.51,  aic = 543.02

AUTO.ARIMA =&gt;ARIMA(0,0,0) with non-zero mean : sigma^2 estimated as 141005:  log likelihood=-352.67
AIC=709.33   AICc=709.6   BIC=713.07

Holt-Winter : ETS(A,A,A) 
  Smoothing parameters:
    alpha = 0.0298 
    beta  = 1e-04 
    gamma = 0.0133 
sigma:  306.1749
sigma^2 : 93743.06939001
     AIC     AICc      BIC 
767.3367 784.8851 797.2759 
</code></pre>

<p>it seems arima(1,0,1)(1,1,1) gives better AIC and log-likelihood that result of auto arima , also HW detects seasonality , is that auto.arima stucking at some local optima</p>
"
"0.131990919337114","0.128458748884677","154104","<p>I am trying to estimate parameter $d$ for ARFIMA model using different methods: Hurst, ML, fdSperio, fdGPH and the function <code>arfima</code> which selects the best fit automatically. The results shown are too large in difference, and I wonder why. </p>

<p>I attached an image of my coding:</p>

<p><img src=""http://i.stack.imgur.com/ZKz6B.png"" alt=""""></p>

<p>Especially parameter $d$ from <code>arfima</code>, the value is too extreme. Other values are having big differences as well.</p>

<p>Also, this is brief explanation on what I am doing:</p>

<ol>
<li>I am fitting ARFIMA model with my data: monthly Brent Oil Price.</li>
<li>Before that, I will need to estimate the values of $d$, and I am doing that with different methods as mentioned above.</li>
<li>After the values of $d$ are obtained, I will use <code>diffseries</code> to difference the series manually.</li>
<li>Then I will fit those differenced series into ARIMA model to compare their AICs, to find the best model.</li>
</ol>

<p>Am I going to the right way? Please correct me if I am wrong. </p>
"
"0.144588807813564","0.140719508946058","154398","<p>What are the applications of AR and MA model? To put my question exactly, when to use AR model and when to use MA model(for example, like when it is seasonal or when there is a trend, etc). In other words, which model to chose if it is seasonal, if it has  trend, if both seasonal and trend, and if no trend and not seasonal? It will be good if someone explains that with an example.</p>

<p><code>auto.arima()</code> can be used to find the parameters of AR and MA easily in R. But I happen to find lower AIC values when using different parameters other than parameters found by <code>auto.arima()</code>. Which parameters is better to use? If parameters with lower AIC value is only better, then how to find those parameters without trying manually all possible parameters? </p>
"
"0.194782903263593","0.23019259804901","154641","<p>This question is similar to the following <a href=""http://stats.stackexchange.com/questions/32634/difference-time-series-before-arima-or-within-arima"">question</a> in the sense I am currently doing the differencing and mean removal of the time series outside the <code>Arima</code> function in R. And I do not know how to do these steps within <code>Arima</code> function in R. The reason is that I am trying to perform the following procedure (data <code>dowj_ts</code> can be found at the bottom): </p>

<pre><code>dowj_ts_d1 &lt;- diff(dowj_ts) # differencing at lag 1 (1-B)
drift &lt;- mean(diff(dowj_ts))
dowj_ts_d1_demeaned &lt;- dowj_ts_d1 - mean(dowj_ts_d1) # mean removal
# Maximum Likelihood AR(1) for the mean-corrected differences X_t
fit &lt;- Arima(dowj_ts_d1_demeaned, order=c(1,0,0),include.mean=F, transform.pars = T)
</code></pre>

<p>Note that the <code>drift</code> is actually <code>0.1336364</code>. And <code>summary(fit)</code> gives the table below:</p>

<pre><code>Series: dowj_ts_d1_demeaned 
ARIMA(1,0,0) with zero mean     

Coefficients:
         ar1
      0.4471
s.e.  0.1051

sigma^2 estimated as 0.1455:  log likelihood=-35.16
AIC=74.32   AICc=74.48   BIC=79.01

Training set error measures:
                       ME     RMSE       MAE       MPE     MAPE      MASE
Training set -0.004721362 0.381457 0.2982851 -9.337089 209.6878 0.8477813
                    ACF1
Training set -0.04852626
</code></pre>

<p>Ultimately, I want to predict 2-step ahead forecast of <strong>the original series</strong>, and this starts to become ugly: </p>

<pre><code> tail(c(dowj_ts[1], dowj_ts[1] + cumsum(c(dowj_ts_d1_demeaned,forecast.Arima(fit,h=2)$mean) + drift)),2)
</code></pre>

<p>And currently these are all done outside the <code>Arima</code> function from the <code>forecast</code> package. I know I can do differencing within Arima like this: </p>

<pre><code> Arima(dowj_ts, order=c(1,1,0),include.drift=T,transform.pars = F)
</code></pre>

<p>This gives:</p>

<pre><code>Series: dowj_ts 
ARIMA(1,1,0) with drift         

Coefficients:
         ar1   drift
      0.4478  0.1204
s.e.  0.1059  0.0786

sigma^2 estimated as 0.1474:  log likelihood=-34.69
AIC=75.38   AICc=75.71   BIC=82.41
</code></pre>

<p>But the drift term computed by R is different from the <code>drift = 0.1336364</code> that I computed manually.</p>

<p>So <strong>my question is: how can I differenced the series and then remove the mean of the differenced series within the Arima function ?</strong></p>

<p><strong>Second question:</strong> Why is the drift term estimated by <code>Arima</code> different from the drift term I computed ? In fact, what does the <strong>mathematical model</strong> look like when <code>include.drift = T</code> ? This really confuses me. </p>

<p>Data can be found below: </p>

<pre><code>structure(c(110.94, 110.69, 110.43, 110.56, 110.75, 110.84, 110.46, 
110.56, 110.46, 110.05, 109.6, 109.31, 109.31, 109.25, 109.02, 
108.54, 108.77, 109.02, 109.44, 109.38, 109.53, 109.89, 110.56, 
110.56, 110.72, 111.23, 111.48, 111.58, 111.9, 112.19, 112.06, 
111.96, 111.68, 111.36, 111.42, 112, 112.22, 112.7, 113.15, 114.36, 
114.65, 115.06, 115.86, 116.4, 116.44, 116.88, 118.07, 118.51, 
119.28, 119.79, 119.7, 119.28, 119.66, 120.14, 120.97, 121.13, 
121.55, 121.96, 122.26, 123.79, 124.11, 124.14, 123.37, 123.02, 
122.86, 123.02, 123.11, 123.05, 123.05, 122.83, 123.18, 122.67, 
122.73, 122.86, 122.67, 122.09, 122, 121.23), .Tsp = c(1, 78, 
1), class = ""ts"")
</code></pre>
"
"0.142381215461566","0.190535115826549","156449","<p>I am working with workersâ€™ remittance quarterly data for Bangladesh. Here I am doing time series forecasting using R. I am applying auto.arima model and exponential smoothing model. I want to compare between them to check which best fits the data and gives better forecast.</p>

<p>Here is the output:</p>

<p>fit1 &lt;- auto.arima(lremit, d=1, D=NA, stationary=FALSE,
+                    seasonal=TRUE,ic=""aic"",trace=TRUE,
+                    allowdrift=FALSE,allowmean=TRUE)</p>

<p>Best model: ARIMA(2,1,3)(0,1,1)[4]                    </p>

<blockquote>
  <p>summary(fit1)
  Series: lremit 
  ARIMA(2,1,3)(0,1,1)[4]<br>
  Coefficients:
            ar1      ar2     ma1     ma2      ma3     sma1
        -0.5024  -0.1691  0.3940  0.1516  -0.1899  -0.9605
  s.e.   0.6321   0.4860  0.6298  0.4465   0.1060   0.1098
  sigma^2 estimated as 0.007314:  log likelihood=135.59
  AIC=-257.18   AICc=-256.3   BIC=-236.84</p>
</blockquote>

<p>Training set error measures:
                       ME       RMSE        MAE         MPE     MAPE      MASE         ACF1
Training set -0.003608938 0.08398593 0.06532171 -0.09985958 1.110818 0.4381367 -0.004851439</p>

<blockquote>
  <p>fit1 &lt;- Arima(lremit,order=c (2,1,3),seasonal=c (0,1,1))
  h11=plot(forecast(fit1,h=20))
  h11
  $mean
           Qtr1     Qtr2     Qtr3     Qtr4
  2015 8.256047 8.283843 8.300686 8.341204
  2016 8.372717 8.406483 8.413318 8.457855
  2017 8.489041 8.522291 8.529440 8.573906
  2018 8.605075 8.638346 8.645488 8.689954
  2019 8.721124 8.754394 8.761536 8.806002</p>
  
  <h2>ETS</h2>
  
  <p>fit2&lt;-ets(lremit)
  summary(fit2)</p>
</blockquote>

<p>ETS(A,A,N) </p>

<p>Call:
 ets(y = lremit) </p>

<p>Smoothing parameters:
    alpha = 0.8594 
    beta  = 1e-04 </p>

<p>Initial states:
    l = 4.2135 </p>

<p>sigma:  0.0858</p>

<pre><code> AIC     AICc      BIC 
</code></pre>

<p>12.20515 12.50145 23.97172 </p>

<p>Training set error measures:
                        ME       RMSE        MAE         MPE     MAPE      MASE         ACF1
Training set -7.229862e-05 0.08579429 0.06800397 -0.01942594 1.169213 0.4561276 -0.002900248</p>

<p>It is my first work using R and I am facing problems regarding this. They are:</p>

<ol>
<li>auto.arima output shows seasonality in every 4th quarter, but exponential smoothing shows non seasonality, what is the interpretation of this contradictory result?</li>
<li>How can I compare between them, what is the proper measure?</li>
<li>What is the command for in sample forecast in auto.arima? If I write h=0, then it shows error</li>
<li>Where can I find elaborate interpretation of auto.arima and exponential smoothing output and about the comparison?</li>
<li>Which error measure should I prefer like ME, MAPE, RMSE etc. as they are almost same for the two models?</li>
<li>In case of auto.arima it shows same output for allowing drift or no drift</li>
</ol>
"
"0.18008604512981","0.207133508706445","159769","<p>I was checking for seasonality and other dependencies and this is the curve I get . There's no apparent seasonality....but what exctly does the falling slope mean? Any help would be appreciated. Thanks!</p>

<p>Actual Time Series:
<img src=""http://i.stack.imgur.com/JtJiu.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/xTvlQ.png"" alt=""Autocorelation curve for a time series with data over last 9 years""></p>

<p>EDIT: Adding other plots:
<img src=""http://i.stack.imgur.com/hhStk.png"" alt=""PACF of original time series""></p>

<p>Here is the ACF and PACF of once-differenced
<img src=""http://i.stack.imgur.com/pGGJM.png"" alt=""ACF,once differentiated"">
 <img src=""http://i.stack.imgur.com/DyHFW.png"" alt=""PACF, differentiated once""></p>

<p>Some results from running the ARIMA model:</p>

<pre><code>Call:
**arima(x = meanT2$MeanUnit, order = c(1, 0, 1))**

Coefficients:
         ar1      ma1  intercept
      0.9840  -0.3525  1002.8215
s.e.  0.0169   0.0927   184.1689

Call:
**arima(x = meanT2$MeanUnit, order = c(1, 0, 0))**

Coefficients:
         ar1  intercept
      0.9456  1031.3660
s.e.  0.0319   110.2209

sigma^2 estimated as 5312:  log likelihood = -651.81,  aic = 1309.62

Call:
**arima(x = meanT2$MeanUnit, order = c(2, 0, 1))**

Coefficients:
         ar1      ar2      ma1  intercept
      1.0455  -0.0599  -0.4046   1002.210
s.e.  0.2324   0.2258   0.2091    185.629

Call:
**arima(x = meanT2$MeanUnit, order = c(2, 0, 0))**

Coefficients:
         ar1     ar2  intercept
      0.6907  0.2753  1016.3942
s.e.  0.0898  0.0912   151.4378

sigma^2 estimated as 4908:  log likelihood = -647.45,  aic = 1302.91
</code></pre>

<p>Edited to add: The residuals plot of the ARIMA(1,0,1) case. For ARIMA(1,0,0) its almost the same 
<img src=""http://i.stack.imgur.com/8sK5s.png"" alt=""enter image description here""></p>

<p>ACF plot of ARIMA(1,0,1)<img src=""http://i.stack.imgur.com/P63P8.png"" alt=""enter image description here""></p>
"
"0.156173761888606","0.130280949812384","160612","<p>I have manually discovered that the best model for my time series is the next one (AIC = 244.9):</p>

<p><img src=""http://i.stack.imgur.com/EaH3R.jpg"" alt=""enter image description here""></p>

<p>But auto.arima function tells me that the best model is (0,1,0) with AIC = 247.93:</p>

<p><img src=""http://i.stack.imgur.com/oKgyN.jpg"" alt=""enter image description here""></p>

<p>If I look at the trace I see: </p>

<p><img src=""http://i.stack.imgur.com/C03Vi.jpg"" alt=""enter image description here""></p>

<p>Why can't auto.arima calculate AIC values for the majority of the models and returns Inf? </p>
"
"0.177084400830287","0.215431871108035","160675","<p>I'm interested in determining both the slope regression coefficient and plotting regression lines for autocorrelated time-series datasets of rainfall.  Specifically, I'd like to identify the best approach in R that would allow me to visualize the regression line on the original (undifferenced) time-series plot when I need to difference the data to remove stationarity (i.e, when d>0 in an arima model).</p>

<p>As a start, I'm exploring the use of auto.arima (from the forecast package) and sarima (from the astsa package) which can output regression coefficients in the presence of autocorrelation.</p>

<p>For example:</p>

<ol>
<li><p>Using auto.arima. The 'drift' of -5.009 represents the slope (see <a href=""http://robjhyndman.com/hyndsight/arima-trends/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/arima-trends/</a>) </p>

<pre><code>&gt; min.ar &lt;- auto.arima(dec.yr.mmin$min_prcp)
&gt; summary(min.ar)

Series: dec.yr.mmin$min_prcp 
ARIMA(1,1,0) with drift         

Coefficients:
          ar1    drift
      -0.5138  -5.0089
s.e.   0.2465   5.7986

sigma^2 estimated as 949:  log likelihood=-57.82
AIC=121.64   AICc=124.31   BIC=123.34

Training set error measures:
                     ME     RMSE      MAE       MPE     MAPE      MASE       ACF1
Training set -0.9479987 28.52129 23.83494 -2.484233 16.12547 0.7957998 -0.2617352
</code></pre></li>
<li><p>Using sarima to fit the model and output the slope</p>

<pre><code>  &gt; fit.min &lt;- sarima(dec.yr.mmin$min_prcp, 1,1,0,                       reg=dec.yr.mmin$decade)
initial  value 3.542448 
  iter   2 value 3.488927
  iter   3 value 3.386967
  iter   4 value 3.383464
  iter   5 value 3.382408
  iter   6 value 3.382051
  iter   7 value 3.382024
  iter   8 value 3.382020
  iter   9 value 3.381925
  iter   9 value 3.381925
  iter   9 value 3.381925
  final  value 3.381925 
  converged
  initial  value 3.400729 
  iter   2 value 3.399523
  iter   3 value 3.399490
  iter   4 value 3.399488
  iter   4 value 3.399488
  iter   4 value 3.399488
  final  value 3.399488 
  converged
</code></pre></li>
</ol>

<p>3.Output coefficients</p>

<pre><code>      &gt; fit.min$fit$coef
             ar1       xreg 
      -0.5137696 -0.5009045 
</code></pre>

<ol start=""4"">
<li><p>For comparison, this is the output from an OLS regression which may give an incorrect slope due to autocorrelation.</p>

<pre><code>  &gt; m3 &lt;- lm(dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)
  &gt; summary(m3)

  Call:
  lm(formula = dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -45.504  -8.048   1.892  13.650  38.357 

  Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
  (Intercept)        1014.1570   319.9461   3.170  0.00807 **
  dec.yr.mmin$decade   -0.4222     0.1580  -2.672  0.02032 * 
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

  Residual standard error: 23.83 on 12 degrees of freedom
  Multiple R-squared:  0.3731,  Adjusted R-squared:  0.3209 
  F-statistic: 7.142 on 1 and 12 DF,  p-value: 0.02032
</code></pre>

<p>`</p></li>
</ol>

<p>The output from sarima identifies the slope coefficient and also intercept when d=0. When differencing is required (e.g., ARIMA (1,1,0) as output above), sarima only outputs the slope. </p>

<p>My question: when d = 1 or more, what approaches in R would allow me to add/visualize the regression line onto the original undifferenced time-series plot. Is it possible to derive the fitted values of the regression line, or derive intercept values from sarima/auto.arima or other package?</p>

<p>Many thanks in advance for your suggestions.</p>
"
"0.0885422004151433","0.114896997924285","161182","<p>I am trying to fit and forecast log returns of a price data using ARIMA model in R. For reproducibility, data is provided <a href=""https://docs.google.com/spreadsheets/d/1U619rL30yGcNRWxoiiIsfy-C-VOH2W2tnEivuIUOVq4/edit?usp=sharing"" rel=""nofollow"">here</a>. </p>

<p><strong>Steps Followed, Code and Results obtained</strong> </p>

<ol>
<li><p>Check for outliers (Package: <code>forecast</code>) - No outliers detected. </p>

<pre><code>outliers &lt;- tsoutliers(log.rtn)
</code></pre></li>
<li><p>Stationarity Check using ADF test (Package: fUnitRoots) - Series found to be stationary</p>

<pre><code>stationary &lt;- adfTest(log.rtn, lags = m1$order, type = c(""c""))
</code></pre></li>
<li><p>Determination of p,d,q using ACF and PACF (Package: astsa) - Based on my understanding, p = 2, d = 0, q = 2</p>

<pre><code>acf2(log.rtn, lags = 20)
</code></pre></li>
<li><p>Fitting ARIMA (Package: forecast)</p>

<pre><code>fit &lt;- auto.arima(log.rtn, stepwise=FALSE, trace=TRUE, approximation=FALSE)
</code></pre>

<p>Model obtained : ARIMA(2,0,1)</p>

<pre><code>Series: log.rtn 

  ARIMA(2,0,1) with zero mean     

Coefficients:
          ar1     ar2     ma1
      -0.5705  0.1557  0.6025
s.e.   0.1549  0.0532  0.1519

sigma^2 estimated as 0.001086:  log likelihood=775.57
AIC=-1543.14   AICc=-1543.04   BIC=-1527.29
</code></pre></li>
<li><p>Prediction (Package:forecast)</p>

<pre><code>fcast &lt;- forecast(fit, n.ahead=5)
plot(fcast)

    Point Forecast       Lo 80      Hi 80       Lo 95      Hi 95
390   1.416920e-03 -0.04080849 0.04364233 -0.06316127 0.06599511
391   8.228924e-04 -0.04142414 0.04306993 -0.06378837 0.06543416
392  -2.488236e-04 -0.04289257 0.04239493 -0.06546681 0.06496917
393   2.700663e-04 -0.04248622 0.04302635 -0.06512003 0.06566016
394  -1.928045e-04 -0.04303250 0.04264690 -0.06571047 0.06532486
395   1.520366e-04 -0.04273465 0.04303872 -0.06543749 0.06574156
396  -1.167506e-04 -0.04303183 0.04279833 -0.06574971 0.06551621
397   9.027370e-05 -0.04284167 0.04302221 -0.06556846 0.06574901
398  -6.967566e-05 -0.04301167 0.04287232 -0.06574379 0.06560444
399   5.380284e-05 -0.04289419 0.04300179 -0.06562948 0.06573708
</code></pre></li>
</ol>

<p>I am quite confused why the model is predicting so badly.</p>
"
"0.247918161162401","0.275752795018284","162204","<p>I've got two time series (parameters of a model for males and females) and aim to identify an appropriate ARIMA model in order to make forecasts. My time series looks like:</p>

<p><img src=""http://i.stack.imgur.com/t8JkR.jpg"" alt=""enter image description here""></p>

<p>The plot and the ACF show non-stationary (the spikes of the ACF cut off very slowly). Thus, I use differencing and obtain:</p>

<p><img src=""http://i.stack.imgur.com/Zy1kC.jpg"" alt=""enter image description here""></p>

<p>This plot indicate that the series might now be stationary and the application of the kpss test and the adf test support this hypothesis.</p>

<p>Starting with the Male series, we make the following observations:</p>

<ul>
<li>The empirical autocorrelations at Lags 1,4,5,26 and 27 are significant different from zero.</li>
<li>The ACF cuts off (?), but I'm concerned about the relatively big spikes at lag 26 and 27.</li>
<li>Only the empirical partial autocorrelations at Lags 1 and 2 are significant different from zero.</li>
</ul>

<p>On ground of these observations alone, if I had to choose a pure AR or MA model for the differenced time series, I would tend to choose either an AR(2) model by arguing that:</p>

<ul>
<li>We have no significant partial autocorrelations for lag greater than 2 </li>
<li>The ACF cuts off except for the region around lag 27. (Are these few outliers alone an indicator, that a mixed ARMA model would be appropriate?)</li>
</ul>

<p>or an MA(1) model by arguing that:</p>

<ul>
<li>The PACF clearly cuts off</li>
<li>We have for lags greater 1 only 4 spikes exceeding the critical value in magnitude. This is ""only"" one more than the 3 spikes (95% out of 60) which would be allowed to lie outside the dotted area.</li>
</ul>

<p>There are no characteristica of an ARIMA(1,1,1) model and choosing orders of p and q of an ARIMA model on grounds of ACF and PACF for p+q > 2 gets difficult.</p>

<p>Using auto.arima() with the AIC criterion (Should I use AIC or AICC?) gives:</p>

<ol>
<li>ARIMA(2,1,1) with Drift; AIC=280.2783</li>
<li>ARIMA(0,1,1) with Drift; AIC=280.2784</li>
<li>ARIMA(2,1,0) with Drift; AIC=281.437</li>
</ol>

<p>All three considered models show white noise residuals:</p>

<p><img src=""http://i.stack.imgur.com/WM0By.jpg"" alt=""enter image description here""></p>

<p>My summed up questions are:</p>

<ol>
<li>Can you still describe the ACF of the time series as cutting of despite the spikes around lag 26?</li>
<li>Are these outliers an indicator that a mixed ARMA model might be more appropriate?</li>
<li>Which Information Criterion should I choose? AIC? AICC?</li>
<li>The residuals of the three models with the highest AIC do all show white noise behavior, but the difference in the AIC is only very small. Should I use the one with the fewest parameters, i.e. an ARIMA(0,1,1)?</li>
<li>Is my argumentation in general plausible?</li>
<li>Are their further possibilities to determine which model might be better or should I for example, the two with the highest AIC and perform backtests to test the plausibility of forecasts?</li>
</ol>

<p>Thanks!</p>

<p><strong>EDIT:</strong> Here is my data:</p>

<pre><code>-5.9112948202 -5.3429985122 -4.7382340534 -3.1129015623 -3.0350910288 -2.3218904871 -1.7926701792 -1.1417358384 -0.6665592055 -0.2907748318 0.2899480865 0.4637205370  0.5826312749  0.3869227286  0.6268379174  0.7439125292 0.7641139207  0.7613140511  3.0143912244 -0.7339255839  2.0109976796 0.8282394650 -2.5668367983  5.9826406394  1.9569198553  2.3860893476 2.0883339390  1.9761894580  2.2601997245  2.2464027995  2.5131158613 3.4564765529  4.2307335557  4.0298688374  3.7626317439  3.1026407174 2.1690168737  1.5617407254  2.6790460788  0.4652054768 -0.0501046517 -1.0157683791 -0.5113698054 -0.0180401353 -1.9471272198 -0.2550365250 -1.1269988523  0.5152074134  0.2362626753 -2.9978337017  1.4924705528 -1.4907767844 -0.5492041416 -0.7313021018 -0.6531515868 -0.4094159299 -0.5525401626 -0.0611454515 -0.5256272882 -1.1235247363 -1.7299848758 -1.3807763611 -1.6999054476 -4.3155973110 -4.7843298990
</code></pre>
"
"0.0834783871129682","0.0812444463702388","163774","<p>I have a relatively simple problem, but yet taking some time to solve it. I am using the <code>arimax()</code> function from the <code>TSA</code> package. <em>(Note: not <code>arima()</code> from the <code>stats</code> package.)</em> This is the model: </p>

<pre><code>out &lt;- arimax(sub_s_t_series, order=c(2,0,1), xreg=sub_r_t_series, method=c(""ML""))
</code></pre>

<p>and these are my coefficients:</p>

<pre><code>Call
arimax(x = sub_s_t_series, order = c(2, 0, 1), xreg = sub_r_t_series, method = c(""ML""))

Coefficients:
         ar1      ar2      ma1  intercept     xreg
      1.4825  -0.6613  -0.8516  52745.107  -1.0132
s.e.  0.0295   0.0294   0.0064     40.828   0.0012

sigma^2 estimated as 0.08929:  log likelihood = -105.98,  aic = 221.97
</code></pre>

<p>All I am trying to do is to interpret the results. According to my understanding and the help given in the TSA package, the above ARIMAX(2,0,1) model is represented as follows:
$$
{\rm sub\_s\_t\_series\_hat[k]} = {\rm intercept} + xreg\times {\rm sub\_r\_t\_series[k]} + 
\frac{a_{t[k]}+ma1*a_{t[k-1]}}{a_{t[k]}-ar1*a_{t[k-1]}-ar2*a_{t[k-2]}}  \tag{1} 
$$
where $a_t$ are the residuals. When I use e_t = fitted(out)-sub_s_t_series_hat to measure the error / residuals myself, e_t matches exactly to the values obtained by <code>out[[""residuals""]]</code>.</p>

<p>But when I use (1) as follows: e_t_hat = sub_s_t_series_hat - sub_s_t_series, 
e_t_hat does not match with <code>out[[""residuals""]]</code>, in fact the results deviate by a magnitude of almost 4.</p>

<p><strong>My questions is:</strong> did an ARIMAX(2,0,1) fit would result in (1) or am I missing something?</p>
"
"0.0885422004151433","0.114896997924285","163878","<p>I want to estimate an ARIMA model on my timeseries, then represent it in state space format, mainly because it will be more responsive to change in pattern.
I used <code>auto.arima</code> from <code>forecast</code> package to find the best ARIMA, and then, if it didn't need differencing, tried to estimate the same ARMA using <code>dlm</code>package. </p>

<pre><code>fit.arima &lt;- auto.arima(training_data_list, seasonal = FALSE)
Series: training_data_list 
ARIMA(1,0,1) with zero mean     

Coefficients:
         ar1      ma1
      0.8247  -0.4913
s.e.  0.0395   0.0618

sigma^2 estimated as 489.7:  log likelihood=-3161.25
AIC=6328.5   AICc=6328.53   BIC=6342.15
</code></pre>

<p>Now the same thing using <code>dlm</code> package:</p>

<pre><code>test.model.arma &lt;- function(u){
  arma  &lt;- dlmModARMA(ar = ARtransPars(u[1]), ma = u[2], sigma2 = exp(u[3]))   # ma = c(u[4]),
  return(arma)
}
init &lt;-c(rep(0,3))
outMLE2 &lt;- dlmMLE(training_data_list, init, test.model.arma,method = ""Nelder-Mead"")
# outMLE2 &lt;- dlmMLE(training_data_list, init, test.model.arma)
if(outMLE2$convergence != 0) {print(""MLE did NOT converge!"")}

dlmModel4 &lt;- test.model.arma(outMLE2$par)
</code></pre>

<p>Now the loglikelihood from the later one is $2523.158$, while the <code>auto.arima</code> gives $3161.25$. the coefficients for $ar$ and $ma$ are very close, so I think it might be unlikely to be the reason.</p>

<pre><code>dput(training_data_list)
structure(c(0.92, 3.76, 2.64, 2.72, -4.48, 4.68, 6.2, 4.16, 22.32, 
28.96, 5.72, 3.44, 29.56, 37.28, 38.16, 31.28, 32.04, 21.32, 
2.88, 7.08000000000001, 52.32, 9.80000000000001, 8.56, 2.24000000000001, 
29.8, 49.2, 23.88, 42.32, -0.08, 3.76, -0.359999999999999, 2.72, 
8.52, 26.68, 7.2, -18.84, -13.68, -3.03999999999999, 10.72, -14.56, 
-16.44, 44.28, -17.84, -8.72000000000003, 3.04000000000002, 30.32, 
-21.12, -13.92, -3.68000000000001, -17.2, -16.44, -7.75999999999999, 
39.8, -1.80000000000001, 25.88, 9.31999999999999, -0.08, 1.76, 
2.64, -5.28, -4.48, 3.68, 1.2, -10.84, -21.68, -1.03999999999999, 
-6.28, 22.44, 62.56, 27.28, 1.16000000000003, -8.72000000000003, 
23.04, -11.68, -40.12, -33.92, -43.68, -26.2, -58.44, 5.24000000000001, 
-33.2, -18.8, -48.12, -57.68, -0.08, 3.76, -2.36, -7.28, -1.48, 
-1.32, 2.2, -6.84, 14.32, 23.96, -6.28, -11.56, -13.44, 14.28, 
-38.84, -31.72, 4.04000000000002, 4.31999999999999, 24.88, -3.91999999999999, 
-19.68, 30.8, 13.56, 4.24000000000001, 22.8, 36.2, 44.88, -17.68, 
-0.08, -1.24, -4.36, -1.28, -5.48, 7.68, 1.2, 14.16, -1.68000000000001, 
9.96000000000001, -3.28, 8.44, 59.56, 33.28, 23.16, 72.28, 48.04, 
36.32, 41.88, 7.08000000000001, 13.32, 20.8, 19.56, -0.759999999999991, 
-4.19999999999999, 31.2, 40.88, -10.68, -0.08, 6.76, 2.64, 2.72, 
7.52, 1.68, 1.2, -8.84, -7.68000000000001, 11.96, 31.72, -5.56, 
38.56, 27.28, -15.84, 60.28, 21.04, -22.68, 33.88, 55.08, 41.32, 
24.8, 6.56, 20.24, 60.8, 36.2, -9.12, 42.32, -0.08, 2.76, 2.64, 
1.72, -1.48, 2.68, 8.2, -18.84, -24.68, -4.03999999999999, 2.72, 
-31.56, -9.44, -11.72, -48.84, -5.72000000000003, 2.04000000000002, 
-19.68, -2.12, 27.08, 7.31999999999999, -22.2, 42.56, 11.24, 
9.80000000000001, 17.2, 25.88, 55.32, -0.08, -3.24, -4.36, 1.72, 
-6.48, 0.68, -0.799999999999997, 6.16, -13.68, -14.04, -2.28, 
12.44, -24.44, -15.72, 13.16, 1.27999999999997, -16.96, -40.68, 
-76.12, -50.92, -49.68, -58.2, -42.44, -36.76, -8.19999999999999, 
-43.8, -71.12, -62.68, -0.08, -3.24, -2.36, -1.28, 8.52, 4.68, 
-0.799999999999997, 18.16, 16.32, -18.04, 14.72, -18.56, 6.56, 
14.28, 53.16, 41.28, 35.04, -40.68, -12.12, -5.91999999999999, 
22.32, 11.8, -16.44, -6.75999999999999, -36.2, 3.19999999999999, 
-8.12, -12.68, -0.08, 3.76, 2.64, -3.28, -0.48, 13.68, 11.2, 
10.16, 9.31999999999999, -15.04, -6.28, -22.56, 14.56, -5.72000000000003, 
53.16, 36.28, 45.04, -12.68, -0.120000000000005, 21.08, -24.68, 
-10.2, -17.44, 3.24000000000001, 8.80000000000001, -11.8, -26.12, 
-27.68, -0.08, 2.76, 0.640000000000001, 2.72, -4.48, -2.32, -4.8, 
18.16, 7.31999999999999, 12.96, 45.72, 37.44, -22.44, 9.27999999999997, 
36.16, 51.28, 7.04000000000002, 54.32, 5.88, -1.91999999999999, 
25.32, 27.8, 15.56, 22.24, 18.8, -38.8, 31.88, 31.32, -0.08, 
6.76, -0.359999999999999, -0.279999999999999, 1.52, 1.68, -3.8, 
18.16, -8.68000000000001, 7.96000000000001, -7.28, 30.44, 12.56, 
51.28, 9.16000000000003, 16.28, -14.96, 50.32, -18.12, -32.92, 
10.32, -8.19999999999999, -5.44, 22.24, 23.8, 33.2, -12.12, 51.32, 
-0.08, -4.24, -2.36, -0.279999999999999, -2.48, -4.32, -4.8, 
0.159999999999997, -17.68, 7.96000000000001, 9.72, 9.44, 1.56, 
-15.72, 4.16000000000003, 6.27999999999997, -28.96, -38.68, -18.12, 
-36.92, -34.68, -25.2, -23.44, -43.76, -59.2, -37.8, -54.12, 
-61.68, -0.08, 0.76, 2.64, 9.72, -1.48, 2.68, 21.2, 13.16, -1.68000000000001, 
10.96, 20.72, 22.44, 23.56, 18.28, -3.83999999999997, -16.72, 
-19.96, 46.32, 47.88, 25.08, 0.319999999999993, 21.8, 14.56, 
32.24, 1.80000000000001, -5.80000000000001, -12.12, 1.31999999999999, 
-0.08, 5.76, -4.36, 4.72, -3.48, -5.32, 4.2, 4.16, -12.68, -14.04, 
-11.28, 0.439999999999998, -13.44, 25.28, 58.16, 29.28, 32.04, 
38.32, 14.88, 32.08, 39.32, 47.8, 16.56, 17.24, -25.2, 13.2, 
27.88, -6.68000000000001, -0.08, -6.24, -2.36, -6.28, -3.48, 
3.68, -1.8, -9.84, 11.32, 4.96000000000001, -26.28, -32.56, -38.44, 
-49.72, -62.84, -40.72, -20.96, -13.68, -52.12, -24.92, -49.68, 
-25.2, -19.44, -82.76, -36.2, -51.8, -60.12, -65.68, 0.92, -3.24, 
-1.36, -0.279999999999999, -19.48, -34.32, -22.8, 1.16, 19.32, 
-2.03999999999999, 27.72, 4.44, 12.56, -53.72, -35.84, -60.72, 
24.04, -15.68, 27.88, 11.08, 12.32, 33.8, 5.56, 3.24000000000001, 
24.8, -21.8, 18.88, -15.68, -0.08, -3.24, -1.36, -0.279999999999999, 
8.52, -5.32, 7.2, -12.84, 9.31999999999999, -0.039999999999992, 
-19.28, 13.44, -4.44, -5.72000000000003, -75.84, -82.72, -39.96, 
-60.68, 17.88, 23.08, 11.32, 38.8, 2.56, -6.75999999999999, 26.8, 
36.2, 27.88, 48.32, -0.08, -2.24, 1.64, 2.72, 2.52, -5.32, 0.200000000000003, 
-10.84, -4.68000000000001, -16.04, -15.28, -9.56, -27.44, -32.72, 
-63.84, -32.72, -49.96, -37.68, 38.88, 27.08, 7.31999999999999, 
5.80000000000001, 5.56, 10.24, -13.2, -20.8, 39.88, 36.32, -0.08, 
-3.24, 1.64, -9.28, -3.48, 1.68, -13.8, -2.84, -12.68, -15.04, 
8.72, 16.44, 7.56, 1.27999999999997, -23.84, -4.72000000000003, 
21.04, -22.68, -47.12, -16.92, -29.68, -32.2, -25.44, -40.76, 
-72.2, -50.8, -54.12, -99.68, -0.08, -4.24, 0.640000000000001, 
11.72, 4.52, -10.32, 1.2, 8.16, 15.32, -5.03999999999999, -14.28, 
-12.56, -34.44, -1.72000000000003, -11.84, -56.72, -25.96, -5.68000000000001, 
10.88, -4.91999999999999, 4.31999999999999, -40.2, 46.56, 20.24, 
5.80000000000001, 21.2, -1.12, 38.32, -0.08, -2.24, 2.64, -1.28, 
4.52, -1.32, -3.8, 1.16, 13.32, -0.039999999999992, 3.72, 2.44, 
19.56, 4.27999999999997, -41.84, -21.72, -29.96, 15.32, 39.88, 
-22.92, 28.32, -11.2, 67.56, 23.24, 25.8, -9.80000000000001, 
54.88, 18.32, -0.08, -1.24, 2.64, -1.28, 10.52, -4.32, -7.8, 
2.16, 16.32, -9.03999999999999, -0.280000000000001, 8.44, -47.44, 
-35.72, 51.16, 12.28, 15.04, 27.32, 19.88, 38.08, 50.32, 35.8, 
-16.44, 25.24, -1.19999999999999, 15.2, 25.88, 21.32, -0.08, 
-2.24, 1.64, 0.720000000000001, -0.48, 2.68, 6.2, -5.84, 13.32, 
11.96, -33.28, -32.56, -39.44, -50.72, 42.16, -38.72, -12.96, 
49.32, -4.12, -21.92, -13.68, -11.2, 2.56, 25.24, 16.8, 34.2, 
25.88, 56.32, -0.08, -2.24, -1.36, -6.28, 6.52, -4.32, -13.8, 
-12.84, -26.68, -15.04, -30.28, -0.560000000000002, 2.56, -28.72, 
58.16, 52.28, -51.96, -30.68, -36.12, -1.91999999999999, -56.68, 
-22.2, -26.44, -21.76, -27.2, -12.8, -58.12, -13.68), .Tsp = c(1, 
25.9642857142857, 28), class = ""ts"")
</code></pre>
"
"0.105592735469691","0.128458748884677","163922","<p>I am trying to find any evidence of warming in monthly times series data of water temperature over a 21-year period that is serially correlated. Essentially I am looking to determine a global trend, like what can be done with OLS regression with data that is from independent observations. I am at a crossroads in trying to determine whether a seasonal ARIMA model or a linear mixed model with a trend component as detailed by Crawley on page 799 of ""The R Book"" (2nd ed.) is the most appropriate method to use. I therefore explored both techniques, but got very contradicting answers!</p>

<p>ARIMA modelling gave me a seasonal ARIMA of form (2,0,2)(0,0,1)[12], indicating that no differencing is required and therefore that the series is stationary with NO trend.</p>

<p>However, the linear mixed affects modelling, comparing two models with and without a trend component using ANOVA and maximum likelihood indicated a highly significant trend (R notation):</p>

<pre><code>model2: ave ~ sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

model1: ave ~ index + sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

ANOVA(model2,model1)

      Df  AIC     BIC     logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
model2 5 346.82   364.49    -168.41   336.82                           
model1 6 338.54   359.74    -163.27   326.54 10.28      1   0.001345 **
</code></pre>

<p>How can this be? What am I missing? Is it about assuming whether the trend is a parametric form (appropriate for linear mixed model) or whatever weird shape (appropriate for ARIMA)? If so how do I go about choosing which approach to adopt?</p>

<p>Thank you kindly for any advice.</p>
"
"0.156173761888606","0.151994441447782","165004","<p>I'm trying to find the best fit line for this data below but no matter what I try, the fit line seems to never be able to account for the lower values as shown below.</p>

<p>The x-values are just dates from 1/1/2014 to 7/20/2015 (566 values), but I don't know how to give you guys the y-values. I have it in my Environment but I don't know how to give you that without copying and pasting from the Console output.</p>

<p><a href=""http://i.stack.imgur.com/KA2LC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KA2LC.png"" alt=""Data with sinusodial fit""></a></p>

<p>This is the code that I'm using to get that fit line:</p>

<pre><code>wb.loglik=function(theta,y,x,null=NA)
{
  a=theta[1]
  b=theta[2]
  c=theta[3]
  d=theta[4]

  if(!is.na(null))
  {
    d=null
  }

  s2=theta[5]
  n=length(y)
  return((-n/2)*log(s2)-1/(2*s2)*sum((y-(a+b*cos(2*pi*((x-c)/d))))^2))
}
result=optim(par=c(mean(wbbcf),sd(wbbcf),1,365.25,var(wbbcf)/2),
fn=wb.loglik,x=Time,y=wbbcf,control=list(fnscale=-1))
theta=result$par
theta
value=result$value
value
</code></pre>

<p>This is the code to get the plot above:</p>

<pre><code>plot(date,wbbcf,xlim=c(as.Date(""2014-01-01""),as.Date(""2015-07-
    20"")),ylim=range(c(-3.2,0)),xlab=""Date (1/1/2014 to
    7/20/2015)"",ylab=""Total Net with Storage (bcf)"",main=""Total Burn with
    Model"")
par(new=T)
curve(-0.9740582-0.7857229*cos(2*pi*(x-5.9582996)/385.1581090),1,566,
    ylim=range(c(-3.2,0)),col=""blue"",xlab="""",ylab="""",xaxt='n',yaxt='n')
</code></pre>

<p>What else can I do to generate a better fit line for this data? Also, if there's a good way to predict future data, I would appreciate help with that as well.</p>

<p>Sorry in advance, if I'm not giving enough information. Please feel free to ask for any information you need and I will promptly edit the post.</p>

<p>EDIT: I have added the work I did with ARIMA below.</p>

<p>I inputted the following code and got the following results:</p>

<pre><code>forecast::auto.arima(wbbcf)
fit.arima = arima(wbbcf,order=c(0,1,2))
pred.arima = predict(fit.arima,n.ahead=500)
plot(wbbcf,xlim=c(1,800),ylim=range(c(-3.2,1)))
lines(pred.arima$pred,col=""red"")
lines(pred.arima$pred+1.96*pred.arima$se,col=""blue"",lty=3)
lines(pred.arima$pred-1.96*pred.arima$se,col=""green"",lty=3)
</code></pre>

<p><a href=""http://i.stack.imgur.com/W8Hqt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/W8Hqt.png"" alt=""ARIMA predictions""></a></p>

<p>Here's the <code>auto.arima()</code> output:</p>

<pre><code>Series: wbbcf 
ARIMA(0,1,2)                    

Coefficients:
          ma1      ma2
      -0.3023  -0.3188
s.e.   0.0397   0.0396

sigma^2 estimated as 0.05788:  log likelihood=3.02
AIC=-0.04   AICc=0.01   BIC=12.98
</code></pre>

<p>Is something wrong with what I'm doing with ARIMA?</p>
"
"0.0834783871129682","0.0812444463702388","167432","<p>I'm using <code>auto.arima</code> function to analyze my data. And here's what I get:</p>

<pre><code>Series: data 
ARIMA(2,1,2) with drift         

Coefficients:
         ar1      ar2      ma1     ma2   drift
      1.6679  -0.8005  -1.2424  0.3125  0.4225
s.e.  0.1007   0.1107   0.1396  0.1413  0.1895

sigma^2 estimated as 17.34:  log likelihood=-438.37
AIC=888.73   AICc=889.3   BIC=906.99
</code></pre>

<p>I'm trying to perform a heteroscedastic test to my residual model using bptest from <code>lmtest</code> package with this code:</p>

<pre><code>&gt; bp&lt;-bptest(lm(residuals(model)~1))
&gt; bp

    studentized Breusch-Pagan test

data:  lm(residuals(model) ~ 1)
BP = 1.231e-30, df = 0, p-value &lt; 2.2e-16
</code></pre>

<p>Am I doing <code>bptest</code> right? When I analyze another data with this code I always get the same <code>df</code> and <code>p-value</code>. </p>

<p><strong>edit:</strong>
here is the data :</p>

<pre><code>    World_Oil_Prices
[1-10] [11-20] [21-30] [31-40] [41-50] [51-60] [61-70] [71-80] [81-90] [91-100] [101-110] [111-120] [121-130] [131-140] [141-150] [151-156]
17.79   22.25   18.73   12.72   16.12   27.49   25.95   18.69   28.28   28.59   37.63   48.75   59.67   53.53   75.91   131.22
17.69   23.51   20.12   12.49   16.24   23.45   27.24   18.52   27.53   29.68   35.54   46.00   54.17   57.22   81.27   121.87
19.46   23.29   19.16   13.80   18.75   27.23   25.02   19.15   24.79   26.88   37.93   43.67   56.63   50.14   90.54   96.85
20.78   20.54   17.24   13.26   20.21   29.62   25.66   19.98   27.89   29.01   42.08   52.55   66.85   54.46   89.76   69.16
19.12   19.42   15.07   11.88   22.37   28.16   27.55   23.64   30.77   29.12   41.65   52.24   63.49   57.78   85.53   46.03
18.56   17.98   14.18   10.41   22.19   29.41   26.97   25.43   32.88   29.95   46.87   58.74   62.26   63.25   93.51   38.60
19.56   19.47   13.24   11.32   24.22   32.08   24.80   25.69   30.36   31.40   42.23   58.20   68.08   66.75   99.32   
20.19   18.02   13.39   10.75   25.01   31.40   25.81   24.49   25.49   31.32   39.09   53.32   66.45   68.29   111.03  
22.14   18.45   13.97   12.86   25.21   32.33   25.03   25.75   26.06   33.67   42.76   49.41   56.38   73.69   123.35  
23.43   18.79   12.48   15.73   27.15   25.28   20.73   26.78   27.91   33.71   44.35   51.66   53.58   67.10   128.33  
</code></pre>

<p>And this is the R code:</p>

<pre><code>library(forecast)
model&lt;-auto.arima(data)
model
library(lmtest)
bp&lt;-bptest(lm(residuals(model)~1))
bp
</code></pre>
"
"NaN","NaN","167434","<p>I am running an ARIMA model for my forecasts in R</p>

<p>My data set is 1 month's data.<br>
It has 2976 observations which has a frequency of 15 min. I recieve data every 15 min.<br>
There is little seasonality in the data.<br>
I am trying to fit an ARIMA model using <code>auto.arima</code> for forecast of 24 tie steps.</p>

<p>This error shows up: <code>AIC value approximated</code>.<br>
Can one ignore this? If not then what is the way around?</p>
"
"0.224384562873093","0.256917497769354","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.146087177447694","0.162488892740478","169299","<p>I use auto.arima function in R to fit a TS model to a annual data composed of electricity demand. The series is transformed w.r.t Box-Cox lambda due to the prevailing heteroscedasticity and then it is twiced differenced to eliminate the trend in the data. The first ACF/PACF plot (w/o transformation) suggest that an ARIMA model should be fitted to the model; whereas the second ACF/PACF (with transformation) plot suggests that an AR model should be fitted. However both of them depend on the same data. 
In both of the case, the auto.arima function selects the best model as ARIMA(1,2,1) which can be expected according to the first plot but not according to the second plot due to the one spike in PACF.     </p>

<p><a href=""http://i.stack.imgur.com/qoYJH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qoYJH.jpg"" alt=""untransformed""></a>
<a href=""http://i.stack.imgur.com/FOzNv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FOzNv.jpg"" alt=""transformed""></a></p>

<pre><code>dmnd=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,
</code></pre>

<p>114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,257.2)</p>

<pre><code>x=ts(dmnd,frequency=1)

sdx=diff(x,differences = 2)
x1&lt;-acf(sdx,length(sdx),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(sdx,length(sdx),ylab=""Sample PACF"",main ="""")

library(FitAR)

#transformation
fit=arima(x,order=c(0,2,0))
BoxCox(fit, interval = c(-1, 1), type = ""BoxCox"")

library(forecast)
tx=BoxCox(x, -0.049)

sdx=diff(tx,differences = 2)
x1&lt;-acf(sdx,length(sdx),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(sdx,length(sdx),ylab=""Sample PACF"",main ="""")


fit=auto.arima(x,d = 2,D = 0,start.p=0, start.q=0, max.p=5, max.q=5,stationary=FALSE,seasonal=FALSE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=TRUE,ic=""aicc"",lambda=-0.049)

par(mfrow=c(1,2)) 
x1&lt;-acf(fit$residuals,length(fit$residuals),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(fit$residuals,length(fit$residuals),ylab=""Sample PACF"",main ="""")

Box.test(fit$residuals, lag = length(fit$residuals)/5, type = c(""Ljung-Box""), fitdf = length(fit$ coef))

shapiro.test(fit$residuals)

library(TSA)
x.standard=rstandard.Arima(fit)
qqnorm(x.standard,main ="""")
qqline(x.standard)
</code></pre>

<p><strong><em>Results of ARIMA(3,1,0) Model</em></strong> </p>

<p><a href=""http://i.stack.imgur.com/MmgeP.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MmgeP.jpg"" alt=""Summary""></a></p>

<p><a href=""http://i.stack.imgur.com/ojFY1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ojFY1.jpg"" alt=""sample ACF/PACf""></a>
<a href=""http://i.stack.imgur.com/Gh94D.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Gh94D.jpg"" alt=""Normality""></a></p>

<pre><code>Shapiro-Wilk normality test

data:  fit$residuals
W = 0.8557, p-value = 5.153e-05

Box-Ljung test

data:  fit$residuals
X-squared = 14.2044, df = 6, p-value = 0.02743
</code></pre>

<p><strong><em>Diagnostics of Residuals ARIMA(1,1,1) transformed</em></strong>
<a href=""http://i.stack.imgur.com/lQGXi.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lQGXi.jpg"" alt=""ACF/PACF""></a>
<a href=""http://i.stack.imgur.com/twQf2.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/twQf2.jpg"" alt=""Normal""></a></p>

<p><strong><em>auto.arima result for the series without the observations ""32, 40, 41""</em></strong>
<a href=""http://i.stack.imgur.com/9XI2O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9XI2O.jpg"" alt=""without some observation""></a></p>
"
"0.21394228630372","0.269457344910234","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.118056267220191","0.114896997924285","169564","<p>The <code>arimax</code> function in the <code>TSA</code> package is to my knowledge the only <code>R</code> package that will fit a transfer function for intervention models. It lacks a <a href=""http://stats.stackexchange.com/questions/34106/forecasting-with-arimax-model-including-xtransf"">predict function</a> though which is sometimes needed.</p>

<p>Is the following a work-around for this issue, leveraging the excellent <code>forecast</code> package? Will the predictive intervals be correct? In my example, the std errors are ""close"" for the components.</p>

<ol>
<li>Use the forecast package arima function to determine the pre-intervention noise series and add any outlier adjustment.</li>
<li>Fit the same model in <code>arimax</code> but add the transfer function</li>
<li>Take the fitted values for the transfer function (coefficients from <code>arimax</code>) and add them as xreg in <code>arima</code>. </li>
<li>Forecast with <code>arima</code></li>
</ol>

<blockquote>
<pre><code>library(TSA)
library(forecast)
data(airmiles)
air.m1&lt;-arimax(log(airmiles),order=c(0,0,1),
              xtransf=data.frame(I911=1*(seq(airmiles)==69)),
              transfer=list(c(1,0))
              )
</code></pre>
  
  <p>air.m1</p>
</blockquote>

<p>Output:</p>

<pre><code>Coefficients:
  ma1  intercept  I911-AR1  I911-MA0
0.5197    17.5172    0.5521   -0.4937
s.e.  0.0798     0.0165    0.2273    0.1103

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.09   BIC=-155.02
</code></pre>

<p>This is the filter, extended out 5 more periods that the data</p>

<pre><code>tf&lt;-filter(1*(seq(1:(length(airmiles)+5))==69),filter=0.5521330,method='recursive',side=1)*(-0.4936508)
forecast.arima&lt;-Arima(log(airmiles),order=c(0,0,1),xreg=tf[1:(length(tf)-5)])
forecast.arima
</code></pre>

<p>Output:</p>

<pre><code>Coefficients:
         ma1  intercept  tf[1:(length(tf) - 5)]
      0.5197    17.5173                  1.0000
s.e.  0.0792     0.0159                  0.2183

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.28   BIC=-157.74
</code></pre>

<p>Then to Predict</p>

<pre><code>predict(forecast.arima,n.ahead = 5, newxreg=tf[114:length(tf)])
</code></pre>
"
"0.237583654806805","0.256917497769354","172226","<p>Let's assume an analytical model predicts an epidemic trend over time, i.e. number of infections over time. We also have a computer simulation results over time to verify the performance of the model. The goal is to prove the simulation results and predicted values of the analytical model (which are both a time series) are statistically close or similar. By similarity I mean the model predicts the values close to what simulation is providing.</p>

<p><strong>Background</strong>:
Researching around this topic, I came across the following posts:</p>

<ol>
<li><p><a href=""http://stackoverflow.com/questions/13835924/similarity-of-trends-in-time-series-analysis"">http://stackoverflow.com/questions/13835924/similarity-of-trends-in-time-series-analysis</a></p></li>
<li><p><a href=""http://stats.stackexchange.com/questions/19103/how-to-statistically-compare-two-time-series"">How to statistically compare two time series?</a></p></li>
</ol>

<p>Both discussions suggest three approaches, where I am interested in two of them basically:</p>

<p>(1). Use of ARIMA; 
 (2). Use of Granger test</p>

<p>For the first suggested solution, this is what has been written there in regards to ARIMA, in (1):</p>

<blockquote>
  <p>Run ARIMA on both data sets. (The basic idea here is to see if the same set of parameters (which make up the ARIMA model) can describe both your temp time series. If you run auto.arima() in forecast (R), then it will select the parameters p,d,q for your data, a great convenience.</p>
</blockquote>

<p>I ran auto.arima on the simulation values and then ran forecast, here are the results:</p>

<pre><code>ARIMA(2,0,0) with zero mean     

Coefficients:
         ar1      ar2
      1.4848  -0.5619
s.e.  0.1876   0.1873

sigma^2 estimated as 121434:  log likelihood=-110.64
AIC=227.27   AICc=229.46   BIC=229.4
</code></pre>

<p>I ran auto.arima on predicted model values and then forecast. This is the result of the predicted model:</p>

<pre><code>ARIMA(2,0,0) with non-zero mean 

Coefficients:
         ar1      ar2  intercept
      1.5170  -0.7996  1478.8843
s.e.  0.1329   0.1412   290.4144

sigma^2 estimated as 85627:  log likelihood=-108.11
AIC=224.21   AICc=228.21   BIC=227.05
</code></pre>

<p><strong>Question 1</strong> What are the values that need to be compared to prove that the two series are similar especially the trend over time?</p>

<p>Regarding the second suggested option, I have read about it and found that Granger test is usually used to see if the values of series <em>A</em> at time <em>t</em> can predict the values of Series <em>B</em> at time <em>t+1</em>. </p>

<p><strong>Question 2</strong> Basically, in my case I want to compare the values of time series A and B at the same time, how this one is relevant to my case then?</p>

<p><strong>Question 3</strong> Is there any available method can be used to prove that the trend of two time-series over time is similar?</p>

<p>FYI. I saw another method which is using Pearson Correlation Coefficient and I could follow the reasoning there. Moreover, verifying analytical models with simulations has been widely used in the literature. see:</p>

<ol>
<li><a href=""http://users.ece.gatech.edu/~jic/tnn05.pdf"" rel=""nofollow"">Spatial-Temporal Modeling of Malware Propagation in Networks Modeling</a></li>
<li><a href=""http://cs.ucf.edu/~czou/research/emailWorm-TDSC.pdf"" rel=""nofollow"">Modeling and Simulation Study of the Propagation and Defense of Internet Email Worm</a></li>
</ol>
"
"0.131990919337114","0.128458748884677","175833","<p>I am a forecasting professional and have recently started using R.</p>

<p>I'm currently trying to forecast this using this code:</p>

<pre><code>library(forecast)
library(tseries)
p=scan()
#scans 54 variables
p.ts=ts(p, frequency=12, start=c(2011, 01))
p.ts

       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2011 102.0 102.2 102.8 103.2 103.3 103.5 103.6 104.0 104.2 103.9 104.2 104.1
2012 104.5 104.8 104.9 105.3 105.5 105.5 105.4 105.1 105.6 105.8 106.3 106.4
2013 106.4 106.4 106.6 106.8 106.4 107.0 107.5 107.4 107.6 107.9 107.9 107.7
2014 107.8 108.1 108.2 108.9 108.7 109.0 109.1 109.4 109.1 109.9 109.8 109.9
2015 109.8 109.5 109.6 109.5 109.5 109.7 110.2 110.6

plot(p.ts)
</code></pre>

<p><a href=""http://i.stack.imgur.com/rIbtu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rIbtu.png"" alt=""Looks like it isn&#39;t a stationary process""></a></p>

<pre><code>plot(diff(p.ts))
</code></pre>

<p><a href=""http://i.stack.imgur.com/2Tv0R.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2Tv0R.png"" alt=""looks like stationary but with high variability""></a></p>

<p>Then I had a look at the ACF and PACF plots
<img src=""http://i.stack.imgur.com/35gUK.jpg"" alt=""enter image description here""></p>

<p>It looks like a MA(1) process too. But not an AR process at all. </p>

<p>Hence, I chose to model it as ARIMA(0,1,1) process</p>

<pre><code>a=arima(p.ts, order=c(0,1,1))
summary(a)

Call:
arima(x = p.ts, order = c(0, 1, 1))

Coefficients:
         ma1
      0.0757
s.e.  0.1119

sigma^2 estimated as 0.09556:  log likelihood = -13.47,  aic = 30.95

Training set error measures:
                    ME      RMSE       MAE     MPE      MAPE      MASE
Training set 0.1450371 0.3066561 0.2472274 0.13619 0.2314932 0.9853266
                   ACF1
Training set -0.2479716
</code></pre>

<p>Then forecasted the numbers.</p>

<pre><code>f=forecast(a)

plot(f)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Km94S.jpg"" alt=""The forecast is just not true.""></p>

<p>Can you help me understand where I went wrong? And how can I correct this?</p>

<p>This is one of five cases that I forecast. In such a case I generally model it with HoltWinters and get a decent response (one that comes very close to the realized values too).</p>
"
"0.263069981584485","0.309369634833634","176129","<p>I've been working on some various time series forecasts and I've begun to notice a trend (pardon the pun) in my analyses. For about 5-7 datasets that I've worked with so far, it would be helpful to allow for multiple seasonal periods along with an option for holiday dummies. I've tried various methods and usually stick with <code>tbats</code> since <code>auto.arima()</code> with regressors has been giving me issues. By this point, it's probably obvious I'm working in R.</p>

<p>Before I get too far, let me give some sample data. Hopefully the following link works: <a href=""https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0"" rel=""nofollow"">https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0</a>.</p>

<p>This data yields the following time series plot:
<a href=""http://i.stack.imgur.com/FYS1x.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FYS1x.jpg"" alt=""Time Series Plot""></a>
The large dips are around Christmas and New Years, however there are also smaller dips around Thanksgiving. In the code below, I name this dataset <code>traindata</code>.</p>

<p>Now, <code>ets</code> and ""plain"" <code>auto.arima</code> don't look so hot in the long run since they are limited to only one seasonal period (I choose weekly). However for my test set that I held out they performed fairly well for the month's worth of data (with the exception of Labor Day weekend). This being said, forecasting out for a year would be ideal.</p>

<p>I next tried <code>tbats</code> with weekly and yearly seasonal periods. That results in the following forecast:
<a href=""http://i.stack.imgur.com/kcXmd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kcXmd.jpg"" alt=""TBATS Forecast""></a></p>

<p>Now this looks pretty good. From the naked eye it looks great at taking into account the weekly and yearly seasonal periods as well as Christmas and New Years effects (since they obviously fall on the same dates each year). It would be best if I could include the holidays (and the days around them) as dummy variables. Hence my attempts at <code>auto.arima</code> with <code>xreg</code> regressors.</p>

<p>For ARIMA with regressors, I've followed Dr. Hyndman's suggestions for the fourier function (given here: <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as well as his selection of the number of fourier terms (given here: <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a>)</p>

<p>My code is as follows:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep=""""),period,sep=""_"")
  return(X)
}

fcdaysout&lt;-365
m1&lt;-7
m2&lt;-30.4375
m3&lt;-365.25

hol&lt;-cbind(traindata$CPY_HOL, traindata$DAY_BEFORE_CPY_HOL, traindata$DAY_AFTER_CPY_HOL)
hol&lt;-as.matrix(hol)

n &lt;- nrow(traindata)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0

for(i in 1:m1)
{
    fake_xreg = cbind(fourier(1:n,i,m1), fourier(1:n,i,m3), hol)
    fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = fake_xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
    	if(fit$aicc &lt; bestfit$aicc)
    {
        bestfit &lt;- fit
        bestk &lt;- i
    }
    else
    {
    }
}

k &lt;- bestk
k
##k&lt;-3

xreg&lt;-cbind(fourier(1:n,k,m1), fourier(1:n,k,m3), hol)
xreg&lt;-as.matrix(xreg)

aacov_fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aic"", allowdrift=TRUE)
summary(aacov_fit)
</code></pre>

<p>Where my issues come in is inside the for loop to determine the <code>k</code>, the number of fourier terms, that minimizes AIC. In all of my attempts at ARIMA with regressors, it always produces an error when <code>k&gt;3</code> (or <code>i&gt;3</code> if we're talking about inside my loop). The error being <code>Error in solve.default(res$hessian * n.used, A) : system is computationally singular: reciprocal condition number = 1.39139e-34</code>. Simply setting <code>k=3</code> gives some decent results for my test set but for the next year it doesn't appear to adequately catch the steep drops around the end of the year and is much smoother than imagined as evidenced in this forecast:<a href=""http://i.stack.imgur.com/rj30h.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rj30h.jpg"" alt=""AutoArima with Covariates (k=3)""></a></p>

<p>I assume this general smoothness is due to the small number of fourier pairs. Is there an oversight in my code in that I'm just royally screwing up the procedure provided by Dr. Hyndman? Or is there a theoretical issue that I'm unknowingly running into by trying to find more than 3 pairs of fourier terms for the multiple seasons I'm attempting to account for? Is there a better way to include the multiple seasonalities and dummy variables?</p>

<p>Any help in getting these covariates into the arima model with an appropriate number of fourier terms would be appreciated. If not, I'd at least like to know whether or not what I'm attempting is possible in general with larger number of fourier pairs.</p>
"
"0.0590281336100955","0.0574484989621426","177117","<p>I use auto.arima function to model the below provided time series data. At the end of the analysis, the best model is given as ARIMA(1,2,1). The log- likelihood=93.69 is positive which is unusual. It is clear for me that the log-likehood is not as same as the probability. But how can this originate from the analysis? Does it depend on the data? Or just due to sign convention? </p>

<pre><code>  srs=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,
114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,257.2)

x=ts(srs,frequency=1)

fit=auto.arima(x,d = 2,D = 0,start.p=0, start.q=0, max.p=5, max.q=5,stationary=FALSE,seasonal=FALSE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=TRUE,ic=""aicc"",lambda=-0.049)

library(forecast)
tx=BoxCox(x, -0.049)
</code></pre>

<h1>result of transformation</h1>

<pre><code> tx=(2.042209,2.159383,2.278396,2.368590,2.443563,2.575967,2.723460,2.832405, 2.879977, 2.928574, 2.964082,3.021106, 3.083437, 3.121522, 3.221002, 3.295802, 3.385064, 3.470876, 3.533058, 3.601728, 3.664871, 3.716566,3.802248, 3.873901, 3.921001, 3.998007, 4.079888, 4.165227, 4.226782, 4.257449, 4.320209, 4.311558, 4.346176,4.395557, 4.442923, 4.497221, 4.561284, 4.626783, 4.659033, 4.643283, 4.705451, 4.774832, 4.814009, 4.826510,4.859228)
</code></pre>

<h1>result of density function given observation</h1>

<pre><code>dnorm(tx ,mean(tx), sd(tx),log=TRUE)
Time Series:
Start = 1 
End = 45 
Frequency = 1 
[1] -2.7168796 -2.4462962 -2.1917838 -2.0125389 -1.8724951 -1.6450191 -1.4214597 -1.2765217 -1.2186144
[10] -1.1628381 -1.1242424 -1.0660746 -1.0078703 -0.9750711 -0.8992890 -0.8517306 -0.8055615 -0.7720362
[19] -0.7543945 -0.7414067 -0.7354801 -0.7349191 -0.7424969 -0.7569828 -0.7705473 -0.7996330 -0.8399630
[28] -0.8923108 -0.9366056 -0.9607175 -1.0143004 -1.0065755 -1.0381350 -1.0861521 -1.1355218 -1.1961059
[37] -1.2730668 -1.3578864 -1.4019282 -1.3802323 -1.4679575 -1.5724597 -1.6345414 -1.6548185 -1.7089570
</code></pre>
"
"0.177976519326958","0.173213741660499","184880","<p>So I wanted to generate $500$ data points from an $ARMA(1,1)$ distribution in R, use the first $400$ as my training data and use the training data and the <code>predict</code> function to both see if I could obtain the correct model via AIC and then plot my prediction. I wanted to generate a whole bunch of different $ARMA$ models to use for the AIC which is why I basically built a grid, however I get a bunch of warnings and errors with my current method, despite the fact it seems to work. Note it is the estimation and not the actual prediction that raises the problems. Can I use a grid like so to produce AICs for various model types?</p>

<pre><code>####### ARMA(1,1) ############
# Causal stationary - root of the function phi(z) &gt; 1
theta1&lt;- 0.5
phi1&lt;- 0.6

arma11&lt;- arima.sim(n = 500,list(ar = c(phi1), ma = c(theta1)), sd = sqrt(1))
tr.data&lt;-arma11[1:400]
AIC&lt;-c()
for (i in 0:3){
  for (j in 0:3){
    aic.ij&lt;- arima(tr.data,order = c(i,0,j))$aic
    AIC&lt;- c(AIC,aic.ij) 
  }
}
AIC&lt;- matrix(AIC, ncol=4, byrow =T)
colnames(AIC)&lt;- c(""q=0"",""q=1"",""q=2"",""q=3"")
rownames(AIC)&lt;- c(""p=0"",""p=1"",""p=2"",""p=3"")
AIC
index&lt;-which(AIC==min(AIC),arr.ind=T)
index
prd&lt;-predict(arima(tr.data, order = c(index[1]-1,0,index[2]-1)),n.ahead =100)
vld.data&lt;-arma11[401:500]

# plot the training data with the next 100 predictions and the 95% confidence intervals
plot.ts(arma11, xlim=c(0,500),ylim=c(floor(min(arma11)),ceiling(max(arma11)))) 
lines(prd$pred, col='blue')
lines(prd$pred-(1.96*prd$se), col='red')
lines(prd$pred+(1.96*prd$se), col='red')
</code></pre>

<p>For this particular $ARMA$ I am error free but receive various size mismatch errors with other models under the same method, I receive this warning though. </p>

<pre><code>Warning message:
In arima(tr.data, order = c(i, 0, j)): possible convergence problem: optim gave code = 1Warning message:
In arima(tr.data, order = c(i, 0, j)): possible convergence problem: optim gave code = 1         q=0      q=1      q=2      q=3
p=0 1608.907 1296.137 1207.432 1187.231
p=1 1244.182 1178.820 1180.004 1181.993
p=2 1190.716 1180.072 1182.002 1183.776
p=3 1179.731 1181.688 1183.767 1181.305
    row col
p=1   2   2
</code></pre>
"
"0.096392538542376","0.140719508946058","184899","<p>I have written the following code to generate 500 data points from a $SARIMA$ model, use $400$ as training data and then predict the following $100$, while estimating the model with AIC. It appeared to me that I did everything correctly as I could see the AIC correctly choosing the model with certain phi values, etc. etc., however my plot outputs for the estimations is very very dense and incomprehensible and I'm not sure why. I checked the number of data points, the window size, etc. and am not sure what I have done wrong in my implementation. </p>

<pre><code>library(CombMSC)
library(forecast)


#####################
#generate data
sdat1&lt;- sarima.Sim(n=500, period=12, model = list(order = c(1,0,0), ar=0.5),list(order= c(1,0,0), ar = 0.5))
#procure training data
x.tr &lt;- window(sdat1, start=1, end=400)

#candidate models
op1 &lt;- arima(x.tr, order=c(0,0,0), list(order= c(0,1,0)))
op2 &lt;- arima(x.tr, order=c(1,0,0), list(order= c(1,0,1)))
op3 &lt;- arima(x.tr, order=c(1,0,0), list(order= c(0,2,0)))
op4 &lt;- arima(x.tr, order=c(0,1,0), list(order= c(0,0,0)))
op5 &lt;- arima(x.tr, order=c(1,0,1), list(order= c(0,0,0)))
op6 &lt;- arima(x.tr, order=c(1,0,0), list(order= c(1,0,0)))

models &lt;- c(op1,op2,op3,op4,op5,op6)
models.AIC &lt;- c(op1$aic,op2$aic,op3$aic,op4$aic,op5$aic,op6$aic)
mod.best = NULL
if (min(models.AIC) == op1$aic){
        mod.best=op1
    } else if (min(models.AIC) == op2$aic){
    mod.best=op2
} else if (min(models.AIC) == op3$aic){
        mod.best=op3
    } else if (min(models.AIC) == op4$aic){
    mod.best=op4
} else if (min(models.AIC) == op5$aic){
        mod.best=op5 
    } else if (min(models.AIC) == op6$aic){
    mod.best=op6
}
models.AIC
mod.best
modpred &lt;- predict(mod.best, n.ahead=100)
vld.data1&lt;-  sdat1[401:500]
plot.ts(sdat1, ylim=c(floor(min(sdat1)),ceiling(max(sdat1))))
plot.ts(sdat1, xlim=c(0,400),ylim=c(floor(min(sdat1)),ceiling(max(sdat1)))) 
lines(modpred$pred, col='blue')
    lines(modpred$pred-(1.96*modpred$se), col='red')
    lines(modpred$pred+(1.96*modpred$se), col='red')
</code></pre>

<p><a href=""http://i.stack.imgur.com/mUMEK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mUMEK.png"" alt=""enter image description here""></a></p>
"
"0.163714586481645","0.207133508706445","186164","<p>I am new to time series and am trying to fit some time series data.</p>

<p>I understand the general concept of ARIMA model. However, as I read more textbooks and articles from Rob Hyndman, I realized I could put some regressors using the <code>xreg</code> argument for the functions <code>auto.arima</code> or <code>arima</code> in R to get an ARMAX model. Therefore, I wonder if it is still necessary to include seasonality in <code>ts(...,frequency)</code> as everything can be specified as dummy variable within the <code>xreg</code> matrix and a more complicated seasonality structure (e.g. monthly seasonality) can be specified. </p>

<p>In addition, what would be a good way to check the accuracy of the forecast? I am fitting multiple time series data with a hierarchical structure. Using <code>auto.arima</code>, I am able to select the best model and validate the model by looking at the residuals (check whether they are white noise). However, is there a way to even improve on the model if the prediction is still far from the actual data?</p>

<p>To sum up, </p>

<ol>
<li>Is the <code>frequency</code> argument in <code>ts</code> function really necessary?  Can I just specify everything in the <code>xreg</code> matrix?</li>
<li>What would be a normal routine to improve on model after selecting the appropriate ARIMA model with the lowest AIC?</li>
</ol>

<p>Updates (Dec 17):</p>

<p>I am now able to fit an ARIMA model with SARIMA error by specifying <code>xreg</code> argument and <code>seasonal=F</code>. One issue that I have with that is, my <code>xreg</code> matrix is not invertible (I assumed) and its not due to the presence of intercept term. Thus <code>auto.arima()</code> only fit a <code>c(0,0,0)</code> model.</p>

<p>I then tried using <code>Arima()</code> to manually select model and it outputted the following error</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
non-finite value supplied by optim
</code></pre>

<p>I check the <code>xreg</code> matrix and it turns out column 48 (Day) and column 52 (2015) is causing the issue. Could you check if there's something wrong with my <a href=""https://drive.google.com/file/d/0B-b9YsAB5mpnam1oN0hYcFRwLXM/view?usp=sharing"" rel=""nofollow"">matrix structure</a> ? </p>

<p>If you think this additional updates should be asked in stack overflow or additional question, I will move it.</p>
"
"0.118056267220191","0.114896997924285","194400","<p>I have data on newspaper articles about police cracking down on crime, and data on crimes reported to the police. The data are daily, covering about six months and the same city.</p>

<p>I want to try an ARIMA model on these data to forecast the deterrent effect of newspaper coverage of law enforcement operations on crime.</p>

<p>I'm using the most recent forecast package in R, but as I have little experience with this sort of stuff I am unsure of the results. I am hoping that someone more knowledgeable can tell me if I need to massage the data for the analysis. I have read up on the forecast package and I'm unsure of how much of the hard work the code below is actually doing for me.</p>

<pre><code>rate &lt;- ts(crime$rate,frequency=7)
news &lt;- ts(crime$news,frequency=7)
fit &lt;- auto.arima(rate, xreg=news)
summary(fit)
</code></pre>

<p>My understanding, based on the answer to <a href=""http://stats.stackexchange.com/questions/14742/auto-arima-with-daily-data-how-to-capture-seasonality-periodicity"">this</a> question, is that the <code>frequency=7</code> here adjusts the data for daily seasonality (which is present in both datasets. There are fewer articles published and fewer crimes reported on Sundays). Are further adjustments required for these seasonal effects?</p>

<p>Output:</p>

<pre><code>ARIMA(2,1,0)(1,0,0)[7]                    

Coefficients:
          ar1      ar2    sar1         m2
      -0.4469  -0.2135  0.6080  1598.3622
s.e.   0.0907   0.0865  0.0727   612.6812

sigma^2 estimated as 1.93e+09:  log likelihood=-1635.99
AIC=3281.99   AICc=3282.45   BIC=3296.55

Training set error measures:
                    ME     RMSE      MAE       MPE     MAPE      MASE         ACF1
Training set -601.4693 43614.13 34018.82 -4.343747 20.00794 0.8864897 -0.004283847
</code></pre>

<p>Assuming that there is no missing data, and if the seasonality is already adjusted for, are there other things I need to check?</p>
"
"0.144588807813564","0.140719508946058","194468","<p>I am using function <code>prewhiten</code> from ""TSA"" package in R. I get an error about <code>NA</code> values, but I don't understand it, because I don't have <code>NA</code> values in my data. Here is the error message:</p>

<pre><code>whitedata &lt;- prewhiten(xhr, ypred, mod1)

Error in na.omit.ts(as.ts(x)) : all times contain an NA
</code></pre>

<p>It works fine for some data files, but not for others. When I print <code>xhr</code> and <code>ypred</code> I don't see any <code>NA</code> values. </p>

<p>Both are time series: </p>

<pre><code>xhr &lt;- ts(data$hr_z,start=1,frequency=10) #convert to a time series
ypred &lt;- ts(data$pred_z,start=1,frequency=10) #convert to a time series
</code></pre>

<p>Strangely, if I run it with a different model (one built on <code>ypred</code>), it runs just fine. The model I am using is: </p>

<pre><code>ARIMA(2,1,2)                    

Coefficients:
         ar1      ar2      ma1     ma2
      1.4835  -0.7641  -0.9574  0.4021
s.e.  0.1136   0.0826   0.1365  0.0910

sigma^2 estimated as 0.02589:  log likelihood=79.98
AIC=-149.96   AICc=-149.65   BIC=-133.55
</code></pre>
"
"0.096392538542376","0.0469065029820195","194941","<p>I'm using <code>auto.arima</code> to get the best model for the <code>MASS</code> dataset <code>deaths</code>. However, <code>auto.arima</code> does not seem to give the best model by measures of AIC, AICc, or BIC. <code>auto.arima</code> code below:</p>

<pre><code>&gt; data(deaths, package='MASS')
&gt; deaths
      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
1974 3035 2552 2704 2554 2014 1655 1721 1524 1596 2074 2199 2512
1975 2933 2889 2938 2497 1870 1726 1607 1545 1396 1787 2076 2837
1976 2787 3891 3179 2011 1636 1580 1489 1300 1356 1653 2013 2823
1977 3102 2294 2385 2444 1748 1554 1498 1361 1346 1564 1640 2293
1978 2815 3137 2679 1969 1870 1633 1529 1366 1357 1570 1535 2491
1979 3084 2605 2573 2143 1693 1504 1461 1354 1333 1492 1781 1915

&gt; library(forecast)
&gt; auto.arima(deaths)
Series: deaths 
ARIMA(1,0,0)(2,0,0)[12] with non-zero mean 

Coefficients:
         ar1    sar1    sar2  intercept
      0.4418  0.3098  0.5078  2058.2234
s.e.  0.1345  0.0973  0.0998   175.8665

sigma^2 estimated as 79455:  log likelihood=-515.07
AIC=1040.13   AICc=1041.04   BIC=1051.51
</code></pre>

<p>If I instead fit a ARIMA(2,1,1)x(1,1,1) model, it is better by all measures: </p>

<pre><code>&gt; Arima(deaths, order=c(2,1,1), seasonal=c(1,1,1))
Series: deaths 
ARIMA(2,1,1)(1,1,1)[12]                    

Coefficients:
         ar1      ar2      ma1     sar1     sma1
      0.2729  -0.3270  -1.0000  -0.2985  -0.9999
s.e.  0.1356   0.1396   0.1305   0.1426   1.0106

sigma^2 estimated as 39753:  log likelihood=-413.08
AIC=838.17   AICc=839.79   BIC=850.64
</code></pre>
"
"0.198132981305659","0.207663941095326","195443","<p>I am looking at two time series, from 01/01/2000 to the present: <br></p>

<ul>
<li>The <a href=""https://research.stlouisfed.org/fred2/series/NAPMNOI/"" rel=""nofollow"" title=""ISM Manufacturing: New Orders Index"">ISM Manufacturing: New Orders Index</a>, only available seasonally adjusted</li>
<li>The manufacturing industry unemployment rate, only available unadjusted (<a href=""https://research.stlouisfed.org/fred2/series/LNU04032232"" rel=""nofollow"">https://research.stlouisfed.org/fred2/series/LNU04032232</a>)</li>
</ul>

<p>I was <em>hoping</em> to construct a multivariate ts model, and use the <strong>New Orders Index</strong> to forecast the <strong>manufacturing industry unemployment rate</strong>. However, am I correct in assuming it is not 'ideal' to use seasonally adjusted data to predict another time series? Because doesn't SA cause (ideally) all the seasonal time series structure to be removed from the data?</p>

<h3>EDIT:</h3>

<p>Sorry, it just now hit me to link to the data I was using by putting it on Google Drive. It's in .csv files, for easy viewing with any program.</p>

<ul>
<li>Manufacturing new orders index data, in <strong>OrdersIndex.csv</strong><br><a href=""https://drive.google.com/file/d/0B2Y54SySHrVwZXczR1N4LXZMdXc/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B2Y54SySHrVwZXczR1N4LXZMdXc/view?usp=sharing</a></li>
<li>Manufacturing industry unemployment rate, in <strong>Unem.csv</strong>
<br><a href=""https://drive.google.com/file/d/0B2Y54SySHrVweFVpRjJFanAwQmc/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B2Y54SySHrVweFVpRjJFanAwQmc/view?usp=sharing</a></li>
</ul>

<p>Below is the New Orders Index time series, with the dashed line indicating the mean of 54.61. It looks fairly stationary to me; a decent spike in 2008, but definitely reverts to the mean.</p>

<pre><code>&gt; plot.ts(OrdersIndex[,2])
&gt; mean(OrdersIndex[,2])
[1] 54.60829
&gt; abline(h=c(54.61), lty=2)
&gt; 
</code></pre>

<p><a href=""http://i.stack.imgur.com/C61sm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/C61sm.png"" alt=""New Orders Index""></a></p>

<p>The ACF and PACF of the series are below. ACF displays dampened sine-wave behavior, PACF has a sharp cut-off after lag 1. This suggests an AR(1) model, as the ACF's slow dying off (at lags > 1) is due to the auto correlation at lag 1.</p>

<pre><code>&gt; Acf(OrdersIndex[,2], plot=T)   #the Acf() function is part of 'forecast' package
&gt; Acf(OrdersIndex[,2], plot=T, type=c('partial'))
&gt;
</code></pre>

<p><a href=""http://i.stack.imgur.com/Dg2Es.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Dg2Es.png"" alt=""ACF plot""></a>
<a href=""http://i.stack.imgur.com/0PqBR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0PqBR.png"" alt=""PACF plot""></a></p>

<p>After running an arima(1,0,0) model with a mean, the ACF and PACF of the residuals do not show significant spikes at any lags.</p>

<pre><code>&gt; OrdersIndex100 &lt;- arima(OrdersIndex[,2], order=c(1,0,0))
&gt; OrdersIndex100

Call:
arima(x = OrdersIndex[, 2], order = c(1, 0, 0))

Coefficients:
         ar1  intercept
      0.8738    54.6979
s.e.  0.0341     1.9399

sigma^2 estimated as 12.39:  log likelihood = -517.44,  aic = 1040.88
&gt;
</code></pre>

<p>Running an Ljung-Box test on the residuals indicates there is not any time series structure left in the data.</p>

<pre><code>&gt; LBQPlot(OrdersIndex100$residuals, k=1)   # LBQPlot() is part of 'FitAR' package
&gt;
</code></pre>

<p><a href=""http://i.stack.imgur.com/xXQKc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXQKc.png"" alt=""Ljung-Box Test""></a></p>

<h3>Conclusion</h3>

<p>The conclusion I arrive at is that the seasonally adjusting done to the data by the ISM (Institute of Supply Management) effectively removed all the seasonality from the data. So, this SA data would be less useful in modeling than non-SA data (this is assuming that I would be using this data series as the Input, and the unemployment data series as the Output). Is this a valid conclusion? You all see any glaring problems with my analysis?</p>
"
"0.12049067317797","0.117266257455049","197625","<p>I have been fitting ARIMA models to panel data.  My goal is to develop a single ARIMA model that explains growth across a number of different regions.  To do this, I followed Rob Hyndman's excellent advice in this CV thread: <a href=""http://stats.stackexchange.com/questions/23036/"">Estimating same model over multiple time series</a>.  Hyndman suggests applying <code>auto.arima</code> to a single time series separated by NAs.</p>

<p>Using this approach, <code>auto.arima</code> enables me to automatically select from a range of different AR models. In theory, <code>auto.arima</code> should also consider ARMA and MA models.</p>

<p>But <code>auto.arima</code> dismisses all models with MA terms. <code>auto.arima</code> produces an AIC of <code>Inf *</code> <strong>every</strong> single time it investigates a model with one or more MA terms.  </p>

<p>The thread <a href=""http://stats.stackexchange.com/questions/160612/""><code>auto.arima</code> doesn't calculate AIC values for the majority of models</a> suggests that <code>auto.arima</code> may produce infinite AICs because the MA terms are non-invertible.  </p>

<p>But I don't think non-invertible MAs are the problem! The <code>trace</code> command shows that <code>auto.arima</code> is producing infinite AICs EVERY single time, and across a range of industries. <code>auto.arima</code> also produces infinite AICs even if the MAs do not look non-invertible. For example, <code>auto.arima</code> deemed the model below to be non-invertible:</p>

<pre><code>arima(x = reim_arma$demean_gdp_growth, order = c(0, 0, 1), include.mean = FALSE)

Coefficients: 
ma1   -0.18
s.e.   0.08

sigma^2 estimated as 0.00823:  log likelihood = 132,  aic = -261
</code></pre>

<p>Does anyone have any idea why <code>auto.arima</code> would work fine with AR terms but <strong>always</strong> produce infinite AICs when MA terms are included? I am wondering if it is some way related to my inclusion of NA terms?</p>
"
"0.10223972648865","0.0995037190209989","201669","<p>I have read that auto.arima choses the model with the best AIC.</p>

<p>I am looking to create a model that best neutralises the autocorrelation, as it will be used for prewhitening.</p>

<p>Can I use auto.arima for that, or should I use a different function/is this at all possible in R? I am looking to find the best model that removes the acf.</p>
"
"0.142381215461566","0.155892367494449","202319","<p>I have daily sales data for a department store for the past 850 days. I have indicators on the major holidays and the days leading up to the major holidays. The number of days before the holidays that are included was chosen by AIC. The issue I'm having is that there are outliers throughout the data that I'm not sure how to handle. Or, at least that's what I think is happening since I don't seem to get accurate forecasts. I'm using a CV to calculate the MAPE of forecasts two weeks out, using the first 450 days as the initial training set and the rest to see how well the model forecasts the data.</p>

<p>I've used tso() from the tsoutliers package and tsoutliers from the forecast package to find outliers. They both give different results.</p>

<pre><code>tsoutliers(data$Sales)

$index
[1] 230 270 271 328 635

$replacements
[1] 2222.160 2088.573 2231.577 1812.380 2138.655

train = 454
trainingdata = data$Sales[1:train]
trainingdata = ts(trainingdata,frequency = 7)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))

Series: trainingdata 
ARIMA(2,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ma1    sar1    sar2      AO52      TC68       TC80      AO86
  0.2872  0.1331  -0.9717  0.3567  0.4607  885.2061  890.3690  -863.4296  836.8638
s.e.  0.0508  0.0480   0.0107  0.0436  0.0429  169.2521  163.4243   166.0282  169.8535
     AO111     AO121      TC229     AO259      TC270     AO328     AO416
  754.1791  691.0849  1236.8523  711.3954  1790.0292  764.9712  920.1783
s.e.  169.2042  167.7273   163.1458  167.9835   163.9663  170.0103  168.9235

sigma^2 estimated as 44080:  log likelihood=-3064.92
AIC=6152.24   AICc=6153.65   BIC=6222.21

Outliers:
type ind  time coefhat  tstat
1    AO  52  8:03   885.2  5.230
2    TC  68 10:05   890.4  5.448
3    TC  80 12:03  -863.4 -5.200
4    AO  86 13:02   836.9  4.927
5    AO 111 16:06   754.2  4.457
6    AO 121 18:02   691.1  4.120
7    TC 229 33:05  1236.9  7.581
8    AO 259 37:07   711.4  4.235
9    TC 270 39:04  1790.0 10.917
10   AO 328 47:06   765.0  4.500
11   AO 416 60:03   920.2  5.447
</code></pre>

<p>Running BoxCox on the data it recommends a transform of the data</p>

<pre><code>lambda &lt;- BoxCox.lambda(data$Sales)
trainingdata = BoxCox(trainingdata,lambda)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))
Series: trainingdata 
ARIMA(3,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ar3      ma1    sar1    sar2      LS3    AO52     AO53    TC68
  0.3918  0.0993  -0.0587  -0.9856  0.3632  0.4144  13.5805  5.7218  -7.7957  6.3960
s.e.  0.0383  0.0418   0.0416   0.0142  0.0361  0.0341   1.3201  1.2980   1.3041  1.2763
      AO80   AO121   TC229   TC270   AO416     AO445   TC634   AO780
  -23.3707  5.5352  5.8088  7.0446  7.9304  -23.6372  5.5475  6.7194
s.e.    1.2376  1.2307  1.2594  1.2640  1.2476    1.2393  1.2598  1.2353

sigma^2 estimated as 2.332:  log likelihood=-1482.63
AIC=3003.26   AICc=3004.23   BIC=3092.34

Outliers:
type ind   time coefhat   tstat
1    LS   3   1:03  13.581  10.287
2    AO  52   8:03   5.722   4.408
3    AO  53   8:04  -7.796  -5.978
4    TC  68  10:05   6.396   5.012
5    AO  80  12:03 -23.371 -18.883
6    AO 121  18:02   5.535   4.498
7    TC 229  33:05   5.809   4.612
8    TC 270  39:04   7.045   5.573
9    AO 416  60:03   7.930   6.356
10   AO 445  64:04 -23.637 -19.073
11   TC 634  91:04   5.547   4.404
12   AO 780 112:03   6.719   5.439
</code></pre>

<p>Some of these outliers are already taken care of since they're the holidays. I'm not sure how to handle the rest of the outliers when fitting the model and in the CV.</p>

<p>What is the best way to go about taking care of the outliers? I can reset the values of the training data where it's predicted as an outliers to the recommended value if it's not a holiday for fitting the model and then still calculate the MAPE off of the original data. However, there's a LS at index 3 so I'm not sure that would make sense for that.</p>
"
"0.206598467635334","0.201069746367499","206188","<p>I'm trying to forecast data that has an hourly and weekly pattern.  The model I made using predictors created using seasonaldummy does a nice job of picking up the hourly weekly pattern, but it takes a long time to train the model.  I tried to create a similar forecast using fourier function, but it doesn't seem to be picking up the hourly pattern as well.  Am I setting up fourier correctly to try to achieve the effect I've gotten with seasonaldummy?  Should the frequency in ts be something other than 168?  My data is hourly.  I've provided some sample data below.  </p>

<p>My end goal is to combine the predictors for the hourly weekly pattern with other predictors, that's why I'm not just using tbats. I've provided examples below of how I'm trying to combine dummy variables for the hourly weekly pattern with other predictors.  </p>

<p>Code:</p>

<pre><code>##BoxCox

TTTlambda &lt;- BoxCox.lambda(tsData)

##Partitioning Time Series
EndTrain&lt;-1344
ValStart&lt;-EndTrain+1
ValEnd&lt;-ValStart+336

tsTrain &lt;-tsData[1:EndTrain]
tsValidation&lt;-tsData[ValStart:ValEnd]
tsTest &lt;- tsData[TestStart:TestEnd]

##Predictors
xregTrain&lt;-dfPredictors[1:EndTrain,]
xregVal&lt;-dfPredictors[ValStart:ValEnd,]
xregTest&lt;-dfPredictors[TestStart:TestEnd,]

##Seasonal Dummies
x=ts(tsData,freq=168) 
dummies=seasonaldummy(x)
xreg2Train&lt;-dummies[1:EndTrain,]
xreg2Val&lt;-dummies[ValStart:ValEnd,]
xreg2Test&lt;-dummies[TestStart:TestEnd,]


##Fourier Terms

tsTTT&lt;-ts(tsData, freq=168)

bestfit &lt;- list(aicc=Inf)
for(i in 1:25)
{
  fit &lt;- auto.arima(tsTTT, xreg=fourier(tsTTT, K=i), seasonal=FALSE)
  if(fit$aicc &lt; bestfit$aicc)
    bestfit &lt;- fit
  else break;
}

bestfit$coef ## K=2

xreg3&lt;-fourier(tsTTT,2)

xreg3Train&lt;-xreg3[1:EndTrain,]
xreg3Val&lt;-xreg3[ValStart:ValEnd,]
xreg3Test&lt;-xreg3[TestStart:TestEnd,]


##hourly weekly
Arima.fit_D &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=xreg2Train, stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

Arima.fit_D_P &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=cbind(xreg2Train,xregTrain$Predictor), stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

##Fourier hourly weekly
Arima.fit_F &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=xreg3Train, stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

Arima.fit_F_P &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=cbind(xreg3Train,xregTrain$Predictor), stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

##Forecast Model

Acast_D&lt;-forecast(Arima.fit_D,xreg=xreg2Val, h=336)

Acast_D_P&lt;-forecast(Arima.fit_D_P,xreg=cbind(xreg2Val,xregVal$Predictor), h=336)

Acast_F&lt;-forecast(Arima.fit_F,xreg=xreg3Val, h=336)

Acast_F_P&lt;-forecast(Arima.fit_F_P,xreg=cbind(xreg3Val,xregVal$Predictor), h=336)
</code></pre>

<p>Data:</p>

<pre><code>dput(tsData[1:1681])
</code></pre>

<p>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5, 4, 7, 7.5, 
NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 8, 8, 5, 2, 7, 4, 6.5, 4.5, 
10, 6, 4.5, 6.5, 9, 2, 6, 3.5, NA, 5, 7, 3.5, 4, 4.5, 13, 19, 
8.5, 10, 8, 13, 10, 10, 6, 13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 
6, 5, 8.5, 3, 12, 10, 9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 
3, 4.5, 4, 5, 5, 3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 
5.5, 5, 7.5, 3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 
5, 5.5, 9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 
5, 4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 7, 
13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 4, 7, 
5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 10.5, 4, 11, 
9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 7, 9.5, 6, 10, 
5.5, 6.5, 12, 10, 10, 6.5, 2, 8, NA, 10, 5, 4, 4.5, 5, 7.5, 12, 
22, 5, 8.5, 2.5, 3, 10.5, 4, 7, 13, 4, 3, 5, 6.5, 3, 9, 9.5, 
16, NA, 4, 12, 4.5, 7, 5.5, 8, 14, 3, 8, 12, 14, 7, 8, 6, 8.5, 
6, 6.5, 15.5, 13, 3.5, 12, 7, 6, NA, 3, 5.5, 8.5, 9, 12, 13, 
8, 6.5, 8, 3, 5, 16.5, 2, 7, 6, 2, 5, 6.5, 3, 3, 7, 2, NA, 13, 
7, 16, 13, 12.5, 12, 7, 13, 11, 21.5, 16, 20, 3, 4, 5, 7, 11, 
7, 9, 11, 7, 13, 4, 14, 5, 12, 6, 7, 9, 12, 7, 12.5, 6.5, 16, 
5, 12, 9, 9.5, 9, 7, 9.5, 3, 13, 8, 7, 7, 7, 9, 6, 6, 11, 15, 
9, 6, 19, 10.5, 4, 6, 14.5, 9, 17, 14, 4, 16, 5, 6.5, 10, 9, 
17, 11.5, 3, 5, 9, 8, 16, 10, NA, 7, 5, 12.5, 12, 11, 3, 3, 3.5, 
14, 12, 7, 4, NA, 6, NA, 6, 10, 8, 10, 2, NA, 4, 5.5, 14, 4, 
4.5, 8.5, 13, 21, 10, 11.5, 18, 5, 3, 2, 6, 11, 3, 7.5, 6, 3, 
5, 9, 7.5, 7.5, 5, 9, 17, 3, 9.5, 5.5, 9.5, 15, 14.5, 10, 9, 
13.5, 12, 12, 3, 11, 6, 4, 8, 17.5, 7.5, 7.5, NA, 7, 4, 6, 6, 
6, 6, 6, 5, 8.5, 6, 6, 5, 6, 7, 5, 5, 5, 5, 7, 6, 8, 14, 6.5, 
9.5, 5, 18.5, 5, 8, 10, 16, 12, 13, 7, 6, 13, 9, 18, 17, 8, 7, 
3, 8, 2, 9, 11, 5, 2, 5.5, 6.5, 7, 10, 2, 3, 2, 3, 5, 4, 5, 6, 
3, 5, 3.5, 5, 4, 9, NA, 10.5, 16, NA, 11, 8.5, 13, 4, 12.5, 12, 
13, 18.5, 21, 5, 9, 4.5, 3, 3, 4, 3, 4, 4, 2, 8, 4.5, 4, 5, 9, 
5, 4.5, 4, 7.5, 6, 7, 22, 5, 8, 5, 7, 4, 8, 5.5, 3, 8, 7, 6, 
7.5, 6, 15, 13.5, 10, 7, 2.5, 7.5, 9, 9.5, 8, 19, 8, 8, 10, 6, 
9, 5, 4.5, 9, 3.5, 4, 3.5, 8, 5, 3.5, 8.5, 9, 12.5, 7, 8, 10.5, 
10, 1.5, 5, 10, 9, 2, 5, 8, 11, 3, 4.5, 2, 8.5, 4, 8, 2, 3, 4, 
5.5, 2, 4, 6, 4.5, 6, 6.5, 0, 2, 3.5, 10, 7, 14, 14, 12.5, 3, 
7, 8, 3, 7, 12, 12.5, 2, 2.5, 3, 9, 10.5, 8, 6, 6.5, 8.5, 5, 
10.5, 9, 3.5, 7, 5, 8, 5, 5, 5.5, 4, 9, 8, 5.5, 5, 6, 10.5, 4, 
9, 6, 5, 11, 10.5, 10.5, 4, 11.5, 11, 6, 2, 9, 5, 9, 5, 5.5, 
7, 4, 10, 5, 3, 9, 9, 19.5, 13, 6, 15, 7, 10, 8, 10.5, 8, 16, 
7, 10.5, 8.5, 10.5, 8, 8, 7, 5, 5, 6, 6, 5, 4, 9, 6.5, 4, 7, 
7, 5, 4, 7, 6, 3, 6, 8.5, 8.5, 4, 5.5, 7, 8, 5, 6, 3, 9, 12, 
6, 7.5, 4, 3, 5.5, 2, 5.5, 7, NA, 8.5, 2, 5, 8, 8, 4, 3, 6, 4, 
4.5, 5, 3, 7.5, 9, 13, 8, 10, 12, 6.5, 3, 3.5, 8.5, 9, NA, 12, 
8, 9, 4, 6, 8, 8, 9.5, 8, 6, 5, 4, 10.5, 6.5, 4, 3.5, 5, 7, 7, 
5, 9, 6, NA, 6, 6, 5, 10, 7, 9, 9, 5, 4, 5, 4, 6, 8, 5, 3, 2.5, 
2.5, 13, 4.5, 2.5, 2, 3, 9.5, 3, 5.5, 6, 10, 9, 10, 13, 14.5, 
9, 7, 6, 5, 4, 4, 4, 5, 6.5, 11, 13.5, 11, 12, 3, 3, 14, 11, 
6, 8, 5.5, 9, 8, 8, 7, 7, 5.5, 3.5, 10.5, 6, 5.5, 8, 8, 15, 6.5, 
8, 9.5, 6.5, 5, 7, 6, 4, 14.5, 4, 2.5, 5, 8, 18, 13, 10, 6, 7, 
18, 4.5, 7, 6.5, 5, 17, 7, 3, 5.5, 4, 6.5, 5.5, 6, 8, NA, 9.5, 
14, 9, 11, 8, 7, 17, 7, 8, 8, 9, 2, 2, 4, 3, 8, 4, 9, 6, 9, 11, 
13, 7.5, 8.5, 6, 6, 10, 17.5, 18.5, 14, 8.5, 4, 5, 6, 3, 2, 4, 
4, 12, 11, 5, 2.5, 8, 6, 10, 5, 8, 8, 10.5, 14, 7, 16, 15, 6, 
4.5, 10, 19, 3, 3, 4.5, 6.5, 4, 7.5, 8, 6, 20, 6, 7, 13, 13, 
4, 10, 6, 5, 4.5, 6, 10, 6, 4, 8.5, 7.5, 3, 3.5, 3, 2, 2, 20.5, 
6, 18, 5.5, 7.5, 5, 3.5, 8, 6, 6.5, 3, 4, 8, 5, 15.5, 4, 5, 8, 
5, 3, 4, 5, 3, 3, 3, 6, 4, 12, 8, 10, 12, 5.5, 9.5, NA, 5, 4.5, 
7, 16, 7, 4.5, 5, 5, 10, 6, 19, 8, 15, 7, 19.5, 10, 7.5, 9, 9, 
7, 8, 3, 6, 5.5, 6, 7, 8, 14, 8, 13, 5.5, 3.5, 5, 9, 4.5, 4, 
4, 3, 7.5, 4, 5, 6.5, 9, 4, NA, 12, 5.5, 6, 12.5, 6.5, 6.5, 5, 
11, 4.5, 8, 2, 4, 5, 5, 3, 2.5, 6, 7, 4, 17, 4, 3, 5, 6, 2, 8, 
8.5, 6.5, 4, 10, 12.5, 11, 6.5, 9, 12.5, 5.5, 5, 7.5, 16, 11.5, 
4, 5.5, 3.5, 4, 3, 6, 4, NA, 5, 6, 7, 3, 4.5, 7, 5.5, 4, 7, 11, 
7, 3, 3, 4, 3.5, 9, 4.5, 8, 5, 6, 8, 5, 5.5, 8, 5, 9, 8, 8, 6.5, 
6, 10, 7, 7, 9, 12, 8, 13, 6.5, 6, 4, 5.5, 6, 3, 7, 8, 15, 10, 
8, 3, NA, 5, 7, 7, 6, 9, 19, 13, 7, 7.5, 11, 8.5, 4, 7.5, 6, 
13.5, 17, 9, 5, 6.5, 6, 4, 5, NA, 3, 6, 10.5, 6, 14, 6, 9.5, 
6, 10, 11, 10, 3, 7, 9, 16.5, 5.5, 12.5, 8, 5, 10, 6, 1, 5, 6.5, 
10, 8.5, 5, NA, 9.5, 13, 10, 10, 20, 7, 8, 5, 3, 3, 4.5, 3.5, 
2, 5, 11, 3, 7.5, NA, 5.5, NA, 6, 6, 11, 12, 7, 5, 15, 11, 6, 
17.5, 13.5, 16, 16.5, 5, 4, 3, 5.5, 3, 8, 11, 8, 12, 14, NA, 
10, 6, 4, 5, 8, 10, 12.5, 6, 3, 6, 5, 8, 6, 11, 12.5, 7, 6, 9.5, 
2, 8.5, 9.5, 8, 8, 2, 7.5, NA, 6, 2.5, 4, 5, 5, 6, 9, 4, 7, 6, 
2, 4.5, 3, 4, 4, 5, 4, 3, 7.5, 8.5, NA, 12, 9, 11, 9, 3, 2.5, 
7, 4, 4, 7, 8.5, 12.5, 3.5, 6.5, 10, 6, 8, 7, 13, 13.5, 12, 13, 
8, NA, 8, 9, 15, NA, 4, 3.5, 2, 7, 8, 7.5, 9.5, 1.5, 5, 4, 8, 
11, 5, 12, 4, 3, 11, 8, 7.5, 5.5, 13, 11, NA, 12, 7, 8, 6, 13, 
8, 5, 4, 7, 8, 2, 3, 4, 4, 5, 5.5, 5.5, 6, 5, 6, 14, 12, 6, 11.5, 
13, 5, 5, 8, 9, 2, 5, 6, 10, 4.5, 4, 7, 7.5, 7, 4, 10, 6.5, 6, 
10)</p>

<pre><code>dput(dfPredictors$Predictor[1:1681])
</code></pre>

<p>c(2, 6, 3, 5, 3, 2, 2, NA, 2, 6, 12, 11, 9, 10, 13, 9, 11, 7, 
12, 8, 6, 4, 10, 6, 2, 7, 2, 1, 3, 2, 1, 3, 8, 7, 7, 8, 13, 13, 
13, 11, 12, 4, 12, 18, 12, 7, 5, 4, 6, 4, 3, 3, NA, 4, 2, 8, 
8, 8, 7, 3, 5, 3, 7, 8, 7, 7, 11, 8, 10, 3, 10, 6, 5, 5, 3, 1, 
2, 1, 1, 3, 4, 8, 8, 5, 9, 12, 12, 11, 8, 5, 9, 10, 7, 8, 4, 
6, 4, 1, 3, 1, 3, NA, 2, 1, 4, 10, 7, 13, 6, 9, 6, 16, 12, 11, 
10, 12, 9, 7, 7, 7, 6, 2, 3, 1, 1, 2, 2, 3, 11, 10, 9, 8, 9, 
13, 6, 6, 10, 9, 11, 10, 8, 7, 6, 4, 2, 3, 5, 3, 2, 4, 4, 4, 
8, 5, 12, 8, 7, 12, 9, 12, 12, 12, 13, 12, 9, 8, 9, 10, 4, 7, 
4, 2, 2, 4, 1, 7, 6, 6, 8, 11, 11, 5, 7, 6, 9, 12, 15, 9, 11, 
5, 10, 5, 4, 4, 2, 3, 3, 2, 5, 4, 7, 8, 6, 6, 5, 12, 10, 8, 10, 
10, 4, 13, 12, 6, 8, 6, 3, 1, 4, 2, NA, 4, 3, 2, 6, 5, 8, 10, 
4, 13, 2, 13, 8, 11, 13, 8, 9, 10, 9, 5, 1, NA, 1, 1, 2, NA, 
1, 7, 6, 10, 7, 8, 12, 12, 9, 5, 6, 8, 13, 13, 13, 8, 8, 1, 5, 
7, 6, 2, NA, 2, 1, 2, 7, 9, 12, 12, 10, 10, 10, 6, 8, 2, 8, 3, 
4, 5, 6, 2, 2, 1, 4, 1, NA, 3, 1, 3, 8, 8, 11, 11, 12, 5, 7, 
14, 9, 10, 14, 11, 8, 6, 8, 7, 5, 4, 3, 4, 9, NA, 2, 4, 5, 8, 
2, 12, 8, 15, 12, 8, 9, 12, 9, 9, 12, 7, 7, 8, 7, 5, 4, NA, 1, 
NA, NA, 4, 9, 8, 8, 8, 12, 13, 7, 11, 8, 14, 12, 13, 15, 8, 6, 
4, 4, 5, 2, NA, 2, 5, 4, 5, 6, 15, 11, 10, 16, 10, 5, 5, 10, 
13, 10, 9, 8, 7, 5, 4, 5, 6, NA, 2, 5, 4, 1, 6, 5, 8, 4, 3, 10, 
11, 8, 12, 10, 10, 10, 12, 10, 10, 7, 5, 7, 3, 4, 3, 3, 3, 3, 
8, 4, 8, 10, 5, 10, 10, 10, 11, 10, 11, 7, 10, 7, 6, 7, 7, 3, 
3, NA, 3, 6, 5, 3, 3, 5, 6, 6, 13, 14, 14, 7, 13, 9, 10, 4, 9, 
10, 8, 3, 6, 10, 5, 2, 1, NA, 3, 4, 4, 12, 12, 11, 12, 11, 13, 
10, 9, 11, 11, 14, 10, 13, 10, 7, 11, 1, 3, 1, 4, 1, 2, 2, 3, 
9, 6, 9, 9, 8, 9, 7, 12, 17, 13, 9, 10, 8, 8, 10, 2, 3, 3, 6, 
2, 2, 1, 6, 8, 7, 9, 5, 11, 8, 8, 12, 13, 14, 10, 7, 5, 11, 11, 
8, 5, 7, 3, 2, 3, 5, NA, 1, 3, 3, 4, 9, 12, 12, 3, 5, 12, 10, 
9, 14, 15, 12, 7, 8, 7, 3, 4, 1, 5, 6, 4, NA, 5, 9, 6, 7, 8, 
15, 13, 9, 12, 9, 7, 7, 6, 7, 8, 8, 6, 4, 5, 4, 1, 5, 1, NA, 
5, 4, 12, 7, 20, 12, 14, 10, 11, 11, 12, 6, 6, 11, 5, 6, 7, 4, 
7, 5, 1, 2, NA, 2, 7, 16, 9, 4, 12, 14, 12, 9, 8, 12, 7, 6, 11, 
9, 15, 9, 4, 4, 3, 3, 2, 5, 2, 1, 6, 8, 3, 12, 11, 14, 9, 6, 
3, 12, 11, 10, 14, 10, 10, 12, 2, 3, 3, 5, 3, 2, 3, 3, 5, 9, 
5, 10, 14, 9, 14, 11, 9, 12, 9, 15, 13, 12, 15, 11, 4, 7, 3, 
3, 3, 2, 5, 5, 11, 4, 2, NA, 3, 6, 10, 8, 5, 9, 9, 10, 11, 8, 
9, 8, NA, 3, NA, 1, 1, 4, 3, 3, NA, 4, 8, 3, 9, 6, 12, 9, 7, 
11, 6, 6, 12, 5, 4, 11, 7, 1, 2, 3, 2, 4, 8, 2, 6, 5, 9, 3, 7, 
8, 8, 8, 14, 10, 12, 5, 12, 9, 13, 7, 3, 5, 3, 4, 2, 4, 2, NA, 
5, 5, 9, 8, 7, 11, 9, 5, 6, 10, 13, 10, 9, 16, 11, 7, 5, 6, 2, 
5, 3, 5, 2, 2, 6, 5, 11, 7, 13, 6, 10, 7, 9, 7, 8, 9, 12, 7, 
7, 5, 3, 5, 3, 3, 5, 3, 1, 2, 10, 11, 8, 1, 9, 10, 14, 12, 7, 
11, 11, 10, 7, 5, 9, 8, 6, NA, 4, 2, NA, 3, 4, 4, 6, 10, 9, 6, 
8, 9, 10, 9, 14, 12, 8, 11, 16, 16, 13, 8, 7, 4, 4, 1, 3, 4, 
2, 2, 5, 9, 9, 3, 8, 12, 6, 11, 10, 6, 8, 15, 12, 12, 7, 6, 7, 
3, 2, 1, 3, 2, 2, 5, 7, 11, 8, 3, 4, 5, 5, 9, 6, 10, 9, 7, 17, 
12, 3, 8, 6, 4, 4, 4, 3, 4, 2, 1, 4, 7, 12, 5, 4, 8, 7, 15, 8, 
6, 6, 10, 5, 10, 7, 3, 4, 4, 1, 5, 3, 4, 5, 2, 1, 8, 6, 8, 9, 
7, 13, 11, 11, 6, 9, 9, 9, 9, 8, 1, 8, 1, 3, 2, 2, 1, 3, 2, 3, 
8, 9, 10, 12, 6, 9, 12, 7, 8, 8, 14, 10, 10, 8, 10, 4, 4, 4, 
4, 3, 2, 4, 2, 3, 16, 3, 9, 8, 15, 9, 11, 10, 12, 12, 7, 15, 
13, 8, 9, 3, 8, 1, 2, 2, 3, 3, 3, 10, 7, 5, 10, 4, 8, 8, 10, 
8, 9, 9, 6, 7, 7, 4, 6, 8, 4, 5, 5, 1, 1, 1, 5, 7, 3, 3, 12, 
8, 7, 10, 7, 12, 11, 7, 6, 14, 13, 9, 14, 3, 4, 6, 3, 2, 3, NA, 
2, 3, 7, 6, 9, 7, 5, 9, 8, 10, 7, 7, 6, 11, 11, 9, 7, 5, 2, 3, 
2, 2, 5, NA, 2, 5, 6, 7, 8, 14, 11, 6, 9, 10, 10, 9, 8, 16, 9, 
6, 6, 3, 2, 5, 1, 1, NA, 3, 3, 2, 5, 10, 9, 10, 13, 11, 8, 17, 
13, 5, 11, 10, 11, 6, 9, 6, 4, 5, 6, 5, 2, 4, 1, 2, 5, 9, 12, 
10, 10, 18, 9, 13, 16, 8, 13, 5, 16, 11, 8, 10, 3, 6, 3, 1, 2, 
2, 6, 2, 12, 9, 5, 8, 10, 11, 4, 10, 9, 9, 9, 19, 11, 8, 9, 4, 
4, 4, 7, 5, 2, 2, 1, 2, 6, 10, 11, 12, 9, 9, 12, 9, 8, 14, 8, 
5, 3, 5, 7, 6, 3, 4, 6, 3, 3, NA, 2, 2, 7, 12, 11, 14, 7, 10, 
10, 13, 12, 5, 8, 8, 9, 10, 4, 9, 4, 5, 3, 1, 4, 2, 1, 7, 9, 
10, 10, 11, 9, 8, 6, 3, 8, 10, 9, 9, 10, 8, 11, 4, 1, 1, 3, 1, 
1, 3, 6, 2, 3, 10, 4, 9, 6, 10, 11, 6, 7, 8, 6, 11, 8, 4, 8, 
5, 5, 1, 7, 1, 3, 3, 3, 6, 6, 8, 8, 9, 7, 6, 10, 9, 10, 6, 13, 
10, 20, 7, 9, 2, 6, 3, 2, 1, 1, 2, 1, 3, 13, 7, 6, 7, 11, 7, 
9, 7, 9, 10, 9, 10, 6, 11, 7, 5, 5, 5, 4, 6, 4, NA, 4, 4, 11, 
11, 9, 8, 9, 9, 12, 8, 11, 12, 3, 9, 12, 10, 8, 7, 3, 5, 2, 2, 
3, 2, 3, 4, 8, 13, 6, 8, 7, 4, 7, 13, 10, 9, 11, 10, 10, 5, 12, 
4, 3, 7, NA, 2, 2, 1, 6, 2, 8, 8, 9, 6, 11, 7, 7, 9, 11, 9, 6, 
8, 11, 10, 6, 5, 3, 5, 5, 1, 1, 2, 2, 5, 7, 13, 11, 7, 17, 10, 
4, 10, 9, 10, 6, 5, 8, 4, 6, 6, 3, 5, NA, 5, 1, 3, 1, 5, 3, 8, 
11, 6, 8, 9, 7, 11, 5, 12, 10, 10, 8, 9, 8, 7, 3, 2, 4, 5, 1, 
3, 1, 2, 9, 10, 4, 7, 10, 11, 6, 11, 8, 8, 7, 10, 9, 9, 11, 9, 
2, 3, 4, 3, NA, 2, 5, 5, 8, 13, 7, 14, 11, 4, 7, 10, 15, 10, 
15, 8, 6, 9, 5, 4, 2, 1, 3, NA, 1, 1, 2, 3, 13, 6, 8, 6, 12, 
10, 13, 5, 14, 11, 14, 14, 10, 10, 5, 3, 4, 2, 4, 2, 3, 2, 2, 
NA, 10, 9, 6, 14, 10, 7, 7, 12, 3, 13, 12, 12, 14, 8, 11, 3, 
6, NA, 2, NA, 3, 2, 5, 5, 11, 3, 5, 7, 7, 12, 8, 5, 11, 5, 2, 
8, 9, 6, 7, 3, 3, 1, 1, NA, 1, 3, 1, 3, 5, 10, 10, 7, 3, 5, 8, 
11, 9, 11, 7, 9, 9, 10, 5, 10, 4, 3, 3, 1, 2, NA, 3, 4, 6, 6, 
7, 11, 9, 11, 9, 6, 6, 8, 7, 9, 12, 12, 9, 5, 4, 3, NA, 1, 2, 
1, 2, 2, 2, 6, 9, 8, 8, 8, 12, 8, 13, 7, 12, 7, 8, 12, 8, 5, 
5, 1, NA, 4, 3, 1, NA, 5, 6, 10, 7, 15, 12, 12, 6, 11, 12, 12, 
10, 16, 10, 8, 11, 8, 5, 4, 2, 1, 2, NA, 5, 2, 8, 9, 7, 9, 14, 
7, 13, 3, 6, 9, 13, 10, 8, 6, 4, 6, 4, 5, 1, 3, 3, 2, 1, 4, 4, 
9, 11, 4, 8, 9, 11, 8, 8, 9, 19, 9, 7, 7, 11, 6, 8)</p>
"
"0.0590281336100955","0.114896997924285","206327","<p>I have time series data on drug trade revenue in a city over a four-month period (DP). I also have data on police crackdowns and raids in the same period, in the same city (IV). I want to see if the crackdowns/raids have any effect on the trade volume.</p>

<p>This is fairly straightforward and easy to follow, I think. However, when I do an ARIMA model with the DP and a bunch of lagged versions of the IV, the output is... Hard to understand, and even harder to explain. </p>

<p>Take the plot below, for example. In this model the IV is included (as xreg), but the graph is not very intuitive. Is there a way to illustrate both variables and how they impact each other (if at all) in a more intuitive way?</p>

<p><a href=""http://i.stack.imgur.com/gSAWY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gSAWY.png"" alt=""enter image description here""></a></p>

<p>I also have a hard time understanding the actual output. The model below is the correct ARIMA model, but as the coefficients are useless in ARIMA models, what can I really say based on these numbers? Can I say anything about how the IV affects the DP?</p>

<p>Most textbook resources I've found online, such as R.H. Shumway &amp; D.S. Stoffer's and Hyndman's, are mostly about how to find the best ARIMA model etc. and relatively little about how we can interpret and explain findings from an ARIMA+xreg model.</p>

<p>I hope someone can help!</p>

<pre><code>Series: revenue 
ARIMA(0,1,2)(2,0,0)[7]                    

Coefficients:
          ma1      ma2    sar1    sar2  (media)1  (media)2  (media)3
      -0.4801  -0.4825  0.4974  0.1835           14530.95           14153.26          -40388.58
s.e.   0.0834   0.0818  0.0945  0.1016           12270.92           16683.66           31806.25
      (media)4  (media)5  (media)6  (media)7  (media)8
              -20874.23          -32847.12          -22016.12           29745.91           13306.47
s.e.           29056.25           25904.39           34676.39           39702.54           55336.30
      (media)9  (media)11  (media)13  (media)17  (media)23
               12164.75            29435.90            87281.65            19487.28          -187469.01
s.e.           48021.79            54428.17            55982.59            57683.76            55452.48
      (media)25
                42058.20
s.e.            53967.83

sigma^2 estimated as 3.325e+09:  log likelihood=-1549.47
AIC=3136.93   AICc=3144.1   BIC=3190.82

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1
Training set 4718.048 57203.09 43598.36 -2.884122 16.07953 0.7763964 0.0171537
</code></pre>
"
"0.10223972648865","0.0663358126806659","208271","<p>I simulated a MA(3) process using: </p>

<pre><code>set.seed(66)
w &lt;- rnorm(100,0,3.6)
p1 &lt;- 0.4; p2 &lt;- -0.2; p3 &lt;- 0.3;
ma3 &lt;- w[1]
ma3[2] &lt;- w[2] + p1*w[1] 
ma3[3] &lt;- w[3] + p1*w[2] + p2*w[1]
for (t in 4:100) ma3[t] &lt;- w[t] + p1*w[t-1] + p2*w[t-2] + p3*w[t-3]
</code></pre>

<p>Running auto.arima on the time series gives:</p>

<pre><code>&gt; auto.arima(ma3)                                   
Series: ma3 
ARIMA(0,0,1) with zero mean     

Coefficients:
         ma1
      0.3854
s.e.  0.1152

sigma^2 estimated as 14.41:  log likelihood=-275.39
AIC=554.77   AICc=554.89   BIC=559.98
</code></pre>

<p>However, fitting the series to a MA(3) model gives a lower AIC:</p>

<pre><code>&gt; arima(ma3, order=c(0,0,3))

Call:
arima(x = ma3, order = c(0, 0, 3))

Coefficients:
         ma1      ma2     ma3  intercept
      0.4039  -0.0836  0.5125     0.2752
s.e.  0.1158   0.0905  0.1039     0.6078

sigma^2 estimated as 11.2:  log likelihood = -264.67,  aic = 539.34
</code></pre>

<p>I'm not sure what's going on. I thought that auto.arima selected the best model based on the AIC. </p>
"
"0.167997013412975","0.163501294390371","208526","<p>Assume I have a time series $ x_t $ that I want to fit using an ARIMA(1,1,0) model of the form:</p>

<blockquote>
  <p>$ \Delta x_t = \alpha \Delta x_{t-1} + w_t  $</p>
</blockquote>

<p>This could be rewritten as: </p>

<blockquote>
  <p>$ x_t - x_{t-1} = \alpha ( x_{t-1} - x_{t-2} )+ w_t  $</p>
  
  <p>$ x_t = ( 1 + \alpha)x_{t-1} - \alpha x_{t-2} + w_t  $</p>
</blockquote>

<p>The last equation describes an AR(2) model with coefficients $1+\alpha$ and $-\alpha$. I recognize that, depending on $\alpha$, this AR(2) model might be non-stationary. However, if I was taking a diff to begin with, then the series I am modeling shouldn't be stationary.</p>

<p>I know that if the model is non-stationary, a diff should be used. But how would the results differ if I used a AR(2) model vs an ARIMA(1,1,0) model? I assume (as hinted by R) that it has an issue with convergence. However, when I ask R to perform the fits, it will do both of them, and the coefficients are (mostly) consistent with my observations above. The forecasts are definitely different, though.</p>

<p>If anyone could shed some light on this, or point me to a good reference, I would appreciate it. </p>

<p>Here is the R code I used to generate both models. </p>

<pre><code>&gt; set.seed(2)
&gt; x &lt;- arima.sim(n = 1000, model=list(order=c(1,1,0), ar=c(0.3)))
&gt; plot(x)
&gt; arima(x, order=c(1,1,0))

Call:
arima(x = x, order = c(1, 1, 0))

Coefficients:
         ar1
      0.3291
s.e.  0.0298

sigma^2 estimated as 1.03:  log likelihood = -1433.91,  aic = 2871.81
&gt; arima(x, order=c(2,0,0))

Call:
arima(x = x, order = c(2, 0, 0))

Coefficients:
         ar1      ar2  intercept
      1.3290  -0.3294    50.9803
s.e.  0.0298   0.0299    35.9741

sigma^2 estimated as 1.03:  log likelihood = -1438.93,  aic = 2885.86
Warning messages:
1: In log(s2) : NaNs produced
2: In log(s2) : NaNs produced
3: In log(s2) : NaNs produced
4: In arima(x, order = c(2, 0, 0)) :
  possible convergence problem: optim gave code = 1
</code></pre>
"
"0.230213704049884","0.237232611884919","209790","<h2>Background</h2>

<p>I'm working on a project which aims to use the history data about a water flux to detect whether there is a leakage happened. The data is hourly collected and among about 4 months.  </p>

<p>I've already read the book which Professor Hyndman write about the forecast and some posts about outliers/anomaly detection on the site, but still I get confused how to realize this in R. In the meantime, I think I've got things mixed up and want to know the basic procedure to accomplish it.</p>

<h2>What I've tried</h2>

<p>At first, I think the basic idea is to fit a model on my train data and forecast it with the test part. Then use the model to check the residual in the whole data whether they are all normal distributed or at least has zero mean.  </p>

<p>So according to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, I've tried ARIMA, Exponential Soomthing and TBATS, but the result isn't ideal. And I'm also afraid that this could lead to a flaw since I didn't consider the outliers and anomaly.<br>
Here is my code</p>

<pre><code>model &lt;- list(
   mod_arima &lt;- auto.arima(train_h, ic = ""aic""),
   mod_exp &lt;- ets(train_h, ic = ""aic""),
   mod_tbats &lt;- tbats(train_h,ic = ""aic"")
)
forecasts &lt;- lapply(model, forecast, h = 24)
par(mfrow = c(2,2));
for (i in forecasts) {plot(i); lines(test_h,col = ""red"")}
</code></pre>

<p><a href=""http://i.stack.imgur.com/a80fL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/a80fL.jpg"" alt=""enter image description here""></a> </p>

<p>Then according to <a href=""http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series"">Simple algorithm for online outlier detection of a generic time series</a>, I find I could detect those single point that in my data through the answer by professor Hyndman, but I fail to change to detect the small level shift. (I've tried to create a 0.05*mean shift level, removing the outliers, then using the tso to detech the level shift, however it fail totally...)</p>

<h2>My Problems</h2>

<p>My problems mainly falls in the following two parts:  </p>

<ol>
<li><p>Even though it seems that there is a relativity between the flux and the flux an hour ago(Looking from the plot), could I use the hourly data directly to fit a model or should I first select the data at the same time each day to fit a model each?  </p>

<p><a href=""http://i.stack.imgur.com/dhPFL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dhPFL.jpg"" alt=""The plot of the relation between the data &amp;the data an hour ago""></a>  </p></li>
<li><p>Now I think my problem could be partly solved by directly detecting the level shift in the data, but I think that the leakage in the flux data should be relatively small if any(maybe just 5%,10% of the mean). While I've mannually create a shift in a try, when I use the tsoutliers::tso in R directly, the result isn't ideal. Is this idea right or should I fit a model still? And how could I detect such a small change in the level shift in a time series, particularly in R?   </p></li>
</ol>

<p>ps:Since I'm new to Cross Validated, I fail to find a way to upload the data may be easy for you solve my problem, is there any advice?</p>
"
"0.0681598176590997","0.0663358126806659","210885","<p>I can't seem to make sense of the following results. The time-series looks more non-stationary than stationary, and when I fit an ARMA(1, 0, 0) it estimates the AR(1) term to be very close to unity (0.99). From this I would expect a simple Dickey-Fuller test with no augmented autoregressive components to either fail or just marginally succeed to reject the null of a unit root. However, the test rejects the null with a p-value of less than 0.01.</p>

<p>I've read all the other questions on the ADF test, but still fail to understand the intuition behind these results.</p>

<p>(when the ADF test is run without specifying the k-order it picks an order of 6, if that helps).</p>

<p><a href=""http://i.stack.imgur.com/TTqiE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TTqiE.png"" alt=""enter image description here""></a></p>

<pre><code>df &lt;- structure(c(31.5, 29.2, 30.5, 30.5, 28.1, 27.7, 24.7, 24.3, 23, 
            19.8, 16.6, 14.9, 15.4, 13.4, 10.7, 9.4, 9.7, 8.9, 10.1, 10.3, 
            9.5, 9.4, 9.2, 8.5, 6.5, 6.3, 6.7, 6.9, 6.9, 6, 5.2, 4.5, 4.3, 
            4.7, 3.5, 3.1, 3.1, 2.8, 2.2, 1.8, 1.1, 1.8, 0.8, 1, 1.9, 0.3, 
            0.2, 0.4, 0.9, 0.9, 1, 0.9, 0.5, 1.3, 1.4, 1, 0.5, 1.3, 1.7, 
            1.7, -0.1, 0.1, 0.8, 1, 2, 2, 1.4, 2.7, 2.3, 2.4, 2, 2, 3.2, 
            2.8, 1.7, 1.4, 0.5, -0.4, 0.1, -1, -1.4, -1, -0.9, -0.9, -1.8, 
            -1.9, -1.1, -0.9, -0.8, -0.3, -0.8, -1, -0.8, -1.3, -1, -1.3, 
            -1.2, -1.2, -0.9, -0.6, 1, 1, 1.8, 2.2, 3.1, 3.1, 2.9, 2.8, 2.9, 
            3.2, 3.3, 3.2, 1.9, 2, 1.8, 2.3, 2.6, 3, 2.9, 3, 3.5, 3.4, 3.1, 
            3.4, 3.6, 3.7, 4.4, 4.2, 3.3, 3.7, 4.4, 4.6, 4, 4.4, 4.7, 4.9, 
            5, 5, 5.1, 5.5, 7.1, 7.6, 7.9, 8.2, 10, 10.9, 11.4, 11.9, 12.3, 
            12.7, 12.4, 12.2, 11.3, 10.7, 9.2, 8.5, 9.5, 8.5, 7.4, 5.9, 4.9, 
            3.9, 2.6, 2.2, 2.3, 1, 1.3, 1.2, -0.3, -0.6, -0.4, 0.2, 0.5, 
            0.9, 1.8, 1.8, 1.8, 2.6, 2.5, 3.6, 2.8, 3, 3.7, 4.4, 5, 4.8, 
            4.6, 4.4, 4.7, 4.2, 4.4, 3.5, 3.4, 3.7, 3.7, 3.3, 2.6, 2.6, 2.9, 
            3.4, 3.3, 3.2, 2.8, 2.9, 2.7, 2.3, 1.6, 1.4, 1.5, 1.3, 0.6, 0.5, 
            0.5, 0.5, 0.6, 0.5, 0.2, 0.3, 0.4, 0.3, 0.1, 0.3, 0.5, 0.3, 0, 
            0.3, 0.4, -0.1, -1.4, -1.5, -1.1, -0.6, 0, -0.2, -0.2, -1, -0.8, 
            -0.4, -0.5, -0.2, 0.7, 0.5, 0.8), .Tsp = c(1996, 2016.16666666667, 
                                                       12), class = ""ts"")

arima(df, order = c(1, 0, 0))

&gt; Call: arima(x = df, order = c(1, 0, 0))
&gt; 
&gt; Coefficients:
&gt;          ar1  intercept
&gt;       0.9986    14.2496 s.e.  0.0018    13.6263
&gt; 
&gt; sigma^2 estimated as 0.6027:  log likelihood = -286.21,  aic = 578.42

tseries::adf.test(df, k = 0)

&gt;   Augmented Dickey-Fuller Test
&gt; 
&gt; data:  df Dickey-Fuller = -5.6878, Lag order = 0, p-value = 0.01
&gt; alternative hypothesis: stationary
</code></pre>
"
"0.0834783871129682","0.0812444463702388","211213","<p>I'm fitting a
$$ARIMA(p,d,q)\times (P,D,Q)_{12}$$
model. The first loop fits p and q. The second loop fits P and Q. Here, d and D are both assumed to be 0 since I'm looking at the interaction between two cycles.</p>

<pre><code>&lt;&lt;arima,echo=FALSE, fig.show='hold', tidy=TRUE&gt;&gt;=
aic_table &lt;- function(data,P,Q,xreg=NULL){
  table &lt;- matrix(NA,(P+1),(Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
      table[p+1,q+1] &lt;- arima(data,order=c(p,0,q),xreg=xreg, method=""ML"")$aic
    }
  }
  dimnames(table) &lt;- list(paste(""AR"",0:P, sep=""""),paste(""MA"",0:Q,sep=""""))
  table
}
u_aic_table &lt;- aic_table(cars_hp$cycle,5,4,xreg=steel_hp$cycle)
@

#a and b are found from the previous step
&lt;&lt;sarima,echo=FALSE, fig.show='hold', tidy=TRUE&gt;&gt;=
aic_table2 &lt;- function(data,P,Q,xreg=NULL){
  table &lt;- matrix(NA,(P+1),(Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
      table[p+1,q+1] &lt;- arima(data,order=c(5,0,3),xreg=xreg, seasonal = list(order = c(p,0,q), Period=12))$aic
    }
  }
  dimnames(table) &lt;- list(paste(""AR"",0:P, sep=""""),paste(""MA"",0:Q,sep=""""))
  table
}
u_aic_table2 &lt;- aic_table2(cars_hp$cycle,5,4,xreg=steel_hp$cycle)
</code></pre>

<p>The thing is when I try this, I can get ACF values under the ARIMA just fine. It's when I run the second loop that I get issues. Is there a workaround for this? Is it because I'm using the cyclic decompositions of each time series? I want to understand how my cars time series relates to the steel time series.</p>

<pre><code>     Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
  non-finite finite-difference value [1] 
</code></pre>
"
"NaN","NaN","212110","<p><code>auto.arima</code> gives me that the best model is <code>arima(0,1,0)</code>. But using Arima and fitting <code>(0,1,0)(0,1,0)[6]</code> I have a better fit according to BIC, AIC, AICc. <code>auto.arima</code> is not evaluating that model</p>

<p>Here is the code</p>

<pre><code>model1&lt;-auto.arima(data.set,xreg = X1,trace = T,
                                   stepwise=F,approximation = F,
                                   parallel = F, num.cores = 2,
                   allowdrift = T,allowmean = T)
</code></pre>

<p>Why is <code>auto.arima</code> not evaluating <code>(0,1,0)(0,1,0)[6]</code>?</p>
"
"0.243755686641053","0.250412201434081","217955","<p>I'm doing some time series modeling using R and the <code>forecast</code> package, and found a minor difference I couldn't figure out. I'll reproduce my steps below. </p>

<p>First, I generate some data. While I have ""real"" data, I'll just use simulated data so that anyone can reproduce them (it makes no difference). The generated data is divided into training and test sets.</p>

<pre><code>&gt; set.seed(1234)
&gt; mydata &lt;- arima.sim(list(order = c(1,0,0), ar = 0.8), n = 500)
&gt; training &lt;- mydata[1:400]   # training set
&gt; testing &lt;- mydata[401:500]  # test set
</code></pre>

<p>Then, I fit a model to my training data:</p>

<pre><code>&gt; library(forecast)
&gt; (fit &lt;- Arima(training, order=c(1,0,0)))
Series: training 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept
      0.8336     0.0462
s.e.  0.0274     0.2987

sigma^2 estimated as 1.013:  log likelihood=-570.68
AIC=1147.37   AICc=1147.43   BIC=1159.34
</code></pre>

<p>Next, I calculate one-step ahead forecasts using the test set:</p>

<pre><code>&gt; refit &lt;- Arima(testing, model=fit)
</code></pre>

<p>For my purposes, a forecast horizon of 1 is fine. So, I should evaluate model accuracy comparing the one-step ahead forecasts -- given by  <code>fitted(forecast(refit))</code> -- to the test set (<code>testing</code>).</p>

<p>I thought the first forecast value obtained using the original model (<code>fit</code>) should be equal to the first point forecast using the <code>refit</code> model, since (I assume) both forecasts are calculated from the training data. However, they're different:</p>

<pre><code>&gt; fitted(refit)[1]
[1] 0.02706320

&gt; forecast(fit)$mean[1]
[1] 1.3180435
</code></pre>

<p>Could anyone explain this difference, please? Am I assuming something wrong here?</p>

<p>For what it's worth, this particular system has R 3.2.5 with <code>forecast</code> version 5.4, but an installation with the latest <code>forecast</code> exhibits the same behavior.</p>

<pre><code>&gt; R.version.string
[1] ""R version 3.2.5 (2016-04-14)""

&gt; packageVersion(""forecast"")
[1] â€˜5.4â€™
</code></pre>

<p>EDIT 1: I had erroneously fit the model to the entire dataset, not just the training set. I corrected it above.</p>

<p>EDIT 2: Stephan's answer below prompted me to dig a little deeper. <code>forecast(refit)</code> gives forecasts past the end of the test set:</p>

<pre><code>&gt; forecast(refit, h=3)
    Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
101     -0.1714176 -1.633258 1.290423 -2.407110 2.064275
102     -0.1352187 -2.038402 1.767965 -3.045887 2.775450
103     -0.1050416 -2.262407 2.052323 -3.404447 3.194363
</code></pre>

<p>So, it doesn't seem to be what I want (one-step ahead forecasts using observed data).</p>

<p>The AR(1) model obtained using <code>auto.arima()</code> is $\hat{y}_t=0.8336y_{t-1} + 0.0462 + e_t$. I calculated by hand the first few forecasts using this model:</p>

<pre><code>&gt; (test.5 &lt;- mydata[400:404])  # last observation from the training set, first four from the test set
[1]  1.571841404  0.003474084  0.744644046 -0.627186378 -2.420643234

&gt; 0.8336*test.5 + 0.0462  # forecasts for y(401)...y(405)
[1]  1.3564870  0.0490960  0.6669353 -0.4766226 -1.9716482

&gt; fitted(refit)[1:5]
[1]  0.02706320  0.01057917  0.62845310 -0.51516887 -2.01027834
</code></pre>

<p>With the exception of the first forecast, the numbers agree (assuming the differences are due to rounding). On the other hand, the first forecast calculated by hand (1.3565) is not too different from the first forecast given by <code>forecast(fit)</code>, which is 1.3180. So, it seems that <code>fitted(refit)</code> is what I'm after, I just don't understand why it gives a different value for the first forecast.</p>

<p>EDIT 3: Rob's answer below mostly solves the issue. I'm still puzzled by the fact that the forecasts given by <code>forecast()</code> differ from those calculated by hand, and by a seemingly fixed amount:</p>

<pre><code>&gt; (by.hand &lt;- coef(fit)['ar1']*test.5 + coef(fit)['intercept'])
[1]  1.35654540  0.04908109  0.66695502 -0.47666695 -1.97177642

&gt; (auto &lt;- c(forecast(fit)$mean[1], fitted(refit)[2:5]))
[1]  1.31804348  0.01057917  0.62845310 -0.51516887 -2.01027834

&gt; by.hand - auto
[1] 0.03850192 0.03850192 0.03850192 0.03850192 0.03850192
</code></pre>

<p>Can anybody shed some light on this?</p>
"
"0.0737851670126194","0.0718106237026783","220299","<p>I'm completely new to forecasting so please correct me if I'm wrong.</p>

<p>I'm trying to forecast sales data using R. My main concern is that when I decompose the data using <code>stl()</code> from <code>stats</code> package, it shows a seasonal component whereas when I use <code>ets()</code> or <code>auto.arima()</code> commands, they do not take a seasonal component into account. Can anyone please suggest to me where I am going wrong? Which method should I prefer?</p>

<p>I would like to do forecast for Aug15-Dec15.</p>

<p>My data are as follows:</p>

<pre><code>Month      Year Amount
January    2010 7632
February   2010 6686
March      2010 3442
April      2010 4556
May        2010 7796
June       2010 1534
July       2010 1466
August     2010 3535
September  2010 2503
October    2010 7534
November   2010 1197
December   2010 5861
January    2011 8846
February   2011 7219
March      2011 5066
April      2011 13177
May        2011 7833
June       2011 5585
July       2011 6392
August     2011 5787
September  2011 13488
October    2011 9413
November   2011 7610
December   2011 11301
January    2012 14912
February   2012 13578
March      2012 12091
April      2012 14628
May        2012 10703
June       2012 7373
July       2012 13638
August     2012 10794
September  2012 12186
October    2012 8137
November   2012 7874
December   2012 7707
January    2013 11569
February   2013 13446
March      2013 10339
April      2013 19086
May        2013 15201
June       2013 11741
July       2013 19368
August     2013 15755
September  2013 12214
October    2013 13859
November   2013 13096
December   2013 14548
January    2014 16191.1
February   2014 23122.3
March      2014 21421.6
April      2014 20904.5
May        2014 19711.5
June       2014 9481.9
July       2014 18699
August     2014 21271.9
September  2014 19515.5
October    2014 19890.6
November   2014 16789
December   2014 31409.3
January    2015 21917.2
February   2015 24911.4
March      2015 26072.4
April      2015 23919.3
May        2015 26980.8
June       2015 41661.2
July       2015 27065.4
August     2015 
September  2015 
October    2015 
November   2015 
December   2015 
</code></pre>

<p>My R code:</p>

<pre><code>x.ts &lt;- structure(c(7632, 6686, 3442, 4556, 7796, 1534, 1466, 3535,
    2503, 7534, 1197, 5861, 8846, 7219, 5066, 13177, 7833, 5585, 6392, 
    5787, 13488, 9413, 7610, 11301, 14912, 13578, 12091, 14628, 10703, 
    7373, 13638, 10794, 12186, 8137, 7874, 7707, 11569, 13446, 10339, 
    19086, 15201, 11741, 19368, 15755, 12214, 13859, 13096, 14548, 
    16191.1, 23122.3, 21421.6, 20904.5, 19711.5, 9481.9, 18699, 21271.9, 
    19515.5, 19890.6, 16789, 31409.3, 21917.2, 24911.4, 26072.4, 
    23919.3, 26980.8, 41661.2, 27065.4, NA, NA, NA, NA, NA),
  .Tsp = c(2010, 2015.91666666667, 12), class = ""ts"")

fit &lt;- stl(x.ts,na.action = na.omit,s.window = ""periodic"",robust = T)
plot(fit)
summary(ets(x.ts)) 
fit2 &lt;- auto.arima(x = x.ts, stepwise = F, approximation = F)
summary(fit2)  
</code></pre>

<p>EDIT:</p>

<pre><code>ets(x.ts)$aicc
[1] 1404.23  


ETS       AICc     
AAN    1404.26631   
ANN    1404.23046   
MNN    1411.95791   
MAN    1404.40096   
MMN    1400.49486   
</code></pre>
"
"0.0340799088295499","0.0995037190209989","220680","<p>I am trying to fit a time series with function <code>auto.arima</code> in the ""forecast"" package in R that is choosing the best model automatically. Since I am using it for my thesis, I have to show the trace of stepwise selection algorithm manually (without adding line ""trace"" in <code>auto.arima</code> code) with my web apps here <a href=""https://nugraha92.shinyapps.io/Analisis/"" rel=""nofollow"">https://nugraha92.shinyapps.io/Analisis/</a></p>

<p>But, when I manually checked it, I found that <code>auto.arima</code> did not choose minimum AIC values.</p>

<p>Then, I input the <code>trace=TRUE</code> line and the result shows that the AIC/BIC value is not the same between the ""trace ARIMA list"" and ""choosen ARIMA model"". For example, in the image it is shown that the best model is ARIMA(3,1,3), BIC score at the list is -252.4451 while at the equation below is -250.89.</p>

<p>Why and which one is the correct one? Did I make some mistakes?</p>

<p><a href=""http://i.stack.imgur.com/KWSPh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KWSPh.png"" alt=""enter image description here""></a></p>
"
"0.110431526074847","0.199598843322149","221881","<p>This question is sort of a follow up of <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r"">this</a> great thread.</p>

<p>I have a Time Series Analysis project in which I have to create a model to predict new values given historical univariate data using R. My plan is to run several models, assess their accuracies and choose the best one to use in production.</p>

<p>I must admit that I'm a beginner in TS analysis, so I'm not sure about the procedure that I must take. The main question is: Should I do a seasonal adjust in the data before fitting it into models?</p>

<p>I've done some simulation myself and the results are quite interesting:</p>

<h2>Raw data (without adjusting)</h2>

<pre><code>library(seasonal)
library(forecast)
data(""AirPassengers"")

training &lt;- window(AirPassengers, end = c(1959, 12))
test &lt;- window(AirPassengers, start = c(1960, 1))

models &lt;- list(
  mod_arima = auto.arima(training, ic='aicc', stepwise=FALSE),
  mod_etrainingp = ets(training, ic='aicc', restrict=FALSE),
  mod_neural = nnetar(training, p=12, size=25),
  mod_tbats = tbats(training, ic='aicc', seasonal.periods=12),
  mod_bats = bats(training, ic='aicc', seasonal.periods=12),
  mod_stl = stlm(training, s.window=12, ic='aicc', robust=TRUE, method='ets'),
  mod_sts = StructTS(training)
)

forecasts &lt;- lapply(models, forecast, 12)

acc &lt;- lapply(forecasts, function(f){
  accuracy(f, test)[2,,drop=FALSE]
})
acc &lt;- Reduce(rbind, acc)
row.names(acc) &lt;- names(forecasts)
(acc &lt;- round(acc, 2))
                   ME   RMSE    MAE   MPE  MAPE MASE ACF1 Theil's U
mod_arima      -15.72  22.96  17.91 -3.68  4.05 0.59 0.08      0.51
mod_etrainingp   4.99  19.01  14.40  0.75  3.03 0.47 0.27      0.41
mod_neural      12.97  26.08  23.38  2.55  4.94 0.77 0.16      0.55
mod_tbats      -15.49  25.67  18.20 -3.71  4.14 0.60 0.17      0.58
mod_bats         0.69  23.12  16.48 -0.35  3.45 0.54 0.38      0.50
mod_stl         31.35  57.93  39.95  5.31  7.43 1.31 0.65      1.01
mod_sts        177.41 199.41 177.41 36.02 36.02 5.83 0.77      3.76
</code></pre>

<h2>Adjusted data</h2>

<pre><code>adj &lt;- seas(AirPassengers)
adj &lt;- final(adj)

adj_training &lt;- window(adj, end = c(1959, 12))
adj_test &lt;- window(adj, start = c(1960, 1))

models_adj &lt;- list(
  mod_arima = auto.arima(adj_training, ic='aicc', stepwise=FALSE),
  mod_etrainingp = ets(adj_training, ic='aicc', restrict=FALSE),
  mod_neural = nnetar(adj_training, p=12, size=25),
  mod_tbats = tbats(adj_training, ic='aicc', seasonal.periods=12),
  mod_bats = bats(adj_training, ic='aicc', seasonal.periods=12),
  mod_stl = stlm(adj_training, s.window=12, ic='aicc', robust=TRUE, method='ets'),
  mod_sts = StructTS(adj_training)
)

forecasts_adj &lt;- lapply(models_adj, forecast, 12)

acc_adj &lt;- lapply(forecasts_adj, function(f) accuracy(f, adj_test)[2,,drop=FALSE])
acc_adj &lt;- Reduce(rbind, acc_adj)
row.names(acc_adj) &lt;- names(forecasts_adj)
(acc_adj &lt;- round(acc_adj, 2))
                   ME  RMSE   MAE   MPE MAPE MASE ACF1 Theil's U
mod_arima        6.02 10.05  9.49  1.22 1.99 0.31 0.64      1.26
mod_etrainingp  -2.76  7.12  4.43 -0.62 0.96 0.15 0.40      0.90
mod_neural     -30.84 33.27 30.84 -6.47 6.47 1.02 0.56      4.11
mod_tbats      -15.38 16.60 15.38 -3.26 3.26 0.51 0.05      2.09
mod_bats        -7.13 10.41  7.31 -1.55 1.58 0.24 0.43      1.34
mod_stl         -1.77  7.01  4.67 -0.41 1.01 0.15 0.39      0.89
mod_sts         -9.59 11.05  9.59 -2.04 2.04 0.32 0.11      1.38
</code></pre>

<p>Now, a comparison table to highlight the differences:</p>

<pre><code>abs(acc_adj) &lt; abs(acc)
                  ME  RMSE   MAE   MPE  MAPE  MASE  ACF1 Theil's U
mod_arima       TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE     FALSE
mod_etrainingp  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE     FALSE
mod_neural     FALSE FALSE FALSE FALSE FALSE FALSE FALSE     FALSE
mod_tbats       TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE     FALSE
mod_bats       FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE     FALSE
mod_stl         TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE      TRUE
mod_sts         TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE      TRUE
as.data.frame(rowMeans(abs(acc_adj) &lt; abs(acc)))
               rowMeans(abs(acc_adj) &lt; abs(acc))
mod_arima                                  0.750
mod_etrainingp                             0.750
mod_neural                                 0.000
mod_tbats                                  0.875
mod_bats                                   0.500
mod_stl                                    1.000
mod_sts                                    1.000
</code></pre>

<p>The only model that has better accuracy in raw data is <code>mod_neural</code>, created with neural networks.</p>

<p>So, what should I learn from this experiment? Does it make sense to use X13ARIMA-SEATS to adjust a series before fitting models? </p>
"
"0.0681598176590997","0.0995037190209989","223297","<p>I am trying to understand the coefficients retrieved from running <code>auto.arima</code> in R on my monthly time series of the annual change in House prices. When doing so, I obtain the following outcome:</p>

<pre><code>Series: AC.HousePrices 
ARIMA(1,1,1)(0,0,1)[12] with drift         

Coefficients:
         ar1      ma1     sma1   drift
      0.3243  -0.6592  -0.7892  -6e-04
s.e.  0.1733   0.1333   0.1161   4e-04

sigma^2 estimated as 0.0008257:  log likelihood=275.22
AIC=-540.44   AICc=-539.96   BIC=-526.07
</code></pre>

<p>To be honest I do not understand why I have two sets of parameters (p,d,q) and (P,D,Q)? The first set (1,1,1) seems to indicate that the series is first-order autoregressive model, nonstationary and with a simple exponential smoothing with drift? What are the second set of values (0,0,1)[12], is it telling me that my series looks yearly seasonal [12]?</p>
"
"0.133863224475948","0.151994441447782","223379","<p>I'm fitting an <code>arima</code>(1,0,0) model using the <code>forecast</code> package in R on the <code>usconsumption</code> dataset. However, when I mimic the same fit using <code>lm</code>, I get different coefficients. My understanding is that they should be the same (in fact, they give the same coefficients if I model an <code>arima</code>(0,0,0) and <code>lm</code> with only the external regressor, which is related to this post: <a href=""http://stats.stackexchange.com/questions/28472/regression-with-arima0-0-0-errors-different-from-linear-regression"">Regression with ARIMA(0,0,0) errors different from linear regression</a>). </p>

<p>Is this because <code>arima</code> and <code>lm</code> use different techniques to calculate coefficients? If so, can someone explain the difference?  </p>

<p>Below is my code.</p>

<pre><code>&gt; library(forecast)
&gt; library(fpp)
&gt; 
&gt; #load data
&gt; data(""usconsumption"")
&gt; 
&gt; #create equivalent data frame from time-series
&gt; lagpad &lt;- function(x, k=1) {
+   c(rep(NA, k), x)[1 : length(x)] 
+ }
&gt; usconsumpdf &lt;- as.data.frame(usconsumption)
&gt; usconsumpdf$consumptionLag1 &lt;- lagpad(usconsumpdf$consumption)
&gt; 
&gt; #create arima model
&gt; arima(usconsumption[,1], xreg=usconsumption[,2], order=c(1,0,0))

Call:
arima(x = usconsumption[, 1], order = c(1, 0, 0), xreg = usconsumption[, 2])

Coefficients:
         ar1  intercept  usconsumption[, 2]
      0.2139     0.5867              0.2292
s.e.  0.0928     0.0755              0.0605

sigma^2 estimated as 0.3776:  log likelihood = -152.87,  aic = 313.74
&gt; 
&gt; #create lm model
&gt; lm(consumption~consumptionLag1+income, data=usconsumpdf)

Call:
lm(formula = consumption ~ consumptionLag1 + income, data = usconsumpdf)

Coefficients:
    (Intercept)  consumptionLag1           income  
         0.3779           0.2456           0.2614  
</code></pre>
"
"0.131990919337114","0.128458748884677","224380","<p>I have a dataset which contains data from a sensor for every 5 minutes and am trying to predict for example 10 future values based on the first 500 values. My data looks like the following and could be downloaded <a href=""https://github.com/numenta/NAB/blob/master/data/artificialWithAnomaly/art_daily_flatmiddle.csv"" rel=""nofollow"">here</a>:</p>

<pre><code>timestamp,value
2014-04-01 00:00:00,-21.0483826823
2014-04-01 00:05:00,-20.2954768676
2014-04-01 00:10:00,-18.127229468299998
2014-04-01 00:15:00,-20.1716653997
2014-04-01 00:20:00,-21.223761612
2014-04-01 00:25:00,-19.1044911334
</code></pre>

<p><a href=""http://i.stack.imgur.com/iw6O3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iw6O3.png"" alt=""enter image description here""></a></p>

<p>I am taking the following steps:</p>

<pre><code># Read data from file and create time series    
myData &lt;- read.zoo(file=""filePath"", sep = "","", header = TRUE,index = 1, tz = """", format = ""%Y-%m-%d %H:%M:%S"", nrows=500)

# Fit ARIMA model to the data
fit &lt;- auto.arima(z, stepwise=FALSE, trace=TRUE, approximation=FALSE)

# Predict 10 timesteps ahead
pred &lt;- predict(fit, n.ahead = 10)
</code></pre>

<p>But when I print the prediction results they do not seem promising and model always converges to a single value:</p>

<pre><code>$pred
Time Series:
Start = 1396474800 
End = 1396477500 
Frequency = 0.00333333333333333 
 [1] 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789

$se
Time Series:
Start = 1396474800 
End = 1396477500 
Frequency = 0.00333333333333333 
 [1]  7.136100  9.728122 11.762177 13.493007 15.025767 16.416032 17.697417 18.892088 20.015580 21.079276
</code></pre>

<p>And here is the summary of fit:</p>

<pre><code>&gt; summary(fit)
Series: z 
ARIMA(0,1,1)                    

Coefficients:
          ma1
      -0.0735
s.e.   0.0463

sigma^2 estimated as 50.92:  log likelihood=-1688.17
AIC=3380.34   AICc=3380.37   BIC=3388.77

Training set error measures:
                    ME     RMSE      MAE     MPE    MAPE       MASE        ACF1
Training set 0.2215984 7.121813 3.141386 1592726 1592732 0.07197436 0.001426353
</code></pre>

<p>This is my first day with R and I think I might be doing something wrong. Any help would be much appreciated.</p>

<p>Thanks</p>
"
"0.144588807813564","0.140719508946058","226913","<p>I am trying to fit different SARIMA models in R. I first tried to fit on the first 12 data points, I got an error; I thought seasonal differencing requires more data, so I changed it to first 24 data points, still I got an error. Now I am fitting it on the entire data, and still I am getting an error. Following is the code:</p>

<pre><code>install.packages(""MASS"")
install.packages(""forecast"")

# Loading:
require(MASS)
require(forecast)

# Data:
nottem_ts &lt;- ts(nottem, frequency=12, start=1920)

# Problem
p &lt;- 0:2;  d &lt;- 0:1;  q &lt;- 0:2;  P &lt;- 0:2;  D &lt;- 0:1;  Q &lt;- 0:2
comb    &lt;- as.matrix(expand.grid(p,d,q,P,D,Q))
aic_vec &lt;- numeric(nrow(comb))
for(k in 1:nrow(comb)){
  aic_vec[k] &lt;- AIC(Arima(nottem_ts, order=c(comb[k,1], comb[k,2], comb[k,3]), 
                          seasonal=list(order=c(comb[k,4], comb[k,5], comb[k,6]), 
                          period=12), method=""ML""))
}
aic_mat &lt;- as.matrix(aic_vec)   
result  &lt;- cbind(comb,aic_mat)
sorted  &lt;- result[order(aic_vec),]
</code></pre>

<p>Is there any solution to this problem and why I am having following error?</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  :
non-finite finite-difference value [2]
</code></pre>

<p>There are no errors if I remove seasonal part, thus I know code is correct and that's why I have not asked the question in a different place, such as Stack Overflow.</p>

<p><strong>Remark</strong> Removing period=12 and using nottem_ts with frequency 1  makes the error disappear, so seasonality will be captured daily rather than monthly, does doing this make any sense?</p>

<p><strong>Comment</strong> As far the analysis problem is concerned I can get rid of this by using fourier in Arima and do what I want to do, but I think it is interesting to know the reason of this error.</p>
"
"0.0590281336100955","0.0574484989621426","228576","<p>I carried out a log transformation on some data about patient admission in the hospital, aiming to generate an additive model. I went ahead to forecast the same data using: </p>

<pre><code>MYFORECAST = forecast.Arima(auto.arima(admission, d=3, D=NA, stationary=FALSE, 
                            seasonal=FALSE, ic=""aic"", trace=TRUE, allowdrift=FALSE, 
                            allowmean=TRUE)
</code></pre>

<p>in R. I got the forecasts but I was left wondering:  Should report the forecasts or take exponent of the values? </p>
"
"0","0","228667","<p>Here is the summary of my SARIMA model.  I want to be able to put all the coefficients of ma1 ma2 sma1 sma2 and the coefficients of the exogenous variables into an equation:  I saw post with the same topic but none with the exogeous varaibles part 
$$
Y_t= ?
$$
        ARIMA(0,0,2)(0,0,2)[4] with non-zero mean </p>

<pre><code>Coefficients:
          ma1     ma2     sma1     sma2  intercept  exogeneous_var1  exogeneous_var2
      -0.7756  0.2152  -0.5155  -0.4023     0.0413           -1e-04           0.0028
s.e.   0.1358  0.1782   0.2985   0.2311     0.0068            0e+00           0.0347
      exogeneous_var3  exogeneous_var4
              -0.0054           0.0012
s.e.           0.0035           0.0049

sigma^2 estimated as 0.0001283:  log likelihood=182.13
AIC=-344.26   AICc=-339.68   BIC=-323.49

Training set error measures:
                        ME       RMSE         MAE      MPE     MAPE      MASE        ACF1
Training set -0.0002763522 0.01042586 0.008296043 83.07064 116.4007 0.4182354 0.006248187
</code></pre>
"
"NaN","NaN","229123","<p>Below is the summary of my SARIMA model: </p>

<pre><code>Series: diff(log(data_final_ts)) 
ARIMA(0,0,2)(0,0,2)[4] with non-zero mean 

Coefficients:
          ma1     ma2     sma1     sma2  intercept  exogeneous_var1  exogeneous_var2
      -0.7756  0.2152  -0.5155  -0.4023     0.0413           -1e-04           0.0028
s.e.   0.1358  0.1782   0.2985   0.2311     0.0068            0e+00           0.0347
      exogeneous_var3  exogeneous_var4
              -0.0054           0.0012
s.e.           0.0035           0.0049

sigma^2 estimated as 0.0001283:  log likelihood=182.13
AIC=-344.26   AICc=-339.68   BIC=-323.49

Training set error measures:
                        ME       RMSE         MAE      MPE     MAPE      MASE        ACF1
Training set -0.0002763522 0.01042586 0.008296043 83.07064 116.4007 0.4182354 0.006248187
</code></pre>

<p>I want to know if the following model equation is correct:</p>

<p>$$(1-B)(1-B^4)Y_t=(1+\phi_1 B^4+\phi_2 B^8)(1+\theta_1 B+\theta_2 B^2)e_t+\text{const}+\beta X_t$$</p>

<p>with $\text{const}$ being the mean of $Y_t$ and with $X_t$ including an intercept and a few exogenous variables.</p>
"
"0.142381215461566","0.190535115826549","229721","<p>I have 4 years electrical load data. I split the data into 3 years (75%) training data, 1 year for testing (25%). Also I have the temperature data for each day during the previous period. (The link to the dataset: <a href=""https://drive.google.com/open?id=0B08HdcWBksWcTUxqc1ByOW1UVEU"" rel=""nofollow"">here</a>.) </p>

<p>I want to make use of the temperature data to enhance the forecasting using argument <code>xreg</code> in <code>arima</code> function. </p>

<p>Here is my code:</p>

<pre><code>mydata1&lt;-read.csv(""1st pape/kaggle_data.csv"");
mydata&lt;-ts(mydata1[,2],start = c(2004),frequency = 365)

#split the data into trainData and test data
trainData = window(mydata, end=c(2007))
testData = window(mydata, start=c(2007))
temp&lt;-ts(mydata1[,3],start = c(2004),frequency = 365)

#split the temperature into trainData and test data
trainReg = window(temp, end=c(2007))
testReg = window(temp, start=c(2007))
</code></pre>

<p>Apply ARIMA model without using <code>xreg</code>:</p>

<pre><code>mod_arima &lt;- auto.arima(trainData, ic='aicc', stepwise=FALSE)
summary(mod_arima)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept
      0.9642  -0.2098  -0.2157  -0.1693  24008.122
s.e.  0.0110   0.0322   0.0330   0.0325   1018.007

sigma^2 estimated as 9318421:  log likelihood=-10347.38
AIC=20706.75   AICc=20706.83   BIC=20736.75

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.102332 3045.638 2293.946 -1.519484 9.625694 0.5151126
                    ACF1
Training set 0.004483007

plot(forecast(mod_arima)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2),legend=c(""forecasted data"",""real data""))

y &lt;- msts(trainData, c(7,365)) # multiseasonal ts
x &lt;- msts(trainReg, c(7,365)) # multiseasonal ts

fit &lt;- auto.arima(y, xreg=(fourier(y, K=c(3,30))))
fit_f &lt;- forecast(fit, xreg= fourier(y, K=c(3,30), 365), 365)
plot(fit_f)
</code></pre>

<p>the red line is the actual data, while the blue is the foretasted data. The left plot is appeared before using fourier function, while the right after using it. </p>

<p><a href=""http://i.stack.imgur.com/QxvKC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QxvKC.png"" alt=""enter image description here""></a></p>

<p>Apply ARIMA model using <code>xreg</code>:</p>

<pre><code>mod_arima2 &lt;- auto.arima(trainData ,xreg = trainReg, ic='aicc', stepwise=FALSE)
summary(mod_arima2)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept  trainReg
      0.9709  -0.2403  -0.2108  -0.1609  29984.188  -88.3976
s.e.  0.0094   0.0320   0.0330   0.0321   1468.108   13.1966

sigma^2 estimated as 8955023:  log likelihood=-10325.13
AIC=20664.26   AICc=20664.36   BIC=20699.26

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.030471 2984.292 2267.803 -1.464553 9.529988 0.5092422
                    ACF1
Training set 0.005526977

plot(forecast(mod_arima2,xreg = testReg)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2), legend=c(""forecasted data"",""real data""))

l = (fourier(y, K=c(3,30)))
z = cbind(l,x)
fit2 &lt;- auto.arima(y, xreg=z)
fit_f2 &lt;- forecast(fit, xreg= z, 365)
plot(fit_f2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TgJE5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TgJE5.png"" alt=""enter image description here""></a>
<strong>Questions</strong>:</p>

<ol>
<li>Did I use <code>xreg</code> correctly?</li>
<li>If yes, why is the summary the same without using <code>xreg</code>?</li>
<li>Why are the forecasts far away from the real data?</li>
</ol>
"
"NaN","NaN","232322","<p>I have a little stupid question about <code>forecast</code> package. I want to get information about model formula. For example:</p>

<pre><code>library(forecast)
fit &lt;- auto.arima(WWWusage)
print(fit)
</code></pre>

<p>This produces: </p>

<pre><code>Series: WWWusage 
ARIMA(1,1,1)                    

Coefficients:
         ar1     ma1
      0.6504  0.5256
s.e.  0.0842  0.0896

sigma^2 estimated as 9.995:  log likelihood=-254.15
AIC=514.3   AICc=514.55   BIC=522.08
</code></pre>

<p>So in this case I can see that the model is <code>ARIMA(1,1,1)</code>. But I can't find how to extract this directly.</p>
"
"0.177976519326958","0.173213741660499","234192","<p>I referred to <a href=""https://www.otexts.org/fpp/9/4"" rel=""nofollow"">this link</a> and I have the following questions regarding my data. Let me start by explaining the time series that I am dealing with.</p>

<p>I have <strong>daily</strong> hospital data with various <strong>departments</strong> and numerous <strong>doctors</strong> working in each department. I have several years of data and my forecast horizon is for the next 365 days. My data has weekly and annual seasonality. Moreover I intend to capture the effects of holidays and Sundays in my forecasts. As a result I have not created a hierarchical time series as suggested towards the end of the link(primarily because I am not sure whether we can pass a regressor to it and more so because I do not know how many doctors I end up predicting for in each department). </p>

<p>The reason for this is that some doctors do not have good data(short time series or sparse data). In this case I collect these doctors and aggregate them to form something I call ""OtherDocs"". Typically in <code>DeptXYZ -&gt; Doc1 , Doc2 , Doc3 , Doc4 , Doc5 and Doc6</code> I could end up creating forecasts for <code>DeptXYZ -&gt; Doc1 , Doc3 , Doc4 , Doc6 and OtherDocs</code>. If <code>OtherDocs</code> is still not predictable I generate a naive forecast. In this fashion I created <strong>base forecasts for every level in the hierarchy individually using <code>arima</code> and passing my <code>xreg</code> to it and selecting the best model on the basis of AIC</strong>.</p>

<p>Now, consider this example - </p>

<p><code>Total -&gt; DeptX and DeptY</code></p>

<p><code>DeptX -&gt; DocA and DocB</code></p>

<p><code>DeptY -&gt; Doc1 , Doc2 and Doc3</code></p>

<p>There are cases where <code>DocA</code> has a time series that starts from ""2011-03-11"" and ends on ""2016-09-07"" while <code>DocB</code> has a time series that starts from ""2011-05-17"" and ends on ""2016-09-07"". Generating the base forecasts for <code>DocA</code> and <code>DocB</code> results in the predicted values(<code>fit$mean</code>) being of a time series from ""2016-09-08"" to ""2017-09-07"". As long as the time series refers to the same dates within the Department I believe we are good to go.</p>

<p>In my attempt to reconcile the forecasts from each level I employed the forecasted proportions like so -</p>

<p>$\Largeá»¹_{DocA,365} = \frac{Å·_{DocA,365}*Å·_{DeptX,365}}{(Å·_{DocA,365}+Å·_{DocB,365})*(Å·_{DeptX,365}+Å·_{DeptY,365})}Å·_{Total,365}$</p>

<p><strong>1. Am I doing anything wrong in the above step?</strong></p>

<p><strong>2. Suppose for one moment that the topmost level forecasted values do not capture the low points of data in the case of Holidays and Sundays. Does that intuitively mean that revised forecasts for DocA might not correctly capture the same(being a proportion of $Å·_{Total,365}$)?</strong></p>

<p>Another query I have is to do with the Optimal Combination Approach -</p>

<p>$\Largeá»¹_h = S(Sâ€²S)^{-1}Sâ€²Å·_h$</p>

<p><strong>3. I am unfamiliar with this matrix notation $S'$. Is it the inverse of $S$? Could you shed some light on this? And how do you suggest I calculate the summing up matrix in my case?(Is it absolutely necessary to proceed with the exact knowledge of the number of doctors in each department?)</strong></p>
"
