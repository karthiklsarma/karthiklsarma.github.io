"V1","V2","V3","V4"
"0.127619140222539","0.128247294010644","  5517","<p>The library languageR provides a method (pvals.fnc) to do MCMC significance testing of the fixed effects in a mixed effect regression model fit using lmer.  However, pvals.fnc gives an error when the lmer model includes random slopes.  </p>

<p>Is there a way to do an MCMC hypothesis test of such models?<br>
If so, how?  (To be accepted an answer should have a worked example in R)
If not, is there a conceptual/computation reason why there is no way?</p>

<p>This question might be related to <a href=""http://stats.stackexchange.com/questions/152/is-there-a-standard-method-to-deal-with-label-switching-problem-in-mcmc-estimatio"">this one</a> but I didn't understand the content there well enough to be certain.</p>

<p><strong>Edit 1</strong>: A proof of concept showing that pvals.fnc() still does 'something' with lme4 models, but that it doesn't do anything with random slope models.</p>

<pre><code>library(lme4)
library(languageR)
#the example from pvals.fnc
data(primingHeid) 
# remove extreme outliers
primingHeid = primingHeid[primingHeid$RT &lt; 7.1,]
# fit mixed-effects model
primingHeid.lmer = lmer(RT ~ RTtoPrime * ResponseToPrime + Condition + (1|Subject) + (1|Word), data = primingHeid)
mcmc = pvals.fnc(primingHeid.lmer, nsim=10000, withMCMC=TRUE)
#Subjects are in both conditions...
table(primingHeid$Subject,primingHeid$Condition)
#So I can fit a model that has a random slope of condition by participant
primingHeid.lmer.rs = lmer(RT ~ RTtoPrime * ResponseToPrime + Condition + (1+Condition|Subject) + (1|Word), data = primingHeid)
#However pvals.fnc fails here...
mcmc.rs = pvals.fnc(primingHeid.lmer.rs)
</code></pre>

<p>It says: Error in pvals.fnc(primingHeid.lmer.rs) : 
  MCMC sampling is not yet implemented in lme4_0.999375
  for models with random correlation parameters</p>

<p>Additional question:  Is pvals.fnc performing as expected for random intercept model?  Should the outputs be trusted?</p>
"
"0.0988533609478405","0.0993399267798783","  6927","<p>I'm doing a simulation study which requires bootstrapping estimates obtained from a generalized linear mixed model (actually, the product of two estimates for fixed effects, one from a GLMM and one from an LMM). To do the study well would require about 1000 simulations with 1000 or 1500 bootstrap replications each time. This takes a significant amount of time on my computer (many days). </p>

<p><code>How can I speed up the computation of these fixed effects?</code></p>

<p>To be more specific, I have subjects who are measured repeatedly in three ways, giving rise to variables X, M, and Y, where X and M are continuous and Y is binary. We have two regression equations 
$$M=\alpha_0+\alpha_1X+\epsilon_1$$
$$Y^*=\beta_0+\beta_1X+\beta_2M+\epsilon_2$$
where Y$^*$ is the underlying latent continuous variable for $Y$ and the errors are not iid.<br>
The statistic we want to bootstrap is $\alpha_1\beta_2$. Thus, each bootstrap replication requires fitting an LMM and a GLMM. My R code is (using lme4)</p>

<pre><code>    stat=function(dat){
        a=fixef(lmer(M~X+(X|person),data=dat))[""X""]
        b=fixef(glmer(Y~X+M+(X+M|person),data=dat,family=""binomial""))[""M""]
        return(a*b)
    }</code></pre>

<p>I realize that I get the same estimate for $\alpha_1$ if I just fit it as a linear model, so that saves some time, but the same trick doesn't work for $\beta_2$.</p>

<p>Do I just need to buy a faster computer? :)</p>
"
"0.116499803115499","0.140487871737254"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"0.0988533609478405","0.0993399267798783"," 12709","<p>I have a large dataset and have performed a multilevel regression in Stata, the model is the following:</p>

<p><code>xtmixed dependent independen1 independent2 independent3  independent4    || independendt5:</code></p>

<p>So there is one grouping factor: <code>independent5</code></p>

<p>In R I did the following:</p>

<pre><code>lmer(dependent ~ independent1 + indepdendent2 + independent3 + independent4 + 1 | independent5, REML=TRUE) 
</code></pre>

<p>A few questions: is this identical?</p>

<p>Stata output gives me number of groups, 100, while R gives number of groups as 99.
Furthermore, the variances and standard deviations are not the same.
Also I would like to know how to obtain p values and coefficients from the R ouput.
I have done <code>fixef(model)</code> and <code>ranef(model)</code> but when I do <code>coef(model)</code> it says: <code>Error in coef(model) : unable to align random and fixed effects</code></p>

<p>Also Stata only gives me 1 coefficient for each predictor while the R <code>fixef(model)</code> gives one for each group. </p>

<p>So someone familiar with both Stata and R could help me with this.</p>
"
"0.057073014553535","0.0573539334676404"," 12768","<p>I would like to fit a 3-level hierarchical regression in lmer, however, I don't know how to specify the grouping factor above the second level.
the model would be:</p>

<pre><code>lmer(depedent ~ independent 1 + independent2 + (1|group1)....
</code></pre>

<p>And I would like to specify another group nested within <code>group1</code>.</p>

<p>I've tried <code>(1|group1/group2)</code> but this gives an error message and group1:group2 is an interaction.</p>

<p>I've also tried separately <code>(1|group1) + (1|group2)</code> but i'm not sure if this is correct.<br>
thanks</p>
"
"NaN","NaN"," 12814","<p>I'm trying to run a 3 level hierarchical regression in lmer in R.
The model is specified:</p>

<pre><code>model &lt;- lmer(dependent ~ ind1 + ... + (1|level3/level2), data=data)
</code></pre>

<p>However, when I enter this code, R starts processing it for 20 minutes and doesn't respond. Am I doing something incorrectly?</p>
"
"0.162432647028998","0.163232156274753"," 19772","<p>Can someone please tell me how to have R estimate the break point in a piecewise linear model (as a fixed or random parameter), when I also need to estimate other random effects? </p>

<p>I've included a toy example below that fits a hockey stick / broken stick regression with random slope variances and a random y-intercept variance for a break point of 4. I want to estimate the break point instead of specifying it. It could be a random effect (preferable) or a fixed effect.</p>

<pre><code>library(lme4)
str(sleepstudy)

#Basis functions
bp = 4
b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

#Mixed effects model with break point = 4
(mod &lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy))

#Plot with break point = 4
xyplot(
        Reaction ~ Days | Subject, sleepstudy, aspect = ""xy"",
        layout = c(6,3), type = c(""g"", ""p"", ""r""),
        xlab = ""Days of sleep deprivation"",
        ylab = ""Average reaction time (ms)"",
        panel = function(x,y) {
        panel.points(x,y)
        panel.lmline(x,y)
        pred &lt;- predict(lm(y ~ b1(x, bp) + b2(x, bp)), newdata = data.frame(x = 0:9))
            panel.lines(0:9, pred, lwd=1, lty=2, col=""red"")
        }
    )
</code></pre>

<p>Output:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject) 
   Data: sleepstudy 
  AIC  BIC logLik deviance REMLdev
 1751 1783 -865.6     1744    1731
Random effects:
 Groups   Name         Variance Std.Dev. Corr          
 Subject  (Intercept)  1709.489 41.3460                
          b1(Days, bp)   90.238  9.4994  -0.797        
          b2(Days, bp)   59.348  7.7038   0.118 -0.008 
 Residual               563.030 23.7283                
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)   289.725     10.350  27.994
b1(Days, bp)   -8.781      2.721  -3.227
b2(Days, bp)   11.710      2.184   5.362

Correlation of Fixed Effects:
            (Intr) b1(D,b
b1(Days,bp) -0.761       
b2(Days,bp) -0.054  0.181
</code></pre>

<p><img src=""http://i.stack.imgur.com/HnAfg.jpg"" alt=""Broken stick regression fit to each individual""></p>
"
"NaN","NaN"," 22346","<p>How can one obtain standardized (fixed effect) regression weights from a multilevel regression?</p>

<p>And, as an ""add-on"": What is the easiest way to obtain these standardized weights from a <code>mer</code>-object (from the <code>lmer</code> function of the <code>lme4</code>package in <code>R</code>)?</p>
"
"0.22829205821414","0.229415733870562"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"NaN","NaN"," 27753","<p>Would there be any problem with using principal component analysis (e.g. for reduction of dimensionality) so that principal components scores could be used as predictors in a mixed-model? For non-mixed models this strategy is frequently applied (principal component regression) but I am not sure if it is applicable in the context of mixed-models?</p>

<p>Please see below a dummy example in R:</p>

<pre><code>library(lme4)
USArrests$score &lt;- prcomp(USArrests[,-1], scale = TRUE)$x[,1]
USArrests$group[1:25)] &lt;- ""A""
USArrests$group[26:50] &lt;- ""B""
m1 &lt;- lmer(Murder~1+score+(1|group), data=USArrests)
summary(m1)
</code></pre>
"
"0.151001003081424","0.151744244666721"," 27945","<p>What is the meaning and effect of %in% in a model formula?</p>

<p>It is apparently used for nesting of one variable into another in a variety of analysis (manova, anova, regressions) in a few published articles.</p>

<p>From ?formula, b%in%a is a:b, so why use %in%?<br>
How is a:b nesting?</p>

<p>I am probably mistaken, but my understanding is that nesting b in a should not lead to the same mean square as the interaction of a and b denoted by a:b?</p>

<pre><code>library(lme4)  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>with(sleepstudy, Days%in%Subject)
  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ...  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fit&lt;-aov(data=sleepstudy, Reaction~Days + Days%in%Subject)
anova(fit)


               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
 Days           1 162703  162703  193.23 &lt; 2.2e-16 ***
 Days:Subject  17 269685   15864   18.84 &lt; 2.2e-16 ***
 Residuals    161 135567     842
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
anova(fm1)


      Df Sum Sq Mean Sq F value
 Days  1  29986   29986  45.785
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction~Days + Days%in%Subject + (1|Subject), sleepstudy)
anova(fm1)

Analysis of Variance Table
             Df Sum Sq Mean Sq  F value
Days          1 162703  162703 248.4233
Days:Subject 17  73391    4317   6.5916
</code></pre>
"
"NaN","NaN"," 30822","<p>I am familiar with the debate surrounding Bates's decision to exclude p-values for mixed effects regression coefficients in lmer. However, I operate in a very p-value-focused discipline and am trying to clear something up. </p>

<p>What are the degrees of freedom for group-level predictors in a two-level model? I can use pvals.fnc() to obtain p-values for these coefficients but something tells me it is way overestimating the degrees of freedom for the tests, using the individual-level sample size and not the group-level sample size. Any help would be greatly appreciated. </p>
"
"0.261812016138227","0.263100680279217"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.0988533609478405","0.0993399267798783"," 33151","<p>I have a dataset (<code>data.mrsa</code>) about the MRSA prevalence of elderly in long term care facilities (LTCF) with the following information:</p>

<ul>
<li>mrsa_result: MRSA result of recruited elderly (positive VS negative)</li>
<li>age: residents' age</li>
<li>ltcf: UID for each LTCF (we sampled 30 out of 1000 LTCFs)</li>
<li>ltcf_type: type of LTCF (private VS non-private)</li>
</ul>

<p>I have a multi-level logistic regression model like the one below:</p>

<pre><code>fit2 &lt;- glmer(mrsa_result ~ age + (1|ltcf), family=binomial(""logit""),data=data.mrsa)
</code></pre>

<p>I know I am trying to find out the effect of <code>age</code> on the log-odds of <code>mrsa_result</code>, having the <code>ltcf</code> on the 2nd level gives me a wider CI on the lod-odds.</p>

<p>Now I want to add the <code>ltcf_type</code> into the model, I think this should be a fixed effect, as there can only be private and non-private, but <code>itcf_type</code> should be considered as 2nd level data, right? As this describe the type of LTCF, not the type of elderly.</p>

<p>I am puzzled on where should I put the term <code>ltcf_type</code> into my model, I wonder which of the following lines is correct:</p>

<pre><code>fit2a &lt;- glmer(mrsa_result ~ age + ltcf_type + (1|ltcf), family=binomial(""logit""),data=data.mrsa)
fit2b &lt;- glmer(mrsa_result ~ age + (1|ltcf + ltcf_type), family=binomial(""logit""),data=data.mrsa)
</code></pre>

<p>Thanks.</p>
"
"0.0807134312271262","0.0811107105653813"," 34088","<p>Could you recommend an R package for estimating a (frequentist) multilevel Weibull regression model? </p>

<p>I need to model random intercepts, random slopes, as well as a cross-classified structure. </p>

<p><strong>UPDATE:</strong>
It seems like there is currently no ""easy"" solution for that. I decided to leave it for now by estimating a multilevel discrete hazard model with <code>glmer</code> and a multilevel Cox PH model with <code>coxme</code> (proposed by EddieMcGoldrick). With regards to the latter, I have still to figure out if implementing a cross-classified structure is possible.</p>
"
"0.0988533609478405","0.0993399267798783"," 37714","<p>I would like to make a prediction for a (new) subject to have a certain outcome given the historical data and the model:</p>

<pre><code>glm(outcome ~ age + treatment + history, family=binomial, ...) 
</code></pre>

<p>however in the historical data that will be fitted by the model, I have some sort of repeated measurements on some of the subjects (and I don't know if repeated measures is the appropriate term to be used here, hence using lmer etc is doubtful); example:<br></p>

<pre><code>subject_ID    age    treatment    history    outcome
S_1           33      T_1         H_1        0
S_2           27      T_2         H_2        1
S_2           27      T_3         H_2        1
S_3           56      T_1         H_11       0
etc...
</code></pre>

<p>In this example subject_2 (S_2) has two rows because he had simultaneously two different treatments at the same time. could a logistic regression still be used or should cases like subject_2 be removed from the analysis?</p>
"
"0.057073014553535","0.0573539334676404"," 38541","<p>I used the functions from this <a href=""http://www.math.mcmaster.ca/peter/s4f03/s4f03_0607/rochl.html"" rel=""nofollow"">link</a> for creating ROC curve for logistic regression model. Since the object produced by <code>glmer</code> in <code>lme4</code> package is a S4 object (as far as I know) and the function from the link cannot handle it.</p>

<p>I wonder if there are similar functions for creating ROC curve for multi-level logistic regression model in R.</p>
"
"0.11414602910707","0.114707866935281"," 43664","<p>I would like to use <code>lme4</code> to fit a mixed effects regression and <code>multcomp</code> to compute the pairwise comparisons. I have a complex data set with multiple continuous and categorical predictors, but my question can be demonstrated using the built-in <code>ChickWeight</code> data set as an example:</p>

<pre><code>m &lt;- lmer(weight ~ Time * Diet + (1 | Chick), data=ChickWeight, REML=F)
</code></pre>

<p><code>Time</code> is continuous and <code>Diet</code> is categorical (4 levels) and there are multiple Chicks per diet. All the chicks started out at about the same weight, but their diets (may) affect their growth rate, so the <code>Diet</code> intercepts should be (more or less) the same, but the slopes might be different. I can get the pairwise comparisons for the intercept effect of <code>Diet</code> like this:</p>

<pre><code>summary(glht(m, linfct=mcp(Diet = ""Tukey"")))
</code></pre>

<p>and, indeed, they are not significantly different, but how can I do the analogous test for the <code>Time:Diet</code> effect? Just putting the interaction term into <code>mcp</code> produces an error:</p>

<pre><code>summary(glht(m, linfct=mcp('Time:Diet' = ""Tukey"")))
Error in summary(glht(m, linfct = mcp(`Time:Diet` = ""Tukey""))) : 
  error in evaluating the argument 'object' in selecting a method for function
 'summary': Error in mcp2matrix(model, linfct = linfct) : 
Variable(s) â€˜Time:Dietâ€™ have been specified in â€˜linfctâ€™ but cannot be found in â€˜modelâ€™! 
</code></pre>
"
"0.22829205821414","0.229415733870562"," 46322","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/28936/how-to-compute-significant-interaction-estimates-when-main-effect-is-not-signifi"">How to compute significant interaction estimates when main effect is not significant?</a>  </p>
</blockquote>



<p>I am an applied linguist and I am modelling responses to a vocabulary test taken by second language learners of English; the aim is to test theoretical hypotheses regarding the relationship between the nature of the word and the likelihood of the learnersâ€™ knowing that word. The models I am using to explore the role of explanatory variables are the random item Item Response Theory model LLTM+e (see de Boeck et al. 2011; de Boeck 2008). These are created using the lmer function in the LME4 package in R and treat item and person responses as random. The estimates shown below are effectively like those from a binary logistic regression model, indicating the log-odds of a correct response on a word, given certain properties. Covariates relate to both item and person characteristics. </p>

<p>The nature of my concern is actually a basic regression issue. I have found a significant interaction between ability grouping of the test taker (GRP; with two levels High and Low) and the length of the word in letters (LEN_L). As far as I can see the estimate of the fixed effects for the first model shows that (a) the lower level learners have an overall lower probability of giving a correct answer (b) that LEN_L does not provide a significant explanation across the pattern of responses for the whole test-taker population, and (c) a significant interaction between GRP and LEN_L indicates that the lower ability learners are less likely to give a correct answer for a longer word. This is in keeping with theory. </p>

<p>However, when I model the data without including the main effect for LEN_L, I am not seeing a significant effect for either high or low groups as shown in Model 2. LEN_L does not show as significant if modelled without interaction with grp low (not shown). I feel that I am missing something obvious, but I cannot quite grasp what is happening. And my references have run dry on this particular issue and I am thinking myself around in circles about it.</p>

<p>(NB: in my full model I have many other significant covariates, but this pattern holds true.) 
My query basically regards whether I should use Model 1, including the non-significant main effect, or whether my findings from Model 2 indicate that the finding is a little unstable. Any advice would be much appreciated! (I can post more details if necessary.)
Karen</p>

<p>Model 1 Fixed effects:</p>

<pre><code>              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    0.25244    0.83194   0.303  0.76156    
grp low       -2.09126    0.26406  -7.920 2.38e-15 ***
LEN_L          0.02093    0.13062   0.160  0.87272    
grp low:LEN_L -0.09185    0.03146  -2.919  0.00351 **
</code></pre>

<p>Model 2 Fixed effects:</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.25243    0.83194   0.303    0.762    
grp low        -2.09126    0.26406  -7.920 2.38e-15 ***
grp high:LEN_L  0.02093    0.13062   0.160    0.873    
grp low:LEN_L  -0.07092    0.13202  -0.537    0.591  
</code></pre>

<blockquote>
  <p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A.,
  Tuerlinckx, F. and Partchev, I. (2011) <a href=""http://www.jstatsoft.org/v39/i12/"" rel=""nofollow"">The Estimation of Item
  Response Models with the 'lmer' Function from the lme4 Package in
  R</a>. Journal of Statistical Software (39:12) pp 1-28</p>
</blockquote>
"
"0.189289774928569","0.172928615966519"," 46789","<p>I collected data to find whether the presence or absence of vision, sound, and touch during a task affected the successful completion of that task. However, there were no samples collected where all three senses were absent. So the dependent variable is boolean success but I have a question about how to model the independent variables in a logistic regression.</p>

<p>My initial analysis used a single categorical variable with seven levels representing each combination of senses (seven because there were no cases where all three senses were absent).</p>

<pre><code>summary( glmer( Success ~ Condition + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>When I tried to build a model with the Vision, Sound, and Touch as separate variables, the analysis fails. <a href=""http://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004552.html"" rel=""nofollow"" title=""[R-sig-ME] Structural zeros in lme4"">I believe this is because I have empty cells when including the vision*sound*touch interaction</a> because we did not collect results where all senses were absent.</p>

<pre><code>summary( glmer( Success ~ Vision + Sound + Touch + Vision*Sound + Vision*Touch + 
                Sound*Touch + Vision*Sound*Touch + ( 1 | Participant ), 
                family=binomial, data=trials))
</code></pre>

<p>I followed the suggestion linked above to use the <code>interaction</code> function to drop the unused factor (all three senses absent). However, this seems to create a variable that looks like my original single categorical variable.</p>

<pre><code>senses &lt;- interaction( trials$Vision, trials$Sound, trials$Touch, drop=TRUE )
summary( glmer( Success ~ senses + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>As I try to refine this analysis, is there a way to model the senses as separate variables to make the interaction between these variables clearer? That is, to appropriately model the contribution of vision in the <code>vision</code>, <code>vision*sound</code>, <code>vision*touch</code> and <code>vision*sound*touch</code> conditions. From the initial analysis, the <code>vision*sound*touch</code> interaction is the most interesting.</p>
"
"0.205779680418073","0.206792547967128"," 49832","<p>In a multi-level model, what are the practical and interpretation-related implications of estimating versus not-estimating random effect correlation parameters?  The practical reason for asking this is that in the lmer framework in R, there is no implemented method for estimating p-values via MCMC techniques when estimates are made in the model of the correlations between parameters. </p>

<p>For example, looking at <a href=""http://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet"">this example</a> (portions quoted below), what are the practical implications of M2 versus M3.  Obviously, in one case P5 will not be estimated and in the other it will.</p>

<p>Questions</p>

<ol>
<li>For practical reasons (the desire to get a p-value through MCMC techniques) one might want to fit a model without correlations between random effects even if P5 is substantially non-zero.  If one does this, and then estimates p-values via the MCMC technique, are the results interpretable?  (I know @Ben Bolker has previously mentioned that <a href=""http://stats.stackexchange.com/questions/5517/how-can-one-do-an-mcmc-hypothesis-test-on-a-mixed-effect-regression-model-with-r"">""combining significance testing with MCMC is a little bit incoherent, statistically, although I understand the urge to do so (getting confidence intervals is more supportable)""</a>, so if it will make you sleep better at night pretend I said confidence intervals.)</li>
<li>If one fails to estimate P5, is that the same as asserting that it is 0?</li>
<li>If P5 really is non-zero, then in what way are the estimated values of P1-P4 affected?</li>
<li>If P5 really is non-zero, then in what way are the estimates of error for P1-P4 affected?</li>
<li>If P5 really is non-zero, then in what ways are interpretations of a model failing to include P5 flawed?</li>
</ol>

<p>Borrowing from @Mike Lawrence's answer (those more knowledgeable than I are free to replace this with full model notation, I'm not entirely confident I can do so with reasonable fidelity):</p>

<p>M2:  <code>V1 ~ (1|V2) + V3 + (0+V3|V2)</code> (Estimates P1 - P4)</p>

<p>M3:  <code>V1 ~ (1+V3|V2) + V3</code> (Estimates P1-P5)</p>

<p><em>Parameters that might be estimated:</em></p>

<p><strong>P1</strong>: A global intercept</p>

<p><strong>P2</strong>: Random effect intercepts for V2 (i.e. for each level of V2, that level's intercept's deviation from the global intercept)</p>

<p><strong>P3</strong>: A single global estimate for the effect (slope) of V3</p>

<p><strong>P4</strong>: The effect of V3 within each level of V2 (more specifically, the degree to which the V3 effect within a given level deviates from the global effect of V3), while enforcing a zero correlation between the intercept deviations and V3 effect deviations across levels of V2.</p>

<p><strong>P5</strong>: The correlation between intercept deviations and V3 deviations across levels of V2</p>

<p>Answers derived from a sufficiently large and broad simulation along with accompanying code in R using lmer would be acceptable.</p>
"
"0.161426862454252","0.141943743489417"," 51464","<p>Background: For a project, I am fitting a conditional logit model where I have 5 control cases for every realized case. To do that I use the <code>clogit()</code> function in the package <code>survival</code>. I wanted to graph interactions with the <code>effects</code> package by John Fox et al. It turns out that this package can't handle <code>clogit</code> objects (output of <code>clogit()</code>). </p>

<p>As I believed I remembered that conditional logit were a special case of GLM, I thought the clever/lazy way to get my interaction plots would be to refit the model using a fixed effects glm and then use <code>effect()</code>.
The documentation of <code>clogit</code> seemed to confirm my intuition:</p>

<blockquote>
  <p>It turns out that the logliklihood for a conditional logistic regresson model = loglik from a Cox model with a particular data structure. [...]
  When a well tested Cox model routine is available many packages use this â€˜trickâ€™ rather than writing a new software routine from scratch, and this is what the clogit routine does. </p>
  
  <p>In detail, a stratified Cox model with each case/control group assigned to its own stratum, time set to a constant, status of 1=case 0=control, and using the exact partial likelihood has the same likelihood formula as a conditional logistic regression. The clogit routine creates the necessary dummy variable of times (all 1) and the strata, then calls coxph.</p>
</blockquote>

<p>Based on this description, it seems that I should be able to reproduce the stratification achieved through <code>strata()</code> by using a random intercept for each case/control group with <code>1|group</code> in <code>lmer()</code>. However, when I try, the results of <code>clogit</code> and <code>lmer</code> differ. One thing is that I probably have the wrong likelihood function. I don't really know how to specify this in <code>lmer</code> but more important, I am wondering what else I am missing. </p>

<p>I wonder whether I am completely wrong or somewhat on the right track but missing some pieces? What I would like is to understand what are the difference in terms of how the model is fitted between a conditional logit and a regular one (I understand that might be quite a long answer, so a book reference would be a great start). The my usual references for regression (Gelman and Hill, 2007; Mills 2011) are somewhat silent on the subject.</p>
"
"0.0988533609478405","0.0993399267798783"," 57688","<p>I have a data set from a repeated measures experimental design with different sets of stimuli. I want to know how strong the association between the continuous dependent variable and the continuous predictor is while accounting for the interindividual and interstimulus variation.</p>

<p>My <code>lmer</code> model description in <code>R</code> looks like this</p>

<pre><code>dv ~ pred + (1 |Â subject) + (1 | stimulus) 
</code></pre>

<p><em>Question 1</em>: I understand that it is non-trivial to calculate R squared for random intercept-slope models. Is the same true for random intercept models? Is there an R-implementation of any of the available methods?</p>

<p><em>Question 2</em>: If I z-transform my dependent variable and predictor, will the parameter estimate for the fixed effect reflect the strength of the association such as it would in an ordinary regression model? <strong>Update</strong>: I think I phrased this questions too vaguely. I was wondering if scaling variables would yield standardized regression estimates. I found this question has been answered before in <a href=""http://stats.stackexchange.com/questions/22346/standardized-beta-weights-for-a-multilevel-regression"">another question</a>.</p>

<p><em>Question 3</em>: Is there an entirely different/more appropriate way to quantify the association strength while controlling for the interindividual and interstimulus variation?</p>
"
"NaN","NaN"," 57912","<p>I'm trying to implement a regression model with both fixed and random effects. The package I use is the <code>lme4</code>.</p>

<p>I want to find the relationship between the continuous variables <code>X1</code> and <code>X2</code>, using the categorical variable <code>F1</code> as a fixed effect.</p>

<p>When I try to implement the model <code>model &lt;- lmer(X1 ~ F1 + (1|X2))</code>  I get the following error:</p>

<blockquote>
  <p><code>Number of levels of a grouping factor for the random effects must be less than the number of observations</code></p>
</blockquote>

<p>My understanding for the error is that the <code>X2</code> varies less than the <code>X1</code> (which has some similar values). Why is this a problem?</p>
"
"0.198294261740427","0.214598768819738"," 62000","<p>I've tried to create three models (using R): an intercept only linear regression, a simple mixed effects regression and a by-subject effects mixed effects regression.</p>

<p>An intercept only regression models the grand mean of a response variable plus error. In <code>mtcars</code>, the variable <code>drat</code> may be considered a response variable. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus error?</p>

<pre><code>interceptOnly &lt;- lm(drat ~ 1, data=mtcars)
</code></pre>

<p>A simple mixed effects regression models the grand mean of a response variable, plus subject deviation, plus error. In <code>mtcars</code>, <code>drat</code> may be considered a response variable and <code>cyl</code> a subject deviation. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus the deviation of <code>cyl</code> from <code>drat</code>, plus error?</p>

<pre><code>library(lme4)
simpleMixedEffects &lt;- lmer(drat ~ (1|cyl), data=mtcars)
</code></pre>

<p>A by-subject effects mixed effects regression models the grand mean of a response variable, plus subject deviation, plus condition effect, plus error. In <code>mtcars</code>, <code>drat</code> may be considered a response variable, <code>cyl</code> a subject deviation and <code>wt</code> a condition effect. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus the deviation of <code>cyl</code> from <code>drat</code>, plus the effect of <code>wt</code>, plus error?</p>

<pre><code>bySubjectMixedEffects &lt;- lmer(drat ~ (1|cyl) + wt, data=mtcars)
</code></pre>

<p>I have one further question: </p>

<p>How can I model a by-subject varying condition effect model. This is a mixed effects model which models the grand mean of a response variable, plus group deviation from grand mean (random effect), plus condition effect (fixed effect), plus group deviation from condition effect (random effect), plus error. Could someone provide R code that outputs a ""by-subject varying condition effect model""?</p>
"
"0.228688055143524","0.243332131696144"," 62070","<p>Let's say you have a response variable and an independent variable. Your data is measured across several levels of a categorical independent variable. One approach in analysing these data would be to use linear regression to estimate a slope at each level of the categorical independent variable. This is the approach I've used here, using <code>sleepstudy</code> dataset from the <code>R</code> <code>lme4</code> package (I've stored the betas from each model in <code>lmBetas</code>):</p>

<pre><code>library(lme4); library(plyr); library(ggplot2)
lmBetas &lt;- daply(sleepstudy, .(Subject), function(x) coef(lm(Reaction ~ Days, data=x))[""Days""])
</code></pre>

<p>Another approach in analysing these data would be to use a mixed effects model to estimate slopes for each level of the categorical independent variable, which in this case is <code>Subject</code>. This is the approach I've taken here (I've stored the betas from the model in <code>lmerBetas</code>):</p>

<pre><code>lmerBetas &lt;- coef(lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy))$Subject[,""Days""]
</code></pre>

<p>I have learned that a single mixed effects model, as implemented through the <code>lmer</code> function in R, is more accurate at estimating slopes than a multiple linear regression model applied to multilevel data. This can be demonstrated with this plot of betas from the above models. </p>

<pre><code>betas &lt;- data.frame(method.betas = c(lmerBetas, lmBetas))
betas$method &lt;- c(rep(""lmer"", 18), rep(""lm"", 18))

ggplot(betas, aes(method.betas)) + 
  geom_histogram() +
  facet_grid(method ~ .)
</code></pre>

<p>The top histogram shows betas estimated using linear regression, and the bottom histogram shows betas estimated using mixed effects. You can see betas estimated using linear regression are more widely spread than those estimated through the mixed effects model.</p>

<p><img src=""http://i.stack.imgur.com/YtB22.jpg"" alt=""enter image description here""></p>

<p>So finally, my questions:</p>

<ol>
<li><p>Is a mixed effects model's higher accuracy in betas estimation connected with the fact that it models intercepts and slopes for each level of the categorical independent variable under a joint probability distribution?</p></li>
<li><p>Generally speaking, why is a mixed effects model more accurate in its betas estimation?</p></li>
</ol>
"
"NaN","NaN"," 62483","<p>I've got a conditional logistic regression setup using <code>clogit</code> in <code>R</code> like this: </p>

<pre><code>m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>I wanted to try doing GLMM analysis in <code>R</code>. I'm a little confused on the syntax for the <code>lme4</code> package in <code>R</code>. <code>COVAR[,2]</code> is the matching variable in my data. Can someone explain to me the difference between some of these statements:</p>

<pre><code>m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (x|COVAR[,2]) )
m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (1|COVAR[,2]) )
</code></pre>

<p>Which one is more appropriate? </p>
"
"0.197706721895681","0.182123199096444"," 63330","<p>I'm new to R, and despite trying to read as much as I can about how lmer works in R, I still don't feel like I know how to correctly specify more complex models using the lmer syntax.  </p>

<p>For example, at the moment I want to use lmer for a two-level multilevel model, where the first level is features of a specific course taken by a specific student in a specific semester (with covariates like kind of course, teaching method, etc) and the second level is the student, which also has a number of covariates (e.g. ethnicity, gender, age, gpa at the beginning of the study, and score on a specific instrument).  I also want to assess interactions with the teaching method, for example by including cross-level interactions between the second-level covariates ethnicity, gender, age, gpa, and the instrument score and the first-level covariate teaching method.  </p>

<p>For the sake of readability, let's limit the equation to teaching method, course type, and score.  It seems to me that I have seen three different ways of doing something like what I want to do in R - they are clearly all specifying something somewhat different, but I can't figure out what the underlying math is supposed to be for each case.  So, for example, in different online references and the books on MLM that I have on hand, it seems that one of these three models is recommended for what I want to do:</p>

<pre><code>course_outcome ~ course_type*teaching_method + score*teaching_method + (1|student)
course_outcome ~ course_type*score + (teaching_method|student)
course_outcome ~ course_type*teaching_method + score*teaching_method + (teaching_method|student)
</code></pre>

<p>I'm a little confused about the differences between how lmer interprets each of these three codings - is there any chance that someone who really understands R better could possibly translate this into the basic regression equation(s) structure that would be calculated in each of these three examples?  Or direct me to a reference that might more clearly explain the difference by giving the regression equations for each case?</p>

<p>Thanks for your time!</p>
"
"0.127619140222539","0.128247294010644"," 63566","<p>I have conducted an experiment with multiple (categorical) conditions per subject, and multiple subject measurements.</p>

<p>My data-frame in short: A subject has one property, <code>is_frisian</code> which is either 0 or 1 depending on the subject. And it is tested for two conditions, <code>person</code> and <code>condition</code>. The measurement variable is <code>error</code>, which is either 0 or 1.</p>

<p>My mixed linear model in R is:</p>

<pre><code>&gt; model &lt;- lmer(error~is_frisian*condition*person+(1|subject_id), data=output)
</code></pre>

<p>However, the residuals plot of this model gives an unexpected (?) result.</p>

<p><img src=""http://i.stack.imgur.com/nz2KY.png"" alt=""Residuals lmer model""></p>

<p>I was taught that this plot should show randomly scattered points, and they should be normal distributed. When plotting the density of the fitted and the residuals, it shows a reasonable normal distribution. The lines you can see in the graph, however, how is this to be explained? And is this okay?</p>

<p>The only thing I could come up with is that the graph has two lines due to the categorical variables. The output variable <code>error</code> is either 0 or 1. But I do not have that much knowledge of the underlying system to confirm this. And then again, the lines also seem to have a low negative slope, is this then perhaps a problem?</p>

<p><strong>UPDATE:</strong></p>

<pre><code>&gt; model &lt;- glmer(error~is_frisian*condition*person + (1|subject_id), data=output, family='binomial')
&gt; binnedplot(fitted(model),resid(model))
</code></pre>

<p>Gives the following result:</p>

<p><img src=""http://i.stack.imgur.com/XMXFx.png"" alt=""binned residual plot""></p>

<p><strong>FINAL EDIT:</strong></p>

<p>The density-plots have been omitted, they have nothing to do with satisfaction of assumptions in this case. For a list of assumptions on logistic regression (when using family=binomial), <a href=""https://www.statisticssolutions.com/academic-solutions/resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/"" rel=""nofollow"">see here at statisticssolutions.com</a></p>
"
"0.161426862454252","0.162221421130763"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.213547666489691","0.214598768819738"," 64535","<p>My ecological question is: ""What are the trends in percent coral cover by island and depth across the state of Hawaii from 1999 to 2012?""  </p>

<p>I am trying to analyze this hierarchical data set using R with 10 transects at each depth, 2 depths per site, and site nested in island.</p>

<p>Data structure:</p>

<pre><code>Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.
 WYear: 0-13. It was suggested that I use this factor as a covariate for years.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.
 Year: 1999 â€“ 2012 (14 years)

Dependent variable: PercentCover
</code></pre>

<p>Currently, I am using the <code>lmer</code> function in the <code>lmerTest</code> package and this is the model that I've constructed.</p>

<pre><code>fit1 &lt;- lmer(PercentCover ~ WYear*Island*DepthCat +
             (1+WYear|Island/Site/DepthCat/Transect) + (1|Year), data=Benthic)
</code></pre>

<p>Unfortunately, the data are spotty (i.e., missing data in multiple years for a number of sites) so the model returns <code>[1] ""Asymptotic covariance matrix A is not positive!""</code>, even using arcsin transformed data. I can still run the summary statistics to get results, but I don't feel comfortable with the error message. Perhaps I have not structured the model correctly in terms of organizing the nested factors, but the number of observations for each of the levels in the summary stats seems correct. I tried different and simpler iterations of the model such as:</p>

<pre><code> fit1 &lt;- lmer(PercentCover ~ WYear + Island + DepthCat + (1+WYear|Transect/Site) + 
              (1|Year), data=Benthic)
</code></pre>

<p>which works, but doesn't give me the interaction information and returns a larger AIC suggesting that the model does not fit the data as well.</p>

<p>To deal with all of the missing data, I tried another approach by using the regression slope of percent cover over time as the dependent variable for each site X depth combination.</p>

<pre><code>Data structure:

Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.

Dependent variable: Trend
</code></pre>

<p>I used the following model, but the summary results did not make much sense, even after transforming the data.</p>

<pre><code> fit1&lt;-lmer(Trend ~ Island*DepthCat + (1| Island/Site/DepthCat/Transect), data=Benthic)
</code></pre>

<p>Any suggestions on improving my analytical approach would be appreciated.</p>
"
"0.11414602910707","0.114707866935281"," 65656","<p>My design is as follows.</p>

<ul>
<li>$y$ is Bernoulli response </li>
<li>$x_1$ is a continuous variable </li>
<li>$x_2$ is a categorical (factor) variable with two levels</li>
</ul>

<p>The experiment is completely within subjects. That is, each subject receives each combination of $x_1$ and $x_2$.</p>

<p>This is a repeated measures logistic regression set-up. The experiment will give two ogives for $p(y=1)$ vs $x_1$, one for level1 and one for level2 of $x_2$. The effect of $x_2$ should be that for level2 compared to level1, the ogive should have a shallower slope and increased intercept.</p>

<p>I am struggling with finding the model using <code>lme4</code>. For example,</p>

<pre><code>glmer(y ~ x1*x2 + (1|subject), family=binomial)
</code></pre>

<p>So far as I understand it, the <code>1|subject</code> part says that <code>subject</code> is a random  effect. But I do not see how to specify that $x_1$ and $x_2$ are repeated measures variables. In the end, I want a model that includes a random effect for subjects, and gives estimated slopes and intercepts for level1 and level2.</p>
"
"0.332925918228954","0.344123600805843"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.22829205821414","0.215077250503652"," 68786","<p>I measured a binary response for each subject in 5 different conditions. For each subject and condition, I replicated the experiment 36 times. I thus have 36 binary values per condition per subject.</p>

<p>I am trying to build a model for those data. I suppose a logistic regression is what I'm looking for, and I am working with the <code>lmer</code> package. My aim is to check whether the conditions significantly influence the observed values, so I would have two models:</p>

<pre><code>lmH1&lt;-lmer(value~condition, (random effects), data=dataset, family=binomial)
</code></pre>

<p>and</p>

<pre><code>lmH0&lt;-lmer(value~1, (random effects), data=dataset, family=binomial) 
</code></pre>

<p>By looking at the output from <code>anova(lmH0, lmH1)</code>, I would be able to determine the significance of the effect of my condition.</p>

<p>I am just not sure what to specify as random effect; the models I defined so far are:</p>

<pre><code>lmH1 &lt;- lmer( value ~ condition + ( 1 | subject ), data = dataSet, family = binomial )
</code></pre>

<p>and </p>

<pre><code>lmH2 &lt;- lmer( value ~ condition + ( 1 | subject/condition ), data = dataSet, family = binomial )
</code></pre>

<p>However I am not sure about how lmer handles the replicates, so I don't know whether I should include those replicates in my random effects or not. I could modify the proposed models so that the grouping defined by the random effects refers to a specific binary values instead of a group of binaries values. My new models would then be</p>

<pre><code>lmH1a &lt;- lmer( value ~ condition + ( 1 | subject/(condition:replicate) ), data = dataSet, family = binomial )
</code></pre>

<p>and</p>

<pre><code>lmH2a &lt;- lmer( value ~ condition + ( 1 | subject/condition/replicate ), data = dataSet, family = binomial )
</code></pre>

<p>With those models R returns the warning message <code>Number of levels of a grouping factor for the random effects is equal to n, the number of observations</code>. But the model is still computed.</p>

<p>All 4 models return very similar values for the fixed effects and for the random effects that they have in common (e.g. the subject random effects are very similar for all 4 models and the condition within subject random effects are very similar for <code>lmH2</code> and <code>lmH2a</code>).</p>

<p>How can I check which random effect structure is the most appropriate for my design and collected data?</p>
"
"0.171219043660605","0.172061800402921"," 70227","<p>I want to model the infection rates in bees based on weather conditions. The weather variables are rolling means for different time periods and durations. Dependent data is infection levels gathered in March and the independent variables are the weather aggregates (e.g. from 30 day period from Jan1-Jan30, 90 day period from Dec1-Feb28), a few thousands of them and highly correlated.</p>

<p>PCA techniques did not work since the infections are not so strongly related to weather. I have also tried Bayesian Model Averaging and Boosted Regression Trees, since variables could be selected based on variable importance they calculate.</p>

<p>But since, my data is longitudinal and my apiaries have a fixed location, I think mixed-models are a good choice. Is there a way to do variable selection based on mixed-models?</p>

<p>What I have done now is to<br>
1. run <code>glmer</code> for each of the independent variables separately,<br>
2. remove those variable whose p-values for fixed-effect estimates are below 0.05 (not sure if this is a right thing - if the estimate for a variable is not significant, that variable being the only one in the model, it is right to drop that variable, is it?)<br>
3. from the variables that are left over, test for correlation between the variables<br>
4. remove the variables that are highly correlated, giving preference to the variable that has the lowest AIC.  </p>

<p>Or should I at this stage, not worry about p-values of Intercepts and only focus on AIC (or BIC)? since some of the variables have high p-values but AICs lower than than those with low p-values. </p>

<p>I have tried reading up a lot, and there is no one fool-proof solution for variable selection, but would like to know if there is anything inherently wrong with my method. As I am not a statistician, equations often look like beautiful Arabic calligraphy and there lies my dead-end.</p>
"
"0.127619140222539","0.128247294010644"," 71070","<p>I'm modeling the amount of organic content in bird bones (a percentage) in two different conditions and also over two time periods. The design is repeated measures - observations in both conditions and time periods come from the same bone (divided into pieces). I want to test the hypotheses: 1) there is no difference in E(Y) across conditions, 2) there is no difference in E(Y) across time, 3) there is no difference in difference of E(Y) (i.e., time*condition interaction). I've tried the following (here with dummy data):</p>

<pre><code>set.seed(6753)
dat &lt;- data.frame(
    id = rep(1:15, each = 4),
    pc.organic = rnorm(60, 0.11, 0.055),
    condition = factor(rep(c(""raw"", ""advanced""), times = 30)),
    year = factor(rep(c(1, 1, 2, 2), times = 15))
    )

library(lme4)
fm1 &lt;- lmer(pc.organic ~ condition * year + (1 | id), data = dat)
summary(fm1)
</code></pre>

<p>This, I think, is an appropriate model to account for the non-independence of observations in the repeated measures design. I'm unsure, however, whether this is ok given the nature of the response variable. The response varies between about .01 (1%) and .2 (20%). It is bounded at zero (obviously), but also at 40% (this is the maximum amount of organic content in any bone - 60% is inorganic). Another option would be to use 40% as the denominator when I define the percentage, thus, the previous values would be .025 and .5 respectively. However, this would still leave the response bounded between 0 and 1.</p>

<p>I've read about beta regression and also about using a logit transformation to linearize the data. If possible, I'd  like to avoid going down these paths, as other researchers in my field are not familiar with these methods. Any suggestions are most welcome. </p>
"
"0.161426862454252","0.162221421130763"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.11414602910707","0.0860309002014606"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.127619140222539","0.128247294010644"," 85555","<p>I would like to run a lagged random effects regression.</p>

<p>The data is from an experiment in which participants were assigned to groups of five and participated in an interactive game for 20 rounds.</p>

<p>Participants could exchange something during the experiment, which is the dependent variable.</p>

<p>Now I would like to predict/explain, how much participant received from other participants based on the behaviour of previous rounds.</p>

<p>Since the data is clustered on three levels: subject, group and time (rounds), I am a little bit lost how to correctly formulate the model.</p>

<p>I am currently using the lme4 package in R. 
I transformed the dependent variable to a 0/1 (nothing received/something received) variable, due to high skewness, so I would need to specify a multilevel logistic model.</p>

<p>So far, I specified and ran the following models:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * subject | group), family = binomial)
</code></pre>

<p>and:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * group | subject), family = binomial)
</code></pre>

<p>*predictors are on subject-level.</p>

<p>I get similar (although not the same) estimates for both models, however in model1, z-values are much higher (and therefore p-values much lower).</p>

<p>Can someone help me on that?</p>

<p>What I want to know is; Can previous behaviour (that is behaviour from round x-1 etc.) predict how much a participant received in round x.
But control/acknowledge that participants are clustered in groups and that behaviour is correlated over time (rounds).</p>
"
"0.235318067376639","0.236476325731729"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.180480718921108","0.181369062527503"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.11414602910707","0.114707866935281"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.0988533609478405","0.0993399267798783"," 88036","<p>I am new to R, and don't see these questions answered anywhere in documentation (though I could be wrong).</p>

<ol>
<li><p>I am using the following nomenclature to run my mixed-effects logistic regression, based on instructions from another site:  </p>

<p><code>output &lt;- glmer(DV ~ IV1 + IV2 + (1 | RE), family = binomial, nAGQ = 10)</code><br>
RE is a factor with several levels.</p>

<p>This works. But I'm wondering why it's necessary to use the <code>(1 | RE)</code> syntax instead of just <code>DV~IV1+IV2 | RE</code>.</p></li>
<li><p>I am running two mixed effects logistic regressions. On one of them I can view the random effects intercepts using <code>ranef()</code>. But I get all 0s when I run ranef on the output of the other one. Both regressions/data are ostensibly the same. What do all 0s for the random effects intercepts mean?</p></li>
</ol>
"
"0.057073014553535","0.0573539334676404"," 88388","<p>I would like to use cross-validation to test how predictive my mixed-effect logistic regression model is (model run with glmer). Is there an easy way to do this using a package in R? I've only seen cross validation functions in R for use with linear models.</p>
"
"0.180480718921108","0.181369062527503"," 90799","<p>I am having trouble training a model for nested data about house prices. Lets say my data looks like following:</p>

<pre><code>  logPrice bedCount bathCount                city
 0.6517920        4       2-3        Redwood City
 0.4402192        1       1-2 South San Francisco
 0.5922396        2       1-2           San Mateo
 0.4606918        3       1-2 South San Francisco
 0.7592523    5plus       3-4           San Mateo
 0.4710397        1       1-2        Redwood City
</code></pre>

<p><code>bedCount</code>, <code>bathCount</code> and <code>city</code> are factors.</p>

<p>As a baseline, I trained a simple linear model ignoreing nested structure of the data (houses are nested within cities).</p>

<pre><code>lm.model = lm(formula = logPrice ~ 1 + bedCount + bathCount + city)
</code></pre>

<p>which corresponds to following assumption:</p>

<p><code>logPrice</code>$_i = \beta_0 + \beta_1\cdot$ <code>bedCount</code>$_i + \beta_2\cdot$ <code>bathCount</code>$_i + \beta_{3,j[i]}\cdot I$(<code>city</code>$_{j[i]}) + \epsilon_i$</p>

<p>where </p>

<p>$\epsilon_i \sim N(0, \sigma^2_{logPrice})$ and $I$(<code>city</code>$_{j[i]}$) is the indicator function for city of the $i^{th}$ house (which is 1).</p>

<p>Now, I trained a 2-level hierarchical model:</p>

<pre><code>lmer.model = lmer(formula = logPrice ~ 1 + bedCount + bathCount + (1 | city))
</code></pre>

<p>which corresponds to the following assumption:</p>

<p><code>logPrice</code>$_i = \beta_0 + \beta_1\cdot$ <code>bedCount</code>$_i + \beta_2\cdot$ <code>bathCount</code>$_i + \beta_{3,j[i]}\cdot I$(<code>city</code>$_{j[i]}) + \epsilon_i$</p>

<p>where
$\epsilon_i \sim N(0, \sigma^2_{logPrice})$ and $\beta_{3,j} \sim N(0, \sigma^2_{\beta_3})$</p>

<p>Now, on the training data, <code>lm.model</code> gives me lesser average RMSE than <code>lmer.model</code> which shouldn't happen because linear regression is a special case of multilevel linear regression (I didn't care to check average RMSE on test data because that on training data itself should be lower for 2nd model than that for 1st model). In fact, my data has multiple levels (houses nested within subdivisions, which are nested within zipcodes, which in turn are nested within cities) and the performance gets worse as I add more and more levels to the model (i.e. model with random effect <code>(1 | subdivision)</code> does worse than that with random effect <code>(1 | zipcode) + (1 | zipcode:subdivision)</code>, which in turn does worse than a model with random effect <code>(1 | city) + (1 | city:zipcode) + (1 | city:zipcode:subdivision)</code>).</p>

<p>What am I missing?</p>
"
"0.228688055143524","0.243332131696144"," 92737","<p>In my data, I have two treatment conditions with repeated measures for each subject. I would like to run a mixed logistic regression separately for each of my two conditions where my binary outcome DV (dependent variable) is regressed on my IV (independent variable) and also have a random slope and intercept fitted for each subject.</p>

<p>So, I run the following:</p>

<pre><code>modelT0 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D0, family = binomial)
modelT1 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D1, family = binomial)
</code></pre>

<p>In the above, D0 and D1 are data sets restricted to treatment conditions 0 and 1, respectively. What I would like to do is compare the estimated fixed effects coefficient on IV across conditions to see if it significantly changes. To do this, I pool D0 and D1 into a single data set, D, and create a treatment indicator that takes value 0 in D0 and 1 in D1. I then run:</p>

<pre><code>model &lt;- glmer(DV ~ IV + treatment + treatment:IV + (1 + treatment|subject:treatment) 
               + (0 + IV + treatment:IV|subject:treatment), data = D, family = binomial)
</code></pre>

<p>I should be able to look at the fixed effects coefficient on treatment:IV to get my answer, but the issue is that for whatever combination of random effects I seem to specify, the coefficients from the pooled regression are slightly different from the regressions specified by treatment. So for instance, the fixed effect coefficient on treatment:IV plus the one on IV in model is not equal to the coefficient on IV in model1.</p>

<p>Any idea about what I might be doing wrong or how to answer the question I have? Thanks!</p>

<p>EDIT:</p>

<p>As per Henrik's suggestion, I'm copying the random effects output of the models below:</p>

<p>summary(modelT0):</p>

<pre><code>    Random effects:
    Groups    Name        Variance  Std.Dev. 
    subject   (Intercept) 1.412e-07 0.0003758
    subject.1 IV          1.650e+00 1.2844341
</code></pre>

<p>summary(modelT1):</p>

<pre><code>    Random effects:
    Groups    Name        Variance Std.Dev.
    subject   (Intercept) 0.00378  0.06148 
    subject.1 IV          0.26398  0.51379 
</code></pre>

<p>summary(model):</p>

<pre><code>    Random effects:
    Groups              Name         Variance  Std.Dev. Corr 
    subject.treatment   (Intercept)  0.0005554 0.02357       
                        treatment    0.0066042 0.08127  -0.88
    subject.treatment.1 IV           1.6500112 1.28453       
                        IV:treatment 1.0278663 1.01384  -0.93
</code></pre>
"
"0.0807134312271262","0.0811107105653813"," 94619","<p>Stata allows for fixed effects and random effects specification of the logistic regression through the <code>xtlogit fe</code> and <code>xtlogit re</code> commands accordingly. I was wondering what are the equivalent commands for these specifications in R.</p>

<p>The only similar specification I am aware of is the mixed effects logistic regression </p>

<pre><code>&gt; mymixedlogit &lt;- glmer(y ~ x1 + x2 +  x3 + (1 | x4), data = d, family = binomial)
</code></pre>

<p>but I am not sure whether this maps to any of the aforementioned commands.</p>
"
"0.0988533609478405","0.0993399267798783"," 97834","<p>I ran a mixed model using lme4::glmer for a logistic regression and consistently got these warning messages. I noticed there are still regular results even so, but are they accurate estimates?</p>

<pre><code>    &gt; glmm.ms1&lt;-glmer(as.formula(paste(paste(y[1], x, sep=""~""), mix[1], sep=""+"")),
    +             data=rtf2,control=glmerControl(optimizer=""bobyqa"",
    +             optCtrl=list(maxfun=100000),family=binomial)
Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.8766 (tol = 0.001)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; coef(summary(glmm.ms1))
                       Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)           1.810e+00  6.558e-01   2.760464 5.772e-03
lepidays             -3.340e+00  2.770e-01 -12.059620 1.726e-33
cldaysbirth          -1.555e+00  5.224e-01  -2.975934 2.921e-03
rotaarm              -2.057e-01  3.209e-01  -0.641102 5.215e-01
cldaysbirth2         -3.072e-01  2.955e-01  -1.039510 2.986e-01
bfh2                 -1.043e+01  1.160e+03  -0.008996 9.928e-01
bfh3                  4.653e-01  4.806e-01   0.968103 3.330e-01
bfh4                  2.547e-01  4.994e-01   0.509966 6.101e-01
bfh5                  3.744e-01  9.926e-01   0.377213 7.060e-01
ruuska               -1.020e-01  5.928e-02  -1.720396 8.536e-02
genderMale           -4.008e-01  2.645e-01  -1.515453 1.297e-01
epiexlbf              6.078e-04  2.796e-03   0.217391 8.279e-01
haz.epi              -7.211e-02  1.373e-01  -0.525039 5.996e-01
cldaysbirth:rotaarm   6.928e-01  4.771e-01   1.452148 1.465e-01
rotaarm:cldaysbirth2  5.181e-01  3.352e-01   1.545527 1.222e-01
Warning messages:
1: In vcov.merMod(object, use.hessian = use.hessian) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
2: In vcov.merMod(object, correlation = correlation, sigm = sig) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
</code></pre>

<p>Due to data's sensitivity, I can't post the whole process for generating same messages, but I would like to know how to handle these warnings. I don't think it's suitable to keep my eye blind here.</p>
"
"0.127619140222539","0.128247294010644","103340","<p>I have a question about SAS and R. For a research, I used a longitudinal data and I initially used SAS (<code>GLIMMIX</code>) and then I analyzed the data with R (<code>glmer</code>) programming. There are differences between p-values of SAS and R. I expected that regression coefficient and standard error could be different for R and SAS. But there are differences for p value for some variables, which are significant in R, are not significant in SAS. </p>

<p>My R model and SAS model are respectively :</p>

<pre><code>#R
m3.glmm &lt;- glmer(y ~ timebefore + timeafter + x1 + x2 +...+ x11 +      
                     (1+timebefore+timeafter|id), 
                 data=data, family=binomial(link=""logit""), nAGQ=3)

#SAS
proc glimmix data=data METHOD=QUAD(QPOINTS=3) NOCLPRINT ;
  class id x2 x3 x4 x5;
  model y(event='1')=timebefore timeafter x1 x2 x3 x4 x5 
        x6 x7  x8 x9 x10 x11 /solution CL link = logit dist = binary;
  random intercept timebefore timeafter/subject = id GCORR SOLUTION;
run;
</code></pre>

<p>Eg: variable ""x1""(defined as age) was significant (p val= 0.04) in SAS but not in R (p val=0.1). But others were similar. It means that significant variables in SAS are found significant in R, or insignificant variables in SAS are insignificant in R. </p>

<p>Does anybody know about the differences?</p>
"
"0.171219043660605","0.172061800402921","106360","<p>I am running a binomial mixed effects logistic regression in R using <code>glmer</code> for a sociolinguistics project. I was asked to used deviation (effect) coding. From what I gather, in deviation coding the last level in a factor is assigned -1, because this is the level that is never compared to the other levels within that variable. Is it possible to obtain the <code>Estimate</code> (<code>Exp(B)</code> value) for the last level as well by using function <code>relevel</code>? I need to report the estimates for all the levels.</p>

<p>For example, my model has the independent variable called <strong>Orthography</strong> with four levels (<code>s</code>, <code>sh</code>, <code>s1</code>, <code>sh1</code>). The dependent variable is <strong>produced sibilant</strong>. In deviation coding the fourth level (<code>sh1</code>) will not be compared to the other three levels, and estimates will be available for the first three (<code>s</code>, <code>sh</code>, <code>s1</code>). The intercept is the mean of the means of all four levels (<code>s + sh + s1 + sh1 / 4</code>). I am interested in obtaining the estimate for the last level (<code>sh1</code>) as well. Does anyone know how to get that? Do I have to rerun the model by changing levels? If so, does anyone know how to do that? I have been unsuccessful with using function <code>relevel</code> to do this.</p>

<p>I have other terms in my model as well:  </p>

<ul>
<li>following segment, which has two levels (<code>vowel</code>, <code>consonant</code>), </li>
<li>position of sibilant in word (<code>initial</code>, <code>medial</code>, <code>final</code>), </li>
<li>grammatical function (<code>noun</code>, <code>verb</code>, <code>adjectives</code>), and </li>
<li>language of instruction (<code>English</code>, <code>Gujarati</code>).</li>
</ul>

<p>This is the code for my model:</p>

<pre><code>model.final_si = glmer(prod_sib ~ orthography + foll_segment + word_position + 
                                  grammatical_func + language_instruction + 
                                  (1|participant) + (1|item), 
                       family=""binomial"",data=data)
</code></pre>
"
"0.139799763738599","0.140487871737254","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.242140293681379","0.229813679935247","110033","<p>I am running a post-hoc analysis on the data collected during an experiment in which 15 unique stimuli were presented to participants. Having run a least squares regression using the lm() function in R I have found significant results for a subset of the data including 90 observations from 6 participants with two continuous variables and their interaction.</p>

<p>Taking advice from an article by Judd, Westfall &amp; Kenny (2012) I attempted to use a combination of the lmer() function found in the lme4 package in combination with a Kenward-Roger approximation through the KRmodcomp() function in the pbkrtest package (see the appendix in the article) in order to control for random effects:</p>

<pre><code>lmer(Prediction_Difference_Scale~Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale + (1|Unique_ID) + (Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale|Block),data=Data)
</code></pre>

<p>The first variable after the DV is the fixed effect, the second variable in parentheses indicates that the intercept is random with respect the unique stimuli (Unique_ID) and the third variable in parentheses indicates that both the intercept and the Condition slopes are random with respect to participant (Block) and that a covariance between the effects should be estimated. </p>

<p>When running the lmer() function I get the following error message:</p>

<pre><code>Error in checkNlevels(reTrms$flist, n = n, control) : 
  number of levels of each grouping factor must be &lt; number of observations
</code></pre>

<p>This is obviously because the number of observations equal the number of unique stimuli.</p>

<p>The function works when excluding the (1|Unique_ID) random  effect, which if I understand correctly is the same as carrying out a 'by stimulus' analysis. However, the authors warn against this by stating: ""Conceptually, a significant by-participant result suggests that experimental results would be likely to replicate for a new set of participants, but only using the same sample of stimuli. A significant by-stimulus result, on the other hand, suggests that experimental results would be likely to replicate for a new set of stimuli, but only using the same sample of participants. However, it is a fallacy to assume that the conjunction of these two results implies that a result would be likely to replicate with simultaneously new samples of both participants and stimuli.""</p>

<p>I would like to control for the random effects of both stimuli and participants, but I am unsure how to proceed?</p>

<p>The article can be accessed here: <a href=""http://jakewestfall.org/publications/JWK.pdf"" rel=""nofollow"">http://jakewestfall.org/publications/JWK.pdf</a></p>

<hr>

<p>To clarify the question regarding the 15 unique stimuli, this is 15 unique stimuli per participant, meaning the sample of 90 observations consists of 6 participants. The stimuli for all of the 90 observations are unique however.</p>

<p>I suppose what my question boils down to is whether there is even a need to include the (1|Unique_ID) 'variable' in the function formula as there is no error dependence between any of the stimuli?</p>
"
"0.302181715792653","0.293197735804187","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.189950474232067","0.206792547967128","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.235318067376639","0.236476325731729","122336","<p>I have a problem with coding of a 2-level categorical predictor variable in R, and subsequently using it as a random slope in lmer().</p>

<p>I can keep the factor as numeric, coded using the treatment coding:</p>

<pre><code>&gt; unique (b$multi)
[1] 0 1
</code></pre>

<p>Running lmer() using a dataset coded in this way yields:</p>

<pre><code>&gt; l1 = glmer(OK ~ multi + (0 + multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
&gt; summary(l1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4806.5   4838.9  -2398.3   4796.5     4792 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-7.8294 -0.5560 -0.1548  0.5623 14.3342 

Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 1.84379  1.3579  
 item    (Intercept) 2.40306  1.5502  
 item.1  multi       0.04145  0.2036  
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>Above there is only one random slope related to <code>multi</code>. However, something very different happens when I convert the variable into a factor:</p>

<pre><code>&gt; b$multi = as.factor(b$multi)
&gt; levels (b$multi)
[1] ""0"" ""1""
</code></pre>

<p>When I fit a model using <code>multi</code> as a random slope variable:</p>

<blockquote>
  <p>l2 = glmer(OK ~ multi + (0+multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
      Warning message:
      In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
        Model failed to converge: degenerate  Hessian with 1 negative eigenvalues</p>
</blockquote>

<p>... the model fails to converge and I get a very different random effects structure:</p>

<pre><code>&gt; summary(l2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4807.8   4853.1  -2396.9   4793.8     4790 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-8.3636 -0.5608 -0.1540  0.5627 15.2515 

Random effects:
 Groups  Name        Variance Std.Dev. Corr
 subject (Intercept) 1.8375   1.3555       
 item    (Intercept) 0.9659   0.9828       
 item.1  multi0      1.5973   1.2638       
         multi1      1.0224   1.0111   1.00
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>The number of parameters in the model clearly change (reflected by the change in AIC, etc.), and I get two random slopes. </p>

<p>My question is which way of coding the categorical variable is better? Intuition tells me that it is the first one, but I have seen recommendations for both ways of coding in various tutorials and classes about running GLMMs in R and this is why it baffles me. Both types of the predictor variable work identically in ordinary regression using lm().</p>
"
"0.0807134312271262","0.0811107105653813","128653","<p>I'd like to do a regression analysis with interactions, my data has two levels (school classes and pupils). My variables are: Predictor = dummy variable on Level 1, dependent Variable = metric on level 1, moderator variable = metric, Level 2</p>

<p>I used the following command, but I am not sure if that right:</p>

<p><code>model &lt;- lmer(DV ~ 1 + IV1 + IV2 + IV1 * IV2 + (1|cluster), daten)</code></p>

<p>Can you tell me if that's correct?</p>
"
"0.057073014553535","0.0573539334676404","130476","<p>This model is a simple linear regression:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ wt, mtcars)
</code></pre>

<p>And this model adds <code>cyl</code> as a random effect:</p>

<pre><code>library(lme4)
mtcars_mixed_effects &lt;- lmer(mpg ~ wt + (1 | cyl), mtcars)
</code></pre>

<p>Is there a way to test whether adding <code>cyl</code> as random effect is worthwhile? I tried this but it threw an error:</p>

<pre><code>anova(mtcars_mixed_effects, mtcars_lm)
</code></pre>

<p>(please disregard the fact that <code>cyl</code> only has three groups, I'm just using one of R's built in datasets to make question reprodicible).</p>
"
"0.189289774928569","0.190221477563171","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.057073014553535","0.0573539334676404","135292","<p>How do you fit a multilevel regression model in R where you want to include a group-level regression? I think that the answer is using STAN, but I've been trying to use lmer() from the lme4 package.</p>

<p>Say you have data on radon levels in houses within counties. You have a house-level predictor ""Dust"" and a county-level measure of ""Uranium"" and ""Plutonium.""</p>

<p>County level: $ \alpha_j \sim N\big (\gamma_0 + \gamma_1 plut_j + \gamma_2 uran_j, \sigma^2_{alpha}\big)$</p>

<p>House level: $radon_i = \alpha_{j[i]} + \beta dust_i + \epsilon_i$</p>

<p>How would you fit a model at the county level, like the following:
county ~ uranium + plutonium
and then incorporate the new county coefficients in the first model?</p>
"
"0.180480718921108","0.181369062527503","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"0.191570456899202","0.207322107215682","139861","<p>I'm examining the effect of income (categorized into quintiles) on a response variable during different years (from 2003 to 2014). I adjust for some other covariates and have repeated measurements on the same individual.</p>

<p><strong>The problem:</strong> To obtain the effect of income on the response variable each year, I include an interaction term between year and income quintile. This gives me implausible results, since those who are rich tend to have higher adjusted values, which is unlikely. So I redid the analysis, fitting one separate regression model each year, and that gave me plausible results; i.e the rich had lower adjusted values.</p>

<p><em>Shouldn't the two methods yield somewhat similar results?</em></p>

<p>My calculations:</p>

<pre><code>    &gt; # START
    &gt; 
    &gt; # Converting the year variable into a factor
    &gt; data$year &lt;- as.factor(data$year)
    &gt; 
    &gt; # Fitting a model with random effects for person
&gt; require(lme4)
&gt; fit  &lt;- lmer(response ~ income*year + sex + age + education + biomarker + (1 | id), data=data)
&gt; # Using lsmeans to predict means (95% confidence intervals)
&gt; require(lsmeans)
&gt; results1 &lt;- lsmeans(fit, ~ income*year)
&gt; summary(results1)
 income     year   lsmean        SE       df lower.CL upper.CL
 Quintile 1 2003 64.95472 0.2826723 190216.2 64.40069 65.50875
 Quintile 2 2003 65.49504 0.2716893 189962.8 64.96254 66.02755
 Quintile 3 2003 65.39961 0.2713204 189648.3 64.86782 65.93139
 Quintile 4 2003 65.51872 0.2715941 189734.3 64.98640 66.05103
 Quintile 5 2003 65.47502 0.2744592 190425.5 64.93709 66.01296
 Quintile 1 2004 64.03440 0.2541402 191982.7 63.53630 64.53251
 Quintile 2 2004 65.04364 0.2472738 191720.0 64.55899 65.52829
 Quintile 3 2004 64.98069 0.2425866 191644.4 64.50523 65.45616
 Quintile 4 2004 65.14219 0.2459067 191477.1 64.66022 65.62416
 Quintile 5 2004 65.25515 0.2496510 191947.6 64.76584 65.74446
 Quintile 1 2005 63.62453 0.2345767 192988.5 63.16476 64.08429
 Quintile 2 2005 64.18179 0.2294655 192853.2 63.73205 64.63154
 Quintile 3 2005 64.00250 0.2308264 192458.4 63.55008 64.45491
 Quintile 4 2005 63.79620 0.2276547 192775.7 63.35000 64.24240
 Quintile 5 2005 64.70116 0.2344171 193212.2 64.24171 65.16062
 Quintile 1 2006 64.17463 0.2228096 193482.8 63.73793 64.61134
 Quintile 2 2006 64.37025 0.2158588 193425.3 63.94717 64.79333
 Quintile 3 2006 64.30762 0.2141169 193366.8 63.88795 64.72728
 Quintile 4 2006 64.38315 0.2168376 193329.6 63.95815 64.80814
 Quintile 5 2006 64.22019 0.2198772 193526.5 63.78923 64.65114
 Quintile 1 2007 63.84188 0.2185725 193602.5 63.41349 64.27028
 Quintile 2 2007 63.92346 0.2104634 193527.6 63.51096 64.33597
 Quintile 3 2007 63.67954 0.2094212 193511.7 63.26908 64.09000
 Quintile 4 2007 64.08718 0.2103912 193478.5 63.67482 64.49955
 Quintile 5 2007 64.03627 0.2113704 193649.9 63.62199 64.45055
 Quintile 1 2008 64.65587 0.2043770 193372.1 64.25529 65.05644
 Quintile 2 2008 64.30141 0.1957655 193499.2 63.91772 64.68511
 Quintile 3 2008 65.04454 0.1955247 193589.4 64.66131 65.42776
 Quintile 4 2008 64.94073 0.1947715 193488.8 64.55898 65.32247
 Quintile 5 2008 65.00096 0.1979129 193119.2 64.61305 65.38886
 Quintile 1 2009 64.74611 0.1941592 191979.5 64.36556 65.12666
 Quintile 2 2009 64.68663 0.1872505 192801.9 64.31962 65.05363
 Quintile 3 2009 64.89048 0.1858611 192919.0 64.52620 65.25476
 Quintile 4 2009 65.19469 0.1848586 192734.9 64.83238 65.55701
 Quintile 5 2009 65.18344 0.1896058 192025.8 64.81182 65.55506
 Quintile 1 2010 64.99407 0.1863001 188874.1 64.62893 65.35922
 Quintile 2 2010 65.14159 0.1758624 190272.3 64.79691 65.48628
 Quintile 3 2010 65.21003 0.1740553 190655.7 64.86889 65.55118
 Quintile 4 2010 65.65492 0.1731845 190157.8 65.31548 65.99435
 Quintile 5 2010 65.42802 0.1774762 189156.3 65.08017 65.77587
 Quintile 1 2011 65.77711 0.1807193 179035.8 65.42290 66.13131
 Quintile 2 2011 65.81373 0.1719076 186787.1 65.47679 66.15066
 Quintile 3 2011 66.25649 0.1692650 187174.9 65.92473 66.58824
 Quintile 4 2011 66.24046 0.1672252 185968.9 65.91271 66.56822
 Quintile 5 2011 66.31895 0.1717513 184631.5 65.98232 66.65558
 Quintile 1 2012 66.45412 0.1815487 178604.9 66.09829 66.80995
 Quintile 2 2012 66.39743 0.1691640 185466.1 66.06587 66.72899
 Quintile 3 2012 66.61760 0.1674829 185934.4 66.28934 66.94586
 Quintile 4 2012 66.69909 0.1672601 185774.9 66.37126 67.02692
 Quintile 5 2012 66.74211 0.1697808 183081.3 66.40934 67.07487
 Quintile 1 2013 66.20088 0.1804738 177511.5 65.84716 66.55461
 Quintile 2 2013 66.05285 0.1710873 185354.9 65.71752 66.38817
 Quintile 3 2013 65.57061 0.1667456 185103.9 65.24379 65.89742
 Quintile 4 2013 65.96563 0.1669031 184335.3 65.63851 66.29276
 Quintile 5 2013 66.19121 0.1723801 183746.1 65.85335 66.52907
 Quintile 1 2014 65.37137 0.3060358 191891.8 64.77155 65.97120
 Quintile 2 2014 66.37503 0.2882805 188873.8 65.81001 66.94006
 Quintile 3 2014 65.49851 0.2876551 188265.5 64.93471 66.06231
 Quintile 4 2014 66.08503 0.2867954 188729.4 65.52291 66.64714
 Quintile 5 2014 65.98435 0.2901786 189169.5 65.41561 66.55309

Results are averaged over the levels of: sex, education 
Confidence level used: 0.95 
&gt; # As you can see above, the richest (quintile 5) has higher values of a harmful biomarker, this is not likelybiomarker, this is not likely
    &gt; 
    &gt; # Redo the analysis by stratification, i.e one regression each year (I'll give 4 examples, years 2005, 2008, 2010 and 2014)
    &gt; fit2005  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2005"",])
        &gt; fit2008  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2008"",])
    &gt; fit2010  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2010"",])
        &gt; fit2014  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2014"",])
    &gt; 
    &gt; lsmeans(fit2005, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 64.57677 0.3644627 7104.87 63.86232 65.29123
     Quintile 2 63.65426 0.3715929 7114.04 62.92582 64.38269
     Quintile 3 63.97948 0.3773103 7119.28 63.23984 64.71912
     Quintile 4 63.46368 0.3727073 7117.62 62.73306 64.19429
     Quintile 5 63.57556 0.3853513 7117.67 62.82016 64.33096

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2008, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 64.27986 0.3268165 9880.81 63.63924 64.92049
     Quintile 2 65.07827 0.3288624 9866.53 64.43363 65.72291
     Quintile 3 64.98781 0.3265577 9859.02 64.34769 65.62793
     Quintile 4 64.89630 0.3305190 9868.49 64.24842 65.54419
     Quintile 5 63.66509 0.3428045 9898.35 62.99313 64.33706

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2010, ""income"")
     income       lsmean        SE       df lower.CL upper.CL
     Quintile 1 65.39530 0.2961521 11897.22 64.81480 65.97581
     Quintile 2 65.61791 0.2911321 11892.26 65.04724 66.18858
     Quintile 3 65.48423 0.2947050 11892.60 64.90656 66.06190
     Quintile 4 65.14303 0.2914349 11925.62 64.57177 65.71429
     Quintile 5 63.89145 0.3030998 11935.15 63.29733 64.48558

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2014, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 67.05015 0.4888236 4523.42 66.09182 68.00849
     Quintile 2 65.41100 0.4801968 4523.32 64.46958 66.35242
     Quintile 3 65.35658 0.4740396 4525.63 64.42723 66.28592
     Quintile 4 65.03556 0.4793119 4526.56 64.09587 65.97524
     Quintile 5 64.94690 0.5001898 4529.88 63.96628 65.92751

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
</code></pre>

<p>I must have misunderstood something (important) here...</p>

<p>Any advice?</p>
"
"0.180480718921108","0.181369062527503","140183","<p>The age old question of comparing sums of squares (SS) between programs has reared its ugly head again. </p>

<p>I am trying to replicate output in SPSS, that was computed using Type 3 Sums of Squares, in R. </p>

<p>I understand that with multiple regressions, there are several ways to get Type 3 SS in R (to match Type 3 output from SPSS). </p>

<p>However, I am running a mixed model using aov (which uses Type 1 SS) and even when I try all the ""usual"" fixes,"" my estimates don't match the Type 3 SS output from SPSS. </p>

<p>First of all, when I run the SPSS syntax using ""/METHOD=SSTYPE(1)"" the results match those I get using this code:</p>

<pre><code>  mymodel&lt;-aov(data=longdat,  DV ~ 1 + Task + Cue + Compatibility + Cue:Task + Compatibility:Task + Cue:Compatibility + Cue:Compatibility:Task + Error(subject/Cue/Compatibility/Cue*Compatibility))
  summary(mymodel)
</code></pre>

<p>So I know the analyses are the same when they use Type 1 SS. </p>

<p>However, when I use:</p>

<pre><code> options(contrasts = c(""contr.sum"",""contr.poly""))
 tt&lt;-lm(DV ~ 1 + Task + Cue + Compatibility + Cue:Task  + Compatibility:Task + Cue:Compatibility + Cue:Compatibility:Task + 1/subject/Cue/Compatibility    /(Cue*Compatibility), data=longdat)
 drop1(tt, ~., test=""F"")
</code></pre>

<p>The results do not match the SPSS Type 3 output. </p>

<p>In attempts to get matching output, I have also tried the Anova function (which can give Type 3 SS)</p>

<pre><code>  Anova(mymodel, type=3, test.statistic=""F"") 
</code></pre>

<p>but I get this error <code>""Error in terms.formula(formula, data = data) : 'data' argument is of the wrong type.""</code></p>

<p>I have also tried using <code>lmer</code>.</p>

<p>Can someone help me get Type 3 Sums of Squares for a mixed model in R?</p>

<p>Thank you! </p>
"
"0.221475828119189","0.222565953629863","140972","<p>Iâ€™m using a maximal logistic regression model to analyze some data. I would like to keep using this technique if possible, just include more data in the model. The main data Iâ€™m looking at is counts of a particular behavior over items in a sequence, and I would like my analysis to also include data from a post-experiment questionnaire (8 items, 1-9 Likert scored). Hereâ€™s some info about my data:</p>

<pre><code>'data.frame':
Pair          : Factor w/ 36 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
SpeakerID     : Factor w/ 72 levels ""10A"",""10B"",""11A"",..: 21 22 21 22 22 21 22 21 21 22 ...
Speaker       : Factor w/ 2 levels ""A"",""B"": 1 2 1 2 2 1 2 1 1 2 ...
Condition1     : Factor w/ 4 levels ""ANTI"",""CONTROL"",..: 1 1 1 1 1 1 1 1 1 1 ...
..- attr(*, ""contrasts"")= num [1:4, 1:3] -0.333 1 -0.333 -0.333 0.25 ...
.. ..- attr(*, ""dimnames"")=List of 2
.. .. .. : chr  ""ANTI"" ""CONTROL"" ""IN"" ""OUT""
.. .. .. : NULL
Condition2         : Factor w/ 3 levels ""0"",""90"",""180"": 2 3 1 1 2 1 1 2 2 3 ...
..- attr(*, ""contrasts"")= num [1:3, 1:2] 0 -0.5 0.5 -0.5 0.25 0.25
.. ..- attr(*, ""dimnames"")=List of 2
.. .. ..$ : chr  ""0"" ""90"" ""180""
    .. .. ..$ : NULL
Item         : Factor w/ 16 levels ""MAP1"",""MAP10"",..: 1 9 10 11 12 13 14 15 16 2 ...
Foo       : num  0.847 1.099 1.946 -1.099 -0.452 ...
wtsFoo          : num  0.952 0.889 2.286 0.889 0.468 ...
Close      : num  -1.798 0.202 -1.798 0.202 0.202 ...
Similar    : num  0.505 0.505 0.505 0.505 0.505 ...
Like       : num  -0.833 0.167 -0.833 0.167 0.167 ...
Task1Hard   : num  -0.89 4.11 -0.89 4.11 4.11 ...
Task2Hard: num  -1.02 2.98 -1.02 2.98 2.98 ...
</code></pre>

<p>My analysis is based on this guide to empirical logit analyses:
<a href=""http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html"" rel=""nofollow"">http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html</a>
So far, so good. In my regression model, Iâ€™m testing the fixed effects of Condition1 (4 levels) and Condition2 (3 levels) on Foo (the behavior, expressed as a proportion converted into empirical logit form, see link for how and why). Pair, Pair:Subject (Subject nested within Pair) and Item are included as random effects. Condition1 is between-subjects/pairs and Condition2 is within-subjects. Hereâ€™s the model Iâ€™m using in R:</p>

<pre><code>model &lt;- lmer(Foo ~ Condition1*Condition2 + (1+Condition1 | Pair) 
+ (1+Condition1 | Pair:Subject) + (1+Condition2 | Item), weights=1/wtsFoo, data)
</code></pre>

<p>This all works fine, but hereâ€™s where it gets fun. Where should the questionnaire data go? </p>

<p>Bad idea #1: Each participant has one score for each questionnaire item, so each questionnaire item type should be included as a fixed effect, so that Foo can be predicted by any of the variables discovered in the post-experiment questionnaire (things like social closeness and task difficulty). This is a terrible idea because the questionnaire items are NOT independent variables from Condition1 and Condition2, and if I include them as fixed effects it will introduce a mess of multicollinearity and will just be flat-out wrong.</p>

<p>Bad idea #2: Analyzing the questionnaire data separately. Not such a bad idea, just one that my committee doesnâ€™t like. </p>

<p>Less bad ideas: please suggest a model that allows me to observe the effects of Condition1 and Condition2 on questionnaire items (Close, Similar, Like, Task1Hard, Task2Hard) AND allows me to observe the effects of questionnaire items on Foo. Failing that, explain to me why the only good thing to do is analyze the questionnaire separately from the observation data.</p>

<p>I've read around on Stackexchange and I haven't seen this particular problem covered, although some answers come close to looking useful, I don't yet have the R or stats chops to make them work for me. If I've missed something obvious, please clue me.</p>
"
"0.139799763738599","0.140487871737254","142914","<p>I want to control for a nuisance covariate in a linear model. Since the covariate interacts significantly with one of the fixed factors, the homogeneity of regression slopes assumption is violated for an ANCOVA approach. My understanding is that this can be handled in a multi-level, or mixed effects, model.</p>

<p>My data:</p>

<pre><code>&gt; str(seeds)
'data.frame':   186 obs. of  4 variables:
 $ fixed.A : Factor w/ 31 levels ""A1"",""A2"",""A3"",..: 7 7 7 7 7 7 10 10 10 10 ...
     $ fixed.B : Factor w/ 2 levels ""Y"",""N"": 2 2 2 1 1 1 2 2 2 1 ...
 $ cov     : num  10.3 10.5 11 12.8 12.9 ...
     $ response: num  10.8 11 11.1 14.7 15.3 ...
</code></pre>

<p>The covariate <code>cov</code> has a significant interaction with <code>fixed.A</code>. To fit a random intercept and slope for <code>cov</code> conditioned on <code>fixed.A</code>, it seems an approach using <code>lmer</code> (in lme4) might be:</p>

<pre><code>&gt; lmer(response ~ fixed.A*fixed.B + (1 + cov | fixed.A), data = seeds)
</code></pre>

<p>I'm aware that this is not the usual approach in <code>lmer</code> since grouping factors tend to be random and therefore don't also appear as fixed factors in the model formula. However, since I'm interested in the interaction between my two fixed factors, I don't see how else to proceed. Any help is greatly appreciated.</p>
"
"0.180480718921108","0.181369062527503","143165","<p>I am attempting to build a model for the following dataset:</p>

<p>Level 1 Observations (Product-Level): 89000<br>
Level 2 Observations (""BU_SBU"" Department-Level): 135</p>

<p>Unfortunately I cannot share a sample of my data, since it is confidential.</p>

<p>The dependent variable in the model is a percentage (Delivery Reliability, 0-100%). Fixed effects include roughly 20 variables at level 1 and 5 variables at level 2. The only random effects are the intercepts at level 2. Having run the regression, I have a number of questions regarding the violation of model assumptions which I cannot answer myself:</p>

<ol>
<li><p>Constant variance of residuals: The graphic shows that there appears to be an upper- and lower-bound of the residuals. My guess is that this is due to the limitation of the dependent variable. But do the upper- and lower-bounds shown in the graphic actually indicate a violation of model assumptions? I have also run a GLMER model with a <code>binomial(logit)</code>-link but this did not resolve the issue. The diagnostic plots look almost identical in all three cases.</p></li>
<li><p>Distribution of residuals: Is there a way to compute confidence intervals for residual QQ-plots of LMER models? And is it possible to compute heteroskedasticity-robust standard errors via the lme4-package?</p></li>
<li><p>Normal distribution of level-2 intercepts: The level-2 intercepts do not appear to be Normally distributed. Is this an issue and if so, how can I resolve it?</p></li>
</ol>

<p>I would greatly appreciate, if someone could help me at least with some of these questions. I am currently stuck and was not able to find any resources that provide answers. I am also grateful for recommendations to helpful literature.  </p>

<p><img src=""http://i.stack.imgur.com/WK1NL.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/ZU4x5.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/GUgmq.png"" alt=""enter image description here""></p>

<p><a href=""https://www.dropbox.com/sh/dji9f95kg88i8kg/AACjQCTA4lW86aYJNa8CHaUha?dl=0"" rel=""nofollow"">Dropbox to Diagnostic Plots</a></p>
"
"0.127619140222539","0.128247294010644","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"0.11414602910707","0.114707866935281","153113","<p>How can I calculate adjusted means for a regression model with fixed and random effects? I'd like to calculate the adjusted means for a lme regression with this formula</p>

<pre><code>mymodel &lt;- myDV ~ experiment_condition + (1|subject_aptitude) + (1|subjects_teacher/subjects_class) 
</code></pre>

<p>where myDV is the dependent variable, experiment_condition is an independent fixed effect and subject_aptitude (participants past class average) and subjects_teacher/subjects_class (classroom nested within teacher) are random effects</p>

<p>The ultimate goal here is to visualize this data with adjusted means because the raw means (before the random effects variance is removed) do not accurately depict the results of the LMER </p>
"
"0.171219043660605","0.172061800402921","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.197706721895681","0.198679853559757","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.197706721895681","0.198679853559757","153846","<p>I am familiar with linear regression models, but I am in the process of learning about linear mixed effects models.</p>

<p>My data consists of measurements for each month for a set of subjects over a long period of time (~15 years). The subjects and time frames are partially crossed - subjects do not appear for each time point. I also have a number of covariates measured at the per date per subject level, and a single boolean variable indicating a whether a <code>count</code> is before or after a particular time point. The point of this particular model is to measure whether or not a particular event (occurring at the ""mid date"") had an effect on the <code>count</code> variable. Due to the partially crossed, longitudinal nature of my data and the general discontinuity of my data over time, I don't believe that simple paired t-tests can properly answer this question. My data frame is as follows:</p>

<p><code>head</code></p>

<pre><code>   subject_id date_monthly count subject_join_date covar1_per_subject_per_date subject_group after_mid_date_bool covar2_per_subject_per_date covar3_per_subject_per_date
1:          0   2013-05-01     3        2011-07-01                           1     afteronly                TRUE                    22.33333                    195.7986
2:          0   2013-04-01     1        2011-07-01                           1     afteronly                TRUE                    21.33333                    194.7986
3:          0   2013-02-01    19        2011-07-01                           1     afteronly                TRUE                    19.36806                    192.8333
4:          0   2013-12-01     3        2011-07-01                           1     afteronly                TRUE                    29.46806                    202.9333
5:          0   2013-10-01     4        2011-07-01                           1     afteronly                TRUE                    27.43333                    200.8986
</code></pre>

<hr>

<p><code>tail</code></p>

<pre><code>       subject_id date_monthly count subject_join_date covar1_per_subject_per_date subject_group after_mid_date_bool covar2_per_subject_per_date covar3_per_subject_per_date
22407:       6911   2013-08-01     3        2011-08-01                           1     afteronly                TRUE                    24.36667                    198.8653
22408:       6911   2013-07-01     1        2011-08-01                           1     afteronly                TRUE                    23.33333                    197.8319
22409:       6911   2013-06-01     1        2011-08-01                           1     afteronly                TRUE                    22.33333                    196.8319
22410:       6931   2009-05-01     7        2009-05-01                           1    beforeonly               FALSE                     0.00000                    147.0986
22411:        238   2013-09-01     1        2012-10-01                           1     afteronly                TRUE                    11.16667                    199.8986
</code></pre>

<p><code>count</code> is the response I am looking to model.</p>

<p>I've read through all of Bates' lme4 paper, but I am still confused as to how to specify the random effects part of my model.</p>

<p>My attempt at a model specification is:</p>

<pre><code>lmer(log(count) ~ covar1_per_subject_per_date + covar2_per_subject_per_date + 
covar3_per_subject_per_date + after_mid_date_bool + 
subject_group + subject_join_date + (1|subject_id) + (1|date_monthly),
data=df, REML=F)
</code></pre>

<p>Which ""works"" (no errors from <code>lmer</code>). However, my primary question is:</p>

<p>Is this the correct specification for a mixed effects model with random, uncorrelated intercepts for <code>subject_id</code> and <code>date_monthly</code>? Correct here means that we model independent fixed effects for each of the fixed effects specified in the model, accounting for multiple trials of the same subject over time with subjects not appearing at every time point.</p>

<p>A secondary but related question is:</p>

<p>Have I organized my data frame in the proper way? My worry is that the <code>after_mid_date</code> column may be specified improperly.</p>

<p>I apologize if this is long-winded or too-specific of a question. My intention of providing my exact data is to be as clear as possible with my question.</p>
"
"0.189289774928569","0.190221477563171","155040","<p>I am struggling with interpreting coefficients from a multiple regression analysis with multiple categorical (dummy) variables. I am running a linear mixed model with biodiversity (<code>LnS_Add1</code>) as independent variable, and several continuous and categorical dependent variables.</p>

<p>With a single categorical/dummy variable (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3</code>; where <code>AREA_AM_2.5</code> is continuous and <code>System_Type3</code> is categorical with 3 levels, i.e. <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em>) this is pretty straightforward. In this case the intercept represents the mean of the reference dummy variable (e.g. <em>Arable</em>) and the mean of the 2nd and 3rd levels <em>Grassland</em> and <em>Orchard</em> can be calculated manually by adding intercept to the slope coefficient.</p>

<pre><code>globmod1 &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + 
     (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1)
</code></pre>

<p>Which returns</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
(Intercept)            0.3585534  0.1238470   2.895
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Grassland -0.5227684  0.0915722  -5.709
System_Type3Orchard   -0.4057969  0.5477567  -0.741
</code></pre>

<p>To get a summary output that shows the means of both <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em> in R I suppress the intercept by adding a -1 (or +0) to the model.</p>

<pre><code>globmod1.coef &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 -1 +
                   (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1.coef)
</code></pre>

<p>Which returns:</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Arable     0.3585534  0.1238470   2.895
System_Type3Grassland -0.1642149  0.1341851  -1.224
System_Type3Orchard   -0.0472434  0.5457304  -0.087
</code></pre>

<p>But what do I do if I have multiple categorical variables (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + Habitat2</code>; where Habitat2 is a categorical variable with 3 levels, i.e. <em>Farm aggregated</em>, <em>Outside field</em>, and <em>Within field</em>)?. Now the intercept represents the mean of the reference level of a combination of <code>System_Type3</code> and <code>Habitat2</code> (e.g. all data in arable systems and measured at farm aggregate level). But what I am interested in are the means for the different levels of each of my 2 categorical variables, holding everything else constant.</p>

<p>How do I create a summary table that contains means of all levels of all categorical variables in my model? The -1 command doesnt help me anymore, as it removes the intercept but the intercept now represents a mean of 2 reference dummy variables. I am only interested here in the fixed effect estimates, not in any hypothesis testing.</p>
"
"0.11414602910707","0.114707866935281","156519","<p>I've been reading through Gelmans book: Data Analysis Using Regression and Multilevel/Hierarchical Models trying to learn more about how to implement  hierarchal models. I have a dataset that I think is appropriate for this type of modeling however I want to get some other opinions. Basically the data I have is structured like this:</p>

<pre><code>BRAND       YEAR         Y           X1          X2         X3
company_1   2012    0.638042396 0.226787359 0.192104136 0.929220784
company_2   2012    0.983422117 0.308550049 0.527779594 0.106629747
company_n   2012    0.209276388 0.700314863 0.741787081 0.491451885
company_1   2013    0.833955686 0.735844101 0.518474158 0.117670754
company_2   2013    0.480778935 0.290739025 0.156177295 0.212643611
company_n   2013    0.69922326  0.188574282 0.448743735 0.609844836
company_1   2014    0.942147995 0.176500074 0.820207708 0.388313924
company_2   2014    0.503095705 0.987218933 0.834039587 0.42661805
company_n   2014    0.46569344  0.310693712 0.852694246 0.17574502 
</code></pre>

<p>where I have about 15 different companies for each year. My thought was to have a model like this: </p>

<pre><code>lmer(Y ~  X1 + X2 + X3 + (1 | BRAND) , h.data)
</code></pre>

<p>where I have a varying intercept for each company. So my question here is whether or not it makes sense to use a hierarchal model and if my data fits the archetype of hierarchal data? Also should I be including YEAR into the model somehow?</p>
"
"0.057073014553535","0.0573539334676404","156804","<p>I have estimated a mixed-effects logistic regression with glmer
and want to draw a bootstrapped confidence-region for the mean predicted probability for two subgroups of the sample.</p>

<p>I have a $1000 \times 2$ Matrix $X$ containing the bootstrapped mean predicted probabilities for the two groups.
One could now compute the empirical covariance matrix $S$ and draw a
circle around the means using the metric induced by $S^{-1}$,
i.e. drawing a confidence ellipsoid based on normality-assumption.</p>

<p>Are there any widely used alternatives to this approach that do not imply distributional assumptions? </p>

<p>skeletor</p>
"
"0.139799763738599","0.140487871737254","157186","<p>Let's say you're trying to fit a model to a dataset that includes categorical variables, group (A or B) and treatment (1, 2, 3 or 4).</p>

<p>In R, your model formula would be <strong>DV ~ group * treatment</strong> (DV stands for dependent variable) and your model output will look like this:</p>

<pre><code>(intercept)                 [...]
groupB                      [...]
treatment2                  [...]
treatment3                  [...]
treatment4                  [...]
groupB:treatment2           [...]
groupB:treatment3           [...]
groupB:treatment4           [...]
</code></pre>

<p>My question is how to interpret this kind of output. Below is what I believe is right for the interpretation of the main effects, and what puzzles me about the interaction parameters.</p>

<pre><code>(intercept)
</code></pre>

<p>This the reference value, i.e. for treatment 1 in group A.</p>

<pre><code>groupB
</code></pre>

<p>This is the difference between group A and group B for treatment 1 only.</p>

<pre><code>treatment2
</code></pre>

<p>This is the difference between treatment 1 and treatment 2, within group A only. It indeed still refers to the intercept value.
Same logic for the two following estimates (""treatment3"" and ""treatment4"").</p>

<pre><code>groupB:treatment2
</code></pre>

<p>Here is where I get puzzled. Is this testing if <strong>the difference between treatment1 and treatment2 is the same in groupB compared to groupA</strong>, or is it testing if if <strong>the difference between groupA and groupB is the same for treatment1 compared to treatment2</strong>.</p>

<p>I thought this question would be very basic, but I went through several R books with no luck and found inconsistent answers on here (see <a href=""http://stats.stackexchange.com/questions/87412/how-to-interpret-2-way-and-3-way-interaction-in-lmer"">http://stats.stackexchange.com/questions/87412/how-to-interpret-2-way-and-3-way-interaction-in-lmer</a> for support for the first idea and <a href=""http://stats.stackexchange.com/questions/33709/interpreting-the-regression-output-from-a-mixed-model-when-interactions-between"">http://stats.stackexchange.com/questions/33709/interpreting-the-regression-output-from-a-mixed-model-when-interactions-between</a> for the other way).</p>

<p>If that matters, I'm working with the glmer function of the lme4 package.</p>

<p>Thanks!</p>
"
"0.197706721895681","0.198679853559757","157851","<p>In the <code>lmer</code> function within <code>lme4</code> in <code>R</code> there is a call for constructing a model matrix of random effects, $Z$, as explained <a href=""http://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf"" rel=""nofollow"">here</a>, pages 7 - 9.</p>

<p>Calculating $Z$ entails KhatriRao and/or Kronecker products of two matrices, $J_i$ and $X_i$.  </p>

<p>The matrix $J_i$ is a mouthful: ""Indicator matrix of grouping factor indices"", but it seems to be a sparse matrix with dummy coding to select which unit (for example, subjects in repetitive measurements) corresponding to higher hierarchical levels are ""on"" for any observation. The $X_i$ matrix seems to act as a selector of measurements in the lower hierarchical level, so that the combination of both ""selectors"" would yield a matrix, $Z_i$ of the form illustrated in the paper via the following example:</p>

<pre><code>(f&lt;-gl(3,2))

[1] 1 1 2 2 3 3
Levels: 1 2 3

(Ji&lt;-t(as(f,Class=""sparseMatrix"")))

6 x 3 sparse Matrix of class ""dgCMatrix""
     1 2 3
[1,] 1 . .
[2,] 1 . .
[3,] . 1 .
[4,] . 1 .
[5,] . . 1
[6,] . . 1

(Xi&lt;-cbind(1,rep.int(c(-1,1),3L)))
     [,1] [,2]
[1,]    1   -1
[2,]    1    1
[3,]    1   -1
[4,]    1    1
[5,]    1   -1
[6,]    1    1
</code></pre>

<p>Transposing each of these matrices, and performing a Khatri-Rao multiplication:</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp;. &amp;. &amp;. &amp;.\\.&amp;.&amp;1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;1&amp;1 \end{smallmatrix}\right]\ast \left[\begin{smallmatrix}\,\,\,\,1 &amp; 1 &amp;\,\,\,\,1 &amp;1 &amp;\,\,\,\,1 &amp;1\\-1&amp;1&amp;-1&amp;1&amp;-1&amp;1 \end{smallmatrix}\right]=
\left[\begin{smallmatrix}\,\,1 &amp; 1 &amp;.&amp;.&amp;.&amp;.\\\,\,\,\,-1 &amp;1&amp;.&amp;.&amp;.&amp;.\\ .&amp;.&amp;\,\,\,\,\,1 &amp;1&amp;.&amp;.\\.&amp;.&amp;\,\,-1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;\,\,\,1&amp;1\\.&amp;.&amp;.&amp;.&amp;-1&amp;1 \end{smallmatrix}\right]$</p>

<p>But $Z_i$ is the transpose of it:</p>

<pre><code>(Zi&lt;-t(KhatriRao(t(Ji),t(Xi))))

6 x 6 sparse Matrix of class ""dgCMatrix""

[1,] 1 -1 .  . .  .
[2,] 1  1 .  . .  .
[3,] .  . 1 -1 .  .
[4,] .  . 1  1 .  .
[5,] .  . .  . 1 -1
[6,] .  . .  . 1  1
</code></pre>

<p>It turns out that the authors make use of the database <code>sleepstudy</code> in <code>lme4</code>, but don't really elaborate on the design matrices as they apply to this particular study. So I'm trying to understand how the made up code in the paper reproduced above would translate into the more meaningful <code>sleepstudy</code> example.</p>

<p>For visual simplicity I have reduced the data set to just three subjects - ""309"", ""330"" and ""371"":</p>

<pre><code>require(lme4)
sleepstudy &lt;- sleepstudy[sleepstudy$Subject %in% c(309, 330, 371), ]
rownames(sleepstudy) &lt;- NULL
</code></pre>

<p>Each individual would exhibit a very different intercept and slope should a simple OLS regression be considered individually, suggesting the need for a mixed-effect model with the higher hierarchy or unit level corresponding to the subjects:</p>

<pre><code>    par(bg = 'peachpuff')
    plot(1,type=""n"", xlim=c(0, 12), ylim=c(200, 360),
             xlab='Days', ylab='Reaction')
    for (i in sleepstudy$Subject){
                fit&lt;-lm(Reaction ~ Days, sleepstudy[sleepstudy$Subject==i,])
            lines(predict(fit), col=i, lwd=3)
            text(x=11, y=predict(fit, data.frame(Days=9)), cex=0.6,labels=i)
        }
</code></pre>

<p><img src=""http://i.stack.imgur.com/opwVvm.png"" alt=""enter image description here""></p>

<p>The mixed-effect regression call is:</p>

<pre><code>fm1&lt;-lmer(Reaction~Days+(Days|Subject), sleepstudy)
</code></pre>

<p>And the matrix extracted from the function yields the following:</p>

<pre><code>parsedFormula&lt;-lFormula(formula= Reaction~Days+(Days|Subject),data= sleepstudy)
parsedFormula$reTrms

$Ztlist
    $Ztlist$`Days | Subject`
6 x 12 sparse Matrix of class ""dgCMatrix""

309 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . . . . . . . . . . . .
309 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . . . . . . . . . . . .
330 . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . .
330 . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . .
371 . . . . . . . . . . . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1
371 . . . . . . . . . . . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9
</code></pre>

<p>This seems right, but if it is, what is linear algebra behind it? I understand the rows of <code>1</code>'s being the selection of individuals like. For instance, subject <code>309</code> is on for the baseline + nine observations, so it gets four <code>1</code>'s and so forth. The second part is clearly the actual measurement: <code>0</code> for baseline, <code>1</code> for the first day of sleep deprivation, etc.</p>

<p><strong>But what are the actual</strong> $J_i$ <strong>and</strong> $X_i$ <strong>matrices and the corresponding</strong> $Z_i= (J_i^{T}âˆ—X_i^{T})^âŠ¤$ <strong>or</strong> $Z_i= (J_i^{T}\otimes X_i^{T})^âŠ¤$, <strong>whichever is pertinent?</strong></p>

<p>Here is a possibility,</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.  &amp;. &amp;. &amp;. &amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.\\
.&amp;.&amp;.&amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.\\&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1\end{smallmatrix}\right]\ast \left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1&amp;1&amp;1&amp;1 &amp; 1 &amp; 1 &amp; 1\\0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right]=$</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\0 &amp; 1 &amp; 2 &amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\ &amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0 &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\\&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right] $</p>

<p>The problem is that it is not the transposed as the <code>lmer</code> function seems to call for, and still is unclear what the rules are to create $X_i$.</p>
"
"0.0988533609478405","0.0993399267798783","159735","<p>I have two factors that are fully crossed, the levels of the factor are each coded 0 and 1. I am running a regression testing for one main effect and one interaction. The following is my logistic regression formula:</p>

<pre><code>m1=glmer(y~1+A+A:B+(1|Participants)+(1|Word),data=data, family = ""binomial"")
</code></pre>

<p>I am wondering if this is acceptable (only testing for one main effect and an interaction), and also why I am getting two interaction terms in my output:</p>

<pre><code>Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.18740    0.21600  -0.868  0.38561   
A1           0.74546    0.28399   2.625  0.00867 **
A0:B1        0.01537    0.28244   0.054  0.95662   
A1:B1        0.15884    0.28650   0.554  0.57929   
</code></pre>
"
"0.151001003081424","0.151744244666721","162164","<p>I'm a masters student trying to model changes in behaviour, heart rate variation and faecal cortisol as welfare measures in sheep over the course of 22 days. Days -4 to -1 is used as baseline, day 0 has no measures as they were in surgery and days 1 to 17 are what I'm interested in. </p>

<p>For behaviour my supervisor told me not to look at each day as a factor individually because there is bound to be some false positives for significance, but to divide them into blocks of four or five to look at larger changes. This worked pretty well to model as a mixed model in lme4. She also told me to get someone else to check my stats as she doesn't know R. The university only started teaching it last year.</p>

<p>This other person told me I should plot my changes against time as a regression and look at my plots for explanations which I can do with lmer(). But he also said I should probably model against days squared because that would account better for the curves on my plots (there's a nice dip in most of my plots on day 1-3 or so indicating pain/stress). This seemed to make sense at the time but now I'm confused. Why would it make sense to model day squared rather than day alone? Modelling frequency of behaviours against type of behaviour and day gives the best fit model and perfectly distributed residuals. Using ""DaySq"" rather than ""Day"" gives slightly different values but almost identical residuals.</p>

<p>This is the start of my dataset for behaviour. It goes down to about 500 lines.</p>

<pre><code>      Behaviour Sheep Day DaySq Block Observed Frequency
1      Standing     2  -4    16     0       49  71.01449
2         Lying     2  -4    16     0       12  17.39130
3        Eating     2  -4    16     0       36  52.17391
4    Ruminating     2  -4    16     0       16  23.18841
5 Moving in pen     2  -4    16     0        0   0.00000
6  XDisturbance     2  -4    16     0        9  13.04348
</code></pre>

<p>This is the model I used:</p>

<pre><code>lmer(data = behdata2, Frequency ~ Behaviour * Day + (1|Sheep))
</code></pre>
"
"0.057073014553535","0.0573539334676404","164973","<p>I am trying to fit a survival analysis in <code>R</code> with non-recurrent events and time-varying coefficients. The baseline distribution is exponential or Weibull and the frailty distribution is gamma distributed. I have roughly 900.000 rows. </p>

<p>So far I have tried the <code>parfm</code> and <code>frailtypack</code>. Though, neither has worked â€“ they just keep running and never return. The calls for <code>parfm</code> and <code>frailtypack</code> are similar to respectively: </p>

<pre><code>frailtyPenal(Surv(stop-start,event)~.-start-stop-event-year+cluster(temp$year), 
             data= regressionData, hazard=""Weibull"", RandDist=â€Gammaâ€)

parfm(Surv(stop-start,event)~.-start-stop-event-year,cluster=â€yearâ€, 
      regressionData, dist=""exponential"", frailty = ""gamma"")
</code></pre>

<p>Where <code>event</code> is zero-one coded. My guess so far is to use the <code>lme4</code> package with the function <code>glmer</code> where the family is Poisson, the respond are zero-one coded, the offset is the difference in time and random effect is an intercept for the year factor. I.e. something like:</p>

<pre><code>glmer(event ~.-start-stop-event-year+(1|year), family = Poisson(), offset=stop-start)
</code></pre>

<p>I know that this will yield Gaussian distributed random effects and not Gamma. Further, I am not sure that I get the model I want. My goal is to have exponential distributed conditional waiting times and hence I chose the Poisson distribution. Question is whether this is correct? Any suggestions on other packages that will do the job?</p>
"
"NaN","NaN","167466","<p>I ran a probit regression using the following code:</p>

<pre><code>    m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>

<p>However, I am now unsure how to compute the marginal effects and their corresponding standard errors?</p>

<p>Thanks a lot.</p>
"
"0.057073014553535","0.0573539334676404","167757","<p>I've run a probit regression in R with a random effect and can find no way to get the marginal effects with s.e. and p values.  I have therefore tried to calculate the marginal effects 'by hand' by using the probit scalars and regression coefficients.  However, I do not know how to get p values or standard errors and as far as I have found there is no easy way to do this for a mixed effects probit regression.</p>

<p>My model m1 is</p>

<pre><code> m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>
"
"0.057073014553535","0.0573539334676404","167780","<p>I am getting the exact same results for a probit regression and post-hoc tests (simultaneous tests for linear hypotheses) - is this because I have used a dummy variable in the probit model and so it is effectively comparing each factor level to the reference group thus when I run the post-hoc, which is comparing differences between the two groups, that I get the same answers?</p>

<p>This is the model I fitted:</p>

<pre><code> m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>

<p>and this is the post hoc test that I did:</p>

<pre><code> summary(glht(m1, lsm(pairwise ~ Name.Origin)))
</code></pre>
"
"0.0807134312271262","0.0811107105653813","167854","<p>I am running a probit glmer, with a binary response varaible and a categorical explanatory variable with three dummy levels and have tried to calculate the marginal effect using the following code:</p>

<pre><code>    ProbitScalar&lt;- mean(dnorm(predict(m1,type = ""link""))) 
</code></pre>

<p>The ProbitScalar value is then multiplied by the coefficient estimates from the regression output.</p>

<p>I get the following values: </p>

<p>-0.2946806 (referring to the intercept and reference level)
-0.1527443
-0.07252501</p>

<p>I am slightly confused how to interpret them as they seem quite low compared to what I would expect from the raw data.</p>

<p>Is it correct that the second variable has a 15% lower chance of achieving success (the binary response variable) than the reference group and the final variable has a 7% less chance of achieving success than the reference group?</p>
"
"0.0988533609478405","0.0993399267798783","168482","<p>I am running a probit regression with a random effect:</p>

<pre><code>m1&lt;-glmer(Binary~Explan+(1|Random),family=binomial(link=""probit""))
</code></pre>

<p>where Explan is a three-level categorical variable. </p>

<p>I want to calculate the mean predicted probabilities for each level of Explan. I tried doing so using this code:</p>

<pre><code>newdata=data.frame(Explan=""First"")
predict(m1,newdata,type=""response"")
</code></pre>

<p>where First is a level of the categorical Explan variable.</p>

<p>However I get the following error message:</p>

<pre><code>Error: (p &lt;- ncol(X)) == ncol(Y) is not TRUE
</code></pre>

<p>Were this a logit model, I would simply strip the model of the intercept and then back-transform the model summary coefficients to get the predicted values that I'm after, but I am unsure of how I would go about this with a mixed-effects probit model. </p>

<p>Any help in extracting the predicted probabilities would be greatly appreciated.</p>
"
"0.189289774928569","0.172928615966519","173996","<p>I'm using R (package lmer) to run linear mixed model My study looks at allergy levels of skin patches from patients and readings (repeated 5 times) are measured over 4 time points.</p>

<p>I need to determine if the allergy level for skin patch changes over time
(e.g., if allergy level from skin patch 1 for patient 1 at time 0 is different from allergy level for skin patch 1 for patient 1 at time 1 etc.) I do not want to see the difference between skin patch 1 and skin patch 2. Using package lmer:  </p>

<pre><code>model &lt;- lmer(allergy_level ~ time +(time|patient/patch))
</code></pre>

<p><strong>Results from this model indicate that time is not significant - the average patient allergy level for individual skin patches does not change over time</strong> (see below for output). However, <strong>I need to be able to tell if there is a significant difference for individual patches for individual patients over time</strong>.</p>

<p>If I run individual regression models for each skin patch for each patient, this will result in a large number of models as I have There are 16 skin patches per patient. (10 patients in total) 5 readings are taken at each of the 4 time points. I thought linear mixed models would be an appropriate method to answer my question (I need to be able to tell if there is a significant difference for individual patches for individual patients over time). </p>

<p>Output:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr             
 ID:patch (Intercept) 17.4109  4.1726                    
          time1        2.7109  1.6465   -0.30            
          time2        3.0082  1.7344   -0.26  0.60      
          time3        5.7643  2.4009   -0.35  0.15  0.54
 patch    (Intercept) 19.1576  4.3769                    
          time1        0.2103  0.4586   -0.56            
          time2        0.4372  0.6612   -0.94  0.48      
          time3        0.5895  0.7678   -0.48  0.96  0.49
 Residual              4.9467  2.2241                    
Number of obs: 2956, groups:  ID:patch, 149; patch, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.44763    1.15028   5.605
time1       -0.01907    0.21237  -0.090
time2       -0.03172    0.24759  -0.128
time3       -0.01124    0.29940  -0.038

model1: AllergyLevel ~ 1 + (1 + time | patch/ID)
model2: AllergyLevel ~ time + (1 + time | patch/ID)
         Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
model11 22 14281 14413 -7118.5    14237                         
model12 25 14287 14437 -7118.4    14237 0.0208      3     0.9992
</code></pre>

<p>I have extracted the random coefficients from model 1:</p>

<pre><code>ranef(model1)

`ID:patch`
      (Intercept)       time1        time2        time3
1:11    5.9845070  0.34088535  0.431998708  1.590906238
1:12    5.1236456 -0.03178611 -0.149784278 -0.116150278
1:13    6.3746877 -0.76853294 -0.550037715  0.842518786
   :
   :
</code></pre>
"
"0.171219043660605","0.172061800402921","174057","<p>This is probably an embarrassingly easy question, but where else can I turn to... </p>

<p>I'm trying to put together examples of regression with mixed effects using <code>lmer</code> {lme4}, so that I can present [R] code that automatically downloads toy datasets in Google Drive and run every instance in <a href=""http://stats.stackexchange.com/a/13173/67822"">this blockbuster post</a>. </p>

<p>And starting with the first case (i.e. <code>V1 ~ (1|V2) + V3</code>, where <code>V3</code> is a continuous variable acting as a fixed effect, and <code>V2</code> is <code>Subjects</code>, both trying to account for <code>V1</code>, a continuous DV), I was expecting to retrieve different intercepts for each one of the <code>Subjects</code> and a single slope for all of them. Yet, this was not the case consistently.</p>

<p>I don't want to bore you with the origin or meaning of the datasets below, because I'm sure most of you get the idea without much explaining. So let me show you what I get... If you're so inclined you can just copy and paste in [R]... it should work if you have {lme4} in your Environment:</p>

<h1>Expected Output:</h1>

<pre><code>politeness &lt;- read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
head(politeness)

  subject   gender scenario  attitude frequency
1      F1      F        1      pol     213.3
2      F1      F        1      inf     204.5
3      F1      F        2      pol     285.1
4      F1      F        2      inf     259.7    


library(lme4)

fit &lt;- lmer(frequency ~ (1|subject) + attitude, data = politeness)

coefficients(fit)
            $subject
               (Intercept) attitudepol
            F1    241.1352   -19.37584
            F2    266.8920   -19.37584
            F3    259.5540   -19.37584
            M3    179.0262   -19.37584
            M4    155.6906   -19.37584
            M7    113.2306   -19.37584
</code></pre>

<h1>Surprising Output:</h1>

<pre><code>library(gsheet)
recall &lt;- read.csv(text = 
    gsheet2text('https://drive.google.com/open?id=1iVDJ_g3MjhxLhyyLHGd4PhYhsYW7Ob0JmaJP8MarWXU',
              format ='csv'))
head(recall)

 Subject Time Emtl_Value Recall_Rate Caffeine_Intake
1     Jim    0   Negative          54              95
2     Jim    0    Neutral          56              86
3     Jim    0   Positive          90             180
4     Jim    1   Negative          26             200

fit &lt;- lmer(Recall_Rate ~ (1|Subject) + Caffeine_Intake, data = recall)

coefficients(fit)
        $Subject
               (Intercept) Caffeine_Intake
        Jason     51.51206        0.013369
        Jim       51.51206        0.013369
        Ron       51.51206        0.013369
        Tina      51.51206        0.013369
        Victor    51.51206        0.013369
</code></pre>

<p>Here is the output of (<code>summary(fit)</code>):</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Recall_Rate ~ (1 | Subject) + Caffeine_Intake
   Data: recall

REML criterion at convergence: 413.9

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.54125 -0.98422  0.04967  0.81465  1.83317 

Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.0     0.00   
 Residual             601.2    24.52   
Number of obs: 45, groups:  Subject, 5

Fixed effects:
                Estimate Std. Error t value
(Intercept)     51.51206    5.92408   8.695
Caffeine_Intake  0.01337    0.03792   0.353

Correlation of Fixed Effects:
            (Intr)
Caffen_Intk -0.787
</code></pre>

<h1>Question:</h1>

<p><strong>Why are all the Intercepts for the different subjects the same in the second example? The structure of the datasets and the <code>lmer</code> syntax appear very similar... and the boxplots don't seem to support the result:</strong></p>

<p><a href=""http://i.stack.imgur.com/xXYdS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXYdS.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance!</p>
"
"NaN","NaN","175418","<p>I ran the mixed effects model using the lme4 package as follows:
lmer9&lt;- lmer(y ~ station + (1|station:tow) + (1|tow), data=my.data)
 where station is the fixed factor and tow a random one...</p>

<p>However I am interested in obtaining the ANOVA for the regression model. 
I ran anova(lmer9) but it gives the ANOVA (F stat ) only for the fixed factor and doesn't give details about the random effects term and the interaction term.</p>

<p>Please suggest ways in which I could obtain a complete ANOVA...</p>

<p>Thanks,
Praniti</p>
"
"0.161426862454252","0.162221421130763","175652","<p>I am familiar with fixed-effects linear regression models, and have done reading on mixed-effects models.</p>

<p>I am attempting to fit a model based on observational data, where treatments come at varying times and do not exist at all for a majority of subjects.</p>

<p>I am interested in whether or not the treatment has an effect on the trajectory of a subject's response over time. Graphically:</p>

<p><img src=""http://i.imgur.com/szfwOL6.png"" alt=""Varying treatment time mixed model""></p>

<p>The most relevant analogous model I have found would be the one specified <a href=""http://www.ats.ucla.edu/stat/seminars/mlm_longitudinal/"" rel=""nofollow"">here</a>, specifically Part 3. However, this example does not use R. I have read through all of Bates' lme4 paper, but I am still uncertain how to specify this effect.</p>

<p>An excerpt of my data:</p>

<pre><code>     ID RESPONSE ID.CONST.1 ID.VAR.1 ID.VAR.2 TREATMENT_ACTIVE RESPONSE.TIME
1077415        7         41        0        5            FALSE           314
1077415        8         41        1        6            TRUE            316
1077415        9         41        10       7            TRUE            319
1077688        1         59        0        1            FALSE           313
1079475        1         85        0        1            FALSE           313
1080811        1         24        0        1            FALSE           314
1081156        1        502        0        1            FALSE           314
1082437        1         50        0        0            FALSE           315
1083154        1        257        0        0            FALSE           315
1083154        2        257        0        0            TRUE            316
1083527        1         69        0        0            FALSE           315
1086283        1         31        0        0            FALSE           316
1088810        1        120        2        1            FALSE           317
1090019        1         93        2        1            TRUE            317
1091048        1         27        0        0            FALSE           317
1091114        1         62        0        1            FALSE           317
</code></pre>

<p>Each subject (<code>ID</code>) has time-varying measurements (<code>ID.VAR.X</code>), constant measurements (<code>ID.CONST.X</code>), as well as the time of observation (<code>RESPONSE.TIME</code>). <code>TREATMENT_ACTIVE</code> indicates whether or not the treatment is active for a given subject at the corresponding <code>RESPONSE.TIME</code>. Some subjects have a single observation, others have multiple observations, and treatment times are rarely the same between subjects.</p>

<p>I've attempted to fit models as:</p>

<pre><code>lmer(RESPONSE ~ ID.CONST.1 + ID.VAR.1 + ID.VAR.2 + TREATMENT_ACTIVE + RESPONSE.TIME + (1|ID) + (1|RESPONSE.TIME)
lmer(RESPONSE ~ ID.CONST.1 + ID.VAR.1 + ID.VAR.2 + RESPONSE.TIME + (1|ID) + (1+TREATMENT_ACTIVE|RESPONSE.TIME)
</code></pre>

<p>However, I'm fairly certain this is misspecified. I am not sure how to specify the random effects to ensure that the <code>TREATMENT_ACTIVE</code> variable is interpreted as I intend. I am interested in testing both an intercept-only model as well as a intercept+slope model for the treatment effect.</p>
"
"0.0988533609478405","0.0993399267798783","177869","<p>I am running mixed effects regression in R, utilizing <code>glmer</code>, and am hoping someone can help clarify the difference between using <code>coef</code> and <code>ranef</code> on the results. Specifically, I have fixed effects $f_1,f_2,f_3$ and random effects $r_1,r_2,r_3.$ When I run <code>coef</code> I get certain coefficent values for each of the fixed and random effects. Additionally when I use the <code>ranef</code> function I get coefficients for my random effects. These two coefficients are not equal for each respective random effect $r_1,r_2,r_3.$ Why are these coefficients different and what information each coefficient tells us?</p>
"
"0.235682055324081","0.25","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.11414602910707","0.114707866935281","179748","<p>I have a dataset, say $A$:</p>

<pre><code>x    y
20   3.4
30   3.3
35   4.5 
</code></pre>

<p>I am fitting a regression model, a mixed model of R's <code>lme4</code> family to be exact, to predict $y$ given $x$. 
I have a set of <code>newdata</code> which don't have an observed $y$. 
Let $\hat{y}$ be the predicted value for this set $B$:</p>

<pre><code>x   yhat
20   3.3
100  6.6 
</code></pre>

<p>I need to report whether the predicted value is expected to be reasonably accurate. 
As you can see in dataset $B$, <code>x[2]</code> is somewhat of an outlier, and therefore $\hat{y}$ is also a value which is rare.
It happens to fall outside the 95% confidence interval of predicted values. $\hat{y}$ is very close to a real observation , in real life. </p>

<p>What kind of metric is used to report that $\hat{y}$ is actually quite a good prediction? 
I hope my question is clear even though I've struggled to explain it...</p>

<h3>Edited to add in response to comment below:</h3>

<p>The model has only one fixed parameter ($x$, continuous) while also having a random effects group parameter . 
Model looks like this </p>

<pre><code>LMER2&lt;-lmer(y~x + (1 |group), training_data)
</code></pre>

<p><code>lme4</code> does give me the coefficients and standard errors , and I have used the <code>bootMer</code> function to calculate $\hat{y}$ confidence interval following <a href=""http://www.r-bloggers.com/confidence-intervals-for-prediction-in-glmms/"" rel=""nofollow"">this article</a>.</p>
"
"0.197706721895681","0.198679853559757","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"0.0988533609478405","0.0993399267798783","182068","<p>I have data where I'm interested in the effect of treatment on individual decisions: Options 1, 2, &amp; 3. Individuals made multiple decisions (level 1) in groups (level 2).</p>

<p>I want to know the effect of treatment on selecting a particular option over not selecting that option. To do this, I dichotomized each option into 3 variables: Option 1 or not Option 1, Option 2 or not Option 2, Option 3 or not Option 3.</p>

<p>I tried to run a binomial regression using glmer: e.g., <code>glmer(Option1 ~ Predictors + (1|Level_1) + (1|Level_2), family=binomial)</code>, which did not converge.</p>

<p>I ran the same analysis using nnet::multinom: e.g., <code>multinom(Option1 ~ Predictors, random=~ 1| Level_1/Level_2, family=binomial)</code>. This did converge and gave sensible results.</p>

<p>What's going on? Is it justifiable to use multinom instead of glmer on the dichotomized data?</p>

<p>Any insight would be appreciated!</p>
"
"NaN","NaN","182467","<p>I'm running a logistic regression (presence/absence response) in R, using glmer (lme4 package). Ben Bolker'sÂ <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">overdisp_fun</a> (see link) tells me my model is overdispersed, so I decided to include an individual-level random effect. This is not solving my problem, as I get convergence issues and overdispersion is not reduced. Could anyone recommend an alternative?</p>

<p>Thanks!! </p>
"
"0.151001003081424","0.151744244666721","183603","<p>Please apologize for this potentially ""stupid"" question. But I am currently attempting to test a mediation in for a multilevel dataset. Unfortunately, the residuals of the regressions do not follow a Normal distribution and they do not have a constant variance. Ideally, I would thus use bootstrapping to obtain confidence intervals. However, the Mediation package in R does not provide this function for multilevel datasets. Instead, it calculates Quasi-Bayesian intervals in this case.</p>

<p>My question is:
<strong>Can I use Quasi-Bayesian Confidence intervals, if I am aware the residuals do not follow a Normal distribution and that they are heteroskedastic? If not, which package/functions could I use instead?</strong></p>

<p>Here is the code I have used. Unfortunately I cannot share a sample of my data, since it is confidential.</p>

<pre><code>&gt; med.fit &lt;- lmer(OTIF ~  LDLV + COLT + Slack2 + (1 | BU), data = Data_P5)
&gt; out.fit &lt;- lmer(EBIT ~ OTIF + Slack2 + LDLV + COLT  + (1 | BU), data = Data_P5)
&gt; med.out &lt;- mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"", sims = 100)
&gt; summary(med.out)

Causal Mediation Analysis 
Quasi-Bayesian Confidence Intervals
Mediator Groups: BU 
Outcome Groups: BU 

Output Based on Overall Averages Across Groups 

            Estimate 95% CI Lower 95% CI Upper p-value
ACME            5.12e-03     1.49e-03     1.06e-02    0.00
ADE            -5.64e-03    -1.77e-02     8.46e-03    0.50
Total Effect   -5.24e-04    -1.38e-02     1.41e-02    0.86
Prop. Mediated -3.21e-01    -1.34e+01     6.01e+00    0.86

Sample Size Used: 167 


Simulations: 100 
</code></pre>

<p>Since bootstrapping is not available for multilevel models I get an error message:</p>

<pre><code>&gt; med.fit &lt;- lmer(OTIF ~  LDLV + COLT + Slack2 + (1 | BU), data = Data_P5)
&gt; out.fit &lt;- lmer(EBIT ~ OTIF + Slack2 + LDLV + COLT  + (1 | BU), data = Data_P5)
&gt; med.out &lt;- mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"", sims = 100, 
+ boot = TRUE)
Error in mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"",  : 
'boot' must be 'FALSE' for models used
</code></pre>
"
"0.141248504647471","0.162221421130763","183908","<p>I have a binary logistic regression with just one binary fixed factor predictor. The reason I don't do it as a Chi square or Fisher's exact test is that I also have a number of random factors (there are multiple data points per individual and individuals are in groups, although I don't care about coefficients or significances for those random variables). I do this with R glmer.</p>

<p>I would like to be able to express the coefficient and associated confidence interval for the predictor as a risk ratio rather than an odds ratio. This is because (maybe not for you but for my audience) risk ratio is much easier to understand. Risk ratio here is the relative increase in chance of the outcome being 1 rather than 0 if the predictor is 1 rather than 0.</p>

<p>The odds ratio is trivial to get from the coefficient and associated CI using exp(). To convert an odds ratio to a risk ratio, you can use ""RR = OR / (1 â€“ p + (p x OR)), where p is the risk in the control group"" (source: <a href=""http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/"" rel=""nofollow"">http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/</a>). But, you need the risk in the control group, which in my case means the chance that the outcome is 1 if the predictor is 0. I believe the intercept coefficient from the model is in fact the odds for this chance, so I can use prob=odds/(odds+1) to get that. I'm pretty much so-far-so-good on this as far as the central estimate for the risk ratio goes. But what worries me is the associated confidence interval, because the intercept coefficient also has its own associated CI. Should I use the central estimate of the intercept, or to be conservative should I use whatever limits of the intercept CI make my relative risk CI widest? Or am I barking up the wrong tree entirely?</p>
"
"0.408213294253567","0.402625845619081","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.172081613571426","0.172928615966519","192785","<p><strong>Objective</strong></p>

<p>I have a crossed and implicitly nested design and am trying to validate the correct â€˜maximalâ€™ model (including all linear and pairwise interactions of the variables) for use in <code>lmer()</code>.  I intend to use this as the starting point for some kind of backward stepwise regression, possibly making use of the function <code>mixed()</code> in the  <code>{afex}</code> package.</p>

<p><strong>Experimental design</strong></p>

<p>This a linguistics study.  We have 20 <code>Subjects</code>, each speaking 180 utterances, amounting to 3600 observations in total. Each utterance is initiated via prompting, and an associated Response Time is measured. Log Response Time is the dependent variable. </p>

<p><em>Conditions &amp; Blocks</em></p>

<p>The Response Time for the utterances is affected by 3 <code>Conditions</code> (coded 1 to 3). Each <code>Condition</code> is implemented by prompting the <code>Subject</code> to recite 1 of 4 <code>Blocks</code> of utterances (coded 1 to 12).</p>

<p><em>Words &amp; Tones</em></p>

<p>Each <code>Block</code> brings about its associated <code>Condition</code> via 15-utterance repetition of 3 carefully chosen <code>Words</code>.  There are a total of 12 <code>Words</code> used in the experiment (coded 1 to 12). The <code>Words</code> within each <code>Block</code> can also be categorized by <code>Tone</code> (coded 1 to 2).  There are 6 <code>Words</code> per <code>Tone</code>.  </p>

<p><em>Summary</em></p>

<p>Each of the 20 <code>Subjects</code> utter all 12 <code>Blocks</code> of 15 utterances each.  In doing so, they repeatedly utter all 12 <code>Words</code> (15 utterances per <code>Word</code>), and thereby use both <code>Tones</code> (90 utterances per <code>Tone</code>).</p>

<p>I would like to consider <code>Block</code>, <code>Word</code>, and <code>Subject</code> as random effects, and <code>Condition</code> and <code>Tone</code> as fixed.</p>

<p><strong>Proposed Model</strong></p>

<p>I think the model can be written in the following wayâ€¦</p>

<p><code>RT_log ~ Condition*Tone + (Condition*Tone|Subject) + (Condition|Word) + (Tone|Block)</code></p>

<p><strong>Questions</strong></p>

<p><strong>1.</strong> Is this the 'maximal' model (with linear plus pairwise interactions) appropriate for my experimental design?_ </p>

<p><strong>2.</strong> There is correlation between <code>Block</code> and <code>Condition</code> (there are only 4 possible blocks - out of the total 12 - for each <code>Condition</code>).  There is, similarly, correlation between <code>Word</code> and <code>Tone</code>.  Is it 'okay' to leave this correlation in the model? I don't see a good way of removing it.</p>

<p><strong>3.</strong> How will lme4 handle implicit nesting: I.e., the blocks, which are implicitly nested in the 3 conditions (i.e., only 4 blocks are applicable to each of the 3 conditions, even though the blocks are coded from 1 to 12), and the words, which are implicitly nested within the 2 tones (only 6 words are applicable to each tone, even though words are coded from 1 to 12)?</p>

<p><strong>4.</strong> Some <code>Blocks</code> utilize <code>Words</code> of only a single <code>Tone</code>, whereas other <code>Blocks</code> utilize words of both <code>Tones</code>.  Will that cause problems for the <code>(Tone|Block)</code> term in the model? It will only make sense for certain values of Block.</p>

<p><strong>5.</strong> It has been suggested by some that we might need a ""Subject:Word"" grouping (random effect).  Why might we need this grouping?</p>
"
"0.139799763738599","0.140487871737254","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.0856095218303024","0.114707866935281","208972","<p>So I have 10 bond return time-series dataset (<code>portfolio1</code> to <code>portfolio10</code>). <code>Portfolio1</code> is the past loser bonds with present 1 month holding period return and portfolio10 is the past winner bonds with present 1 month holding period return. Now, I want to see whether <code>portfolio10</code> still out performs <code>portfolio1</code> even after 1 month of holding period but I want to adjust the risk factors. So my regression model looks like this: </p>

<pre><code>r_(p,t) = a_(p)  +  b_(p)*F_(t)   +  e_(p,t)  
</code></pre>

<p>Where <code>r_(p,t)</code> is a return for portfolio <code>p</code> at time <code>t</code>, <code>a_(p)</code> is the <code>alpha</code> for each portfolio, <code>F_(t)</code> is risk factors at time t and <code>e_(p,t)</code> the error term. </p>

<p>I would like to compute alpha for each of the portfolios and consider it as a risk -adjusted return. </p>

<p><strong>How do you compute the t statistic for the difference in alphas between</strong> <code>portfolio1(loser)</code> <strong>and</strong> <code>portfolio10(winner)</code>?</p>

<p>I am using R and I have tried doing:</p>

<pre><code> library(lme4)
 mod1 &lt;- lmer(r_111~(1|decile)+mTERM+(1|ID), data=rg81_10_1)
 summary(mod1)
</code></pre>

<p><code>rg81_10_1</code> is an <code>rbind</code>ed dataset of <code>portfolio10</code> and <code>portfolio1</code>. </p>

<p>Is the t statistic for the intercept the t statistic that I want? </p>
"
"0.127619140222539","0.128247294010644","209773","<p>I am estimating a model of the type (logistic regression with random slopes and random intercepts clustered by the variable ID):</p>

<pre><code>formula = result ~ year + (1 | ID) + (year | ID)
</code></pre>

<p>using</p>

<pre><code>glmer(formula, data = data1, family = binomial, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)
</code></pre>

<p>However, I am getting results for three random effects:</p>

<pre><code>Random effects:
 Groups Name        Variance  Std.Dev.  Corr 
 ID    (Intercept) 3.645e-01 0.6037203      
 ID.1  (Intercept) 1.228e+00 1.1082860      
        year       3.043e-07 0.0005516 -1.00
</code></pre>

<p>Is there something I am specifying incorrectly?</p>

<p>On the other hand, the fixed effects are correct:</p>

<pre><code>Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 55.328196   8.052619   6.871 6.38e-12 ***
year       -0.027933   0.004008  -6.969 3.20e-12 ***
</code></pre>
"
"0.0807134312271262","0.0811107105653813","211094","<p>I am following the advice to ""keep it maximal"", and am analyzing the results from several psycholinguistic experiments. My main interest is in the fixed effects, with the random effect terms in there to provide a better test of these fixed effects.</p>

<p>That being said, I would like to keep things simple and, as much as possible, have the same analyses in each experiment.</p>

<p>In several of the experiments my random subject intercepts and slopes are perfectly, negatively correlated.</p>

<p>Given everything I said, would I be committing a cardinal sin by keeping both of them in the model?</p>

<p>Some details: I am the glmer function in R. I have only one fixed effect. I am running a logistic regression.</p>
"
"0.139799763738599","0.140487871737254","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.11414602910707","0.114707866935281","212446","<p>I'd like to use elastic net regression for coefficient estimate and parameter selection on a data set that includes nested structure. I've been experimenting with lassop{MMS} to do so. I'm not a statistician by training, and I'm having a difficult time deciphering how to translate the example provided with the documentation to a real-data context.</p>

<pre><code>    require(lme4)
    require(lmerTest)
    require(MMS)
    data(grouseticks)
    ?grouseticks # sample data w/ multiple grouping levels
    n&lt;-length(grouseticks$TICKS)
#two dummy variables for additional fixed effects that we'll assume will be selected out
    dv1&lt;-rnorm(n, mean = 0, sd = 1)
    dv2&lt;-rnorm(n, mean=3, sd=2)
#sample saturated ME model, two terms for random intercept. I'm trying to write this in lassop syntax. 
sat_lmm&lt;- lmer(TICKS~YEAR+HEIGHT+YEAR+dv1+dv2+HEIGHT:dv1+(1|BROOD)+(1|LOCATION), data=grouseticks, REML=FALSE)
summary(sat_lmm)
</code></pre>

<p>How would set up the random effects and grouping matrices to mimic the above model formulation? Feel free to rip into this, I know my grouping and random effects matrices are desperately wrong.</p>

<pre><code>x&lt;-getME(sat_lmm,name = c( ""X""))
x&lt;-x[,c(""(Intercept)"" , ""HEIGHT"",""dv1"" ,""HEIGHT:dv1"",  ""dv2""  , ""YEAR96"" , ""YEAR97"")]
#rearrange variables so that first 3 collumns will be frozen in
y&lt;-as.numeric(getME(sat_lmm,name = c( ""y"")))

# this was my naive guess at handling  random effects
zlx&lt;-cbind( factor(grouseticks$BROOD, labels=seq(length(unique(grouseticks$BROOD)))),
            factor(grouseticks$LOCATION, labels=seq(length(unique(grouseticks$LOCATION)))))

#dummy grouping variable
gx&lt;-rbind(rep(1, length(n)) , 
          rep(1, length(n)))

require(glmnet)
lam&lt;-cv.glmnet(x, y, alpha=0.8, standardize=TRUE)
plot(lam)
#value of lambda that gives minimum cross-validation error
lammin&lt;-lam$lambda.min
lamlse&lt;-lam$lambda.lse
melasso.minlam&lt;-lassop(data=x,
                       Y=y,
                       z=zlx, 
                       mu=lammin,
                       fix=3,
                       D=TRUE,
                       alpha=0.8,
                       showit=F)
#as this stands, it won't run.
print(melasso.minlam)
</code></pre>
"
"0.139799763738599","0.140487871737254","213187","<p>I'm very new to R and stats in general but I think I need to do a repeated measures analysis perhaps using a linear mixed model on my data.</p>

<p>Data are behavioural measurements e.g. distance swum for 21 fish of 2 different genotypes. Each fish was tested for 10 minutes, and data taken for each minute. I've made graphs to show this (B and C).</p>

<p><img src=""http://i.stack.imgur.com/8oKg9.png"" alt=""""></p>

<p>My issue is that I can't do just a normal regression as each of the points is linked over time by the same fish. I've been taught to use linear mixed models such as:</p>

<pre><code>model &lt;- lmer(swimdurbot~1+start+(1+start|file), data=tdautodisc)
model2 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
model3 &lt;- lmer(swimdurbot~Genotype+(1+start|file), data=tdautodisc)
model4 &lt;- lmer(swimdurbot~Genotype+start+ Genotype*start +(1+start|file), data=tdautodisc)
model5 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
</code></pre>

<p>and to then perform anovas on them but I honestly can't work out what each model is showing, how they are different from each other and which part of the analysis that R comes up with tells me what I want to know.</p>

<p>I want to find out from my data: Does fish behaviour change over 10 mintutes (gradient)? Is the behaviour different between genotypes? and Does genotype affect the change in behaviour over time?</p>

<p>I would really appreciate some help on this as I can't get my head around it, and the graphs don't show any obvious trend so aren't useful for me to guess which part of the analysis relates to which aspect of the graphs.</p>
"
"0.189289774928569","0.190221477563171","218738","<p>I want to build a linear regression model where I predict a mean of a group of participants (how they rate something on average). Predictors should be </p>

<ol>
<li>age (continuous)</li>
<li>origin (deviation coded, each level compared to grand mean, levels=1,2,3,4)</li>
<li>education (Helmert coded, each level compared to subsequent ones, haven't decided on number of levels yet)</li>
<li>gender/sex (dummy coded, 0/1)</li>
</ol>

<p>Following questions:</p>

<p><strong>1.</strong> In R, I use the following code for the coding, for example for 4) sex:</p>

<pre><code>    data$sex &lt;- factor(data$sex, labels=c(""1"",""2"")) 
    contr.treatment(2) 
    contrasts(data$sex) = contr.treatment(2)  
</code></pre>

<p>That gives me the right (dummy) coding, for the other 2 and 3 a little differently. Can I use run this kind of code for each predictor (except age) and then throw all predictors into a model like this:</p>

<pre><code>    model &lt;- lm(Mean ~ age +  sex + educ..., data)
</code></pre>

<p>It seems wrong because: what is the common intercept going to be with these different coding systems? It's different for each coding system.
But then, how am I going to enter these different predictors into a model?</p>

<p><strong>2.</strong> Can I leave age in there as it is, unchanged, continuous?</p>

<p><strong>3. Quite a different question:</strong> This is my <em>participant analysis</em>. For the <em>item analysis</em>, I used a logistic regression based on medians instead of means. That's because I did four rating surveys with Likert scales. 4 surveys - 4 participant groups - each group rated the items on <strong>one</strong> property only, such that each item was rated on 4 properties by different people.</p>

<p>Given this, is it okay if I use linear models and means in this analysis now? And can I even build my model as I suggested above?</p>

<p><strong>Many thanks</strong> for any input! I've been trying some things, but confusion isn't fading yet...</p>
"
"0.189289774928569","0.190221477563171","221161","<p>I have been having trouble with the predict function underestimating (or overestimating) the predictions from an lmer model with some polynomials. Hopefully my edits make it clearer. I have scaled data that looks like this:</p>

<pre><code>Terr      Date     Year            Age  
T.092     123      0.548425     -0.86392            
T.104     102      1.2072       -0.48185            
T.104     105      1.075445     -0.86392            
T.104     112      0.94369      -1.24599            
T.040     116     -0.2421        2.192652           
T.040     114     -0.37386       1.810581           
T.040     119     -0.50561       1.428509           
T.040     128      0.15316      -0.09978            
T.040     113      0.021405     -0.48185
</code></pre>

<p>Iâ€™m trying to determine how Year affects lay date after controlling for Age, with Terr (territory) as a random variable. I usually include polynomials and do model averaging, but whether I use a single model or do model averaging, the predict function gives predictions that are a bit lower or higher than they should be. I realize that the model below would not be a good model for this data, Iâ€™m just trying to provide a simplified example.  </p>

<p>Below is my code  </p>

<pre><code>library(lme4
m1 &lt;- lmer(Date ~ (1|Terr) + Year + Age + I(Age^2), data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictions=predict(m1, newdata = new.dat, re.form=NA)
pred.l&lt;-cbind(new.dat, Predictions)
pred.l  

      Year          Age Predictions
    1   -2 2.265676e-16    124.4439
    2   -1 2.265676e-16    123.2124
    3    0 2.265676e-16    121.9810
    4    1 2.265676e-16    120.7496
</code></pre>

<p>When plotted with the means, the graph looks like this:</p>

<p><a href=""http://i.stack.imgur.com/mwIpJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mwIpJ.jpg"" alt=""graph1""></a></p>

<p>When I use effects, I get a much better fit  </p>

<pre><code>library(effects)
ef.1c=effect(c(""Year""), m1, xlevels=list(Year=-2:1))
pred.lc=data.frame(ef.1c)
pred.lc

      Year      fit        se    lower    upper
    1   -2 126.0226 0.6186425 124.8089 127.2363
    2   -1 124.7911 0.4291211 123.9493 125.6330
    3    0 123.5597 0.3298340 122.9126 124.2068
    4    1 122.3283 0.3957970 121.5518 123.1048
</code></pre>

<p><a href=""http://i.stack.imgur.com/SvI3f.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SvI3f.jpg"" alt=""graph2""></a></p>

<p>After much trial and error, I have discovered that the problem is with the Age polynomial, because when the Age polynomial is not included, the predicted and fitted are equal and both fit well. Below is the same  model but with Age as a linear term.  </p>

<pre><code>m2 &lt;- lmer(Date ~ (1|Terr) + Year + Age, data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsd=predict(m2, newdata = new.dat, re.form=NA)  
pred.ld&lt;-cbind(new.dat, Predictionsd)
pred.ld

      Year          Age Predictionsd
    1   -2 2.265676e-16     125.9551
    2   -1 2.265676e-16     124.7653
    3    0 2.265676e-16     123.5755
    4    1 2.265676e-16     122.3857

library(effects)
ef.1e=effect(c(""Year""), m2, xlevels=list(Year=-2:1))
pred.le=data.frame(ef.1e)
pred.le

      Year      fit        se    lower    upper
    1   -2 125.9551 0.6401008 124.6993 127.2109
    2   -1 124.7653 0.4436129 123.8950 125.6356
    3    0 123.5755 0.3406741 122.9072 124.2439
    4    1 122.3857 0.4093021 121.5827 123.1887
</code></pre>

<p>I do many similar analyses, and this issue with the predictions being slightly lower (or higher) than they should be often happens when Age is included as a polynomial. When I include a polynomial for Year, there is no problem and the predicted and fitted are equal, so I know the problem is not with all polynomials.</p>

<pre><code>m3 &lt;- lmer(Date ~ (1|Terr) + Year + I(Year^2) + Age, data=data)

new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsf=predict(m3, newdata = new.dat, re.form=NA)  
pred.lf&lt;-cbind(new.dat, Predictionsf)
pred.lf

      Year          Age Predictionsf
    1   -2 2.265676e-16     125.6103
    2   -1 2.265676e-16     124.8494
    3    0 2.265676e-16     123.7483
    4    1 2.265676e-16     122.3070

library(effects)
ef.1g=effect(c(""Year""), m3, xlevels=list(Year=-2:1))
pred.lg=data.frame(ef.1g)
pred.lg

      Year      fit        se    lower    upper
    1   -2 125.6103 0.8206625 124.0003 127.2203
    2   -1 124.8494 0.4615719 123.9438 125.7549
    3    0 123.7483 0.4275858 122.9094 124.5871
    4    1 122.3070 0.4262110 121.4708 123.1431
</code></pre>

<p>I've looked for answers (e.g., <a href=""http://stats.stackexchange.com/questions/180010/overestimated-and-underestimated-predictions-in-regression"">here</a>) but haven't found anything that is directly helpful. I can provide the whole data set if needed. Does anyone have any insight?</p>
"
"0.0807134312271262","0.0811107105653813","224165","<p>I built a mixed linear regression model which includes a dependent variable 'dv', independent variable 'v1' &amp; 'v2', and subject ID 'subject'. </p>

<p>The R syntax is shown below:</p>

<p>output &lt;-lmer(dv ~ v1 + v2 + v1*v2 + (1|subject)+(0+ v1|subject)+(0+ v2 |subject), data=matrix, control=lmerControl(optimizer=""bobyqa"",optCtrl=list(maxfun=2e5)))</p>

<p>The dataset is here:
<a href=""https://1drv.ms/t/s!AitdBHtSjoIpiCXWmzLEnvqX1iuE"" rel=""nofollow"">dataset</a></p>

<p><em>My question:</em></p>

<p>There is a point very strange, the significance of interaction is not consistent with the data pattern (as shown on attached plot), the fixed effect of interaction shows a very small p-value, meaning the slope of two lines should be significantly different, however, the plot does not show an expected pattern. I am worrying there is something wrong, which one (mixed model output or plot result) is correct?</p>

<p><a href=""http://i.stack.imgur.com/IMWEm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IMWEm.png"" alt=""enter image description here""></a></p>
"
"0.180480718921108","0.181369062527503","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.151001003081424","0.151744244666721","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"0.139799763738599","0.140487871737254","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
"0.102095312178031","0.128247294010644","234947","<p>I'm looking to run a linear mixed effect model using lme4, where my dependent variable <code>one_syllable_words / total_words_generated</code> is a proportion and my random effect <code>(1 | participant_ID)</code> reflects the longitudinal nature of the design. Independent, fixed effect variables of interest include <code>age</code>, <code>group</code>, <code>timepoint</code>, and interactions between them. </p>

<p>I've come across two main ways to deal with the proportional nature of the DV:  </p>

<ol>
<li><p><strong>Standard logistic regression / binomial GLM</strong>  </p>

<p>In my scenario, I envision the lme4 equation looking like this:  </p>

<pre><code>glmer(one_syllable_words / total_words_generated ~ age + group +
timepoint + age:timepoint + age:group + timepoint:group + (1 |
participant_ID), family = ""binomial"", weights =
total_words_generated, data = mydat)  
</code></pre></li>
<li><p><strong>Beta regression</strong>  </p>

<p>I would apply a transformation to my DV <code>(DV * (n - 1) + .5)/ n)</code> so that it cannot equal 0 or 1. (There are a few instances where it equals zero, no instances where it equals one.)  </p></li>
</ol>

<p>I'm unclear whether logistic regression or beta regression is preferred in this example. My DV isn't a clear-cut case of successes and failures (unless we stretch the definition of ""success""), so I'm worried logistic regression might not be appropriate. However, I'm having trouble getting a firm grasp on beta regression &amp; all it entails. If beta regression is preferred:  </p>

<ol>
<li>Why is it preferred?  </li>
<li>What is it doing ""behind the scenes"" to the data?  </li>
<li>How can it be applied in R?  </li>
</ol>
"
"0.057073014553535","0.0573539334676404","235187","<p>I get that for each interaction term, there should be the individual covariate term specified in model (<a href=""http://stats.stackexchange.com/questions/27724/do-all-interactions-terms-need-their-individual-terms-in-regression-model"">source</a>), otherwise the model is prone to location shifts  - ie. the scale of variables is starting to count - similarly to as when dropping the intercept (please correct me if this understanding is wrong).</p>

<p>My question is then: does it apply for each level of interaction?
Am I right to do:</p>

<pre><code>glmer(Y ~ X1 + X2 + X3 + X1:X2:X3 + (1|X4), data, family = binomial)
</code></pre>

<p>Or should I include the interactions X1:X2, X2:X3 and X1:X3 also?
I intuitively feel, that no, but experimenting with this shows me differences in significance, hence the question.</p>

<p>I'll add, that my hypothesis concerns only the second level interaction.</p>
"
