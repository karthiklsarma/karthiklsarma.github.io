V1,V2,V3,Post
0.340692572,0.326431275,71414,"<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
0.094491118,0.090535746,71417,"<p>This is a question I originally posted on r-help but it is more suited here. I will post the question and the answer I received from Dr. Winsemius and would be most grateful for any additional answers you can provide. </p>

<p>I am evaluating survival models using Brier score (“peperr”) and Harrell’s C-Index (“Hmisc”).
I am wondering:</p>

<ol>
<li><p>What would be considered a “good fit” according to these scores (like the heuristic levels we have for R square in linear regressions) ?</p></li>
<li><p>Are there any papers to cite on the matter (I couldn’t find any) ?</p></li>
<li><p>Is there any paper to cite that discusses the limitation of using traditional reporting for model fit in survival analysis as opposed to these measures  ?</p></li>
</ol>

<p>Dr. David Winsemius replied:</p>

<ol>
<li><p>Frank Harrell's excellent text ""Regression Modeling Strategies"" has an extensive discussion of ""goodness of fit"" and the principles of model comparison. It's both too involved as well as off-topic for Rhelp. The other text to consult is Steyerberg's ""Clinical Prediction Models"".</p></li>
<li><p>I predict that the RMS bibliography would be an excellent place to start your search.</p></li>
</ol>

<p>Despite getting his name attached to what he calls the 'c-index', I don't think one could call Frank Harrell a proponent of that measure or any of the ""competitors"". It's really just a dressed up/transformed AUC. The message I have taken from reading his book and listening to presentations is that one should apply biologic tests of sensibility as well as careful investigation of the functional relationships between candidate predictors and the outcomes of interest. He speaks very disparagingly about automatic procedures.</p>
"
0.231455025,0.221766381,71418,"<p>Prior CV postings on the matter of GOF measures in generalized linear models:</p>

<p><a href=""http://stats.stackexchange.com/questions/5293/find-out-pseudo-r-square-value-for-a-logistic-regression-analysis/5298#5298"">Find out pseudo R square value for a Logistic Regression analysis</a></p>

<p><a href=""http://stats.stackexchange.com/questions/3559/which-pseudo-r2-measure-is-the-one-to-report-for-logistic-regression-cox-s"">Which pseudo-$R^2$ measure is the one to report for logistic regression (Cox &amp; Snell or Nagelkerke)?</a></p>

<p><a href=""http://stats.stackexchange.com/questions/12641/addressing-model-uncertainty/"">Addressing model uncertainty</a></p>

<p><a href=""http://stats.stackexchange.com/questions/58756/compare-classifiers-based-on-auroc-or-accuracy/"">Compare classifiers based on AUROC or accuracy?</a></p>

<p>""Goodness of fit"" is an elusive notion. Any set of data can be perfectly fit with a complex, saturated model, but such a model will generally be useless despite being perfect. Application of such tests often completely ignores what the model that is being fit to. I find it rather strange that Anderson-Darling and  Kolmogorov-Smirnov tests are being called ""goodness of fit tests"" when they are really being used as ""tests of normality"".</p>

<p>Models need to be both validated and calibrated and the GOF measures generally tell you very little about those aspects. (It should be noted in passing that the 'rms' function <code>print.cph</code> also reports the Brier score along with a pseudo-R^2 and Somers-D as ""discrimination indexes"". And it does not report the c-index, perhaps because the Somers-D is equivalent and preceded it historically and Harrell is tired of people misusing it.)</p>

<p>You will note that Frank told you that your proposed strategy in an earlier rhelp posting where you proposed taking a ""best"" glmnet model and then apply stepwise forward and backward reduction was bad statistical practice. Part of the problem is that you were taking a result from a method which is optimized for prediction (penalized glmnet)  and then applying a procedure that was in all probability <em>lowering</em> its predictive capacity.</p>

<p>Your low Brier score is something I see all the time in my research. I work with large datasets where the outcomes of interest are rather rare (mortality over 5-12 years for basically healthy people). Even a good model will only be predicting a mortality rate of 4-5% for most of the people who die and the ""error rate"" remains high despite many variables being highly significant. Model comparison measures (especially the deviance) are much better guides for decision making than any of the GOF or discrimination measures.</p>
"
0.211288564,0.202444083,26721,"<p>In statistics, the study of streaming data is called <a href=""http://en.wikipedia.org/wiki/Sequential_analysis"" rel=""nofollow"">sequential analysis</a>. Machine learning has the closely related concept of <a href=""http://en.wikipedia.org/wiki/Online_machine_learning"" rel=""nofollow"">online learning</a>, the difference being an emphasis on model fitting (regression), rather than hypothesis testing. From the abstract:</p>

<blockquote>
  <p>A potential clustering with a specified number of clusters is represented by an association hypothesis. Whenever a new report arrives, a posterior distribution over all
  hypotheses is iteratively calculated from a prior distribution, an update model and a likelihood function. The update model is based on an association probability for clusters given the probability of false detection and a derived probability of an unobserved target. The likelihood of each hypothesis is derived from a cost value of associating the current report with its corresponding cluster according to the hypothesis. A set of
  hypotheses is maintained by Monte Carlo sampling. In this case, the state-space, i.e., the space of all hypotheses, is discrete with a linearly growing dimensionality over time. To lower the complexity further, hypotheses are combined if their clusters are close to each other in the observation space. Finally, for each time-step, the posterior distribution is projected into a distribution over the number of clusters.</p>
</blockquote>

<p>To understand <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.2931"" rel=""nofollow"">the suggested paper</a> (<em><a href=""http://dx.doi.org/10.1109/ICIF.2005.1591845"" rel=""nofollow"">Sequential clustering with particle filtering: Estimating the number of clusters from data</a></em>), you will need to familiarize yourself with</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Cluster_analysis#Clustering_algorithms"" rel=""nofollow"">Clustering algorithms</a>.</li>
<li><a href=""http://en.wikipedia.org/wiki/Bayesian_inference"" rel=""nofollow"">Bayesian inference</a>. <a href=""http://videolectures.net/mlss09uk_bishop_ibi/"" rel=""nofollow"">Lecture by Christopher Bishop</a> of the <a href=""http://research.microsoft.com/~cmbishop/prml/"" rel=""nofollow"">PRML book</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Particle_filter"" rel=""nofollow"">Particle filters</a>, also known as <a href=""http://en.wikipedia.org/wiki/Monte_Carlo_method"" rel=""nofollow"">sequential Monte Carlo methods</a>. Here are lectures by <a href=""http://videolectures.net/mlss09uk_godsill_pf/"" rel=""nofollow"">Simon Godsill</a>, <a href=""http://videolectures.net/nips09_doucet_freitas_smc/"" rel=""nofollow"">Arnaud Doucet &amp; Nando de Freitas</a>, and <a href=""http://videolectures.net/mlss07_doucet_smcm/"" rel=""nofollow"">Arnaud Doucet</a> of the <a href=""http://www.springer.com/statistics/physical+%26+information+science/book/978-0-387-95146-1"" rel=""nofollow"">Sequential Monte Carlo Methods in Practice</a> book.</li>
</ul>

<p>There might be easier ad hoc solutions but these are useful tools so I recommend learning them anyway.</p>
"
0.211288564,0.202444083,26769,"<p>I'm attempting to perform hierarchical agglomerative cluster analysis in R. </p>

<p>However, when I use particular clustering methods, I get reversals (upward branching) in the resulting tree, which violates the ultrametric property.</p>

<p><img src=""http://i.stack.imgur.com/WSPo2.jpg"" alt=""enter image description here""></p>

<p>The two methods are: UPGMC and WPGMC (methods=""median"" and ""centroid"" in <code>hclust</code>).  Legendre &amp; Legendre in their Numerical Ecology book suggest some reasons why this may occur (Section 8.6).  However, they provide no solutions to rectify the issue and convert the trees to ultrametric.</p>

<p>I'm curious: is this an unavoidable consequence of the data and the clustering method, or is there a way that I can produce a tree that satisfies the ultrametric property using these two methods?</p>

<p>Here is an example data set and R code to play with:</p>

<pre><code>#Generate data frame with mixed continuous and categorical trait data for 10 species
set.seed(91)
(df=data.frame(trait1=runif(10,0,10),trait2=runif(10,0,10),
               trait3=sample(letters[1:3],10,replace=T),row.names=paste(""sp"",1:10,sep="""")))

#Generate Gower dissimilarity matrix from trait data
library(cluster)
(dist.gower=daisy(df,metric=""gower""))

#Create a vector of clustering methods
tree.methods=c(""ward"",""single"",""complete"",""average"",""mcquitty"",""median"",""centroid"")  
#Build the trees using each method
trees=lapply(tree.methods,function(i) hclust(dist.gower,method=i))  
#Plot the trees
par(mfrow=c(4,2))
for(i in 1:length(trees)) {plot(trees[[i]])}
#The last two trees have reversals...cannot be converted to ultrametric!
</code></pre>
"
0.353553391,0.338753743,141053,"<p><strong>How much you can conclude depends on the assumptions</strong> you are willing to make about the underlying distribution.  Even with no assumptions, though, you can still conclude something, although it might not be a whole lot.  For example, the sample data are not plausibly consistent with an underlying distribution whose mean is $100$ and standard deviation is $10$: Chebyshev's inequality tells us these data must have extremely low likelihood in that case.</p>

<p><strong>The most general answer</strong> will be given in the form of a <a href=""http://en.wikipedia.org/wiki/Probability_box"" rel=""nofollow"">p-box</a>.  This is a simple device to describe a range of distribution functions.  It consists of two graphs, one lying above the other, which thereby determine a region $\mathcal P$ in Cartesian coordinates.  Any CDF whose graph would lie entirely within $\mathcal P$ is an element of that p-box.</p>

<p><strong>As an example,</strong> let's construct a (nonparametric) 90%-confidence p-box for the sample data, $(\lt 5, \lt 5, \lt 5, \lt 10, \lt 10, \lt 10)$.  This p-box should be as ""small as possible"" (in some sense) while containing any distribution law $F$ for which the probability of the data, given $F$, is at least $1 - 90/100 = 1/10$.  Let us (temporarily) call these the ""plausible laws.""</p>

<p>""As small as possible"" can be rigorously defined.  Let $\mathbb{P}$ be the set of all distribution functions.  Associated with any subset $\Omega\subset \mathbb{P}$ is the set $C(\Omega)$ consisting of all points $(x,y)$ such that $x$ is a real number and there exist $F,G\in \Omega$ with $F(x) \le y \le G(x)$.  Letting $\Omega$ be the set of plausible laws, we take the p-box to be $C(\Omega)$.</p>

<p>These data, which have no quantified results at all, tell us just two things about the plausible laws.  When a sample $X_1, X_2, \ldots, X_6$ is taken from such a law $F$, then (1) it should be unlikely that one or more of the $X_i$ equal $10$ or greater and (2) it should be unlikely that four or more of the $X_i$ should equal $5$ or greater.  These are binomial events governed by the parameters $p=F(10)$ and $q=F(5)$, respectively.  Interpreting ""unlikely"" as being a chance of $100-90 = 10\%$ or less, these criteria uniquely determine $p$ and $q$.</p>

<p>For these data, $p = (1/10)^{1/6} \approx 0.681$ and $q \approx 0.201$.</p>

<p>The p-box therefore contains the graphs of all distribution laws $F$ for which (1) $q \le F(5) \le 1$ and (2) $p \le F(10) \le 1$.  A part of this p-box (between $x=0$ and $x=15$) is shaded in gray:</p>

<p><img src=""http://i.stack.imgur.com/0vHBO.png"" alt=""Figure""></p>

<p>Within this p-box I have drawn the graphs of some distributions that come close to its boundary.  All three are two-parameter Exponential distributions.  The dark red one is determined by the two corners it passes through.  The other two (dashed gray ones) show a distribution with a large standard deviation and one with a small standard deviation.</p>

<p>These illustrations should make it clear that if you wished to construct a <em>parametric</em> p-box encompassing only two-parameter Exponential distributions, then it would exclude the region between the thick red curve and the lower line segment between $x=5$ and $x=10$, but it would have to include everything else.  We don't necessarily achieve a lot by making a strong parametric assumption in this example.</p>

<p><strong>What happens when the amount of data increases?</strong>  Suppose as you collect more data that all results continue to be either $\lt 5$ or $\lt 10$ and that their proportions remain roughly 1:1.  Then the value of $p$ approaches $1.0$--there can't be much of a chance of observing anything greater than $10$--and the value of $q$ approaches $1/2$, the proportion of $\lt 5$s in the population.  Making an Exponential distribution assumption allows you to conclude a little more: the chance of observing anything above $5$ grows vanishingly small, too.  Here is the situation upon observing $3000$ values of ""$\lt 5$"" and $3000$ values of ""$\lt 10$"":</p>

<p><img src=""http://i.stack.imgur.com/nJyHc.png"" alt=""Figure 2""></p>

<p>Nevertheless, there is a huge set of possible distributions consistent with such data, even when the dataset becomes enormous.  Intuitively this should be clear: for example, any distribution supported on a set of numbers less than $5$ will be consistent with such data and its graph will fit within this p-box.</p>

<p><strong>This approach to data analysis is not as powerful as, say, a Bayesian analysis.</strong>  However, it requires minimal assumptions: we only suppose that the data are identically and independently distributed.  There is no need to adopt a prior and justify your choice of it, for instance.  (Such checking is rather difficult when all the data are censored!)  If the p-box is sufficiently narrow to let you draw a firm conclusion, then you're done and you know your conclusion is extremely robust.  Otherwise, the p-box can be a useful tool to help you explore the impact of potential additional assumptions on your conclusions.</p>

<hr>

<p>Here is some <code>R</code> code to play with.</p>

<pre class=""lang-r prettyprint-override""><code>k1 &lt;- 3; k2 &lt;- 3     # Counts
x1 &lt;- 5; x2 &lt;- 10    # Quantitation limits
confidence &lt;- 90/100
#
# Find p and q.
#
x &lt;- c(rep(x1, k1), rep(x2, k2))
n &lt;- length(x)
alpha &lt;- 1 - confidence
p &lt;- uniroot(function(p) pbinom(n-1, n, p) - confidence, c(0,1))$root
    q &lt;- uniroot(function(p) pbinom(k1-1, n, p) - confidence, c(0,1))$root
#
# Plot the p-box.
#
plot(c(0, 15), c(0, 1), type=""n"", bty=""n"", xlab=""X"", ylab=""Probability"")
rect(0, 0, x1, 1, col=gray(.975), border=NA)
rect(x1, q, x2, 1, col=gray(.975), border=NA)
rect(x2, p, 15, 1, col=gray(.975), border=NA)
#
# Show a parametric solution.
#
F.inv &lt;- qexp; F &lt;- pexp            # Exponential distribution family
F.p &lt;- F.inv(p); F.q &lt;- F.inv(q)
sigma &lt;- (x2 - x1) / (F.p - F.q)    # Scale parameter
mu &lt;- x2 - sigma*F.p                # Offset parameter
curve(F((x - mu)/sigma), add=TRUE, col=""Red"", lwd=2)
f &lt;- stepfun(c(x1, x2), c(0, q, p)) # Lower limit of the p-box
plot(f, add=TRUE, verticals=FALSE)
curve(1 + 0*x, add=TRUE)            # Upper limit of the p-box
</code></pre>
"
0.133630621,0.12803688,141080,"<p>To generalize results to a greater population, a representative sample is necessary. In developing a psychometrically valid instrument, via confirmatory factor analysis or item response theory, we are interested in understanding the underlying relationships among items, and whether they measure a unified construct with sufficient accuracy and precision. When we conduct such a study, we are not interested in the resulting respondent scores; the respondents serve as a source of data to test whether the structure exists as we hypothesize.</p>

<p>I am looking for various views on whether or not it is necessary to have a representative sample to conduct such a validity study. Measurement invariance or differential item functioning - a key part of establishing validity - requires different subgroups, but having subgroups does not mean the sample overall is representative of a population. My question comes from a practical angle - oftentimes, when developing a new instrument or assessment, it can be challenging to find a representative sample to field it on without already having it validated, kind of a Catch-22. If you are going to make the case that a representative sample is not necessary, please provide citations if possible.</p>
"
0.313391585,0.3002731,141107,"<p>I will demonstrate using an AR(1) model. Consider a variable, $\left(y_{t}\right)_{t=0,1,\ldots}
 $. Then consider the AR(1) model: $y_{t}=\rho y_{t-1}+\varepsilon_{t},\quad\varepsilon_{t}\thicksim N\left(0,\,\sigma^{2}\right)
 $.</p>

<p>Solving the AR(1) model we get:$y_{t}=\rho^{t}y_{0}+\sum_{i=0}^{t-1}\rho^{i}\varepsilon_{t-i}
 $</p>

<p>where $y_{0}
 $ is the initial value of the process. Finding the variance we get: $\sigma^{2}\frac{1-\rho^{2t}}{1-\rho}
 $. Clearly this process is not stationary nor weakly mixing (hence its not ergodic since weakly mixing imlies ergodicity) as both the conditional mean and the conditional variance depend upon time. We can therefore write:$y_{t}=\rho^{t}y_{0}+\sum_{i=0}^{t-1}\rho^{i}\varepsilon_{t-i}\stackrel{D}{=}N\left(\rho^{t}y_{0},\,\sigma^{2}\frac{1-\rho^{2t}}{1-\rho^{2}}\right)
 $</p>

<p>If however the condition $\left|\rho\right|&lt;1
 $ holds then the series is stationary and weakly mixing (hence ergodic since weakly mixing implies ergodicity) and we can write it as:$y_{t}=\sum_{i=0}^{t-1}\rho^{i}\varepsilon_{t-i}\stackrel{D}{=}N\left(0,\,\sigma^{2}\frac{1}{1-\rho^{2}}\right)
 $</p>

<p>since when $\left|\rho\right|&lt;1
 $ holds then $\rho^{t}\rightarrow0
 $ exponentially fast and we have that $\rho^{t}y_{0}\rightarrow0
 $ and $\sigma^{2}\frac{1-\rho^{2t}}{1-\rho^{2}}\rightarrow\sigma^{2}\frac{1}{1-\rho^{2}}
 $ exponentially fast. To summarize, when $\left|\rho\right|&lt;1$
  and $\varepsilon_{t}$
  is normally distributed then the condition $\left|\rho\right|&lt;1$
  is enough to ensure ergodicity of both the mean and the variance. Now we know that $\left|\rho\right|&lt;1
 $ is enough to ensure stationarity and ergodicity of the AR(1) process and in order to test this we could run a unit root test on the series and see if the series had a unit root or not. Other sources of non-stationarity could be a deterministic trend, changing variance and structural breaks, all of which we could model. A unit root however needs a transformation of the data or we could use cointegration analysis if we had more than one series.</p>

<p>To illustrate I will simulate the AR(1) model: $y_{t}=y_{t-1}+\varepsilon_{t}
 $:
<img src=""http://i.stack.imgur.com/BrBwN.png"" alt=""""></p>

<p>We see that the autocorrelation function decays very slowly and from the time series graph the series does not look very stationary. Therefore we can say that clearly the assumption about stationarity and weakly mixing (hence ergodicity) does not hold when $\left|\rho\right|=1
 $. For information about the autocorrelation function see <a href=""http://en.wikipedia.org/wiki/Autocorrelation."" rel=""nofollow"">here</a>.</p>

<p>For the second illustration I will simulate the AR(2), $y_{t}=0.3y_{t-1}+0.4y_{t-1}+\varepsilon_{t}
 $ model but I will try to model it with an AR(1) model to see how the residuals look like.<img src=""http://i.stack.imgur.com/6YOe0.png"" alt=""enter image description here""></p>

<p>We see that the time series graph looks stationary and the ACF goes exponentially towards zero indicating stationarity and weakly mixing. By fitting an AR(1) model to the data we get the following model: $y_{t}=0.54y_{t-1}+\varepsilon_{t}
 $. If we now look at the residual ACF we see that they are not white noise (a martingale diff. seq. is white noise) and the portmanteu test rejects the null of no autocorrelation.<img src=""http://i.stack.imgur.com/vcCvV.png"" alt=""enter image description here""></p>

<p>If we then fit an AR(2) to the simulated data we get an estimated model of: $y_{t}=0.37y_{t-1}+0.32y_{t-1}+\varepsilon_{t}
 $. Now the residuals do look like white noise and we cannot reject the null of no autocorrelation.<img src=""http://i.stack.imgur.com/e8WJo.png"" alt=""enter image description here""></p>

<p>If this did not answer your question then let me know.</p>
"
0.192879187,0.221766381,186725,"<p>Short version: How would one be able to quantify an intervention effect in time-series analysis when the intervention decreases seasonal amplitude variation but doesn't directly effect the median?</p>

<p><a href=""https://www.dropbox.com/s/hb3g7j17igeqnoc/dat.csv?dl=0"" rel=""nofollow"">Here</a> is a link to my raw data.</p>

<p>I have a complex time-series of daily incidence numbers for a population over 7 years, totaling 2557 observations. There is a strong weekly and yearly seasonality (high incidence in winter months and low incidence in summer months). There is a baseline negative trend which is orders of magnitude smaller than the seasonality. An intervention was introduced at time = 1700. This intervention should theoretically not cause a level shift. My aim is to detect whether the intervention increases the baseline negative trend.</p>

<p>I have attempted to fit a dynamic linear regression with ARIMA errors in R using <code>auto.arima()</code> in the <code>forecast</code> package. I modeled the weekly season using a dummy variable for each weekday and the weekend. I modeled the monthly seasonality with harmonics using <code>fourier()</code> function in the <code>forecast</code> package. An the intervention effect was coded in by specifying the time index and post-intervention times as independent variables using the methods described in <a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a>. With these variables specified <code>auto.arima()</code> suggests an ARMA(7,7) process. The coefficients for baseline trend and post-intervention trend are however non-significant.  </p>

<p>I am concerned that by using fourier terms to model away the seasonality I am artificially removing any intervention effect, as visual analysis of the time series indicates that the intervention is specifically decreasing incidence during the winter months and therefore reducing the yearly seasonal variability. </p>
"
0.211288564,0.202444083,140465,"<p>I have just tried two ways to perform a linear discriminant analysis.</p>

<p><strong>Mode 1</strong></p>

<p>This is the data, divided in two tables (printed from screen using R commander):
<img src=""http://i.stack.imgur.com/V43Qx.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/72LJJ.png"" alt=""enter image description here""></p>

<p>They belong respectively to the two already known groups.</p>

<p>I ran the next code to get the coefficients and critic value of the function:</p>

<pre><code>m1=cov.wt(Craneos1)$center
    m1=data.matrix(m1)
    m2=cov.wt(Craneos2)$center
m2=data.matrix(m2)
D=((nrow(Craneos1)-1)*var(Craneos1)+(nrow(Craneos2)-1)*var(Craneos2))/30
d=solve(D)%*%(m1-m2)
d # Estos son los coeficientes de la función
0.5*t(m1+m2)%*%d # Este es el punto crítico
</code></pre>

<p>I get the next results:</p>

<pre><code>&gt; d # Coefficients of the discriminant analysis
           [,1]
x1 -0.089306662
x2  0.155774683
x3  0.005231617
x4 -0.177194601
x5 -0.177408670

&gt; 0.5*t(m1+m2)%*%d # Este es el punto crítico
          [,1]
[1,] -30.46349
</code></pre>

<p>These results are exactly the same as those ones that appeared in a book I am studying.</p>

<p><strong>Mode 2</strong></p>

<p>I reordered the last two tables into only one with the first column as a factor:
<img src=""http://i.stack.imgur.com/sXAk6.png"" alt=""enter image description here""></p>

<p>In this case, I used the function <strong>lda</strong> (from the package MASS), since it has been widely used in examples on internet.</p>

<p>I ran the next command:</p>

<pre><code>lda(group~.,data=Datos)
</code></pre>

<p>Which displayed the next results:</p>

<pre><code>&gt; lda(group~.,data=Datos)
Call:
lda(group ~ ., data = Datos)

Prior probabilities of groups:
CRANEOS1 CRANEOS2 
 0.53125  0.46875 

Group means:
               X1       X2       X3       X4       X5
CRANEOS1 174.8235 139.3529 132.0000 69.82353 130.3529
CRANEOS2 185.7333 138.7333 134.7667 76.46667 137.5000

Coefficients of linear discriminants:
            LD1
X1  0.047726591
X2 -0.083247929
X3 -0.002795841
X4  0.094695000
X5  0.094809401
</code></pre>

<p>My questions are:</p>

<p>Is there anything wrong in the code displayed above? </p>

<p>Why are the both coefficients different depending on the mode used? </p>

<p>Which of them must be more reliable? </p>

<p>How can I get the critic value using the Mode 2?</p>
"
0.25,0.239535069,749,"<p>Regarding shopping cart analysis, I think that the main objective is to individuate the most frequent combinations of products bought by the customers. The <code>association rules</code> represent the most natural methodology here (indeed they were actually developed for this purpose). Analysing the combinations of products bought by the customers, and the number of times these combinations are repeated, leads to a rule of the type ‘if condition, then result’ with a corresponding interestingness measurement. You may also consider <code>Log-linear models</code> in order to investigate the associations between the considered variables.</p>

<p>Now as for clustering, here are some information that may come in handy:</p>

<p>At first consider <code>Variable clustering</code>. Variable clustering is used for assessing collinearity, redundancy, and for separating variables into clusters that can be scored as a single variable, thus resulting in data reduction. Look for the <code>varclus</code> function (package Hmisc in R)</p>

<p>Assessment of the clusterwise stability: function <code>clusterboot</code> {R package  fpc}</p>

<p>Distance based statistics for cluster validation: function <code>cluster.stats</code> {R package  fpc}</p>

<p>As mbq have mentioned, use the silhouette widths for assessing the best number of clusters. Watch <a href=""http://www.google.com/codesearch/p?hl=en#sTQFIWS4uR8/afs/sipb/project/r-project/arch/sun4x_510/lib/R/library/cluster/R-ex/pam.object.R&amp;q=lang%3ar%20%22optimal%20number%20of%20clusters%22&amp;sa=N&amp;cd=6&amp;ct=rc"">this</a>. Regarding silhouette widths, see also the <a href=""http://finzi.psych.upenn.edu/R/library/optpart/html/optsil.html"">optsil</a> function. </p>

<p>Estimate the number of clusters in a data set via the <a href=""http://finzi.psych.upenn.edu/R/library/clusterSim/html/index.GAP.html"">gap statistic</a></p>

<p>For calculating Dissimilarity Indices and Distance Measures see <a href=""http://finzi.psych.upenn.edu/R/library/labdsv/html/dsvdis.html"">dsvdis</a> and <a href=""http://finzi.psych.upenn.edu/R/library/vegan/html/vegdist.html"">vegdist</a></p>

<p>EM clustering algorithm can decide how many clusters to create by cross validation, (if you can't specify apriori how many clusters to generate). <em>Although the EM algorithm is guaranteed to converge to a maximum, this is a local maximum and may not necessarily be the same as the global maximum. For a better chance of obtaining the global maximum, the whole procedure should be repeated several times, with different initial guesses for the parameter values. The overall log-likelihood figure can be used to compare the different final configurations obtained: just choose the largest of the local maxima</em>.
You can find an implementation of the EM clusterer in the open-source project <a href=""http://www.cs.waikato.ac.nz/~ml/weka/"">WEKA</a></p>

<p><a href=""http://zoonek2.free.fr/UNIX/48_R/06.html"">This</a> is also an interesting link.</p>

<p>Also search <a href=""http://www.statsoft.com/textbook/cluster-analysis/#vfold"">here</a> for <code>Finding the Right Number of Clusters in k-Means and EM Clustering: v-Fold Cross-Validation</code></p>

<p>Finally, you may explore clustering results using <a href=""http://had.co.nz/model-vis/"">clusterfly</a> </p>
"
0,0,87813,"<p>Still don't have a good grasp of what you're looking for. But if you're comparing binary classification models I wouldn't recommend just using AUC (a measure of discrimination)            </p>

<p>Usually one uses three metrics together, choosing one from each of the three categories below. :<br>
(1) Global measure:  scaled Brier or N's R-sq. The difference between Brier and N's R-sq may be significant due to different penalty functions<br>
The Brier score is often preferred, but should usually be converted to a scale Brier score (thought this is less of an issue of all analysis on same dataset).<br>
(2) Discrimination: AUC, discrimination slope<br>
(3) Calibration:  calibration slope with calibration or validation graph                   </p>
"
0.133630621,0.12803688,71444,"<p>With the usual definitions of linear and nonlinear with regard to modelling, it's not linearity with respect to the predictors that's the critical aspect, but linearity with respect to the parameters. A nonlinear model is nonlinear because it's not linear in parameters. </p>

<p>For example, the first sentence <a href=""http://en.wikipedia.org/wiki/Nonlinear_regression"">here</a> says:</p>

<blockquote>
  <p>In statistics, nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables.</p>
</blockquote>

<p>By contrast, <a href=""http://en.wikipedia.org/wiki/Generalized_linear_model"">Generalized <em>Linear</em> Models</a> generally have a nonlinear relationship between response and predictors, but the link-transformed mean response (the <em>linear predictor</em>, $\eta$) is linear in the parameters.</p>

<p>[By that definition, I believe your model is nonlinear in the $\theta$s, though if the $\theta$s are specified (known) then that nonlinearity isn't relevant to estimation. If they're being fitted, then the model is nonlinear.]</p>
"
0.327326835,0.313625024,41819,"<h1>Answer to the question</h1>

<p>First of all, it is important to notice that the quantities $P(X\leq Y)$ and $P(X\lt Y)$ are different, given that the variables are not continuous.</p>

<p>Let $X$ and $Y$ be two independent random variables whose distribution is a mixture of a discrete and a continous distribution such that $P(X=0)=p_1&gt;0$ and $P(Y=0)=p_2&gt;0$. Then by the law of total probability we have that </p>

<p>\begin{eqnarray*}
P(X\leq Y)&amp;=&amp;P(X\leq Y \vert Y=0)P(Y=0)+P(X\leq Y \vert Y&gt;0)P(Y&gt;0)\\
&amp;=&amp; P(X=0)P(Y=0)+P(X\leq Y \vert Y&gt;0)[1-P(Y=0)]\\
&amp;=&amp; p_1p_2 +P(\{X\leq Y\} \cap \{X=0 \cup X&gt;0\} \vert Y&gt;0)(1-p_2).
\end{eqnarray*}</p>

<p>Now, $P(\{X\leq Y\} \cap \{X=0 \cup X&gt;0\} \vert Y&gt;0)=p_1+P(X\leq Y\vert X&gt;0,Y&gt;0)(1-p_1)$. With this, we obtain an expression for $\theta=P(X\leq Y)$ in terms of quantities that we can estimate. Note that</p>

<p>\begin{eqnarray*}
P(X\leq Y\vert X&gt;0,Y&gt;0)=\int_0^{\infty}F_X(y)f_Y(y)dy,
\end{eqnarray*}</p>

<p>where $F_X$ is the CDF of the continuos part $X$ and $f_Y$ is the PDF of the continuos part of $Y$ (In your case, a Lomax distribution).</p>

<p>Now, <strong>how to estimate the parameters?</strong> I am going to use nonlinear squares between the CDFs and the empirical CDFs. This method works in your case given the large sample size. Please, find below an R code for conducting this estimation using a simulated sample of size $n=1000$.</p>

<pre><code>rm(list=ls())
p0 = 0.75
alpha0 = 3
lambda0 = 1

# Function for simulating a Lomax variable
rlomax = function(n,alpha,lambda) return( lambda*( (1-runif(n))^(-1/alpha) - 1 ))

# Simulated data, X and Y
set.seed(1)
ns = 1000
simx = simy = rep(0,ns)

for(i in 1:ns){
u = runif(1)
if(u&lt;p0) simx[i] =  0
else simx[i] = rlomax(1,alpha0,lambda0)
}

for(i in 1:ns){
u = runif(1)
if(u&lt;p0) simy[i] =  0
else simy[i] = rlomax(1,alpha0,lambda0)
}

hist(simx,col=""red"")
hist(simy,add=T,col=""blue"")

# Distribution function of the mixture

FM = function(x,p,alpha,lambda){
temp = x
for(i in 1:length(x)){
if(x[i]==0) temp[i]=p
if(x[i]&gt;0) temp[i] = p + (1-p)*( 1-(1+x[i]/lambda)^(-alpha) )
}
return(temp)
}


ecdfdatx = ecdf(simx)(sort(simx))
ecdfdaty = ecdf(simy)(sort(simy))

Datax = data.frame(sort(simx),ecdfdatx)
Datay = data.frame(sort(simy),ecdfdaty)

# Fit for the first data set

nls_fitx = nls(ecdfdatx ~ FM(sort(simx),p,alpha,lambda), data=Datax, start =     list(p = 0.75, alpha = 3,  lambda = 1) )
nls_fitx
plot(ecdf(simx))
lines(sort(simx), predict(nls_fitx), col = ""red"")

# Fit for the second data set

nls_fity = nls(ecdfdaty ~ FM(sort(simy),p,alpha,lambda), data=Datax, start = list(p =     0.75, alpha = 3,  lambda = 1) )
nls_fity
plot(ecdf(simy))
lines(sort(simy), predict(nls_fity), col = ""red"")
</code></pre>

<p>With this code we obtain estimators of the parameters $(p_1,\alpha_X,\lambda_X,p_2,\alpha_Y,\lambda_Y)$. The remaining step consists of calculating $P(X\leq Y\vert X&gt;0,Y&gt;0)=\int_0^{\infty}F_X(y)f_Y(y)dy$.</p>

<pre><code># remaining quantity

px.h = coef(nls_fitx)[1]
py.h =coef(nls_fity)[1]
ax.h = coef(nls_fitx)[2]
ay.h = coef(nls_fity)[2] 
lx.h = coef(nls_fitx)[3] 
ly.h = coef(nls_fity)[3] 

# Lomax PDF
dlomax = function(x,alpha,lambda) return(alpha*(1+x/lambda)^(-(alpha+1))/lambda)

# Lomax CDF
plomax = function(x,alpha,lambda) return(1-(1+x/lambda)^(-alpha) )

tempf = function(x) plomax(x,ax.h,lx.h)*dlomax(x,ay.h,ly.h)

p.l = integrate(tempf,0,Inf)$value

# Estimator of theta
px.h + p.l*(1-px.h)*(1-py.h)
</code></pre>

<p>Similarly, the estimator of $P(X&lt;Y)$ can be calculated as follows</p>

<pre><code># Estimator of theta2
p.l*(1-px.h)*(1-py.h)
</code></pre>

<p>Then the quantity $P(X\leq Y)$ depends on the probabilities $p_1$ and $p_2$ and therefore this quantity may be misleading. For instance, if $X$ and $Y$ are i.i.d. and $p_1,p_2\approx 1$, we have that $P(X\lt Y)\approx 0$ and $P(X \leq Y)\approx 1$.</p>

<p><strong>My conclusion:</strong> The stress-strength coefficient is not what you need for comparing the performance of both companies.</p>

<h1>How to solve the problem?</h1>

<p>I think this problem can be seen as a <a href=""http://en.wikipedia.org/wiki/Decision_theory"" rel=""nofollow"">decision problem</a>. You have two companies providing a programming service and you want to decide which one is better. Consider the hypothetical case where one of the companies produce a large proportion of codes with zero errors but also that when a code contains errors, it is likely that the number of errors is large. Is this better than a company with a lower proportion of codes with zero errors but smaller positive errors?</p>

<p><em>A toy naive example</em>. Suppose that your decision rule is based on the estimated proportions of 0-error codes as follows:</p>

<ol>
<li>Estimate $p_1$ and $p_2$. If $\hat{p}_1/\hat{p}_2\in(0.9,1.1)$, then proceed to estimate $\theta = P(X&lt;Y)$
using only the positive quantities. If a $95\%$ confidence interval for $\theta$ contains the value $0.5$, then there is no criterion for choosing one of the companies. If this value is not contained in the confidence interval, then choose Company X if $P(X&lt;Y)&gt;0.5$ or Company Y is $P(X&lt;Y)&lt;0.5$.</li>
<li>If the ratio of estimators $\hat{p}_1/\hat{p}_2&lt;0.9$, then choose Company Y.</li>
<li>If the ratio of estimators $\hat{p}_1/\hat{p}_2&gt;1.1$, then choose Company X.</li>
</ol>

<p>This (naive, I repeat) criterion favours companies that produce more 0-error codes and proceeds to select one based on the stress-stress coefficient of the continous part when they seem to produce a similar proportion of 0-error codes.</p>

<p>In order to conduct a proper analysis, one would need to select a proper loss-function based on expert's opinion in order to come up with a reasonable selection criterion. This would require more effort and I think it would fall out of the scope of this site but I hope this answer gives you some help.</p>

<p>Some references of possible interest:</p>

<blockquote>
  <p><a href=""http://www.amazon.co.uk/Statistical-Decision-Bayesian-Analysis-Statistics/dp/0387960988/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1351960851&amp;sr=1-2"" rel=""nofollow"">Statistical Decision Theory and Bayesian Analysis</a></p>
  
  <p><a href=""http://www.amazon.co.uk/Bayesian-Theory-Series-Probability-Statistics/dp/047149464X"" rel=""nofollow"">Bayesian Theory</a></p>
  
  <p><a href=""http://www.amazon.co.uk/Bayesian-Decision-Analysis-Principles-Practice/dp/0521764548/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1351960851&amp;sr=1-1"" rel=""nofollow"">Bayesian Decision Analysis: Principles and Practice</a></p>
</blockquote>

<p>It would also help to check the literature on <a href=""http://en.wikipedia.org/wiki/Software_quality_control"" rel=""nofollow"">Software quality control</a> and see the critera adopted by some companies.</p>
"
0.167038276,0.22406454,41822,"<p><strong>EDIT:</strong>
<em>OK, having spent the last couple of hours checking out some pubs on functional data analysis, I am actually starting to feel a little silly having asked the question in the first place! It's becoming obvious (well. Still a tad -ish but getting there.) - Thanks, @PeterFlom for pointing it out! I'll leave the question up for now, should anyone see something worth looking out for as I venture into FDA with my data...?</em></p>

<p>Here's what I have (this is for voice analysis):</p>

<p>Suppose I have three speakers; each speaker produces say 10 tokens of a particular vowel; I've measured each token and fitted time-normalised 3rd order polynomial curves to the vowel formants (I'm using three formants, which in layman's terms are the frequencies determining the vowel sound, which change over time; the formants are determined by vocal tract shape, so they are interrelated). I have also calculated the average polynomial function to each formant for each speaker, to give me a general idea of what this vowel looks like for each speaker on average, leaving me with a set of three related polynomial functions (one for each vowel formant) for each of the three speakers.</p>

<p>Pictures might help clarify! The first figure shows 10 individual tokens (three time-normalised polynomials (because three formants) per token) for three speakers; the second figure shows the average polynomials for each formant for each speaker. (I've indicated the frequency range of each formant, hope that makes a little clearer what I'm talking about!):</p>

<p><img src=""http://i.stack.imgur.com/91AX1.gif"" alt=""Polynomials for individual tokens for three speakers"">
<img src=""http://i.stack.imgur.com/RhTAf.gif"" alt=""Average polynomials for three speakers""></p>

<p>(You can see that the degree of variability is quite massive within a single speaker; but you can still guess from the graphs that speakers 1 &amp; 2 are very similar, at least in terms of these particular vowel tokens (our guess is they're the same speaker), and speaker 3 is someone else (this we happen to know).)</p>

<p>To start with, I would be happy if I could find a way that would give me a basic distance measure between the average polynomials for each speaker. (Something to say, but in numerical terms, e.g. that speaker 1 and speaker 2's formants look very similar to each other (at least with regards to the first and second formants), but quite different to speaker 3's formants.)</p>

<p>Ideally, I want to be able to compare all the speakers' tokens, where each token is described by three polynomial functions, and get a probability to tell me how likely it is that the speakers are the same, based on their productions for this vowel (i.e. taking into account within-speaker variability). But if someone could just help me with my first question I just might be able to figure everything else out from there!</p>

<p>Thank you so much!</p>
"
0.163663418,0.156812512,57093,"<p>Model the actions of the players instead of the payoffs. That is, predict the probability that a player selects to cooperate at a particular round as a function of previous rounds (if the game is repeated in your setting) and your covariates. I think this makes more causal sense, as the players actually select the actions influenced by whatever, and the payoffs are just a deterministic function of the actions. Furthermore, this makes the output variables binary, which simplifies the analysis, as you do not have to think about the potentially difficult dependence between total payoffs.</p>

<p>I guess it is also probably fine to treat the strategies selected by each player as conditionally independent given the covariates&amp;history, which makes the analysis just simple prediction of a binary variable. On the other hand, one could argue that unobserved variables might lead to dependence. </p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3435562/"" rel=""nofollow"">Angel Sanchéz</a> has applied logistic regression to modeling the probability of cooperating in Prisoner's dilemma. Their setting is probably somewhat different as it involves multiple players in a network, but you should still take a look to see if their approach can be modified to your setting.</p>
"
0.094491118,0.090535746,201776,"<p>I have to study tolerance intervals for a distribution of a random variable Z that is given by the difference of a normal X minus a (independent) lognormal Y.</p>

<p>To begin with I tried to get an expression for the PDF of Z, but I got stuck in solving the integral </p>

<p>$f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(x-z)dx$.</p>

<p>where $f_X$ is the PDF of a normal random variable and $f_Y$ is the PDF of a lognormal RV independent from X. I don't even know if this integral is solvable in terms of elementary functions.</p>

<p>Is there any document I could read to progress in my analysis towards a tolerance interval? Do you believe this kind of distribution is tractable in a theoretical way?</p>

<p>Thank you</p>
"
0.169030851,0.202444083,201787,"<p>I am guessing that the reviewer wanted to see a test of the null-hypothesis that the two hazard ratios are the same. A rejection of that null-hypothesis would support your conclusion. Typically, you do so by estimating one model for both groups, add an interaction term, and look at the test whether that interaction term is 0. </p>

<p>With a Cox model there is a complication that by estimating two separate models you allowed the baseline hazard function to be different for both groups. So combining the two models and adding an interaction term is not exactly equivalent to estimating the two models separately.  </p>

<p>You could use stratification for this, this allows separate baseline hazard functions. So a silly example using Stata:</p>

<pre><code>. sysuse cancer, replace
(Patient Survival in Drug Trial)

. stcox i.drug##c.age, strata(drug) nolog

         failure _d:  died
   analysis time _t:  studytime

 Stratified Cox regr. -- Breslow method for ties

 No. of subjects =           48                  Number of obs    =          48
 No. of failures =           31
 Time at risk    =          744
                                                LR chi2(3)       =       10.73
 Log likelihood  =   -59.372185                  Prob &gt; chi2      =      0.0133

------------------------------------------------------------------------------
          _t | Haz. Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
 -------------+----------------------------------------------------------------
        drug |
          2  |          1  (omitted)
          3  |          1  (omitted)
             |
         age |   1.096463   .0533264     1.89   0.058     .9967717    1.206124
             |
  drug#c.age |
          2  |   1.110106   .1132812     1.02   0.306     .9088726    1.355893
          3  |   .9860279   .0992645    -0.14   0.889     .8094646    1.201104
------------------------------------------------------------------------------
                                                            Stratified by drug
</code></pre>

<p>The main effect of drug are omitted because they are captured by the stratification. The main effect of age is the effect of age when you get drug 1, and the interaction effects tell us whether the effect of age differs when you get drug 2 or drug 3 respectively. The tests behind those interaction terms are the ones you would be looking for.</p>
"
0.267261242,0.25607376,231676,"<p>Let the columns of $X$ be $X_1,X_2,\ldots, X_n$, the corresponding entries of $A$ be $a_1, a_2, \ldots, a_n$, the columns of $Y$ be $Y_1, Y_2, \ldots, Y_n$, and the error columns be $e_1, e_2, \ldots, e_n$.</p>

<p>Notice that</p>

<p>$$e_i = Y_i - a_i X_i.$$</p>

<p>Each parameter $a_i$ is involved in only one of these expressions.  Therefore, the sum of squares of the $e_i$, equal to $\sum_{i=1}^n |e_i|^2$, can be minimized by separately and independently finding $a_i$ that minimize the squared norms $|e_i|^2 = e_i^\prime e_i$.  That's a set of $n$ (univariate) regression-through-the-origin problems.  With no constraints on the $a_i$, the solutions would be</p>

<p>$$\hat a_i = \frac{Y_i^\prime X_i}{X_i^\prime X_i}.$$</p>

<p>If any of the $\hat a_i$ lies outside the constraining interval $[0,1]$, the convexity of the objective function shows you only need to examine its values on the boundary $\partial[0,1]=\{0,1\}$. A simple approach is an exhaustive search of both points: that is, compare the values of $|Y_i - X_i|^2$ and $|Y_i|^2$, choosing $\hat a_i = 1$ when the former is smaller and $\hat a_i=0$ otherwise.</p>

<hr>

<p>Here are two examples with $m=100$ rows and $n=5$ columns.  They were generated by creating the $X$ and $A$ matrices randomly and adding random errors to them to obtain $Y$.  Provided the entries in $A$ are all in the range $[0,1]$, the estimated values should be close to the original ones (depending on how large the random errors are).  The left plot in each example is a dotplot of the estimate $\hat a_i$ and the original parameter value $a_i$, enabling visual comparison of the estimates to the parameters.  The right plot in each example is a scatterplot of the residuals of this fit (the $\hat e_i$) against the original errors.  When the constraints are not applied (as in the bottom row), this scatterplot should be tightly focused on the line of equality.  When constraints are applied (the top row), there will be more scatter (contributed by the corresponding columns).</p>

<p><a href=""http://i.stack.imgur.com/7ySTW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7ySTW.png"" alt=""Figure""></a></p>

<hr>

<p>The <code>R</code> code to produce this figure will let you experiment with arbitrary values of $m$ and $n$. The estimate of $A$ consists of four lines exactly paralleling the analysis: computation of the regression coefficient, of the two values at the boundary, and the comparisons needed to select the best one.  It is fast and parallelizable--apart from the plotting step, it will run in seconds even when $n$ is in the millions ($10^6$).</p>

<pre class=""lang-R prettyprint-override""><code>m &lt;- 100
n &lt;- 5
par(mfrow=c(2,2))

for (i in c(23, 19)) {
  #
  # Generate data.
  #
  set.seed(i)
  x &lt;- matrix(rnorm(m*n), m)
  alpha &lt;- rnorm(n, 1/2, 1/2)
  eps &lt;- matrix(rnorm(m*n, 0, 1/4), m)
  y &lt;- t(t(x) * alpha) + eps
  #
  # Compute A.
  #
  a &lt;- colSums(x*y) / colSums(x*x)
  a.0 &lt;- colSums(y*y)
  a.1 &lt;- colSums((y-x)*(y-x))
  a &lt;- ifelse(0 &lt;= a &amp; a &lt;= 1, a, ifelse(a.0 &lt;= a.1, 0, 1))
  #
  # Plot results.
  #
  e &lt;- y - x %*% diag(a)
  u &lt;- rbind(Parameter=alpha, Estimate=a)
  dotchart(u, col=ifelse(abs(u-1/2)&gt;1/2, ""Red"", ""Blue""), cex=0.6, pch=20, 
           xlab=""Parameter value"")
  plot(as.vector(eps), as.vector(e), asp=1, col=""#00000040"", 
       xlab=""Error"", ylab=""Residual"")
}
</code></pre>
"
0.094491118,0.090535746,186659,"<p>Hierarchical Clustering is a clustering algorithm, that can also be used to order elements. Essentially, it will order them such that very similar items are very close.</p>

<p>See the documentation of the <a href=""http://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html"" rel=""nofollow""><code>hclust</code></a> function:</p>

<blockquote>
  <p>Hierarchical cluster analysis on a set of dissimilarities and
       methods for analyzing it.</p>
</blockquote>
"
0.412393049,0.414886909,87851,"<p>Residuals are the errors in the predictions of your linear model for each observation. Because yours is a simple linear model, your errors look a lot like your observations, but they are not <em>exactly</em> identical. Your regression line in the first scatterplot is essentially the same as the horizontal dotted line in your second scatterplot, <code>Residuals vs. Fitted</code>, but angled upward to reflect a positive correlation between year and rainfall, and passing through different coordinates. Hence the pattern only differs between these two scatterplots in terms of that angular rotation and the locations of your coordinates. I.e., your <code>Residuals vs. Fitted</code> scatterplot rotated and relocated the data coordinates so that your scatterplot $x$ and $y$ axes changed from <code>Year</code> and <code>Rainfall</code>, respectively, to:
$$x_{\text{Residuals vs. Fitted}}= \text{Year}\times\text{slope}+\text{intercept }\\\ \
y_{\text{Residuals vs. Fitted}}=\text{Rainfall} - x_{\text{Residuals vs. Fitted}}$$
Thus your slope coefficient determines the change in angle and your intercept coefficient changes the location of the observation pattern in your coordinate space. If either were closer to zero, your patterns would be that much more identical. Either value being exactly zero is a perfectly tolerable and potentially meaningful result for a linear regression. In fact, these represent the null hypotheses of typical $t$ tests for those coefficients, so they have to be tolerable and interpretable results – otherwise, one could not really try to falsify those nulls this way (however, regression is not a <a href=""http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Null_hypothesis_statistical_significant_testing_vs_hypothesis_testing"" rel=""nofollow"">NHST</a> inherently).</p>

<p>@Penguin_Knight's answer is a useful alternative phrasing of my point here, with an additional consideration of multiple regression, which you may want to consider if you have other independent variables you'd like to relate to rainfall. @charles' answer is an interesting response to whether you should've used this particular model. Building on that, here are some other points to consider:</p>

<ul>
<li><p>Your <code>Residuals vs. Fitted</code> plot has a convex, <a href=""http://en.wikipedia.org/wiki/Local_regression"" rel=""nofollow"">locally weighted scatterplot smoothing</a> (<a href=""/questions/tagged/loess"" class=""post-tag"" title=""show questions tagged &#39;loess&#39;"" rel=""tag"">loess</a>) line drawn through it, if I'm not mistaken. Because it's convex, you might want to try <a href=""http://en.wikipedia.org/wiki/Polynomial_regression"" rel=""nofollow"">polynomial regression</a> or other <a href=""/questions/tagged/curve-fitting"" class=""post-tag"" title=""show questions tagged &#39;curve-fitting&#39;"" rel=""tag"">curve-fitting</a> approaches if you can get more data...but with 21 observations only (one per year, right?), these are exercises in <a href=""/questions/tagged/overfitting"" class=""post-tag"" title=""show questions tagged &#39;overfitting&#39;"" rel=""tag"">overfitting</a>. Their utility depends on your purpose.</p>

<ul>
<li>I tried eyeballing your <code>Rainfall</code> data as roughly the following: <code>Rainfall=c(45,36,53,52,52,43,41,47,55,54,68,47,63,42,57,50,61,60,65,55,47)</code><br> then tried <code>summary(lm(Rainfall~Year+Year2))</code> where <code>Year=1990:2010</code> and <code>Year2=scale(Year,scale=F)^2</code> (Can't seem to square <code>Year</code> inside <code>lm()</code>...). Here's what I got:
$$\text{Rainfall}=.64\times\text{Year}-.05\times\text{Year}^2-122$$
This worsened the model's $F$ and $R^2$, and with $t_{(18)}=-.99,p=.34$ for the quadratic term, I wouldn't reject a null hypothesis that denies a curvilinear relationship, but it is prettier:<img src=""http://i.stack.imgur.com/h0WXR.png"" alt="""">produced with <code>curveplot(Year,Rainfall)</code> where<br>
<code>curveplot=function(x,y){plot(x,y);x2=scale(x,scale=F)^2; lines(x,predict(lm(y~x+x2)))
lines(x,predict(lm(y~x+x2),interval='confidence')[,2],lty=3)  #(these two lines plot
lines(x,predict(lm(y~x+x2),interval='confidence')[,3],lty=3)} #</code>(95% <a href=""/questions/tagged/confidence-interval"" class=""post-tag"" title=""show questions tagged &#39;confidence-interval&#39;"" rel=""tag"">confidence-interval</a>s)<br><br>
You may want to run this function on your actual data, since my eyeballs distorted your <code>Rainfall</code> data...or you might not. It's a more complex model, it might be overfitted, and there's really not enough evidence of a curvilinear trend. If you were modeling a longer period of time or trying to predict rainfall for years outside 1990–2010, it's almost certain that you wouldn't want a quadratic <code>Year</code> term, because it causes the $\lim_{\text{Year}\rightarrow\pm\infty}=-\infty$ (so long as it's negative, as here). Hence I only suggest it in case you want to describe the gradually decreasing slope in this specific time frame with a relatively simplistic model (e.g., as evidence against a strong curve). Also, I wanted to show off a simple confidence interval plot!</li>
</ul></li>
<li><p>If you have more precise information about the times of your observations – e.g., if they weren't <em>exactly</em> annual, and you know how many days/weeks/months short of a full year each observation of <code>Rainfall</code> pertains to – you might want to improve the precision of your <code>Year</code> values, because a simple linear regression model fit with OLS assumes your variables are continuous. E.g., if 1995's observation pertains to the sum of rainfall measurements beginning <em>after</em> the last rainfall measurement of 1994 on December 25 and ending with the last rainfall of 1995 on December 6, you might consider setting the value of <code>Year</code> corresponding to that sixth observation of <code>Rainfall</code> to be $\frac{346}{365}$ higher than the previous value, not exactly 1 higher. You might not want to either; it complicates your work, and it shouldn't matter much as long as the real time differences between your observations don't vary too much. </p>

<p>Annual rainfall measurements between 35–70mm implies incredible aridity though – <a href=""http://en.wikipedia.org/wiki/Climate_of_Antarctica#Precipitation"" rel=""nofollow"">even Antarctica gets more precipitation than that, mostly</a>. Even if you actually meant <em>centimeters</em>, <a href=""http://www.ucmp.berkeley.edu/exhibits/biomes/deserts.php"" rel=""nofollow"">this is probably still a desert</a>. If it's not raining often in your region of interest, your real time differences between observations might vary widely. If you can preserve that information, you should. Also, if you have info on separate observations within years that you are summing to simplify your analysis, bear in mind that this is reducing the accuracy of your model too. Annual rainfall accumulates as a <a href=""/questions/tagged/nonlinear"" class=""post-tag"" title=""show questions tagged &#39;nonlinear&#39;"" rel=""tag"">nonlinear</a> function of day-of-year, and in arid climates, <a href=""/questions/tagged/zero-inflation"" class=""post-tag"" title=""show questions tagged &#39;zero-inflation&#39;"" rel=""tag"">zero-inflation</a> may be useful to incorporate in the model, because many days won't see any rain. </p>

<p>If you can model <code>Rainfall</code> over time with disaggregated measurements and specific dates, you'll also have to worry more about seasonal cycles after all, following charles' original reasoning. Never fear though; we'll be happy to help you pick the right model for that kind of data too, if you describe its disaggregated nature in a separate question (i.e., I wouldn't try to change this question to describe it). If your data is necessarily aggregated to the annual level, and if you don't have specific measurement dates, you can disregard most of this point, unfortunately.</p></li>
<li><p>Another problem with OLS regression for these data may be implied by your <code>Normal Q-Q</code> plot. OLS regression also assumes normally distributed residuals. It looks like yours have excess <a href=""/questions/tagged/kurtosis"" class=""post-tag"" title=""show questions tagged &#39;kurtosis&#39;"" rel=""tag"">kurtosis</a>, though maybe not too much. Regardless, you might consider <a href=""/questions/tagged/robust"" class=""post-tag"" title=""show questions tagged &#39;robust&#39;"" rel=""tag"">robust</a> or <a href=""/questions/tagged/nonparametric"" class=""post-tag"" title=""show questions tagged &#39;nonparametric&#39;"" rel=""tag"">nonparametric</a> alternatives to OLS linear regression. Some sacrifice power, but you might prefer to play it safe.</p></li>
</ul>
"
0.133630621,0.12803688,87316,"<p>I agree with you that P(X=1) is equivalent to Y (or E[Y] if stated before Y is determined).
P(X=1) = E[Y] = int( y * fy, y = 0 .. 1) = int( 2y^2, y = 0..1) = [ 2/3 y^3 | y=0..1] = 2/3 * (1 – 0) = 2/3</p>

<p>For 2, I think the problem is much, much harder</p>

<p>Although it is the binomial, you can’t just use E[Y]</p>

<p>P( k of n heads) = choose(n,k) * y^k * (1-y)^(n-k)</p>

<p>E[ P( k of n heads) ] = E[ choose(n,k) * y^k * (1-y)^(n-k) ]</p>

<p>= int( fy * choose(n,k) * y^k * (1-y)^(n-k), y = 0..1 )</p>

<p>= choose(n,k) * int( 2*y * y^k * (1-y)^(n-k), y = 0..1 )</p>

<p>= 2 * choose(n,k) * int( y^(k+1) * (1-y)^(n-k), y = 0..1 )</p>

<p>This is where the beta function comes in</p>

<p>2 * choose(n,k) * By(k+2,n-k+1)</p>

<p>Just to check if we could have used E[Y], suppose that we were interested in 4 of 6 heads 
(the most probable result at y = 2/3)</p>

<p>choose(6,4) * (2/3)^4 * (1/3)^2 = 80/243 or about 0.33</p>

<p>choose(6,4) * int( 2*y * y^4 * (1-y)^2, y = 0..1 ) = 15/84 or about 0.18</p>

<p>Because y takes on many potential values, it spreads pmf for a multi-toss event.<br>
Conducting the same analysis for the 3 case shows that it is more probable with the random y draw, than with the chosen expected value (which should make intuitive sense)</p>

<p>For 3, I don’t think you need beta</p>

<p>Per Bayes, the probability of a given y value, given that tails was flipped:</p>

<p>P[y|T] = P[T|y] * P[y] / P[T]</p>

<p>fyT = P[T|y] * fy / P[T] = (1-y) * 2y / (1/3) = 6y – 6y^2</p>

<p>This new pdf is the conditional one based on the first tails, we find its expected value to determine the probability of heads</p>

<p>Int( y * (6y – 6y^2 ), y = 0..1 ) = 6[1/3-1/4] = ½</p>

<p>So there is a 50% chance of heads (or tails) after if the first toss resulted in tails</p>
"
0.188982237,0.181071492,87394,"<p>It sounds like you want to simulate the formation of tracks and then conduct a Monte Carlo simulation to see how many tracks fall into the red region.  To do this, I would first convert the lines to two functions, one giving direction and the other distance from one point to the next on that track.  Now you can study the probability distributions associated with those two functions.  For example, you might find that distance travelled follows a specific distribution (be careful that the distribution doesn't change over time).  If either variable does change over time then you have to delve into time series analysis (not my field, sorry).</p>

<p>Another thought that comes to mind is that, since the direction of movement in x-y changes gradually in most of the tracks, you might do better examining the change in direction vs time for the tracks.</p>

<p>You will also need to estimate the probability of a track starting at a given x-y co-ordinate with a given direction.  You may want to consider using kernel density estimation to smooth the resulting PDF or, if it appears to follow a distribution for which there is an analytical model then expectation maximisation could be used to fit that distribution to the data.</p>

<p>The Monte Carlo simulation would then draw random samples from these distributions to simulate the shapes of the tracks.  Then you have to simulate a large number of tracks and see how often those tracks pass through the red region.  This could be thousands or millions of tracks, you'll have to experiment to see when the distribution stops changing as you add more tracks.</p>
"
0.25,0.239535069,87398,"<p>Multidimensional scaling (in particular smallest space analysis) is a very sensitive measure, and it is very unlikely two separate data sets (regardless of mirrored variables) will produce an identical geometric space. This is because the common spaces displayed are an overall relationship between every single variable in the data set, and thus easily manipulated when analysis parameters are changed (variables added, removed etc). Thus, unless you had obtained exactly the same results as the original paper, it is highly unlikely you will get the same plot. Although holistically the same, your participants will have had individual variability different to that of the original paper. Thus the common space (e.g. correlation in simple terms) will be slightly different from the original. </p>

<p>What you are looking for is similar patterns in the geometric space. Just because they are not in exactly the same place on your geometric plot, when compared to your geometric plot, doesn't mean the same grouped clusters aren't there. For what it's worth, I'm not 100% sure the groupings from the original paper are replicated in yours. Are you using the same number of variables, exactly? </p>

<p>Would I be correct in saying you are comparing the Euclidean distances between a series of variables? If so I would look to do multidimensional scaling within the PROXSCAL procedure of SPSS and use the common space function. I think this is likely to offer you a much better outcome. Just because the original paper used SSA doesn't mean a different form of multidimensional scaling (PROXSCAL, cluster analysis – gosh, even factor analysis with a specified number of groups, which in this case would be 11) would be inappropriate. You are looking to confirm factors. If you are unable to get the same matched factors, then perhaps there is a methodology or theoretical explanation to it.</p>

<p>It's difficult without theoretical understanding of the topic to fully appreciate whether the common spaces in your output are theoretically interesting (thus challenging Raven) or whether they are inconsistently confusing and disinteresting.</p>
"
0.163663418,0.156812512,121691,"<p>I hope this is not ""opinion"" question</p>

<p>but I am curious as to whether or not the study of statistics and probability theory in the realm of cancer genomics is considered ""less philosophical"" than the pure math studies.</p>

<p>I am only asking because something inside me is very skeptical of pure math, i.e.  algebra and geometry.  I just am lacking a faith to study these abstract notions.</p>

<p>but I am interested in modeling cancer genes, and using probability to study biology.</p>

<p>however, I have some ingrained notion that ""applied"" is not as respected as ""pure"" math.  Can someone help me with this?  </p>

<p>Even my algebra teacher has scoffed at applied studies.  So I am having this dilemma.</p>

<p>For instance,  straying away from functional analysis, and moving more into probability theory seems to me like a ""weaker"" move.   I am sure this is a philosophical question, but I am looking for the guidance of those working in statistics, and biostatistics.   Are these field theoretically rewarding and dense?</p>

<p>Ultimately,
 my question is that ,  do you think that biostatistics is only a particular scope on a specific part of biotechnology revolution? Do you think biostats is a general field, or particular and why?   ""</p>

<p>The reason I am concerned about this, is because I am not interested in studying just a narrow field interested in just a small set of problems, and wish to be as general as possible.  Can someone advise on this?</p>

<p>Thank you very much.</p>
"
0.094491118,0.090535746,41907,"<p>I am not sure exactly what you mean by ""journey"" but it sounds like survival analysis would be a good starting point; at the least, this would let you plot survival functions (time to sale) and deal with any censoring (people who never bought). (In the data shown, there was no censoring, but survival analysis still works well).</p>

<p>After some exploratory analysis and plotting, you could try adding ""journey"" as an IV in (e.g.) a Cox proportional hazards model. However, in the data shown, ""journey"" is highly collinear with ID. How to deal with this would depend on whether ""journey"" takes a limited number of categories or is different for each person. </p>
"
0.25,0.239535069,41925,"<p>I want to check whether a group of patients are significantly different from their control group. However, I also want to check if the p-value is still significant in case a covariate is taken into account. For that I created a simple data frame in R:</p>

<pre><code>data &lt;- data.frame(group = c(rep(""CTRL"", 10), rep(""P"", 10)), 
    response = c(10,11,14,16,17,17,19,20,21,22, 10,11,11,11,12,13,14,14,15,16),
    age = c(40,41,45,43,50,51,55,57,60,62, 40,42,43,43,45,46,46,50,52,54))
</code></pre>

<p>First of all, I performed a normal ANOVA using the <code>lm</code> function. I tried 3 different ways to see if the results are the same. And they are:</p>

<pre><code>anova(lm(data$response ~ data$group))

summary(lm(data$response ~ data$group))

summary(aov(data$response ~ data$group)
</code></pre>

<p>To check if the covariate <code>age</code> is correlated with the <code>response</code> variable I performed a correlation test:</p>

<pre><code>cor.test(data$response, data$age)
</code></pre>

<p>Seeing a high and significant correlation I concluded that the significance between the control group and the patient group might be due to the effect of age.</p>

<p>I am now unsure how to perform the ANCOVA analysis to check if the effect between the two groups is really there or if it is just because of the covariate age.</p>

<p>To check this I did the following:</p>

<pre><code>m1 &lt;- lm(data$response ~ data$group + data$age)

m2 &lt;- lm(data$response ~ data$group)

anova(m1, m2)
</code></pre>

<p>Comparing the two models resulted in a significant p-value. I would have thought of a p-value of much less significant then the one obtained with the normal anova. Is it actually right to compare the two models or can I achieve this by using only one model? I am really stuck here and hoping to find some helpful answers here.</p>
"
0.188982237,0.181071492,140929,"<p>I wish to carry out logistic regression analysis using Firth's method, as implemented in R logistf package, to analyse SNP case/control data, for rare variants, whilst controlling for stratification using PCA eigenvectors as covariates. I wish to obtain p-values for each SNP (additive model).</p>

<p>Previously I have performed logistic regression analysis using PLINK:</p>

<pre><code>plink  --bfile snpdata --logistic --ci 0.95 --covar plink2_pca.eigenvec --covar-number 1-2 --out snpout
</code></pre>

<p>I would like to perform similar analysis, but wish to handle quasi-complete separation of the rare variants in my data sets.</p>

<p>I have followed a SNP analysis example provided with logistf and been able to obtain P values:</p>

<p>A very small sample of the snpdata (cases: case 1, control 0; SNP additive allele counts for minor allele: 0, 1, 2):</p>

<pre><code>           PC01         PC02 case exm226_A exm401_A exm4584_A exm146_A
1  -0.003092320 -0.002737810    1            0       0       0       0
2   0.015637300  0.008232330    1            0       0       0       0
3   0.006746730  0.008704400    1            0       1       0       1
4   0.001438270  0.000875751    0            0       0       0       0
5  -0.004161490  0.011407500    0            0       0       2       0

for(i in 1:ncol(snpdata)) snpdata[,i] &lt;-as.factor(snpdata[,i])
snpdata &lt;- snpdata[sapply(snpdata,function(x) length(levels(x))&gt;=2)] 
fitsnp &lt;- logistf(data=snpdata, formula=case~1, pl=FALSE)
add1(fitsnp)
</code></pre>

<p>But I am not clear on how to pass the eigenvectors in as covariates, or whether I can used the eigenvector values as is, or need to convert to these as factors?   </p>

<pre><code>fitsnp &lt;- logistf(data=snpdata,formula=case~(1+PC01+PC02), pl=FALSE)
</code></pre>

<p>I'm not sure if I am on the right track here and can't find a sufficiently similar example on-line to follow.</p>

<p>I would appreciate any assistance, or explanation if I am going completely wrong here.</p>

<p>Thanks in advance.</p>
"
